{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyP7D9Br/3I+GfTqSh4XdhCb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## Mount the Colab Notebook to Google Drive"],"metadata":{"id":"luYeHBwgVMb4"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5r5CmR2rVJBx","executionInfo":{"status":"ok","timestamp":1744441143751,"user_tz":240,"elapsed":558,"user":{"displayName":"Rohan Pratap Reddy Ravula","userId":"17224012780318323221"}},"outputId":"a054442d-87df-4516-acc8-78b7ca4caddf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["## Data Processing\n","### Load the Required Libraries"],"metadata":{"id":"DkOnsQ9VVaqG"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import os\n","from tqdm import tqdm\n","tqdm.pandas()"],"metadata":{"id":"i6jmDdyMVXvP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_path = \"/content/drive/MyDrive/DATA 6250/Datasets/Updated/Normalized/notable_ai_models_normalized.csv\"\n","df = pd.read_csv(input_path)"],"metadata":{"id":"DCrCJOc4VkPX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":794},"id":"MWyqYvLHVx81","executionInfo":{"status":"ok","timestamp":1744441147017,"user_tz":240,"elapsed":26,"user":{"displayName":"Rohan Pratap Reddy Ravula","userId":"17224012780318323221"}},"outputId":"83cb3c6f-e6e0-4635-c3b4-ed3763ef5fc5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["             Model      Domain    Organization      Country        Date  \\\n","0  EXAONE Deep 32B    Language  LG AI Research  South Korea  2025-03-16   \n","1          QwQ-32B    Language         Alibaba        China  2025-03-06   \n","2          GPT-4.5    Language          OpenAI          USA  2025-02-27   \n","3          GPT-4.5      Vision          OpenAI          USA  2025-02-27   \n","4          GPT-4.5  Multimodal          OpenAI          USA  2025-02-27   \n","\n","         Notability  Training compute (FLOP)  Finetune compute (FLOP)  \\\n","0     Training cost             1.260000e+24             7.040000e+21   \n","1  SOTA improvement             3.510000e+24                      NaN   \n","2     Training cost                      NaN                      NaN   \n","3     Training cost                      NaN                      NaN   \n","4     Training cost                      NaN                      NaN   \n","\n","      data size  Epochs  ...  \\\n","0  1.200000e+10     NaN  ...   \n","1           NaN     NaN  ...   \n","2           NaN     NaN  ...   \n","3           NaN     NaN  ...   \n","4           NaN     NaN  ...   \n","\n","                                            Abstract           Base model  \\\n","0  We present EXAONE Deep series, which exhibits ...       EXAONE 3.5 32B   \n","1  QwQ is the reasoning model of the Qwen series....  Qwen2.5-Coder (32B)   \n","2  We advance AI capabilities by scaling two comp...                  NaN   \n","3  We advance AI capabilities by scaling two comp...                  NaN   \n","4  We advance AI capabilities by scaling two comp...                  NaN   \n","\n","                              Finetune compute notes  Batch size notes  \\\n","0  Table 1 (reported): 7.04 × 10^21 FLOP\\n\\n6ND =...               NaN   \n","1                                                NaN               NaN   \n","2                                                NaN               NaN   \n","3                                                NaN               NaN   \n","4                                                NaN               NaN   \n","\n","             Model accessibility Training code accessibility  \\\n","0  Open weights (non-commercial)                  Unreleased   \n","1    Open weights (unrestricted)                  Unreleased   \n","2                     API access                  Unreleased   \n","3                     API access                  Unreleased   \n","4                     API access                  Unreleased   \n","\n","   Inference code accessibility  \\\n","0                           NaN   \n","1                           NaN   \n","2                           NaN   \n","3                           NaN   \n","4                           NaN   \n","\n","                                 Accessibility notes Frontier model  \\\n","0  https://huggingface.co/LGAI-EXAONE/EXAONE-Deep...            NaN   \n","1      https://huggingface.co/Qwen/QwQ-32B\\nApache 2            NaN   \n","2                                                NaN            NaN   \n","3                                                NaN            NaN   \n","4                                                NaN            NaN   \n","\n","     Training compute estimation method  \n","0  Reported,Operation counting,Hardware  \n","1                                   NaN  \n","2                                   NaN  \n","3                                   NaN  \n","4                                   NaN  \n","\n","[5 rows x 40 columns]"],"text/html":["\n","  <div id=\"df-f243205e-b70c-4d23-bfb7-918d6f2d5582\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Model</th>\n","      <th>Domain</th>\n","      <th>Organization</th>\n","      <th>Country</th>\n","      <th>Date</th>\n","      <th>Notability</th>\n","      <th>Training compute (FLOP)</th>\n","      <th>Finetune compute (FLOP)</th>\n","      <th>data size</th>\n","      <th>Epochs</th>\n","      <th>...</th>\n","      <th>Abstract</th>\n","      <th>Base model</th>\n","      <th>Finetune compute notes</th>\n","      <th>Batch size notes</th>\n","      <th>Model accessibility</th>\n","      <th>Training code accessibility</th>\n","      <th>Inference code accessibility</th>\n","      <th>Accessibility notes</th>\n","      <th>Frontier model</th>\n","      <th>Training compute estimation method</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>EXAONE Deep 32B</td>\n","      <td>Language</td>\n","      <td>LG AI Research</td>\n","      <td>South Korea</td>\n","      <td>2025-03-16</td>\n","      <td>Training cost</td>\n","      <td>1.260000e+24</td>\n","      <td>7.040000e+21</td>\n","      <td>1.200000e+10</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>We present EXAONE Deep series, which exhibits ...</td>\n","      <td>EXAONE 3.5 32B</td>\n","      <td>Table 1 (reported): 7.04 × 10^21 FLOP\\n\\n6ND =...</td>\n","      <td>NaN</td>\n","      <td>Open weights (non-commercial)</td>\n","      <td>Unreleased</td>\n","      <td>NaN</td>\n","      <td>https://huggingface.co/LGAI-EXAONE/EXAONE-Deep...</td>\n","      <td>NaN</td>\n","      <td>Reported,Operation counting,Hardware</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>QwQ-32B</td>\n","      <td>Language</td>\n","      <td>Alibaba</td>\n","      <td>China</td>\n","      <td>2025-03-06</td>\n","      <td>SOTA improvement</td>\n","      <td>3.510000e+24</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>QwQ is the reasoning model of the Qwen series....</td>\n","      <td>Qwen2.5-Coder (32B)</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Open weights (unrestricted)</td>\n","      <td>Unreleased</td>\n","      <td>NaN</td>\n","      <td>https://huggingface.co/Qwen/QwQ-32B\\nApache 2</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>GPT-4.5</td>\n","      <td>Language</td>\n","      <td>OpenAI</td>\n","      <td>USA</td>\n","      <td>2025-02-27</td>\n","      <td>Training cost</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>We advance AI capabilities by scaling two comp...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>API access</td>\n","      <td>Unreleased</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>GPT-4.5</td>\n","      <td>Vision</td>\n","      <td>OpenAI</td>\n","      <td>USA</td>\n","      <td>2025-02-27</td>\n","      <td>Training cost</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>We advance AI capabilities by scaling two comp...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>API access</td>\n","      <td>Unreleased</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>GPT-4.5</td>\n","      <td>Multimodal</td>\n","      <td>OpenAI</td>\n","      <td>USA</td>\n","      <td>2025-02-27</td>\n","      <td>Training cost</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>We advance AI capabilities by scaling two comp...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>API access</td>\n","      <td>Unreleased</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 40 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f243205e-b70c-4d23-bfb7-918d6f2d5582')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-f243205e-b70c-4d23-bfb7-918d6f2d5582 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-f243205e-b70c-4d23-bfb7-918d6f2d5582');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-3c38b45f-b8a8-4aa6-865e-a4ee7947695b\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3c38b45f-b8a8-4aa6-865e-a4ee7947695b')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-3c38b45f-b8a8-4aa6-865e-a4ee7947695b button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df"}},"metadata":{},"execution_count":36}]},{"cell_type":"code","source":["path_1 = \"/content/drive/MyDrive/DATA 6250/Datasets/Updated/Filled/large_scale_ai_models_filled.csv\"\n","df_large_scale_ai = pd.read_csv(path_1)\n","\n","path_2 = \"/content/drive/MyDrive/DATA 6250/Datasets/Updated/Filled/ml_hardware_filled.csv\"\n","df_ml_hardware = pd.read_csv(path_2)"],"metadata":{"id":"0ag9bXG-VzrL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for col in df.columns:\n","  print(repr(col))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yqaGrZ8OYG4P","executionInfo":{"status":"ok","timestamp":1744441150773,"user_tz":240,"elapsed":8,"user":{"displayName":"Rohan Pratap Reddy Ravula","userId":"17224012780318323221"}},"outputId":"5c930006-7235-41d3-d98f-34b83430757b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["'Model'\n","'Domain'\n","'Organization'\n","'Country'\n","'Date'\n","'Notability'\n","'Training compute (FLOP)'\n","'Finetune compute (FLOP)'\n","'data size'\n","'Epochs'\n","'Batch size'\n","'Training time (hours)'\n","'Power draw (W)'\n","'Training compute cost'\n","'Confidence'\n","'Training hardware'\n","'Hardware quantity'\n","'Hardware utilization'\n","'Category'\n","'Authors'\n","'Notability criteria notes'\n","'Parameters'\n","'Parameters notes'\n","'Training compute notes'\n","'Training dataset'\n","'Training dataset notes'\n","'Dataset size notes'\n","'Training time notes'\n","'Training compute cost (2023 USD)'\n","'Compute cost notes'\n","'Abstract'\n","'Base model'\n","'Finetune compute notes'\n","'Batch size notes'\n","'Model accessibility'\n","'Training code accessibility'\n","'Inference code accessibility'\n","'Accessibility notes'\n","'Frontier model'\n","'Training compute estimation method'\n"]}]},{"cell_type":"code","source":["features_notes = ['Notability criteria notes','Parameters notes','Training compute notes',\n","                  'Training dataset notes','Dataset size notes','Training time notes',\n","                  'Compute cost notes','Finetune compute notes','Batch size notes',\n","                  'Accessibility notes']\n","def merge_notes(row_vals):\n","  notes = \"\"\n","  for feature in features_notes:\n","    if not pd.isna(row_vals[feature]):\n","      notes += f\"{feature}: {row_vals[feature]}\\n\"\n","  return notes"],"metadata":{"id":"ELdzJ5CsY270"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['Overall_notes'] = df.progress_apply(merge_notes, axis=1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QMWWzC8yZ_3k","executionInfo":{"status":"ok","timestamp":1744441154057,"user_tz":240,"elapsed":211,"user":{"displayName":"Rohan Pratap Reddy Ravula","userId":"17224012780318323221"}},"outputId":"4e45cfe9-c233-4fd1-c270-9be62e7808f2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 5701/5701 [00:00<00:00, 28859.24it/s]\n"]}]},{"cell_type":"code","source":["df.drop(columns=[col for col in df.columns if col in features_notes], inplace=True)"],"metadata":{"id":"segYZS8fa9bw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for col in df.columns:\n","  print(repr(col))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H4ENkmqGbpWV","executionInfo":{"status":"ok","timestamp":1744441156627,"user_tz":240,"elapsed":10,"user":{"displayName":"Rohan Pratap Reddy Ravula","userId":"17224012780318323221"}},"outputId":"15e7e9f9-72b2-481b-a217-7904e22ae93b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["'Model'\n","'Domain'\n","'Organization'\n","'Country'\n","'Date'\n","'Notability'\n","'Training compute (FLOP)'\n","'Finetune compute (FLOP)'\n","'data size'\n","'Epochs'\n","'Batch size'\n","'Training time (hours)'\n","'Power draw (W)'\n","'Training compute cost'\n","'Confidence'\n","'Training hardware'\n","'Hardware quantity'\n","'Hardware utilization'\n","'Category'\n","'Authors'\n","'Parameters'\n","'Training dataset'\n","'Training compute cost (2023 USD)'\n","'Abstract'\n","'Base model'\n","'Model accessibility'\n","'Training code accessibility'\n","'Inference code accessibility'\n","'Frontier model'\n","'Training compute estimation method'\n","'Overall_notes'\n"]}]},{"cell_type":"code","source":["df['Training compute cost'].fillna(0, inplace=True)\n","df['Training compute cost (2023 USD)'].fillna(0, inplace=True)\n","df['Training compute cost'] = (df['Training compute cost'] + df['Training compute cost (2023 USD)']) / 2.0\n","df.drop(columns=['Training compute cost (2023 USD)'], inplace=True)\n","df['Training compute cost'] = df['Training compute cost'].apply(lambda x: round(x,2))\n","df['Training compute cost'].replace(0, np.nan, inplace=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9lwmhm7ix2n-","executionInfo":{"status":"ok","timestamp":1744441158573,"user_tz":240,"elapsed":18,"user":{"displayName":"Rohan Pratap Reddy Ravula","userId":"17224012780318323221"}},"outputId":"01c9a592-6b4e-40ca-dd03-be8d51b67e0c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-43-cd7d51d5075d>:1: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n","The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n","\n","For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n","\n","\n","  df['Training compute cost'].fillna(0, inplace=True)\n","<ipython-input-43-cd7d51d5075d>:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n","The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n","\n","For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n","\n","\n","  df['Training compute cost (2023 USD)'].fillna(0, inplace=True)\n","<ipython-input-43-cd7d51d5075d>:6: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n","The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n","\n","For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n","\n","\n","  df['Training compute cost'].replace(0, np.nan, inplace=True)\n"]}]},{"cell_type":"code","source":["num_cols = df.select_dtypes(include='number').columns\n","object_cols = df.select_dtypes(include='object').columns"],"metadata":{"id":"s-v6a7yabye1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for col in num_cols:\n","  print(repr(col))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mfFQalwIcZDy","executionInfo":{"status":"ok","timestamp":1744441162252,"user_tz":240,"elapsed":4,"user":{"displayName":"Rohan Pratap Reddy Ravula","userId":"17224012780318323221"}},"outputId":"62ed959c-4701-42e2-a9a3-3ee707b5cf5c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["'Training compute (FLOP)'\n","'Finetune compute (FLOP)'\n","'data size'\n","'Epochs'\n","'Batch size'\n","'Training time (hours)'\n","'Power draw (W)'\n","'Training compute cost'\n","'Hardware quantity'\n","'Hardware utilization'\n","'Parameters'\n"]}]},{"cell_type":"code","source":["for col in object_cols:\n","  print(repr(col))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v4px1KDDcfad","executionInfo":{"status":"ok","timestamp":1744441269770,"user_tz":240,"elapsed":42,"user":{"displayName":"Rohan Pratap Reddy Ravula","userId":"17224012780318323221"}},"outputId":"6d31a7da-4cb6-43d6-c2f9-d49f6553ff41"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["'Model'\n","'Domain'\n","'Organization'\n","'Country'\n","'Date'\n","'Notability'\n","'Confidence'\n","'Training hardware'\n","'Category'\n","'Authors'\n","'Training dataset'\n","'Abstract'\n","'Base model'\n","'Model accessibility'\n","'Training code accessibility'\n","'Inference code accessibility'\n","'Frontier model'\n","'Training compute estimation method'\n","'Overall_notes'\n"]}]},{"cell_type":"code","source":["order_cols = ['Model','Base model','Training hardware'] + num_cols.tolist() + ['Overall_notes']\n","df_new = df[order_cols].copy()\n","df_new.drop_duplicates(inplace=True)\n","df_new.reset_index(drop=True, inplace=True)\n","df_new.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1ch6DA07cood","executionInfo":{"status":"ok","timestamp":1744441271352,"user_tz":240,"elapsed":19,"user":{"displayName":"Rohan Pratap Reddy Ravula","userId":"17224012780318323221"}},"outputId":"5b8241b1-0544-4754-fde7-a783d5e3e1d7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(927, 15)"]},"metadata":{},"execution_count":48}]},{"cell_type":"code","source":["order_cols = ['Model','Base model','Training hardware','Overall_notes', 'Parameters',\n","              'data size','Batch size', 'Epochs','Training time (hours)','Hardware quantity',\n","              'Hardware utilization','Training compute (FLOP)','Finetune compute (FLOP)',\n","              'Power draw (W)', 'Training compute cost']\n","df_new = df_new[order_cols].copy()"],"metadata":{"id":"hhcR4zWow2kH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(df_large_scale_ai.columns)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OkWZmpqUzrGU","executionInfo":{"status":"ok","timestamp":1744441382604,"user_tz":240,"elapsed":12,"user":{"displayName":"Rohan Pratap Reddy Ravula","userId":"17224012780318323221"}},"outputId":"fb5894f8-890e-401d-9729-ac1b6cc66602"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Index(['Model', 'Domain', 'Country', 'Organization', 'Date', 'Category',\n","       'Task', 'Confidence', 'Hardware quantity', 'Training hardware',\n","       'accessibility', 'Training dataset', 'Training code accessibility',\n","       'Parameters', 'data size', 'Training time (hours)',\n","       'Training compute (FLOP)', 'Finetune compute (FLOP)'],\n","      dtype='object')\n"]}]},{"cell_type":"code","source":["num_large_scale_cols = df_large_scale_ai.select_dtypes(include='number').columns\n","order_large_scale_cols = ['Model','Training hardware'] + num_large_scale_cols.tolist()\n","print(order_large_scale_cols)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JTeRulPtzyr3","executionInfo":{"status":"ok","timestamp":1744441501975,"user_tz":240,"elapsed":7,"user":{"displayName":"Rohan Pratap Reddy Ravula","userId":"17224012780318323221"}},"outputId":"5c9e5683-f65b-4784-85e0-8d7c4c71ad88"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['Model', 'Training hardware', 'Hardware quantity', 'Parameters', 'data size', 'Training time (hours)', 'Training compute (FLOP)', 'Finetune compute (FLOP)']\n"]}]},{"cell_type":"code","source":["print(df_ml_hardware.columns)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lgMv6_l_0tSO","executionInfo":{"status":"ok","timestamp":1744441639908,"user_tz":240,"elapsed":6,"user":{"displayName":"Rohan Pratap Reddy Ravula","userId":"17224012780318323221"}},"outputId":"bd5ff767-f858-4bdb-837a-370009c5d424"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Index(['hardware model', 'Manufacturer', 'Type', 'Release date',\n","       'Release price (USD)', 'TDP (W)', 'compute flops', 'compute ops',\n","       'Memory size (bytes)', 'Die Size (mm^2)', 'avg_freq', 'freq_range',\n","       'Tensor cores', 'transistors (10^6)', 'Process size (nm)', 'Foundry',\n","       'ml models'],\n","      dtype='object')\n"]}]},{"cell_type":"code","source":["num_ml_hardware_cols = df_ml_hardware.select_dtypes(include='number').columns\n","order_ml_hardware_cols = ['hardware model','ml models'] + num_ml_hardware_cols.tolist()\n","print(order_ml_hardware_cols)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U1ZXvTCy0x9K","executionInfo":{"status":"ok","timestamp":1744441743029,"user_tz":240,"elapsed":15,"user":{"displayName":"Rohan Pratap Reddy Ravula","userId":"17224012780318323221"}},"outputId":"6e9e57de-64cd-4871-a98f-5d3f04f5825a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['hardware model', 'ml models', 'Release price (USD)', 'TDP (W)', 'compute flops', 'compute ops', 'Memory size (bytes)', 'Die Size (mm^2)', 'avg_freq', 'freq_range', 'Tensor cores', 'transistors (10^6)', 'Process size (nm)']\n"]}]},{"cell_type":"code","source":["df_new_large_scale = df_large_scale_ai[order_large_scale_cols].copy()\n","df_new_ml_hardware = df_ml_hardware[order_ml_hardware_cols].copy()"],"metadata":{"id":"UapkLE5I1eRf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_new_large_scale.drop_duplicates(inplace=True)\n","df_new_large_scale.reset_index(drop=True, inplace=True)\n","df_new_ml_hardware.drop_duplicates(inplace=True)\n","df_new_ml_hardware.reset_index(drop=True, inplace=True)"],"metadata":{"id":"q5pyMps03fGN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_new_large_scale.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ia7sX-qP3lZF","executionInfo":{"status":"ok","timestamp":1744442394892,"user_tz":240,"elapsed":8,"user":{"displayName":"Rohan Pratap Reddy Ravula","userId":"17224012780318323221"}},"outputId":"7d4aee2f-f44c-49e5-c187-4c7fa1117400"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(290, 8)"]},"metadata":{},"execution_count":56}]},{"cell_type":"code","source":["df_new_ml_hardware.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RfoD1mK43pjW","executionInfo":{"status":"ok","timestamp":1744442404332,"user_tz":240,"elapsed":7,"user":{"displayName":"Rohan Pratap Reddy Ravula","userId":"17224012780318323221"}},"outputId":"ecb2cf7e-9eb8-4498-cf2f-05f9828643d2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(879, 13)"]},"metadata":{},"execution_count":57}]},{"cell_type":"markdown","source":["## Using LLMs for filling missing values\n","'llmware/dragon-qwen-7b-ov' for RAG based approach\n","\n","'all-mpnet-base-v2' for sentence embeddings"],"metadata":{"id":"9D2tS3cti3kv"}},{"cell_type":"markdown","source":["### Install all Required Libraries"],"metadata":{"id":"pZ1BrTdv375Q"}},{"cell_type":"code","source":["!pip install -U sentence-transformers transformers\n","!pip install \"llmware[full]\" openvino openvino_genai"],"metadata":{"id":"jfNmyMpbjNUI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Load the required Libraries"],"metadata":{"id":"mTV6Wn5F4AmZ"}},{"cell_type":"code","source":["from sentence_transformers import SentenceTransformer, util\n","from huggingface_hub import snapshot_download\n","import torch"],"metadata":{"id":"vpZfUOJzjib4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["File os setup"],"metadata":{"id":"rRbUE-G34Gnh"}},{"cell_type":"code","source":["import os\n","os.environ[\"PYDEVD_DISABLE_FILE_VALIDATION\"] = \"1\""],"metadata":{"id":"Eh4IE_h3joVc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Initialize the sentence transformer"],"metadata":{"id":"8Bzykum94JVP"}},{"cell_type":"code","source":["sen_model = SentenceTransformer('all-mpnet-base-v2',device=device)"],"metadata":{"id":"uTVH6y_Jjr_V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Creating Embeddings and store it on new data"],"metadata":{"id":"mEMg-1dB4Nvy"}},{"cell_type":"code","source":["def get_df_embeddings(df_all,feature):\n","  embeddings = sen_model.encode(df_all[feature].tolist(),convert_to_tensor=True)\n","  df_all['Embeddings'] = [row for row in embeddings.cpu()]\n","  return df_all"],"metadata":{"id":"fXk_wWoNjvQn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Create a function For calculating valid embeddings for all features"],"metadata":{"id":"ZoDAGb245Jo6"}},{"cell_type":"code","source":["def get_unmasked_embeddings(unmasked_df):\n","  unmasked_df = unmasked_df.reset_index(drop=True)\n","  emb_list = []\n","  for emb in unmasked_df['Embeddings']:\n","    if not isinstance(emb, torch.Tensor):\n","        emb = torch.tensor(emb)\n","    emb_list.append(emb)\n","  unmask_embeddings = torch.stack(emb_list, dim=0)\n","  return unmask_embeddings,unmasked_df"],"metadata":{"id":"FM9va_e-j9xA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Create a function to assign values under features column based on their nearest models value."],"metadata":{"id":"G43WjbBU5Kdh"}},{"cell_type":"code","source":["def assign_values_to_features(row_vals,unmask_df,feature,confidence_val=0.5):\n","  emb = row_vals['Embeddings']\n","  if not isinstance(emb, torch.Tensor):\n","    emb = torch.tensor(emb)\n","  emb = emb.unsqueeze(0)\n","  unmask_embeddings,unmask = get_unmasked_embeddings(unmask_df)\n","  similarities = util.cos_sim(emb, unmask_embeddings)[0]\n","  max_val, max_idx = torch.max(similarities, dim=0)\n","  max_val = max_val.item()\n","  max_idx = max_idx.item()\n","  if max_val > confidence_val:\n","    return (unmask.iloc[max_idx][feature])\n","  else:\n","    return None"],"metadata":{"id":"QSKovsRbkAFs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_max_score_idx(row_vals,unmask_df,feature):\n","  emb = row_vals['Embeddings']\n","  if not isinstance(emb, torch.Tensor):\n","    emb = torch.tensor(emb)\n","  emb = emb.unsqueeze(0)\n","  unmask_embeddings,unmask = get_unmasked_embeddings(unmask_df)\n","  similarities = util.cos_sim(emb, unmask_embeddings)[0]\n","  max_val, max_idx = torch.max(similarities, dim=0)\n","  max_val = max_val.item()\n","  max_idx = max_idx.item()\n","  return max_val,max_idx"],"metadata":{"id":"8UE483-b-TKZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Initializing RAG Model"],"metadata":{"id":"PcLdjK5z5OWc"}},{"cell_type":"code","source":["from llmware.models import ModelCatalog\n","rag_model = ModelCatalog().load_model(\"dragon-qwen-7b-ov\",temperature=0.01)"],"metadata":{"id":"AGhGFkVCkDDc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Creating a Generalized function to extract values using RAG model"],"metadata":{"id":"D3ApR59t5SDb"}},{"cell_type":"code","source":["def extract_feature_from_notes(row_vals, feature,_rag_model):\n","    \"\"\"\n","    Extract a specified feature from the 'Overall_notes' column using the RAG model.\n","\n","    Args:\n","        row_vals (dict): Dictionary containing model data, including 'Model' and 'Overall_notes'.\n","        feature (str): The feature to extract (e.g., 'Training compute (FLOP)', 'Parameters').\n","\n","    Returns:\n","        str: The extracted numerical value or 'na' if not found or on error.\n","    \"\"\"\n","    # Check if 'Overall_notes' is empty or whitespace\n","    if not row_vals.get('Overall_notes') or not row_vals['Overall_notes'].strip():\n","        return \"na\"\n","\n","    # Define feature configurations with questions, descriptions, examples, and format notes\n","    feature_configs = {\n","        \"Training compute (FLOP)\": {\n","            \"question\": f\"What is the training compute in FLOPs for the model '{row_vals['Model']}'?\",\n","            \"description\": \"training compute in FLOPs\",\n","            \"examples\": [\n","                (\n","                    f\"Context for the model 'ExampleModel':\\n\"\n","                    f\"'...trained with 1.2e25 FLOPs...'\\n\"\n","                    f\"Question: What is the training compute in FLOPs for the model 'ExampleModel'?\\n\"\n","                    f\"Answer: 12000000000000000000000000\"\n","                ),\n","                (\n","                    f\"Context for the model 'AnotherModel':\\n\"\n","                    f\"'Not Found._x000D_ Is the training compute noted:_x000D_ <bot>: Not Found._x000D_...'\\n\"\n","                    f\"Question: What is the training compute in FLOPs for the model 'AnotherModel'?\\n\"\n","                    f\"Answer: na\"\n","                )\n","            ],\n","            \"format_note\": \"For example, for '1.2e25 FLOPs' return '12000000000000000000000000', for '464e24 FLOPs' return '464000000000000000000000000'.\"\n","        },\n","        \"Finetune compute (FLOP)\": {\n","            \"question\": f\"What is the finetune compute in FLOPs for the model '{row_vals['Model']}'?\",\n","            \"description\": \"finetune compute in FLOPs\",\n","            \"examples\": [\n","                (\n","                    f\"Context for the model 'ExampleModel':\\n\"\n","                    f\"'...finetuned with 1e24 FLOPs...'\\n\"\n","                    f\"Question: What is the finetune compute in FLOPs for the model 'ExampleModel'?\\n\"\n","                    f\"Answer: 1000000000000000000000000\"\n","                ),\n","                (\n","                    f\"Context for the model 'AnotherModel':\\n\"\n","                    f\"'Not Found._x000D_ Is the fine-tune compute noted:_x000D_ <bot>: Not Found._x000D_...'\\n\"\n","                    f\"Question: What is the finetune compute in FLOPs for the model 'AnotherModel'?\\n\"\n","                    f\"Answer: na\"\n","                )\n","            ],\n","            \"format_note\": \"For example, for '1e24 FLOPs' return '1000000000000000000000000', for '2.5e25 FLOPs' return '25000000000000000000000000'.\"\n","        },\n","        \"data size\": {\n","            \"question\": f\"What is the size of the training dataset for the model '{row_vals['Model']}'?\",\n","            \"description\": \"dataset size (e.g., number of tokens, words, or samples)\",\n","            \"examples\": [\n","                (\n","                    f\"Context for the model 'ExampleModel':\\n\"\n","                    f\"'...trained on 10 trillion tokens...'\\n\"\n","                    f\"Question: What is the size of the training dataset for the model 'ExampleModel'?\\n\"\n","                    f\"Answer: 10000000000000\"\n","                ),\n","                (\n","                    f\"Context for the model 'AnotherModel':\\n\"\n","                    f\"'Not Found._x000D_ Is the dataset size noted:_x000D_ <bot>: Not Found._x000D_...'\\n\"\n","                    f\"Question: What is the size of the training dataset for the model 'AnotherModel'?\\n\"\n","                    f\"Answer: na\"\n","                )\n","            ],\n","            \"format_note\": \"For example, for '10 trillion tokens' return '10000000000000', for '1.2 billion words' return '1200000000', or for '10.7M video-caption pairs' return '10700000'.\"\n","        },\n","        \"Epochs\": {\n","            \"question\": f\"What is the number of epochs used in training the model '{row_vals['Model']}'?\",\n","            \"description\": \"number of epochs\",\n","            \"examples\": [\n","                (\n","                    f\"Context for the model 'ExampleModel':\\n\"\n","                    f\"'...trained for 10 epochs...'\\n\"\n","                    f\"Question: What is the number of epochs used in training the model 'ExampleModel'?\\n\"\n","                    f\"Answer: 10\"\n","                ),\n","                (\n","                    f\"Context for the model 'AnotherModel':\\n\"\n","                    f\"'Not Found._x000D_ Is the number of epochs noted:_x000D_ <bot>: Not Found._x000D_...'\\n\"\n","                    f\"Question: What is the number of epochs used in training the model 'AnotherModel'?\\n\"\n","                    f\"Answer: na\"\n","                )\n","            ],\n","            \"format_note\": \"For example, for '10 epochs' return '10', for '5 epochs' return '5'.\"\n","        },\n","        \"Batch size\": {\n","            \"question\": f\"What is the batch size used in training the model '{row_vals['Model']}'?\",\n","            \"description\": \"batch size\",\n","            \"examples\": [\n","                (\n","                    f\"Context for the model 'ExampleModel':\\n\"\n","                    f\"'...trained with a batch size of 512...'\\n\"\n","                    f\"Question: What is the batch size used in training the model 'ExampleModel'?\\n\"\n","                    f\"Answer: 512\"\n","                ),\n","                (\n","                    f\"Context for the model 'AnotherModel':\\n\"\n","                    f\"'Not Found._x000D_ Is the batch size noted:_x000D_ <bot>: Not Found._x000D_...'\\n\"\n","                    f\"Question: What is the batch size used in training the model 'AnotherModel'?\\n\"\n","                    f\"Answer: na\"\n","                )\n","            ],\n","            \"format_note\": \"For example, for 'batch size of 512' return '512', for 'batch size of 1024' return '1024'.\"\n","        },\n","        \"Training time (hours)\": {\n","            \"question\": f\"What is the training time in hours for the model '{row_vals['Model']}'?\",\n","            \"description\": \"training time in hours\",\n","            \"examples\": [\n","                (\n","                    f\"Context for the model 'ExampleModel':\\n\"\n","                    f\"'...trained for 2500 hours...'\\n\"\n","                    f\"Question: What is the training time in hours for the model 'ExampleModel'?\\n\"\n","                    f\"Answer: 2500\"\n","                ),\n","                (\n","                    f\"Context for the model 'AnotherModel':\\n\"\n","                    f\"'Not Found._x000D_ Is the training time noted:_x000D_ <bot>: Not Found._x000D_...'\\n\"\n","                    f\"Question: What is the training time in hours for the model 'AnotherModel'?\\n\"\n","                    f\"Answer: na\"\n","                )\n","            ],\n","            \"format_note\": \"For example, for '2500 hours' return '2500', for '3 months' convert to hours (assume 30 days per month) and return '2160'.\"\n","        },\n","        \"Power draw (W)\": {\n","            \"question\": f\"What is the power draw in watts during training for the model '{row_vals['Model']}'?\",\n","            \"description\": \"power draw in watts\",\n","            \"examples\": [\n","                (\n","                    f\"Context for the model 'ExampleModel':\\n\"\n","                    f\"'...power draw of 1000 W...'\\n\"\n","                    f\"Question: What is the power draw in watts during training for the model 'ExampleModel'?\\n\"\n","                    f\"Answer: 1000\"\n","                ),\n","                (\n","                    f\"Context for the model 'AnotherModel':\\n\"\n","                    f\"'Not Found._x000D_ Is the power draw noted:_x000D_ <bot>: Not Found._x000D_...'\\n\"\n","                    f\"Question: What is the power draw in watts during training for the model 'AnotherModel'?\\n\"\n","                    f\"Answer: na\"\n","                )\n","            ],\n","            \"format_note\": \"For example, for '1000 W' return '1000', for '1.5 kW' return '1500'.\"\n","        },\n","        \"Training compute cost\": {\n","            \"question\": f\"What is the training compute cost for the model '{row_vals['Model']}'?\",\n","            \"description\": \"training compute cost\",\n","            \"examples\": [\n","                (\n","                    f\"Context for the model 'ExampleModel':\\n\"\n","                    f\"'...training compute cost of $1000000...'\\n\"\n","                    f\"Question: What is the training compute cost for the model 'ExampleModel'?\\n\"\n","                    f\"Answer: 1000000\"\n","                ),\n","                (\n","                    f\"Context for the model 'AnotherModel':\\n\"\n","                    f\"'Not Found._x000D_ Is the training compute cost noted:_x000D_ <bot>: Not Found._x000D_...'\\n\"\n","                    f\"Question: What is the training compute cost for the model 'AnotherModel'?\\n\"\n","                    f\"Answer: na\"\n","                )\n","            ],\n","            \"format_note\": \"For example, for '$1000000' return '1000000', for '1 million USD' return '1000000'.\"\n","        },\n","        \"Hardware quantity\": {\n","            \"question\": f\"What is the quantity of hardware used in training the model '{row_vals['Model']}'?\",\n","            \"description\": \"hardware quantity\",\n","            \"examples\": [\n","                (\n","                    f\"Context for the model 'ExampleModel':\\n\"\n","                    f\"'...trained on 512 GPUs...'\\n\"\n","                    f\"Question: What is the quantity of hardware used in training the model 'ExampleModel'?\\n\"\n","                    f\"Answer: 512\"\n","                ),\n","                (\n","                    f\"Context for the model 'AnotherModel':\\n\"\n","                    f\"'Not Found._x000D_ Is the hardware quantity noted:_x000D_ <bot>: Not Found._x000D_...'\\n\"\n","                    f\"Question: What is the quantity of hardware used in training the model 'AnotherModel'?\\n\"\n","                    f\"Answer: na\"\n","                )\n","            ],\n","            \"format_note\": \"For example, for '512 GPUs' return '512', for '10000 TPUs' return '10000'.\"\n","        },\n","        \"Hardware utilization\": {\n","            \"question\": f\"What is the hardware utilization during training for the model '{row_vals['Model']}'?\",\n","            \"description\": \"hardware utilization\",\n","            \"examples\": [\n","                (\n","                    f\"Context for the model 'ExampleModel':\\n\"\n","                    f\"'...hardware utilization of 80%...'\\n\"\n","                    f\"Question: What is the hardware utilization during training for the model 'ExampleModel'?\\n\"\n","                    f\"Answer: 0.8\"\n","                ),\n","                (\n","                    f\"Context for the model 'AnotherModel':\\n\"\n","                    f\"'Not Found._x000D_ Is the hardware utilization noted:_x000D_ <bot>: Not Found._x000D_...'\\n\"\n","                    f\"Question: What is the hardware utilization during training for the model 'AnotherModel'?\\n\"\n","                    f\"Answer: na\"\n","                )\n","            ],\n","            \"format_note\": \"For example, for '80% utilization' return '0.8', for '50%' return '0.5', for '0.75' return '0.75'.\"\n","        },\n","        \"Parameters\": {\n","            \"question\": f\"What is the number of parameters for the model '{row_vals['Model']}'?\",\n","            \"description\": \"number of parameters\",\n","            \"examples\": [\n","                (\n","                    f\"Context for the model 'ExampleModel':\\n\"\n","                    f\"'...has 130 billion parameters...'\\n\"\n","                    f\"Question: What is the number of parameters for the model 'ExampleModel'?\\n\"\n","                    f\"Answer: 130000000000\"\n","                ),\n","                (\n","                    f\"Context for the model 'AnotherModel':\\n\"\n","                    f\"'Not Found._x000D_ Is the number of parameters noted:_x000D_ <bot>: Not Found._x000D_...'\\n\"\n","                    f\"Question: What is the number of parameters for the model 'AnotherModel'?\\n\"\n","                    f\"Answer: na\"\n","                )\n","            ],\n","            \"format_note\": \"For example, for '130 billion parameters' return '130000000000', for '1.6B parameters' return '1600000000'.\"\n","        },\n","        \"Training compute cost (2023 USD)\": {\n","            \"question\": f\"What is the training compute cost in 2023 USD for the model '{row_vals['Model']}'?\",\n","            \"description\": \"training compute cost in 2023 USD\",\n","            \"examples\": [\n","                (\n","                    f\"Context for the model 'ExampleModel':\\n\"\n","                    f\"'...training compute cost of $1000000 in 2023 USD...'\\n\"\n","                    f\"Question: What is the training compute cost in 2023 USD for the model 'ExampleModel'?\\n\"\n","                    f\"Answer: 1000000\"\n","                ),\n","                (\n","                    f\"Context for the model 'AnotherModel':\\n\"\n","                    f\"'Not Found._x000D_ Is the training compute cost in 2023 USD noted:_x000D_ <bot>: Not Found._x000D_...'\\n\"\n","                    f\"Question: What is the training compute cost in 2023 USD for the model 'AnotherModel'?\\n\"\n","                    f\"Answer: na\"\n","                )\n","            ],\n","            \"format_note\": \"For example, for '$1000000 in 2023 USD' return '1000000', for '1 million 2023 USD' return '1000000'.\"\n","        }\n","    }\n","\n","    # Validate the feature\n","    if feature not in feature_configs:\n","        raise ValueError(f\"Invalid feature: {feature}. Supported features are: {list(feature_configs.keys())}\")\n","\n","    config = feature_configs[feature]\n","\n","    try:\n","        # Construct the context from 'Overall_notes'\n","        context = (\n","            f\"Context for the model '{row_vals['Model']}':\\n\"\n","            f\"{row_vals['Overall_notes']}\"\n","        )\n","\n","        # Construct the two-shot prompt\n","        question = (\n","            f\"{config['question']}\\n\"\n","            f\"Provide the answer as a numerical value representing the {config['description']}. \"\n","            f\"{config['format_note']} \"\n","            f\"If the {config['description']} is not explicitly mentioned or cannot be determined, return 'na'.\\n\\n\"\n","            f\"Example 1:\\n\"\n","            f\"{config['examples'][0]}\\n\\n\"\n","            f\"Example 2:\\n\"\n","            f\"{config['examples'][1]}\"\n","        )\n","\n","        prompt = f\"<human>:{context}\\n{question}\\n<bot>:\"\n","\n","        # Assuming rag_model is an instance of \"llmware/dragon-qwen-7b-ov\" with an inference method\n","        output = _rag_model.inference(prompt)\n","        answer = output.get(\"llm_response\", output).strip()\n","        return answer if answer else \"na\"\n","    except Exception as e:\n","        print(f\"QA extraction error for feature {feature}: {e}\")\n","        return \"na\""],"metadata":{"id":"mGC_H94C5Uwe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Create a Filling Code"],"metadata":{"id":"JTBP1Okj5Z8k"}},{"cell_type":"code","source":["num_cols = df_new.select_dypes(include='number').columns\n","print(num_cols)"],"metadata":{"id":"yExKxDrW5hFb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def step_1(df_notable,_rag_model,feature):\n","  print(f\"Processing {feature} using feature extraction ...\\n\")\n","  mask = df_notable[feature].isna()\n","  df_notable.loc[mask,feature] = df_notable[mask].progress_apply(\n","      lambda row: extract_feature_from_notes(row,feature,_rag_model),axis=1\n","  )\n","  df_notable[feature] = pd.to_numeric(df_notable[feature],errors='coerce')\n","  return df_notable"],"metadata":{"id":"s-0ZWYFz6KvE","executionInfo":{"status":"ok","timestamp":1744448201372,"user_tz":240,"elapsed":6,"user":{"displayName":"Rohan Pratap Reddy Ravula","userId":"17224012780318323221"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["def step_2(df_notable,df_large,_sen_model,feature):\n","  print(f\"Processing {feature} using selective keying from large scale ai dataset ...\\n\")\n","  step2_cols = ['Hardware quantity', 'Parameters', 'data size', 'Training time (hours)',\n","                'Training compute (FLOP)', 'Finetune compute (FLOP)']\n","  base_models_unique = df_notable[\"Base model\"].dropna().unique()\n","  base_model_embeddings = _sen_model.encode(base_models_unique, convert_to_tensor=True)\n","  large_scale_models = df_large[\"Model\"].unique()\n","  large_scale_embeddings = sentence_model.encode(large_scale_models, convert_to_tensor=True)\n","  similarity_matrix = cosine_similarity(base_model_embeddings.cpu(), large_scale_embeddings.cpu())\n","  match_dict = {base_model: large_scale_models[similarity_matrix[i].argmax()]\n","                  if similarity_matrix[i].max() >= 0.8 else None\n","                  for i, base_model in enumerate(base_models_unique)}\n","  df_large.set_index(\"Model\", inplace=True)\n","  for idx, row in df_notable.iterrows():\n","        base_model = row[\"Base model\"]\n","        if pd.isna(base_model) or base_model not in match_dict or match_dict[base_model] is None:\n","            continue\n","        large_scale_model = match_dict[base_model]\n","        if large_scale_model in df_large.index:\n","            for col in step2_cols:\n","                if pd.isna(row[col]) and not pd.isna(df_large.loc[large_scale_model, col]):\n","                    df_notable.at[idx, col] = df_large.loc[large_scale_model, col]\n","    df_large.reset_index(inplace=True)\n","    return df_notable\n"],"metadata":{"id":"vZS2mWWQPAtc"},"execution_count":null,"outputs":[]}]}