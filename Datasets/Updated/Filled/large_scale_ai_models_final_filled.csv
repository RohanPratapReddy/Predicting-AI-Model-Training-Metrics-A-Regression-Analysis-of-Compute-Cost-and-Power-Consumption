Model,Domain,Country,Organization,Date,Category,Task,Confidence,Hardware quantity,accessibility,Training dataset,Training code accessibility,Parameters,data size,Training time (hours),Training compute (FLOP),Finetune compute (FLOP),Hardware unit Release Price (USD),Hardware unit TDP (W),Hardware unit Compute FLOPS,Hardware unit Compute OPS,Hardware unit Memory Size (bytes),Training hardware,Notability,Epochs,Batch size,Power draw (W),Training compute cost,Hardware utilization,Expected power (W),Training Energy,Expected Energy,Abstract,Link,Reference
AFM-on-device,Language,USA,Apple,2024-07-29,Industry,Language modeling/generation,Confident,2048.0,Hosted access (no API),Not-defined,Unreleased,2730000000.0,7588000000000.0,1282.4326530612243,4.5126e+23,1877.1621580193864,13865.543199999987,156.9,459000000000000.0,918000000000000.0,95000000000.0,Google TPU v5p,Significant use,1,18949752,788306.6236983632,28396632.473599974,0.52,321331.2,1010950154.8552281,412085623.3273469,"We present foundation language models developed to power Apple Intelligence features, including a ∼3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute [Apple, 2024b]. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",https://machinelearning.apple.com/research/apple-intelligence-foundation-language-models,Apple Intelligence Foundation Language Models
AFM-server,Language,USA,Apple,2024-07-29,Industry,Language modeling/generation,Likely,8192.0,Hosted access (no API),Not-defined,Unreleased,2730000000.0,7400000000000.0,1282.4326530612243,4.3e+24,1877.1621580193864,6796.58,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Significant use,1,18949752,3466813.5881738467,55677583.36,0.52,1572864.0,4445954947.550488,2017092152.4244895,,https://machinelearning.apple.com/research/apple-intelligence-foundation-language-models,Apple Intelligence Foundation Language Models
AlexaTM 20B,Language,USA,Amazon,2022-08-02,Industry,Language modeling,Confident,128.0,API access,"mC4,Wikipedia",Not-defined,19750000000.0,1319000000000.0,2880.0,2.04374016e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,SOTA improvement,2,2000000,113643.12380745166,1293528.32,0.4935,51200.0,327292196.5654608,147456000.0,"In this work, we demonstrate that multilingual large-scale sequence-to-sequence (seq2seq) models, pre-trained on a mixture of denoising and Causal Language Modeling (CLM) tasks, are more efficient few-shot learners than decoder-only models on various tasks. In particular, we train a 20 billion parameter multilingual seq2seq model called Alexa Teacher Model (AlexaTM 20B) and show that it achieves state-of-the-art (SOTA) performance on 1-shot summarization tasks, outperforming a much larger 540B PaLM decoder model. AlexaTM 20B also achieves SOTA in 1-shot machine translation, especially for low-resource languages, across almost all language pairs supported by the model (Arabic, English, French, German, Hindi, Italian, Japanese, Marathi, Portuguese, Spanish, Tamil, and Telugu) on Flores-101 dataset. We also show in zero-shot setting, AlexaTM 20B outperforms GPT3 (175B) on SuperGLUE and SQuADv2 datasets and provides SOTA performance on multilingual tasks such as XNLI, XCOPA, Paws-X, and XWinograd. Overall, our results present a compelling case for seq2seq models as a powerful alternative to decoder-only models for Large-scale Language Model (LLM) training. ",https://arxiv.org/abs/2208.01448,AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model
AlexaTM 20B,Language,USA,Amazon,2022-08-02,Industry,Question answering,Confident,128.0,API access,"mC4,Wikipedia",Not-defined,19750000000.0,1319000000000.0,2880.0,2.04374016e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,SOTA improvement,2,2000000,113643.12380745166,1293528.32,0.4935,51200.0,327292196.5654608,147456000.0,"In this work, we demonstrate that multilingual large-scale sequence-to-sequence (seq2seq) models, pre-trained on a mixture of denoising and Causal Language Modeling (CLM) tasks, are more efficient few-shot learners than decoder-only models on various tasks. In particular, we train a 20 billion parameter multilingual seq2seq model called Alexa Teacher Model (AlexaTM 20B) and show that it achieves state-of-the-art (SOTA) performance on 1-shot summarization tasks, outperforming a much larger 540B PaLM decoder model. AlexaTM 20B also achieves SOTA in 1-shot machine translation, especially for low-resource languages, across almost all language pairs supported by the model (Arabic, English, French, German, Hindi, Italian, Japanese, Marathi, Portuguese, Spanish, Tamil, and Telugu) on Flores-101 dataset. We also show in zero-shot setting, AlexaTM 20B outperforms GPT3 (175B) on SuperGLUE and SQuADv2 datasets and provides SOTA performance on multilingual tasks such as XNLI, XCOPA, Paws-X, and XWinograd. Overall, our results present a compelling case for seq2seq models as a powerful alternative to decoder-only models for Large-scale Language Model (LLM) training. ",https://arxiv.org/abs/2208.01448,AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model
AlexaTM 20B,Language,USA,Amazon,2022-08-02,Industry,Translation,Confident,128.0,API access,"mC4,Wikipedia",Not-defined,19750000000.0,1319000000000.0,2880.0,2.04374016e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,SOTA improvement,2,2000000,113643.12380745166,1293528.32,0.4935,51200.0,327292196.5654608,147456000.0,"In this work, we demonstrate that multilingual large-scale sequence-to-sequence (seq2seq) models, pre-trained on a mixture of denoising and Causal Language Modeling (CLM) tasks, are more efficient few-shot learners than decoder-only models on various tasks. In particular, we train a 20 billion parameter multilingual seq2seq model called Alexa Teacher Model (AlexaTM 20B) and show that it achieves state-of-the-art (SOTA) performance on 1-shot summarization tasks, outperforming a much larger 540B PaLM decoder model. AlexaTM 20B also achieves SOTA in 1-shot machine translation, especially for low-resource languages, across almost all language pairs supported by the model (Arabic, English, French, German, Hindi, Italian, Japanese, Marathi, Portuguese, Spanish, Tamil, and Telugu) on Flores-101 dataset. We also show in zero-shot setting, AlexaTM 20B outperforms GPT3 (175B) on SuperGLUE and SQuADv2 datasets and provides SOTA performance on multilingual tasks such as XNLI, XCOPA, Paws-X, and XWinograd. Overall, our results present a compelling case for seq2seq models as a powerful alternative to decoder-only models for Large-scale Language Model (LLM) training. ",https://arxiv.org/abs/2208.01448,AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model
AlphaCode,Language,UK,DeepMind,2022-02-02,Industry,Code generation,Not-defined,3750.0,Unreleased,"CodeContests,Unspecified unreleased",Unreleased,41100000000.0,589652798174979.1,147.2,1.63944e+23,1877.1621580193864,6796.58,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,SOTA improvement,4,4718592,1601024.4557014774,25487175.0,1.6774020397208803e-08,720000.0,235670799.87925744,105983999.99999999,"Programming is a powerful and ubiquitous problem-solving tool. Developing systems that can assist programmers or even generate programs independently could make programming more productive and
accessible, yet so far incorporating innovations in AI has proven challenging. Recent large-scale language models have demonstrated an impressive ability to generate code, and are now able to complete
simple programming tasks. However, these models still perform poorly when evaluated on more complex, unseen problems that require problem-solving skills beyond simply translating instructions into
code. For example, competitive programming problems which require an understanding of algorithms
and complex natural language remain extremely challenging. To address this gap, we introduce AlphaCode, a system for code generation that can create novel solutions to these problems that require deeper
reasoning. In simulated evaluations on recent programming competitions on the Codeforces platform,
AlphaCode achieved on average a ranking of top 54.3% in competitions with more than 5,000 participants. We found that three key components were critical to achieve good and reliable performance:
(1) an extensive and clean competitive programming dataset for training and evaluation, (2) large and
efficient-to-sample transformer-based architectures, and (3) large-scale model sampling to explore the
search space, followed by filtering based on program behavior to a small set of submissions.
",https://arxiv.org/abs/2203.07814,Competition-Level Code Generation with AlphaCode
Amazon Nova Pro,Language,USA,Amazon,2024-12-03,Industry,Language modeling/generation,Speculative,13760.0,API access,Not-defined,Not-defined,200000000000.0,4000000000000.0,1152.0,6.000010000000001e+24,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,SOTA improvement,2,2000000,788306.6236983632,139054294.4,9.586150689748849e-10,5504000.0,908129230.5005144,6340608000.0,,https://aws.amazon.com/es/blogs/aws/introducing-amazon-nova-frontier-intelligence-and-industry-leading-price-performance/,Introducing Amazon Nova foundation models: Frontier intelligence and industry leading price performance
Amazon Nova Pro,Language,USA,Amazon,2024-12-03,Industry,Retrieval-augmented generation,Speculative,13760.0,API access,Not-defined,Not-defined,200000000000.0,4000000000000.0,1152.0,6.000010000000001e+24,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,SOTA improvement,2,2000000,788306.6236983632,139054294.4,9.586150689748849e-10,5504000.0,908129230.5005144,6340608000.0,,https://aws.amazon.com/es/blogs/aws/introducing-amazon-nova-frontier-intelligence-and-industry-leading-price-performance/,Introducing Amazon Nova foundation models: Frontier intelligence and industry leading price performance
Amazon Nova Pro,Language,USA,Amazon,2024-12-03,Industry,Video generation,Speculative,13760.0,API access,Not-defined,Not-defined,200000000000.0,4000000000000.0,1152.0,6.000010000000001e+24,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,SOTA improvement,2,2000000,788306.6236983632,139054294.4,9.586150689748849e-10,5504000.0,908129230.5005144,6340608000.0,,https://aws.amazon.com/es/blogs/aws/introducing-amazon-nova-frontier-intelligence-and-industry-leading-price-performance/,Introducing Amazon Nova foundation models: Frontier intelligence and industry leading price performance
Amazon Nova Pro,Multimodal,USA,Amazon,2024-12-03,Industry,Language modeling/generation,Speculative,13760.0,API access,Not-defined,Not-defined,200000000000.0,4000000000000.0,1152.0,6.000010000000001e+24,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,SOTA improvement,2,2000000,788306.6236983632,139054294.4,9.586150689748849e-10,5504000.0,908129230.5005144,6340608000.0,,https://aws.amazon.com/es/blogs/aws/introducing-amazon-nova-frontier-intelligence-and-industry-leading-price-performance/,Introducing Amazon Nova foundation models: Frontier intelligence and industry leading price performance
Amazon Nova Pro,Multimodal,USA,Amazon,2024-12-03,Industry,Retrieval-augmented generation,Speculative,13760.0,API access,Not-defined,Not-defined,200000000000.0,4000000000000.0,1152.0,6.000010000000001e+24,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,SOTA improvement,2,2000000,788306.6236983632,139054294.4,9.586150689748849e-10,5504000.0,908129230.5005144,6340608000.0,,https://aws.amazon.com/es/blogs/aws/introducing-amazon-nova-frontier-intelligence-and-industry-leading-price-performance/,Introducing Amazon Nova foundation models: Frontier intelligence and industry leading price performance
Amazon Nova Pro,Multimodal,USA,Amazon,2024-12-03,Industry,Video generation,Speculative,13760.0,API access,Not-defined,Not-defined,200000000000.0,4000000000000.0,1152.0,6.000010000000001e+24,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,SOTA improvement,2,2000000,788306.6236983632,139054294.4,9.586150689748849e-10,5504000.0,908129230.5005144,6340608000.0,,https://aws.amazon.com/es/blogs/aws/introducing-amazon-nova-frontier-intelligence-and-industry-leading-price-performance/,Introducing Amazon Nova foundation models: Frontier intelligence and industry leading price performance
Amazon Nova Pro,Video,USA,Amazon,2024-12-03,Industry,Language modeling/generation,Speculative,13760.0,API access,Not-defined,Not-defined,200000000000.0,4000000000000.0,1152.0,6.000010000000001e+24,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,SOTA improvement,2,2000000,788306.6236983632,139054294.4,9.586150689748849e-10,5504000.0,908129230.5005144,6340608000.0,,https://aws.amazon.com/es/blogs/aws/introducing-amazon-nova-frontier-intelligence-and-industry-leading-price-performance/,Introducing Amazon Nova foundation models: Frontier intelligence and industry leading price performance
Amazon Nova Pro,Video,USA,Amazon,2024-12-03,Industry,Retrieval-augmented generation,Speculative,13760.0,API access,Not-defined,Not-defined,200000000000.0,4000000000000.0,1152.0,6.000010000000001e+24,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,SOTA improvement,2,2000000,788306.6236983632,139054294.4,9.586150689748849e-10,5504000.0,908129230.5005144,6340608000.0,,https://aws.amazon.com/es/blogs/aws/introducing-amazon-nova-frontier-intelligence-and-industry-leading-price-performance/,Introducing Amazon Nova foundation models: Frontier intelligence and industry leading price performance
Amazon Nova Pro,Video,USA,Amazon,2024-12-03,Industry,Video generation,Speculative,13760.0,API access,Not-defined,Not-defined,200000000000.0,4000000000000.0,1152.0,6.000010000000001e+24,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,SOTA improvement,2,2000000,788306.6236983632,139054294.4,9.586150689748849e-10,5504000.0,908129230.5005144,6340608000.0,,https://aws.amazon.com/es/blogs/aws/introducing-amazon-nova-frontier-intelligence-and-industry-leading-price-performance/,Introducing Amazon Nova foundation models: Frontier intelligence and industry leading price performance
Amazon Titan,Image generation,USA,Amazon,2023-09-28,Industry,Chat,Likely,13760.0,API access,Not-defined,Unreleased,200000000000.0,4000000000000.0,1152.0,4.8e+24,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Training cost,2,2000000,12166402.812291188,139054294.4,0.2696,5504000.0,14015696039.759449,6340608000.0,,https://aws.amazon.com/bedrock/titan/,
Amazon Titan,Image generation,USA,Amazon,2023-09-28,Industry,Code generation,Likely,13760.0,API access,Not-defined,Unreleased,200000000000.0,4000000000000.0,1152.0,4.8e+24,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Training cost,2,2000000,12166402.812291188,139054294.4,0.2696,5504000.0,14015696039.759449,6340608000.0,,https://aws.amazon.com/bedrock/titan/,
Amazon Titan,Image generation,USA,Amazon,2023-09-28,Industry,Image generation,Likely,13760.0,API access,Not-defined,Unreleased,200000000000.0,4000000000000.0,1152.0,4.8e+24,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Training cost,2,2000000,12166402.812291188,139054294.4,0.2696,5504000.0,14015696039.759449,6340608000.0,,https://aws.amazon.com/bedrock/titan/,
Amazon Titan,Image generation,USA,Amazon,2023-09-28,Industry,Language modeling/generation,Likely,13760.0,API access,Not-defined,Unreleased,200000000000.0,4000000000000.0,1152.0,4.8e+24,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Training cost,2,2000000,12166402.812291188,139054294.4,0.2696,5504000.0,14015696039.759449,6340608000.0,,https://aws.amazon.com/bedrock/titan/,
Amazon Titan,Image generation,USA,Amazon,2023-09-28,Industry,Semantic search,Likely,13760.0,API access,Not-defined,Unreleased,200000000000.0,4000000000000.0,1152.0,4.8e+24,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Training cost,2,2000000,12166402.812291188,139054294.4,0.2696,5504000.0,14015696039.759449,6340608000.0,,https://aws.amazon.com/bedrock/titan/,
Amazon Titan,Image generation,USA,Amazon,2023-09-28,Industry,Text-to-image,Likely,13760.0,API access,Not-defined,Unreleased,200000000000.0,4000000000000.0,1152.0,4.8e+24,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Training cost,2,2000000,12166402.812291188,139054294.4,0.2696,5504000.0,14015696039.759449,6340608000.0,,https://aws.amazon.com/bedrock/titan/,
Amazon Titan,Image generation,USA,Amazon,2023-09-28,Industry,Translation,Likely,13760.0,API access,Not-defined,Unreleased,200000000000.0,4000000000000.0,1152.0,4.8e+24,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Training cost,2,2000000,12166402.812291188,139054294.4,0.2696,5504000.0,14015696039.759449,6340608000.0,,https://aws.amazon.com/bedrock/titan/,
Amazon Titan,Language,USA,Amazon,2023-09-28,Industry,Chat,Likely,13760.0,API access,Not-defined,Unreleased,200000000000.0,4000000000000.0,1152.0,4.8e+24,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Training cost,2,2000000,12166402.812291188,139054294.4,0.2696,5504000.0,14015696039.759449,6340608000.0,,https://aws.amazon.com/bedrock/titan/,
Amazon Titan,Language,USA,Amazon,2023-09-28,Industry,Code generation,Likely,13760.0,API access,Not-defined,Unreleased,200000000000.0,4000000000000.0,1152.0,4.8e+24,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Training cost,2,2000000,12166402.812291188,139054294.4,0.2696,5504000.0,14015696039.759449,6340608000.0,,https://aws.amazon.com/bedrock/titan/,
Amazon Titan,Language,USA,Amazon,2023-09-28,Industry,Image generation,Likely,13760.0,API access,Not-defined,Unreleased,200000000000.0,4000000000000.0,1152.0,4.8e+24,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Training cost,2,2000000,12166402.812291188,139054294.4,0.2696,5504000.0,14015696039.759449,6340608000.0,,https://aws.amazon.com/bedrock/titan/,
Amazon Titan,Language,USA,Amazon,2023-09-28,Industry,Language modeling/generation,Likely,13760.0,API access,Not-defined,Unreleased,200000000000.0,4000000000000.0,1152.0,4.8e+24,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Training cost,2,2000000,12166402.812291188,139054294.4,0.2696,5504000.0,14015696039.759449,6340608000.0,,https://aws.amazon.com/bedrock/titan/,
Amazon Titan,Language,USA,Amazon,2023-09-28,Industry,Semantic search,Likely,13760.0,API access,Not-defined,Unreleased,200000000000.0,4000000000000.0,1152.0,4.8e+24,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Training cost,2,2000000,12166402.812291188,139054294.4,0.2696,5504000.0,14015696039.759449,6340608000.0,,https://aws.amazon.com/bedrock/titan/,
Amazon Titan,Language,USA,Amazon,2023-09-28,Industry,Text-to-image,Likely,13760.0,API access,Not-defined,Unreleased,200000000000.0,4000000000000.0,1152.0,4.8e+24,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Training cost,2,2000000,12166402.812291188,139054294.4,0.2696,5504000.0,14015696039.759449,6340608000.0,,https://aws.amazon.com/bedrock/titan/,
Amazon Titan,Language,USA,Amazon,2023-09-28,Industry,Translation,Likely,13760.0,API access,Not-defined,Unreleased,200000000000.0,4000000000000.0,1152.0,4.8e+24,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Training cost,2,2000000,12166402.812291188,139054294.4,0.2696,5504000.0,14015696039.759449,6340608000.0,,https://aws.amazon.com/bedrock/titan/,
Anthropic LM 175B,Language,USA,Anthropic,2023-02-15,Industry,Language modeling/generation,Confident,1024.0,Unreleased,Not-defined,Unreleased,175000000000.0,180000000000.0,793.5,4.3e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Significant use,1,2000000,909984.4296839886,11628103.68,1.3376883720930233e-08,409600.0,722072644.954245,325017600.0,"We test the hypothesis that language models trained with reinforcement learning from human feedback (RLHF) have the capability to ""morally self-correct"" -- to avoid producing harmful outputs -- if instructed to do so. We find strong evidence in support of this hypothesis across three different experiments, each of which reveal different facets of moral self-correction. We find that the capability for moral self-correction emerges at 22B model parameters, and typically improves with increasing model size and RLHF training. We believe that at this level of scale, language models obtain two capabilities that they can use for moral self-correction: (1) they can follow instructions and (2) they can learn complex normative concepts of harm like stereotyping, bias, and discrimination. As such, they can follow instructions to avoid certain kinds of morally harmful outputs. We believe our results are cause for cautious optimism regarding the ability to train language models to abide by ethical principles.",https://arxiv.org/abs/2302.07459,The Capacity for Moral Self-Correction in Large Language Models
Anthropic LM 175B,Language,USA,Anthropic,2023-02-15,Industry,Question answering,Confident,1024.0,Unreleased,Not-defined,Unreleased,175000000000.0,180000000000.0,793.5,4.3e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Significant use,1,2000000,909984.4296839886,11628103.68,1.3376883720930233e-08,409600.0,722072644.954245,325017600.0,"We test the hypothesis that language models trained with reinforcement learning from human feedback (RLHF) have the capability to ""morally self-correct"" -- to avoid producing harmful outputs -- if instructed to do so. We find strong evidence in support of this hypothesis across three different experiments, each of which reveal different facets of moral self-correction. We find that the capability for moral self-correction emerges at 22B model parameters, and typically improves with increasing model size and RLHF training. We believe that at this level of scale, language models obtain two capabilities that they can use for moral self-correction: (1) they can follow instructions and (2) they can learn complex normative concepts of harm like stereotyping, bias, and discrimination. As such, they can follow instructions to avoid certain kinds of morally harmful outputs. We believe our results are cause for cautious optimism regarding the ability to train language models to abide by ethical principles.",https://arxiv.org/abs/2302.07459,The Capacity for Moral Self-Correction in Large Language Models
Aquila2 34B,Language,China,Beijing Academy of Artificial Intelligence / BAAI,2023-10-13,Academia,Chat,Likely,512.0,Open weights (unrestricted),Unspecified unreleased,Open source,34000000000.0,2000000000000.0,4096.0,3.7800000000001e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,SOTA improvement,1,4000000,197336.5951439333,5174113.28,1.5216137566137164e-08,204800.0,808290693.7095507,838860800.0,"We announce that our Aquila2 series is now open source, comprising Aquila2 (the base language models: Aquila2-7B, Aquila2-34B and Aquila2-70B-Expr) and AquilaChat2 (the chat models, namely AquilaChat2-7B, AquilaChat2-34B and AquilaChat2-70B-Expr, as well as the long-text chat models, namely AquilaChat2-7B-16k and AquilaChat2-34B-16k). You can find the links in the following table. Kindly click on them to access the model cards.","https://github.com/FlagAI-Open/Aquila2
https://huggingface.co/BAAI/Aquila2-34B",
Aquila2 34B,Language,China,Beijing Academy of Artificial Intelligence / BAAI,2023-10-13,Academia,Language modeling/generation,Likely,512.0,Open weights (unrestricted),Unspecified unreleased,Open source,34000000000.0,2000000000000.0,4096.0,3.7800000000001e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,SOTA improvement,1,4000000,197336.5951439333,5174113.28,1.5216137566137164e-08,204800.0,808290693.7095507,838860800.0,"We announce that our Aquila2 series is now open source, comprising Aquila2 (the base language models: Aquila2-7B, Aquila2-34B and Aquila2-70B-Expr) and AquilaChat2 (the chat models, namely AquilaChat2-7B, AquilaChat2-34B and AquilaChat2-70B-Expr, as well as the long-text chat models, namely AquilaChat2-7B-16k and AquilaChat2-34B-16k). You can find the links in the following table. Kindly click on them to access the model cards.","https://github.com/FlagAI-Open/Aquila2
https://huggingface.co/BAAI/Aquila2-34B",
BIG-G 137B,Language,USA,Google,2022-06-09,Industry,Language modeling/generation,Confident,12288.0,Unreleased,GLaM dataset,Unreleased,137000000000.0,681000000000.0,42.0,5.6e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Training cost,0,3200000,788306.6236983632,124178718.72,1.0270892857142856e-08,4915200.0,33108878.195331253,206438400.0,"Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 450 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI's GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide a strong baseline. Findings include: model performance and calibration both improve with scale, but are poor in absolute terms (and when compared with rater performance); performance is remarkably similar across model classes, though with benefits from sparsity; tasks that improve gradually and predictably commonly involve a large knowledge or memorization component, whereas tasks that exhibit ""breakthrough"" behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting.",https://arxiv.org/abs/2206.04615,Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models
BLOOM-176B,Language,France,BigScience,2022-07-11,Industry,Code generation,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Historical significance,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,France,BigScience,2022-07-11,Industry,Code generation,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Highly cited,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,France,BigScience,2022-07-11,Industry,Language modeling,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Historical significance,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,France,BigScience,2022-07-11,Industry,Language modeling,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Highly cited,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,France,BigScience,2022-07-11,Industry,Translation,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Historical significance,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,France,BigScience,2022-07-11,Industry,Translation,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Highly cited,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,France,BigScience,2022-07-11,Research collective,Code generation,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Historical significance,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,France,BigScience,2022-07-11,Research collective,Code generation,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Highly cited,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,France,BigScience,2022-07-11,Research collective,Language modeling,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Historical significance,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,France,BigScience,2022-07-11,Research collective,Language modeling,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Highly cited,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,France,BigScience,2022-07-11,Research collective,Translation,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Historical significance,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,France,BigScience,2022-07-11,Research collective,Translation,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Highly cited,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,France,Hugging Face,2022-07-11,Industry,Code generation,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Historical significance,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,France,Hugging Face,2022-07-11,Industry,Code generation,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Highly cited,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,France,Hugging Face,2022-07-11,Industry,Language modeling,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Historical significance,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,France,Hugging Face,2022-07-11,Industry,Language modeling,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Highly cited,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,France,Hugging Face,2022-07-11,Industry,Translation,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Historical significance,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,France,Hugging Face,2022-07-11,Industry,Translation,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Highly cited,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,France,Hugging Face,2022-07-11,Research collective,Code generation,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Historical significance,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,France,Hugging Face,2022-07-11,Research collective,Code generation,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Highly cited,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,France,Hugging Face,2022-07-11,Research collective,Language modeling,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Historical significance,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,France,Hugging Face,2022-07-11,Research collective,Language modeling,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Highly cited,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,France,Hugging Face,2022-07-11,Research collective,Translation,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Historical significance,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,France,Hugging Face,2022-07-11,Research collective,Translation,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Highly cited,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,Multinational,BigScience,2022-07-11,Industry,Code generation,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Historical significance,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,Multinational,BigScience,2022-07-11,Industry,Code generation,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Highly cited,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,Multinational,BigScience,2022-07-11,Industry,Language modeling,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Historical significance,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,Multinational,BigScience,2022-07-11,Industry,Language modeling,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Highly cited,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,Multinational,BigScience,2022-07-11,Industry,Translation,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Historical significance,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,Multinational,BigScience,2022-07-11,Industry,Translation,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Highly cited,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,Multinational,BigScience,2022-07-11,Research collective,Code generation,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Historical significance,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,Multinational,BigScience,2022-07-11,Research collective,Code generation,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Highly cited,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,Multinational,BigScience,2022-07-11,Research collective,Language modeling,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Historical significance,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,Multinational,BigScience,2022-07-11,Research collective,Language modeling,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Highly cited,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,Multinational,BigScience,2022-07-11,Research collective,Translation,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Historical significance,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,Multinational,BigScience,2022-07-11,Research collective,Translation,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Highly cited,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,Multinational,Hugging Face,2022-07-11,Industry,Code generation,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Historical significance,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,Multinational,Hugging Face,2022-07-11,Industry,Code generation,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Highly cited,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,Multinational,Hugging Face,2022-07-11,Industry,Language modeling,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Historical significance,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,Multinational,Hugging Face,2022-07-11,Industry,Language modeling,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Highly cited,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,Multinational,Hugging Face,2022-07-11,Industry,Translation,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Historical significance,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,Multinational,Hugging Face,2022-07-11,Industry,Translation,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Highly cited,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,Multinational,Hugging Face,2022-07-11,Research collective,Code generation,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Historical significance,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,Multinational,Hugging Face,2022-07-11,Research collective,Code generation,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Highly cited,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,Multinational,Hugging Face,2022-07-11,Research collective,Language modeling,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Historical significance,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,Multinational,Hugging Face,2022-07-11,Research collective,Language modeling,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Highly cited,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,Multinational,Hugging Face,2022-07-11,Research collective,Translation,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Historical significance,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,Multinational,Hugging Face,2022-07-11,Research collective,Translation,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Highly cited,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,USA,BigScience,2022-07-11,Industry,Code generation,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Historical significance,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,USA,BigScience,2022-07-11,Industry,Code generation,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Highly cited,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,USA,BigScience,2022-07-11,Industry,Language modeling,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Historical significance,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,USA,BigScience,2022-07-11,Industry,Language modeling,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Highly cited,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,USA,BigScience,2022-07-11,Industry,Translation,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Historical significance,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,USA,BigScience,2022-07-11,Industry,Translation,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Highly cited,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,USA,BigScience,2022-07-11,Research collective,Code generation,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Historical significance,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,USA,BigScience,2022-07-11,Research collective,Code generation,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Highly cited,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,USA,BigScience,2022-07-11,Research collective,Language modeling,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Historical significance,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,USA,BigScience,2022-07-11,Research collective,Language modeling,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Highly cited,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,USA,BigScience,2022-07-11,Research collective,Translation,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Historical significance,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,USA,BigScience,2022-07-11,Research collective,Translation,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Highly cited,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,USA,Hugging Face,2022-07-11,Industry,Code generation,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Historical significance,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,USA,Hugging Face,2022-07-11,Industry,Code generation,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Highly cited,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,USA,Hugging Face,2022-07-11,Industry,Language modeling,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Historical significance,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,USA,Hugging Face,2022-07-11,Industry,Language modeling,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Highly cited,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,USA,Hugging Face,2022-07-11,Industry,Translation,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Historical significance,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,USA,Hugging Face,2022-07-11,Industry,Translation,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Highly cited,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,USA,Hugging Face,2022-07-11,Research collective,Code generation,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Historical significance,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,USA,Hugging Face,2022-07-11,Research collective,Code generation,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Highly cited,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,USA,Hugging Face,2022-07-11,Research collective,Language modeling,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Historical significance,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,USA,Hugging Face,2022-07-11,Research collective,Language modeling,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Highly cited,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,USA,Hugging Face,2022-07-11,Research collective,Translation,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Historical significance,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
BLOOM-176B,Language,USA,Hugging Face,2022-07-11,Research collective,Translation,Confident,384.0,Open weights (restricted use),BigScience ROOTS Corpus,Unreleased,176247271424.0,379000000000.0,2808.0,3.65664e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Highly cited,1,4194304,341004.3433715435,4360538.88,0.5,153600.0,957540196.1872941,431308800.0,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",https://arxiv.org/abs/2211.05100,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
Baichuan 2-7B,Not-defined,China,Baichuan,2023-09-20,Industry,Not-defined,Unverified,1024.0,Not-defined,Not-defined,Not-defined,7000000000.0,2600000000000.0,1282.4326530612243,1.092e+23,1877.1621580193864,7406.0,303.0,107206000000000.0,303958000000000.0,54246185353.93311,NVIDIA A800,Training cost,1,4000000,788306.6236983632,7583744.0,9.817399267399268e-09,310272.0,1010950154.8552281,397902944.1306122,"In this technical report, we present Baichuan 2, a series of large-scale multilingual language models containing 7 billion and 13 billion parameters, trained from scratch, on 2.6 trillion tokens. ",https://arxiv.org/pdf/2309.10305,"Baichuan 2: Open Large-scale Language Models
"
Baichuan2-13B,Language,China,Baichuan,2023-09-06,Industry,Chat,Confident,1024.0,Open weights (restricted use),Not-defined,Unreleased,13000000000.0,2275000000000.0,1282.4326530612243,2.03e+23,1877.1621580193864,7406.0,303.0,107206000000000.0,303958000000000.0,54246185353.93311,NVIDIA A800,Training cost,1,4000000,197336.5951439333,7583744.0,5.281083743842365e-09,310272.0,253070893.25650308,397902944.1306122,,"https://huggingface.co/baichuan-inc/Baichuan2-13B-Base, https://arxiv.org/abs/2309.10305",Baichuan 2: Open Large-scale Language Models
Baichuan2-53B,Language,China,Baichuan,2023-08-09,Industry,Chat,Likely,1024.0,Not-defined,Not-defined,Not-defined,53000000000.0,2275000000000.0,4096.0,8.268e+23,1877.1621580193864,7406.0,303.0,107206000000000.0,303958000000000.0,54246185353.93311,NVIDIA A800,Training cost,1,4000000,197336.5951439333,7583744.0,1.2966376390904694e-09,310272.0,808290693.7095507,1270874112.0,"On Tuesday, four-month-old AI startup Baichuan Intelligent Technology unveiled its first closed-source model equipped with 53 billion parameters. Following the Chinese company’s rapid release of two open-source large language models since its founding in April, the new model demonstrates the firm’s fast pace in delivering pre-trained models for larger parameters. The freshly introduced model, Baichuan-53B, is mainly for corporate clients and focused on text generation. A ChatGPT-like chat service built on the model entered internal testing on the same day the model was launched, its official website shows, with plans for the firm to publicly launch APIs and associated components next month.",https://technode.com/2023/08/09/chinese-ai-startup-baichuan-rolls-out-third-llm-in-four-months/,Chinese AI startup Baichuan rolls out third LLM in four months
Baichuan2-53B,Language,China,Baichuan,2023-08-09,Industry,Language modeling/generation,Likely,1024.0,Not-defined,Not-defined,Not-defined,53000000000.0,2275000000000.0,4096.0,8.268e+23,1877.1621580193864,7406.0,303.0,107206000000000.0,303958000000000.0,54246185353.93311,NVIDIA A800,Training cost,1,4000000,197336.5951439333,7583744.0,1.2966376390904694e-09,310272.0,808290693.7095507,1270874112.0,"On Tuesday, four-month-old AI startup Baichuan Intelligent Technology unveiled its first closed-source model equipped with 53 billion parameters. Following the Chinese company’s rapid release of two open-source large language models since its founding in April, the new model demonstrates the firm’s fast pace in delivering pre-trained models for larger parameters. The freshly introduced model, Baichuan-53B, is mainly for corporate clients and focused on text generation. A ChatGPT-like chat service built on the model entered internal testing on the same day the model was launched, its official website shows, with plans for the firm to publicly launch APIs and associated components next month.",https://technode.com/2023/08/09/chinese-ai-startup-baichuan-rolls-out-third-llm-in-four-months/,Chinese AI startup Baichuan rolls out third LLM in four months
BloombergGPT,Language,USA,Bloomberg,2023-03-30,Academia,Language modeling,Confident,512.0,Unreleased,Not-defined,Unreleased,50558868480.0,532000000000.0,1270.0,2.36e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,SOTA improvement,0,4200000,453498.3266248031,5174113.28,0.327,204800.0,575942874.8134999,260096000.0,"The use of NLP in the realm of financial technology is broad and complex, with applications ranging from sentiment analysis and named entity recognition to question answering. Large Language Models (LLMs) have been shown to be effective on a variety of tasks; however, no LLM specialized for the financial domain has been reported in literature. In this work, we present BloombergGPT, a 50 billion parameter language model that is trained on a wide range of financial data. We construct a 363 billion token dataset based on Bloomberg's extensive data sources, perhaps the largest domain-specific dataset yet, augmented with 345 billion tokens from general purpose datasets. We validate BloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite of internal benchmarks that most accurately reflect our intended usage. Our mixed dataset training leads to a model that outperforms existing models on financial tasks by significant margins without sacrificing performance on general LLM benchmarks. Additionally, we explain our modeling choices, training process, and evaluation methodology. We release Training Chronicles (Appendix C) detailing our experience in training BloombergGPT.",https://arxiv.org/abs/2303.17564,BloombergGPT: A Large Language Model for Finance
BloombergGPT,Language,USA,Bloomberg,2023-03-30,Industry,Language modeling,Confident,512.0,Unreleased,Not-defined,Unreleased,50558868480.0,532000000000.0,1270.0,2.36e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,SOTA improvement,0,4200000,453498.3266248031,5174113.28,0.327,204800.0,575942874.8134999,260096000.0,"The use of NLP in the realm of financial technology is broad and complex, with applications ranging from sentiment analysis and named entity recognition to question answering. Large Language Models (LLMs) have been shown to be effective on a variety of tasks; however, no LLM specialized for the financial domain has been reported in literature. In this work, we present BloombergGPT, a 50 billion parameter language model that is trained on a wide range of financial data. We construct a 363 billion token dataset based on Bloomberg's extensive data sources, perhaps the largest domain-specific dataset yet, augmented with 345 billion tokens from general purpose datasets. We validate BloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite of internal benchmarks that most accurately reflect our intended usage. Our mixed dataset training leads to a model that outperforms existing models on financial tasks by significant margins without sacrificing performance on general LLM benchmarks. Additionally, we explain our modeling choices, training process, and evaluation methodology. We release Training Chronicles (Appendix C) detailing our experience in training BloombergGPT.",https://arxiv.org/abs/2303.17564,BloombergGPT: A Large Language Model for Finance
BloombergGPT,Language,USA,Johns Hopkins University,2023-03-30,Academia,Language modeling,Confident,512.0,Unreleased,Not-defined,Unreleased,50558868480.0,532000000000.0,1270.0,2.36e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,SOTA improvement,0,4200000,453498.3266248031,5174113.28,0.327,204800.0,575942874.8134999,260096000.0,"The use of NLP in the realm of financial technology is broad and complex, with applications ranging from sentiment analysis and named entity recognition to question answering. Large Language Models (LLMs) have been shown to be effective on a variety of tasks; however, no LLM specialized for the financial domain has been reported in literature. In this work, we present BloombergGPT, a 50 billion parameter language model that is trained on a wide range of financial data. We construct a 363 billion token dataset based on Bloomberg's extensive data sources, perhaps the largest domain-specific dataset yet, augmented with 345 billion tokens from general purpose datasets. We validate BloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite of internal benchmarks that most accurately reflect our intended usage. Our mixed dataset training leads to a model that outperforms existing models on financial tasks by significant margins without sacrificing performance on general LLM benchmarks. Additionally, we explain our modeling choices, training process, and evaluation methodology. We release Training Chronicles (Appendix C) detailing our experience in training BloombergGPT.",https://arxiv.org/abs/2303.17564,BloombergGPT: A Large Language Model for Finance
BloombergGPT,Language,USA,Johns Hopkins University,2023-03-30,Industry,Language modeling,Confident,512.0,Unreleased,Not-defined,Unreleased,50558868480.0,532000000000.0,1270.0,2.36e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,SOTA improvement,0,4200000,453498.3266248031,5174113.28,0.327,204800.0,575942874.8134999,260096000.0,"The use of NLP in the realm of financial technology is broad and complex, with applications ranging from sentiment analysis and named entity recognition to question answering. Large Language Models (LLMs) have been shown to be effective on a variety of tasks; however, no LLM specialized for the financial domain has been reported in literature. In this work, we present BloombergGPT, a 50 billion parameter language model that is trained on a wide range of financial data. We construct a 363 billion token dataset based on Bloomberg's extensive data sources, perhaps the largest domain-specific dataset yet, augmented with 345 billion tokens from general purpose datasets. We validate BloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite of internal benchmarks that most accurately reflect our intended usage. Our mixed dataset training leads to a model that outperforms existing models on financial tasks by significant margins without sacrificing performance on general LLM benchmarks. Additionally, we explain our modeling choices, training process, and evaluation methodology. We release Training Chronicles (Appendix C) detailing our experience in training BloombergGPT.",https://arxiv.org/abs/2303.17564,BloombergGPT: A Large Language Model for Finance
BlueLM 130B,Language,China,vivo AI lab,2023-11-02,Industry,Chat,Confident,4096.0,Unreleased,Unspecified unreleased,Unreleased,130000000000.0,2592000000000.0,4320.0,7.8e+23,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,4194304,197336.5951439333,61440000.0,7.374435897435898e-09,1638400.0,852494091.0217918,7077888000.0,,https://baijiahao.baidu.com/s?id=1781445143383237948&wfr=spider&for=pc,
BlueLM 130B,Language,China,vivo AI lab,2023-11-02,Industry,Language modeling/generation,Confident,4096.0,Unreleased,Unspecified unreleased,Unreleased,130000000000.0,2592000000000.0,4320.0,7.8e+23,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,4194304,197336.5951439333,61440000.0,7.374435897435898e-09,1638400.0,852494091.0217918,7077888000.0,,https://baijiahao.baidu.com/s?id=1781445143383237948&wfr=spider&for=pc,
BlueLM 130B,Language,China,vivo AI lab,2023-11-02,Industry,Question answering,Confident,4096.0,Unreleased,Unspecified unreleased,Unreleased,130000000000.0,2592000000000.0,4320.0,7.8e+23,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,4194304,197336.5951439333,61440000.0,7.374435897435898e-09,1638400.0,852494091.0217918,7077888000.0,,https://baijiahao.baidu.com/s?id=1781445143383237948&wfr=spider&for=pc,
BlueLM 175B,Language,China,vivo AI lab,2023-11-02,Industry,Chat,Confident,1024.0,Unreleased,Unspecified unreleased,Unreleased,175000000000.0,2592000000000.0,793.5,1.05e+24,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Significant use,1,2000000,909984.4296839886,11628103.68,5.478152380952381e-09,409600.0,722072644.954245,325017600.0,,https://baijiahao.baidu.com/s?id=1781445143383237948&wfr=spider&for=pc,
BlueLM 175B,Language,China,vivo AI lab,2023-11-02,Industry,Language modeling/generation,Confident,1024.0,Unreleased,Unspecified unreleased,Unreleased,175000000000.0,2592000000000.0,793.5,1.05e+24,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Significant use,1,2000000,909984.4296839886,11628103.68,5.478152380952381e-09,409600.0,722072644.954245,325017600.0,,https://baijiahao.baidu.com/s?id=1781445143383237948&wfr=spider&for=pc,
BlueLM 175B,Language,China,vivo AI lab,2023-11-02,Industry,Question answering,Confident,1024.0,Unreleased,Unspecified unreleased,Unreleased,175000000000.0,2592000000000.0,793.5,1.05e+24,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Significant use,1,2000000,909984.4296839886,11628103.68,5.478152380952381e-09,409600.0,722072644.954245,325017600.0,,https://baijiahao.baidu.com/s?id=1781445143383237948&wfr=spider&for=pc,
BlueLM 70B,Language,China,vivo AI lab,2023-11-02,Industry,Chat,Confident,1024.0,Unreleased,Unspecified unreleased,Unreleased,70000000000.0,2592000000000.0,793.5,4.2e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,SOTA improvement,1,4000000,197336.5951439333,11628103.68,1.3695380952380951e-08,409600.0,156586588.24671108,325017600.0,,https://baijiahao.baidu.com/s?id=1781445143383237948&wfr=spider&for=pc,
BlueLM 70B,Language,China,vivo AI lab,2023-11-02,Industry,Language modeling/generation,Confident,1024.0,Unreleased,Unspecified unreleased,Unreleased,70000000000.0,2592000000000.0,793.5,4.2e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,SOTA improvement,1,4000000,197336.5951439333,11628103.68,1.3695380952380951e-08,409600.0,156586588.24671108,325017600.0,,https://baijiahao.baidu.com/s?id=1781445143383237948&wfr=spider&for=pc,
BlueLM 70B,Language,China,vivo AI lab,2023-11-02,Industry,Question answering,Confident,1024.0,Unreleased,Unspecified unreleased,Unreleased,70000000000.0,2592000000000.0,793.5,4.2e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,SOTA improvement,1,4000000,197336.5951439333,11628103.68,1.3695380952380951e-08,409600.0,156586588.24671108,325017600.0,,https://baijiahao.baidu.com/s?id=1781445143383237948&wfr=spider&for=pc,
BlueLM 7B,Language,China,vivo AI lab,2023-10-31,Industry,Chat,Confident,512.0,Open weights (restricted use),Unspecified unreleased,Unreleased,7000000000.0,2592000000000.0,940.0,1.0920000000001e+23,1877.1621580193864,12302.845,450.0,766305000000000.0,503500000000000.0,84000000000.0,"AMD Radeon Instinct MI250X,NVIDIA A100",Significant use,1,4000000,197336.5951439333,6299056.64,7.017445054944413e-08,230400.0,185496399.43529728,216576000.0,"BlueLM is a large-scale open-source language model independently developed by the vivo AI Lab. This release includes 2K and 32K context length versions for both Base and Chat models.

High-quality Data: BlueLM is trained on a high-quality data with 2.6 trillion tokens. Our train corpus mainly consists of Chinese and English data, with a small amount of Japanese and Korean data.
Stronger Performance: BlueLM-7B-Chat achieves a strong competitive performance in C-Eval and CMMLU benchmarks of the same size.
Longer Context: We have extended the context length of both BlueLM-7B-Base-32K and BlueLM-7B-Chat-32K models from 2K to 32K. The models can support longer context understanding while maintaining the same basic capabilities.
Model License: BlueLM weights are open for academic research and commercial use.",https://github.com/vivo-ai-lab/BlueLM/blob/main/BlueLM_technical_report.pdf,BlueLM: An Open Multilingual 7B Language Model
BlueLM 7B,Language,China,vivo AI lab,2023-10-31,Industry,Code generation,Confident,512.0,Open weights (restricted use),Unspecified unreleased,Unreleased,7000000000.0,2592000000000.0,940.0,1.0920000000001e+23,1877.1621580193864,12302.845,450.0,766305000000000.0,503500000000000.0,84000000000.0,"AMD Radeon Instinct MI250X,NVIDIA A100",Significant use,1,4000000,197336.5951439333,6299056.64,7.017445054944413e-08,230400.0,185496399.43529728,216576000.0,"BlueLM is a large-scale open-source language model independently developed by the vivo AI Lab. This release includes 2K and 32K context length versions for both Base and Chat models.

High-quality Data: BlueLM is trained on a high-quality data with 2.6 trillion tokens. Our train corpus mainly consists of Chinese and English data, with a small amount of Japanese and Korean data.
Stronger Performance: BlueLM-7B-Chat achieves a strong competitive performance in C-Eval and CMMLU benchmarks of the same size.
Longer Context: We have extended the context length of both BlueLM-7B-Base-32K and BlueLM-7B-Chat-32K models from 2K to 32K. The models can support longer context understanding while maintaining the same basic capabilities.
Model License: BlueLM weights are open for academic research and commercial use.",https://github.com/vivo-ai-lab/BlueLM/blob/main/BlueLM_technical_report.pdf,BlueLM: An Open Multilingual 7B Language Model
BlueLM 7B,Language,China,vivo AI lab,2023-10-31,Industry,Language modeling/generation,Confident,512.0,Open weights (restricted use),Unspecified unreleased,Unreleased,7000000000.0,2592000000000.0,940.0,1.0920000000001e+23,1877.1621580193864,12302.845,450.0,766305000000000.0,503500000000000.0,84000000000.0,"AMD Radeon Instinct MI250X,NVIDIA A100",Significant use,1,4000000,197336.5951439333,6299056.64,7.017445054944413e-08,230400.0,185496399.43529728,216576000.0,"BlueLM is a large-scale open-source language model independently developed by the vivo AI Lab. This release includes 2K and 32K context length versions for both Base and Chat models.

High-quality Data: BlueLM is trained on a high-quality data with 2.6 trillion tokens. Our train corpus mainly consists of Chinese and English data, with a small amount of Japanese and Korean data.
Stronger Performance: BlueLM-7B-Chat achieves a strong competitive performance in C-Eval and CMMLU benchmarks of the same size.
Longer Context: We have extended the context length of both BlueLM-7B-Base-32K and BlueLM-7B-Chat-32K models from 2K to 32K. The models can support longer context understanding while maintaining the same basic capabilities.
Model License: BlueLM weights are open for academic research and commercial use.",https://github.com/vivo-ai-lab/BlueLM/blob/main/BlueLM_technical_report.pdf,BlueLM: An Open Multilingual 7B Language Model
BlueLM 7B,Language,China,vivo AI lab,2023-10-31,Industry,Question answering,Confident,512.0,Open weights (restricted use),Unspecified unreleased,Unreleased,7000000000.0,2592000000000.0,940.0,1.0920000000001e+23,1877.1621580193864,12302.845,450.0,766305000000000.0,503500000000000.0,84000000000.0,"AMD Radeon Instinct MI250X,NVIDIA A100",Significant use,1,4000000,197336.5951439333,6299056.64,7.017445054944413e-08,230400.0,185496399.43529728,216576000.0,"BlueLM is a large-scale open-source language model independently developed by the vivo AI Lab. This release includes 2K and 32K context length versions for both Base and Chat models.

High-quality Data: BlueLM is trained on a high-quality data with 2.6 trillion tokens. Our train corpus mainly consists of Chinese and English data, with a small amount of Japanese and Korean data.
Stronger Performance: BlueLM-7B-Chat achieves a strong competitive performance in C-Eval and CMMLU benchmarks of the same size.
Longer Context: We have extended the context length of both BlueLM-7B-Base-32K and BlueLM-7B-Chat-32K models from 2K to 32K. The models can support longer context understanding while maintaining the same basic capabilities.
Model License: BlueLM weights are open for academic research and commercial use.",https://github.com/vivo-ai-lab/BlueLM/blob/main/BlueLM_technical_report.pdf,BlueLM: An Open Multilingual 7B Language Model
BlueLM 7B,Language,China,vivo AI lab,2023-10-31,Industry,Translation,Confident,512.0,Open weights (restricted use),Unspecified unreleased,Unreleased,7000000000.0,2592000000000.0,940.0,1.0920000000001e+23,1877.1621580193864,12302.845,450.0,766305000000000.0,503500000000000.0,84000000000.0,"AMD Radeon Instinct MI250X,NVIDIA A100",Significant use,1,4000000,197336.5951439333,6299056.64,7.017445054944413e-08,230400.0,185496399.43529728,216576000.0,"BlueLM is a large-scale open-source language model independently developed by the vivo AI Lab. This release includes 2K and 32K context length versions for both Base and Chat models.

High-quality Data: BlueLM is trained on a high-quality data with 2.6 trillion tokens. Our train corpus mainly consists of Chinese and English data, with a small amount of Japanese and Korean data.
Stronger Performance: BlueLM-7B-Chat achieves a strong competitive performance in C-Eval and CMMLU benchmarks of the same size.
Longer Context: We have extended the context length of both BlueLM-7B-Base-32K and BlueLM-7B-Chat-32K models from 2K to 32K. The models can support longer context understanding while maintaining the same basic capabilities.
Model License: BlueLM weights are open for academic research and commercial use.",https://github.com/vivo-ai-lab/BlueLM/blob/main/BlueLM_technical_report.pdf,BlueLM: An Open Multilingual 7B Language Model
Chameleon-34B,Image generation,USA,Facebook AI Research,2024-05-16,Industry,Language modeling/generation,Confident,3072.0,Open weights (non-commercial),Unspecified unreleased,Unreleased,34000000000.0,4400000000000.0,1394.0,1.6453571041e+24,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Significant use,1,4000000,113138.0969599182,34884311.04,3.495934095806114e-09,1228800.0,157714507.16212597,1712947200.0,"We present Chameleon, a family of early-fusion token-based mixed-modal models capable of understanding and generating images and text in any arbitrary sequence. We outline a stable training approach from inception, an alignment recipe, and an architectural parameterization tailored for the early-fusion, token-based, mixed-modal setting. The models are evaluated on a comprehensive range of tasks, including visual question answering, image captioning, text generation, image generation, and long-form mixed modal generation. Chameleon demonstrates broad and general capabilities, including state-of-the-art performance in image captioning tasks, outperforms Llama-2 in text-only tasks while being competitive with models such as Mixtral 8x7B and Gemini-Pro, and performs non-trivial image generation, all in a single model. It also matches or exceeds the performance of much larger models, including Gemini Pro and GPT-4V, according to human judgments on a new long-form mixed-modal generation evaluation, where either the prompt or outputs contain mixed sequences of both images and text. Chameleon marks a significant step forward in a unified modeling of full multimodal documents.",https://arxiv.org/abs/2405.09818v1,Chameleon: Mixed-Modal Early-Fusion Foundation Models
Chameleon-34B,Image generation,USA,Facebook AI Research,2024-05-16,Industry,Text-to-image,Confident,3072.0,Open weights (non-commercial),Unspecified unreleased,Unreleased,34000000000.0,4400000000000.0,1394.0,1.6453571041e+24,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Significant use,1,4000000,113138.0969599182,34884311.04,3.495934095806114e-09,1228800.0,157714507.16212597,1712947200.0,"We present Chameleon, a family of early-fusion token-based mixed-modal models capable of understanding and generating images and text in any arbitrary sequence. We outline a stable training approach from inception, an alignment recipe, and an architectural parameterization tailored for the early-fusion, token-based, mixed-modal setting. The models are evaluated on a comprehensive range of tasks, including visual question answering, image captioning, text generation, image generation, and long-form mixed modal generation. Chameleon demonstrates broad and general capabilities, including state-of-the-art performance in image captioning tasks, outperforms Llama-2 in text-only tasks while being competitive with models such as Mixtral 8x7B and Gemini-Pro, and performs non-trivial image generation, all in a single model. It also matches or exceeds the performance of much larger models, including Gemini Pro and GPT-4V, according to human judgments on a new long-form mixed-modal generation evaluation, where either the prompt or outputs contain mixed sequences of both images and text. Chameleon marks a significant step forward in a unified modeling of full multimodal documents.",https://arxiv.org/abs/2405.09818v1,Chameleon: Mixed-Modal Early-Fusion Foundation Models
Chameleon-34B,Image generation,USA,Facebook AI Research,2024-05-16,Industry,Vision-language generation,Confident,3072.0,Open weights (non-commercial),Unspecified unreleased,Unreleased,34000000000.0,4400000000000.0,1394.0,1.6453571041e+24,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Significant use,1,4000000,113138.0969599182,34884311.04,3.495934095806114e-09,1228800.0,157714507.16212597,1712947200.0,"We present Chameleon, a family of early-fusion token-based mixed-modal models capable of understanding and generating images and text in any arbitrary sequence. We outline a stable training approach from inception, an alignment recipe, and an architectural parameterization tailored for the early-fusion, token-based, mixed-modal setting. The models are evaluated on a comprehensive range of tasks, including visual question answering, image captioning, text generation, image generation, and long-form mixed modal generation. Chameleon demonstrates broad and general capabilities, including state-of-the-art performance in image captioning tasks, outperforms Llama-2 in text-only tasks while being competitive with models such as Mixtral 8x7B and Gemini-Pro, and performs non-trivial image generation, all in a single model. It also matches or exceeds the performance of much larger models, including Gemini Pro and GPT-4V, according to human judgments on a new long-form mixed-modal generation evaluation, where either the prompt or outputs contain mixed sequences of both images and text. Chameleon marks a significant step forward in a unified modeling of full multimodal documents.",https://arxiv.org/abs/2405.09818v1,Chameleon: Mixed-Modal Early-Fusion Foundation Models
Chameleon-34B,Image generation,USA,Facebook AI Research,2024-05-16,Industry,Visual question answering,Confident,3072.0,Open weights (non-commercial),Unspecified unreleased,Unreleased,34000000000.0,4400000000000.0,1394.0,1.6453571041e+24,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Significant use,1,4000000,113138.0969599182,34884311.04,3.495934095806114e-09,1228800.0,157714507.16212597,1712947200.0,"We present Chameleon, a family of early-fusion token-based mixed-modal models capable of understanding and generating images and text in any arbitrary sequence. We outline a stable training approach from inception, an alignment recipe, and an architectural parameterization tailored for the early-fusion, token-based, mixed-modal setting. The models are evaluated on a comprehensive range of tasks, including visual question answering, image captioning, text generation, image generation, and long-form mixed modal generation. Chameleon demonstrates broad and general capabilities, including state-of-the-art performance in image captioning tasks, outperforms Llama-2 in text-only tasks while being competitive with models such as Mixtral 8x7B and Gemini-Pro, and performs non-trivial image generation, all in a single model. It also matches or exceeds the performance of much larger models, including Gemini Pro and GPT-4V, according to human judgments on a new long-form mixed-modal generation evaluation, where either the prompt or outputs contain mixed sequences of both images and text. Chameleon marks a significant step forward in a unified modeling of full multimodal documents.",https://arxiv.org/abs/2405.09818v1,Chameleon: Mixed-Modal Early-Fusion Foundation Models
Chameleon-34B,Language,USA,Facebook AI Research,2024-05-16,Industry,Language modeling/generation,Confident,3072.0,Open weights (non-commercial),Unspecified unreleased,Unreleased,34000000000.0,4400000000000.0,1394.0,1.6453571041e+24,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Significant use,1,4000000,113138.0969599182,34884311.04,3.495934095806114e-09,1228800.0,157714507.16212597,1712947200.0,"We present Chameleon, a family of early-fusion token-based mixed-modal models capable of understanding and generating images and text in any arbitrary sequence. We outline a stable training approach from inception, an alignment recipe, and an architectural parameterization tailored for the early-fusion, token-based, mixed-modal setting. The models are evaluated on a comprehensive range of tasks, including visual question answering, image captioning, text generation, image generation, and long-form mixed modal generation. Chameleon demonstrates broad and general capabilities, including state-of-the-art performance in image captioning tasks, outperforms Llama-2 in text-only tasks while being competitive with models such as Mixtral 8x7B and Gemini-Pro, and performs non-trivial image generation, all in a single model. It also matches or exceeds the performance of much larger models, including Gemini Pro and GPT-4V, according to human judgments on a new long-form mixed-modal generation evaluation, where either the prompt or outputs contain mixed sequences of both images and text. Chameleon marks a significant step forward in a unified modeling of full multimodal documents.",https://arxiv.org/abs/2405.09818v1,Chameleon: Mixed-Modal Early-Fusion Foundation Models
Chameleon-34B,Language,USA,Facebook AI Research,2024-05-16,Industry,Text-to-image,Confident,3072.0,Open weights (non-commercial),Unspecified unreleased,Unreleased,34000000000.0,4400000000000.0,1394.0,1.6453571041e+24,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Significant use,1,4000000,113138.0969599182,34884311.04,3.495934095806114e-09,1228800.0,157714507.16212597,1712947200.0,"We present Chameleon, a family of early-fusion token-based mixed-modal models capable of understanding and generating images and text in any arbitrary sequence. We outline a stable training approach from inception, an alignment recipe, and an architectural parameterization tailored for the early-fusion, token-based, mixed-modal setting. The models are evaluated on a comprehensive range of tasks, including visual question answering, image captioning, text generation, image generation, and long-form mixed modal generation. Chameleon demonstrates broad and general capabilities, including state-of-the-art performance in image captioning tasks, outperforms Llama-2 in text-only tasks while being competitive with models such as Mixtral 8x7B and Gemini-Pro, and performs non-trivial image generation, all in a single model. It also matches or exceeds the performance of much larger models, including Gemini Pro and GPT-4V, according to human judgments on a new long-form mixed-modal generation evaluation, where either the prompt or outputs contain mixed sequences of both images and text. Chameleon marks a significant step forward in a unified modeling of full multimodal documents.",https://arxiv.org/abs/2405.09818v1,Chameleon: Mixed-Modal Early-Fusion Foundation Models
Chameleon-34B,Language,USA,Facebook AI Research,2024-05-16,Industry,Vision-language generation,Confident,3072.0,Open weights (non-commercial),Unspecified unreleased,Unreleased,34000000000.0,4400000000000.0,1394.0,1.6453571041e+24,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Significant use,1,4000000,113138.0969599182,34884311.04,3.495934095806114e-09,1228800.0,157714507.16212597,1712947200.0,"We present Chameleon, a family of early-fusion token-based mixed-modal models capable of understanding and generating images and text in any arbitrary sequence. We outline a stable training approach from inception, an alignment recipe, and an architectural parameterization tailored for the early-fusion, token-based, mixed-modal setting. The models are evaluated on a comprehensive range of tasks, including visual question answering, image captioning, text generation, image generation, and long-form mixed modal generation. Chameleon demonstrates broad and general capabilities, including state-of-the-art performance in image captioning tasks, outperforms Llama-2 in text-only tasks while being competitive with models such as Mixtral 8x7B and Gemini-Pro, and performs non-trivial image generation, all in a single model. It also matches or exceeds the performance of much larger models, including Gemini Pro and GPT-4V, according to human judgments on a new long-form mixed-modal generation evaluation, where either the prompt or outputs contain mixed sequences of both images and text. Chameleon marks a significant step forward in a unified modeling of full multimodal documents.",https://arxiv.org/abs/2405.09818v1,Chameleon: Mixed-Modal Early-Fusion Foundation Models
Chameleon-34B,Language,USA,Facebook AI Research,2024-05-16,Industry,Visual question answering,Confident,3072.0,Open weights (non-commercial),Unspecified unreleased,Unreleased,34000000000.0,4400000000000.0,1394.0,1.6453571041e+24,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Significant use,1,4000000,113138.0969599182,34884311.04,3.495934095806114e-09,1228800.0,157714507.16212597,1712947200.0,"We present Chameleon, a family of early-fusion token-based mixed-modal models capable of understanding and generating images and text in any arbitrary sequence. We outline a stable training approach from inception, an alignment recipe, and an architectural parameterization tailored for the early-fusion, token-based, mixed-modal setting. The models are evaluated on a comprehensive range of tasks, including visual question answering, image captioning, text generation, image generation, and long-form mixed modal generation. Chameleon demonstrates broad and general capabilities, including state-of-the-art performance in image captioning tasks, outperforms Llama-2 in text-only tasks while being competitive with models such as Mixtral 8x7B and Gemini-Pro, and performs non-trivial image generation, all in a single model. It also matches or exceeds the performance of much larger models, including Gemini Pro and GPT-4V, according to human judgments on a new long-form mixed-modal generation evaluation, where either the prompt or outputs contain mixed sequences of both images and text. Chameleon marks a significant step forward in a unified modeling of full multimodal documents.",https://arxiv.org/abs/2405.09818v1,Chameleon: Mixed-Modal Early-Fusion Foundation Models
Chameleon-34B,Multimodal,USA,Facebook AI Research,2024-05-16,Industry,Language modeling/generation,Confident,3072.0,Open weights (non-commercial),Unspecified unreleased,Unreleased,34000000000.0,4400000000000.0,1394.0,1.6453571041e+24,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Significant use,1,4000000,113138.0969599182,34884311.04,3.495934095806114e-09,1228800.0,157714507.16212597,1712947200.0,"We present Chameleon, a family of early-fusion token-based mixed-modal models capable of understanding and generating images and text in any arbitrary sequence. We outline a stable training approach from inception, an alignment recipe, and an architectural parameterization tailored for the early-fusion, token-based, mixed-modal setting. The models are evaluated on a comprehensive range of tasks, including visual question answering, image captioning, text generation, image generation, and long-form mixed modal generation. Chameleon demonstrates broad and general capabilities, including state-of-the-art performance in image captioning tasks, outperforms Llama-2 in text-only tasks while being competitive with models such as Mixtral 8x7B and Gemini-Pro, and performs non-trivial image generation, all in a single model. It also matches or exceeds the performance of much larger models, including Gemini Pro and GPT-4V, according to human judgments on a new long-form mixed-modal generation evaluation, where either the prompt or outputs contain mixed sequences of both images and text. Chameleon marks a significant step forward in a unified modeling of full multimodal documents.",https://arxiv.org/abs/2405.09818v1,Chameleon: Mixed-Modal Early-Fusion Foundation Models
Chameleon-34B,Multimodal,USA,Facebook AI Research,2024-05-16,Industry,Text-to-image,Confident,3072.0,Open weights (non-commercial),Unspecified unreleased,Unreleased,34000000000.0,4400000000000.0,1394.0,1.6453571041e+24,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Significant use,1,4000000,113138.0969599182,34884311.04,3.495934095806114e-09,1228800.0,157714507.16212597,1712947200.0,"We present Chameleon, a family of early-fusion token-based mixed-modal models capable of understanding and generating images and text in any arbitrary sequence. We outline a stable training approach from inception, an alignment recipe, and an architectural parameterization tailored for the early-fusion, token-based, mixed-modal setting. The models are evaluated on a comprehensive range of tasks, including visual question answering, image captioning, text generation, image generation, and long-form mixed modal generation. Chameleon demonstrates broad and general capabilities, including state-of-the-art performance in image captioning tasks, outperforms Llama-2 in text-only tasks while being competitive with models such as Mixtral 8x7B and Gemini-Pro, and performs non-trivial image generation, all in a single model. It also matches or exceeds the performance of much larger models, including Gemini Pro and GPT-4V, according to human judgments on a new long-form mixed-modal generation evaluation, where either the prompt or outputs contain mixed sequences of both images and text. Chameleon marks a significant step forward in a unified modeling of full multimodal documents.",https://arxiv.org/abs/2405.09818v1,Chameleon: Mixed-Modal Early-Fusion Foundation Models
Chameleon-34B,Multimodal,USA,Facebook AI Research,2024-05-16,Industry,Vision-language generation,Confident,3072.0,Open weights (non-commercial),Unspecified unreleased,Unreleased,34000000000.0,4400000000000.0,1394.0,1.6453571041e+24,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Significant use,1,4000000,113138.0969599182,34884311.04,3.495934095806114e-09,1228800.0,157714507.16212597,1712947200.0,"We present Chameleon, a family of early-fusion token-based mixed-modal models capable of understanding and generating images and text in any arbitrary sequence. We outline a stable training approach from inception, an alignment recipe, and an architectural parameterization tailored for the early-fusion, token-based, mixed-modal setting. The models are evaluated on a comprehensive range of tasks, including visual question answering, image captioning, text generation, image generation, and long-form mixed modal generation. Chameleon demonstrates broad and general capabilities, including state-of-the-art performance in image captioning tasks, outperforms Llama-2 in text-only tasks while being competitive with models such as Mixtral 8x7B and Gemini-Pro, and performs non-trivial image generation, all in a single model. It also matches or exceeds the performance of much larger models, including Gemini Pro and GPT-4V, according to human judgments on a new long-form mixed-modal generation evaluation, where either the prompt or outputs contain mixed sequences of both images and text. Chameleon marks a significant step forward in a unified modeling of full multimodal documents.",https://arxiv.org/abs/2405.09818v1,Chameleon: Mixed-Modal Early-Fusion Foundation Models
Chameleon-34B,Multimodal,USA,Facebook AI Research,2024-05-16,Industry,Visual question answering,Confident,3072.0,Open weights (non-commercial),Unspecified unreleased,Unreleased,34000000000.0,4400000000000.0,1394.0,1.6453571041e+24,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Significant use,1,4000000,113138.0969599182,34884311.04,3.495934095806114e-09,1228800.0,157714507.16212597,1712947200.0,"We present Chameleon, a family of early-fusion token-based mixed-modal models capable of understanding and generating images and text in any arbitrary sequence. We outline a stable training approach from inception, an alignment recipe, and an architectural parameterization tailored for the early-fusion, token-based, mixed-modal setting. The models are evaluated on a comprehensive range of tasks, including visual question answering, image captioning, text generation, image generation, and long-form mixed modal generation. Chameleon demonstrates broad and general capabilities, including state-of-the-art performance in image captioning tasks, outperforms Llama-2 in text-only tasks while being competitive with models such as Mixtral 8x7B and Gemini-Pro, and performs non-trivial image generation, all in a single model. It also matches or exceeds the performance of much larger models, including Gemini Pro and GPT-4V, according to human judgments on a new long-form mixed-modal generation evaluation, where either the prompt or outputs contain mixed sequences of both images and text. Chameleon marks a significant step forward in a unified modeling of full multimodal documents.",https://arxiv.org/abs/2405.09818v1,Chameleon: Mixed-Modal Early-Fusion Foundation Models
Chameleon-34B,Vision,USA,Facebook AI Research,2024-05-16,Industry,Language modeling/generation,Confident,3072.0,Open weights (non-commercial),Unspecified unreleased,Unreleased,34000000000.0,4400000000000.0,1394.0,1.6453571041e+24,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Significant use,1,4000000,113138.0969599182,34884311.04,3.495934095806114e-09,1228800.0,157714507.16212597,1712947200.0,"We present Chameleon, a family of early-fusion token-based mixed-modal models capable of understanding and generating images and text in any arbitrary sequence. We outline a stable training approach from inception, an alignment recipe, and an architectural parameterization tailored for the early-fusion, token-based, mixed-modal setting. The models are evaluated on a comprehensive range of tasks, including visual question answering, image captioning, text generation, image generation, and long-form mixed modal generation. Chameleon demonstrates broad and general capabilities, including state-of-the-art performance in image captioning tasks, outperforms Llama-2 in text-only tasks while being competitive with models such as Mixtral 8x7B and Gemini-Pro, and performs non-trivial image generation, all in a single model. It also matches or exceeds the performance of much larger models, including Gemini Pro and GPT-4V, according to human judgments on a new long-form mixed-modal generation evaluation, where either the prompt or outputs contain mixed sequences of both images and text. Chameleon marks a significant step forward in a unified modeling of full multimodal documents.",https://arxiv.org/abs/2405.09818v1,Chameleon: Mixed-Modal Early-Fusion Foundation Models
Chameleon-34B,Vision,USA,Facebook AI Research,2024-05-16,Industry,Text-to-image,Confident,3072.0,Open weights (non-commercial),Unspecified unreleased,Unreleased,34000000000.0,4400000000000.0,1394.0,1.6453571041e+24,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Significant use,1,4000000,113138.0969599182,34884311.04,3.495934095806114e-09,1228800.0,157714507.16212597,1712947200.0,"We present Chameleon, a family of early-fusion token-based mixed-modal models capable of understanding and generating images and text in any arbitrary sequence. We outline a stable training approach from inception, an alignment recipe, and an architectural parameterization tailored for the early-fusion, token-based, mixed-modal setting. The models are evaluated on a comprehensive range of tasks, including visual question answering, image captioning, text generation, image generation, and long-form mixed modal generation. Chameleon demonstrates broad and general capabilities, including state-of-the-art performance in image captioning tasks, outperforms Llama-2 in text-only tasks while being competitive with models such as Mixtral 8x7B and Gemini-Pro, and performs non-trivial image generation, all in a single model. It also matches or exceeds the performance of much larger models, including Gemini Pro and GPT-4V, according to human judgments on a new long-form mixed-modal generation evaluation, where either the prompt or outputs contain mixed sequences of both images and text. Chameleon marks a significant step forward in a unified modeling of full multimodal documents.",https://arxiv.org/abs/2405.09818v1,Chameleon: Mixed-Modal Early-Fusion Foundation Models
Chameleon-34B,Vision,USA,Facebook AI Research,2024-05-16,Industry,Vision-language generation,Confident,3072.0,Open weights (non-commercial),Unspecified unreleased,Unreleased,34000000000.0,4400000000000.0,1394.0,1.6453571041e+24,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Significant use,1,4000000,113138.0969599182,34884311.04,3.495934095806114e-09,1228800.0,157714507.16212597,1712947200.0,"We present Chameleon, a family of early-fusion token-based mixed-modal models capable of understanding and generating images and text in any arbitrary sequence. We outline a stable training approach from inception, an alignment recipe, and an architectural parameterization tailored for the early-fusion, token-based, mixed-modal setting. The models are evaluated on a comprehensive range of tasks, including visual question answering, image captioning, text generation, image generation, and long-form mixed modal generation. Chameleon demonstrates broad and general capabilities, including state-of-the-art performance in image captioning tasks, outperforms Llama-2 in text-only tasks while being competitive with models such as Mixtral 8x7B and Gemini-Pro, and performs non-trivial image generation, all in a single model. It also matches or exceeds the performance of much larger models, including Gemini Pro and GPT-4V, according to human judgments on a new long-form mixed-modal generation evaluation, where either the prompt or outputs contain mixed sequences of both images and text. Chameleon marks a significant step forward in a unified modeling of full multimodal documents.",https://arxiv.org/abs/2405.09818v1,Chameleon: Mixed-Modal Early-Fusion Foundation Models
Chameleon-34B,Vision,USA,Facebook AI Research,2024-05-16,Industry,Visual question answering,Confident,3072.0,Open weights (non-commercial),Unspecified unreleased,Unreleased,34000000000.0,4400000000000.0,1394.0,1.6453571041e+24,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Significant use,1,4000000,113138.0969599182,34884311.04,3.495934095806114e-09,1228800.0,157714507.16212597,1712947200.0,"We present Chameleon, a family of early-fusion token-based mixed-modal models capable of understanding and generating images and text in any arbitrary sequence. We outline a stable training approach from inception, an alignment recipe, and an architectural parameterization tailored for the early-fusion, token-based, mixed-modal setting. The models are evaluated on a comprehensive range of tasks, including visual question answering, image captioning, text generation, image generation, and long-form mixed modal generation. Chameleon demonstrates broad and general capabilities, including state-of-the-art performance in image captioning tasks, outperforms Llama-2 in text-only tasks while being competitive with models such as Mixtral 8x7B and Gemini-Pro, and performs non-trivial image generation, all in a single model. It also matches or exceeds the performance of much larger models, including Gemini Pro and GPT-4V, according to human judgments on a new long-form mixed-modal generation evaluation, where either the prompt or outputs contain mixed sequences of both images and text. Chameleon marks a significant step forward in a unified modeling of full multimodal documents.",https://arxiv.org/abs/2405.09818v1,Chameleon: Mixed-Modal Early-Fusion Foundation Models
CodeFuse-13B,Language,China,Ant Group,2023-10-10,Industry,Code generation,Confident,512.0,Open weights (unrestricted),"The Stack,GitHub",Unreleased,13000000000.0,1000000000000.0,960.0,3.09e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Training cost,1,36864000,788306.6236983632,5814051.84,1.8615080906148866e-08,204800.0,756774358.7504287,196608000.0,"Code Large Language Models (Code LLMs) have gained significant attention in the industry due to their wide applications in the full lifecycle of software engineering. However, the effectiveness of existing models in understanding non-English inputs for multi-lingual code-related tasks is still far from well studied. This paper introduces CodeFuse-13B, an open-sourced pre-trained code LLM. It is specifically designed for code-related tasks with both English and Chinese prompts and supports over 40 programming languages. CodeFuse achieves its effectiveness by utilizing a high quality pre-training dataset that is carefully filtered by program analyzers and optimized during the training process. Extensive experiments are conducted using real-world usage scenarios, the industry-standard benchmark HumanEval-x, and the specially designed CodeFuseEval for Chinese prompts. To assess the effectiveness of CodeFuse, we actively collected valuable human feedback from the AntGroup's software development process where CodeFuse has been successfully deployed. The results demonstrate that CodeFuse-13B achieves a HumanEval pass@1 score of 37.10%, positioning it as one of the top multi-lingual code LLMs with similar parameter sizes. In practical scenarios, such as code generation, code translation, code comments, and testcase generation, CodeFuse performs better than other models when confronted with Chinese prompts.",https://arxiv.org/abs/2310.06266,CodeFuse-13B: A Pretrained Multi-lingual Code Large Language Model
CodeGen2,Language,USA,Salesforce,2023-05-03,Industry,Code generation,Confident,512.0,Open weights (unrestricted),Stack v1.1,Not-defined,16000000000.0,1000000000000.0,960.0,3.09e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,SOTA improvement,1,4718592,1601024.4557014774,5814051.84,1.8615080906148866e-08,204800.0,1536983477.4734182,196608000.0,"Large language models (LLMs) have demonstrated remarkable abilities in representation learning for program synthesis and understanding tasks. The quality of the learned representations appears to be dictated by the neural scaling laws as a function of the number of model parameters and observations, while imposing upper bounds on the model performance by the amount of available data and compute, which is costly.
In this study, we attempt to render the training of LLMs for program synthesis more efficient by unifying four key components: (1) model architectures, (2) learning methods, (3) infill sampling, and, (4) data distributions. Specifically, for the model architecture, we attempt to unify encoder and decoder-based models into a single prefix-LM. For learning methods, (i) causal language modeling, (ii) span corruption, (iii) infilling are unified into a simple learning algorithm. For infill sampling, we explore the claim of a ""free lunch"" hypothesis. For data distributions, the effect of a mixture distribution and multi-epoch training of programming and natural languages on model performance is explored.
We conduct a comprehensive series of empirical experiments on 1B LLMs, for which failures and successes of this exploration are distilled into five lessons. We will provide a final recipe for training and release CodeGen2 models in size 1B, 3.7B, 7B, and, 16B parameters, along with the training framework as open-source: this https URL.",https://arxiv.org/abs/2305.02309,CodeGen2: Lessons for Training LLMs on Programming and Natural Languages
CodeWhisperer,Language,USA,Amazon,2022-06-24,Industry,Code generation,Unknown,512.0,Hosted access (no API),Unspecified unreleased,Unreleased,16000000000.0,1000000000000.0,960.0,3.09e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Training cost,1,4718592,788306.6236983632,5814051.84,1.8615080906148866e-08,204800.0,756774358.7504287,196608000.0,"We are excited to announce Amazon CodeWhisperer, a machine learning (ML)-powered service that helps improve developer productivity by providing code recommendations based on developers’ natural comments and prior code. With CodeWhisperer, developers can simply write a comment that outlines a specific task in plain English, such as “upload a file to S3.” Based on this, CodeWhisperer automatically determines which cloud services and public libraries are best suited for the specified task, builds the specific code on the fly, and recommends the generated code snippets directly in the IDE.",https://aws.amazon.com/blogs/machine-learning/introducing-amazon-codewhisperer-the-ml-powered-coding-companion/,"Introducing Amazon CodeWhisperer, the ML-powered coding companion"
Codestral,Language,France,Mistral AI,2024-05-29,Industry,Code autocompletion,Confident,3750.0,Open weights (non-commercial),Unspecified unreleased,Unreleased,22200000000.0,589652798174979.1,147.2,1.63944e+23,1877.1621580193864,6796.58,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,SOTA improvement,4,4718592,1601024.4557014774,25487175.0,1.6774020397208803e-08,720000.0,235670799.87925744,105983999.99999999,"We introduce Codestral, our first-ever code model. Codestral is an open-weight generative AI model explicitly designed for code generation tasks. It helps developers write and interact with code through a shared instruction and completion API endpoint. As it masters code and English, it can be used to design advanced AI applications for software developers.

A model fluent in 80+ programming languages
Codestral is trained on a diverse dataset of 80+ programming languages, including the most popular ones, such as Python, Java, C, C++, JavaScript, and Bash. It also performs well on more specific ones like Swift and Fortran. This broad language base ensures Codestral can assist developers in various coding environments and projects.

Codestral saves developers time and effort: it can complete coding functions, write tests, and complete any partial code using a fill-in-the-middle mechanism. Interacting with Codestral will help level up the developer’s coding game and reduce the risk of errors and bugs.",https://mistral.ai/news/codestral/,
Codestral,Language,France,Mistral AI,2024-05-29,Industry,Code generation,Confident,3750.0,Open weights (non-commercial),Unspecified unreleased,Unreleased,22200000000.0,589652798174979.1,147.2,1.63944e+23,1877.1621580193864,6796.58,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,SOTA improvement,4,4718592,1601024.4557014774,25487175.0,1.6774020397208803e-08,720000.0,235670799.87925744,105983999.99999999,"We introduce Codestral, our first-ever code model. Codestral is an open-weight generative AI model explicitly designed for code generation tasks. It helps developers write and interact with code through a shared instruction and completion API endpoint. As it masters code and English, it can be used to design advanced AI applications for software developers.

A model fluent in 80+ programming languages
Codestral is trained on a diverse dataset of 80+ programming languages, including the most popular ones, such as Python, Java, C, C++, JavaScript, and Bash. It also performs well on more specific ones like Swift and Fortran. This broad language base ensures Codestral can assist developers in various coding environments and projects.

Codestral saves developers time and effort: it can complete coding functions, write tests, and complete any partial code using a fill-in-the-middle mechanism. Interacting with Codestral will help level up the developer’s coding game and reduce the risk of errors and bugs.",https://mistral.ai/news/codestral/,
"Cosmos-1.0-
Diffusion-14B Video2World",Robotics,USA,NVIDIA,2025-01-07,Industry,Robotic manipulation,Unverified,10000.0,Open weights (restricted use),Unspecified unreleased,Not-defined,14000000000.0,9000000000000000.0,1282.4326530612243,6.1554816e+24,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,None,4,6472188,91700208.69015627,444898800.0,3.0086354250494388e-09,7000000.0,117599341916.78503,8977028571.42857,"Physical AI needs to be trained digitally first. It needs a digital twin of itself, the policy model, and a digital twin of the world, the world model. In this paper, we present the Cosmos World Foundation Model Platform to help developers build customized world models for their Physical AI setups. We position a world foundation model as a general-purpose world model that can be fine-tuned into customized world models for downstream applications. Our platform covers a video curation pipeline, pre-trained world foundation models, examples of post-training of pre-trained world foundation models, and video tokenizers. To help Physical AI builders solve the most critical problems of our society, we make our platform open-source and our models open-weight with permissive licenses available via this https URL.",https://arxiv.org/abs/2501.03575,Cosmos World Foundation Model Platform for Physical AI
"Cosmos-1.0-
Diffusion-14B Video2World",Robotics,USA,NVIDIA,2025-01-07,Industry,Self-driving car,Unverified,10000.0,Open weights (restricted use),Unspecified unreleased,Not-defined,14000000000.0,9000000000000000.0,1282.4326530612243,6.1554816e+24,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,None,4,6472188,91700208.69015627,444898800.0,3.0086354250494388e-09,7000000.0,117599341916.78503,8977028571.42857,"Physical AI needs to be trained digitally first. It needs a digital twin of itself, the policy model, and a digital twin of the world, the world model. In this paper, we present the Cosmos World Foundation Model Platform to help developers build customized world models for their Physical AI setups. We position a world foundation model as a general-purpose world model that can be fine-tuned into customized world models for downstream applications. Our platform covers a video curation pipeline, pre-trained world foundation models, examples of post-training of pre-trained world foundation models, and video tokenizers. To help Physical AI builders solve the most critical problems of our society, we make our platform open-source and our models open-weight with permissive licenses available via this https URL.",https://arxiv.org/abs/2501.03575,Cosmos World Foundation Model Platform for Physical AI
"Cosmos-1.0-
Diffusion-14B Video2World",Robotics,USA,NVIDIA,2025-01-07,Industry,Video generation,Unverified,10000.0,Open weights (restricted use),Unspecified unreleased,Not-defined,14000000000.0,9000000000000000.0,1282.4326530612243,6.1554816e+24,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,None,4,6472188,91700208.69015627,444898800.0,3.0086354250494388e-09,7000000.0,117599341916.78503,8977028571.42857,"Physical AI needs to be trained digitally first. It needs a digital twin of itself, the policy model, and a digital twin of the world, the world model. In this paper, we present the Cosmos World Foundation Model Platform to help developers build customized world models for their Physical AI setups. We position a world foundation model as a general-purpose world model that can be fine-tuned into customized world models for downstream applications. Our platform covers a video curation pipeline, pre-trained world foundation models, examples of post-training of pre-trained world foundation models, and video tokenizers. To help Physical AI builders solve the most critical problems of our society, we make our platform open-source and our models open-weight with permissive licenses available via this https URL.",https://arxiv.org/abs/2501.03575,Cosmos World Foundation Model Platform for Physical AI
"Cosmos-1.0-
Diffusion-14B Video2World",Video,USA,NVIDIA,2025-01-07,Industry,Robotic manipulation,Unverified,10000.0,Open weights (restricted use),Unspecified unreleased,Not-defined,14000000000.0,9000000000000000.0,1282.4326530612243,6.1554816e+24,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,None,4,6472188,91700208.69015627,444898800.0,3.0086354250494388e-09,7000000.0,117599341916.78503,8977028571.42857,"Physical AI needs to be trained digitally first. It needs a digital twin of itself, the policy model, and a digital twin of the world, the world model. In this paper, we present the Cosmos World Foundation Model Platform to help developers build customized world models for their Physical AI setups. We position a world foundation model as a general-purpose world model that can be fine-tuned into customized world models for downstream applications. Our platform covers a video curation pipeline, pre-trained world foundation models, examples of post-training of pre-trained world foundation models, and video tokenizers. To help Physical AI builders solve the most critical problems of our society, we make our platform open-source and our models open-weight with permissive licenses available via this https URL.",https://arxiv.org/abs/2501.03575,Cosmos World Foundation Model Platform for Physical AI
"Cosmos-1.0-
Diffusion-14B Video2World",Video,USA,NVIDIA,2025-01-07,Industry,Self-driving car,Unverified,10000.0,Open weights (restricted use),Unspecified unreleased,Not-defined,14000000000.0,9000000000000000.0,1282.4326530612243,6.1554816e+24,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,None,4,6472188,91700208.69015627,444898800.0,3.0086354250494388e-09,7000000.0,117599341916.78503,8977028571.42857,"Physical AI needs to be trained digitally first. It needs a digital twin of itself, the policy model, and a digital twin of the world, the world model. In this paper, we present the Cosmos World Foundation Model Platform to help developers build customized world models for their Physical AI setups. We position a world foundation model as a general-purpose world model that can be fine-tuned into customized world models for downstream applications. Our platform covers a video curation pipeline, pre-trained world foundation models, examples of post-training of pre-trained world foundation models, and video tokenizers. To help Physical AI builders solve the most critical problems of our society, we make our platform open-source and our models open-weight with permissive licenses available via this https URL.",https://arxiv.org/abs/2501.03575,Cosmos World Foundation Model Platform for Physical AI
"Cosmos-1.0-
Diffusion-14B Video2World",Video,USA,NVIDIA,2025-01-07,Industry,Video generation,Unverified,10000.0,Open weights (restricted use),Unspecified unreleased,Not-defined,14000000000.0,9000000000000000.0,1282.4326530612243,6.1554816e+24,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,None,4,6472188,91700208.69015627,444898800.0,3.0086354250494388e-09,7000000.0,117599341916.78503,8977028571.42857,"Physical AI needs to be trained digitally first. It needs a digital twin of itself, the policy model, and a digital twin of the world, the world model. In this paper, we present the Cosmos World Foundation Model Platform to help developers build customized world models for their Physical AI setups. We position a world foundation model as a general-purpose world model that can be fine-tuned into customized world models for downstream applications. Our platform covers a video curation pipeline, pre-trained world foundation models, examples of post-training of pre-trained world foundation models, and video tokenizers. To help Physical AI builders solve the most critical problems of our society, we make our platform open-source and our models open-weight with permissive licenses available via this https URL.",https://arxiv.org/abs/2501.03575,Cosmos World Foundation Model Platform for Physical AI
"Cosmos-1.0-
Diffusion-14B Video2World",Vision,USA,NVIDIA,2025-01-07,Industry,Robotic manipulation,Unverified,10000.0,Open weights (restricted use),Unspecified unreleased,Not-defined,14000000000.0,9000000000000000.0,1282.4326530612243,6.1554816e+24,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,None,4,6472188,91700208.69015627,444898800.0,3.0086354250494388e-09,7000000.0,117599341916.78503,8977028571.42857,"Physical AI needs to be trained digitally first. It needs a digital twin of itself, the policy model, and a digital twin of the world, the world model. In this paper, we present the Cosmos World Foundation Model Platform to help developers build customized world models for their Physical AI setups. We position a world foundation model as a general-purpose world model that can be fine-tuned into customized world models for downstream applications. Our platform covers a video curation pipeline, pre-trained world foundation models, examples of post-training of pre-trained world foundation models, and video tokenizers. To help Physical AI builders solve the most critical problems of our society, we make our platform open-source and our models open-weight with permissive licenses available via this https URL.",https://arxiv.org/abs/2501.03575,Cosmos World Foundation Model Platform for Physical AI
"Cosmos-1.0-
Diffusion-14B Video2World",Vision,USA,NVIDIA,2025-01-07,Industry,Self-driving car,Unverified,10000.0,Open weights (restricted use),Unspecified unreleased,Not-defined,14000000000.0,9000000000000000.0,1282.4326530612243,6.1554816e+24,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,None,4,6472188,91700208.69015627,444898800.0,3.0086354250494388e-09,7000000.0,117599341916.78503,8977028571.42857,"Physical AI needs to be trained digitally first. It needs a digital twin of itself, the policy model, and a digital twin of the world, the world model. In this paper, we present the Cosmos World Foundation Model Platform to help developers build customized world models for their Physical AI setups. We position a world foundation model as a general-purpose world model that can be fine-tuned into customized world models for downstream applications. Our platform covers a video curation pipeline, pre-trained world foundation models, examples of post-training of pre-trained world foundation models, and video tokenizers. To help Physical AI builders solve the most critical problems of our society, we make our platform open-source and our models open-weight with permissive licenses available via this https URL.",https://arxiv.org/abs/2501.03575,Cosmos World Foundation Model Platform for Physical AI
"Cosmos-1.0-
Diffusion-14B Video2World",Vision,USA,NVIDIA,2025-01-07,Industry,Video generation,Unverified,10000.0,Open weights (restricted use),Unspecified unreleased,Not-defined,14000000000.0,9000000000000000.0,1282.4326530612243,6.1554816e+24,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,None,4,6472188,91700208.69015627,444898800.0,3.0086354250494388e-09,7000000.0,117599341916.78503,8977028571.42857,"Physical AI needs to be trained digitally first. It needs a digital twin of itself, the policy model, and a digital twin of the world, the world model. In this paper, we present the Cosmos World Foundation Model Platform to help developers build customized world models for their Physical AI setups. We position a world foundation model as a general-purpose world model that can be fine-tuned into customized world models for downstream applications. Our platform covers a video curation pipeline, pre-trained world foundation models, examples of post-training of pre-trained world foundation models, and video tokenizers. To help Physical AI builders solve the most critical problems of our society, we make our platform open-source and our models open-weight with permissive licenses available via this https URL.",https://arxiv.org/abs/2501.03575,Cosmos World Foundation Model Platform for Physical AI
DeepSeek Coder 33B,Language,China,DeepSeek,2024-01-25,Academia,Code generation,Likely,2048.0,Open weights (restricted use),Not-defined,Unreleased,33000000000.0,2000000000000.0,960.0,3.96e+23,1877.1621580193864,38473.85279999992,700.0,1532450000000000.0,1979000000000000.0,80000000000.0,NVIDIA H800 SXM5,Training cost,1,36864000,788306.6236983632,78794450.53439984,3.869823232323232e-08,1433600.0,756774358.7504287,1376256000.0,"The rapid development of large language models has revolutionized code intelligence in software development. However, the predominance of closed-source models has restricted extensive research and development. To address this, we introduce the DeepSeek-Coder series, a range of open-source code models with sizes from 1.3B to 33B, trained from scratch on 2 trillion tokens. These models are pre-trained on a high-quality project-level code corpus and employ a fill-in-the-blank task with a 16K window to enhance code generation and infilling. Our extensive evaluations demonstrate that DeepSeek-Coder not only achieves state-of-the-art performance among open-source code models across multiple benchmarks but also surpasses existing closed-source models like Codex and GPT-3.5. Furthermore, DeepSeek-Coder models are under a permissive license that allows for both research and unrestricted commercial use.",https://arxiv.org/abs/2401.14196,DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence
DeepSeek Coder 33B,Language,China,DeepSeek,2024-01-25,Industry,Code generation,Likely,2048.0,Open weights (restricted use),Not-defined,Unreleased,33000000000.0,2000000000000.0,960.0,3.96e+23,1877.1621580193864,38473.85279999992,700.0,1532450000000000.0,1979000000000000.0,80000000000.0,NVIDIA H800 SXM5,Training cost,1,36864000,788306.6236983632,78794450.53439984,3.869823232323232e-08,1433600.0,756774358.7504287,1376256000.0,"The rapid development of large language models has revolutionized code intelligence in software development. However, the predominance of closed-source models has restricted extensive research and development. To address this, we introduce the DeepSeek-Coder series, a range of open-source code models with sizes from 1.3B to 33B, trained from scratch on 2 trillion tokens. These models are pre-trained on a high-quality project-level code corpus and employ a fill-in-the-blank task with a 16K window to enhance code generation and infilling. Our extensive evaluations demonstrate that DeepSeek-Coder not only achieves state-of-the-art performance among open-source code models across multiple benchmarks but also surpasses existing closed-source models like Codex and GPT-3.5. Furthermore, DeepSeek-Coder models are under a permissive license that allows for both research and unrestricted commercial use.",https://arxiv.org/abs/2401.14196,DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence
DeepSeek Coder 33B,Language,China,Peking University,2024-01-25,Academia,Code generation,Likely,2048.0,Open weights (restricted use),Not-defined,Unreleased,33000000000.0,2000000000000.0,960.0,3.96e+23,1877.1621580193864,38473.85279999992,700.0,1532450000000000.0,1979000000000000.0,80000000000.0,NVIDIA H800 SXM5,Training cost,1,36864000,788306.6236983632,78794450.53439984,3.869823232323232e-08,1433600.0,756774358.7504287,1376256000.0,"The rapid development of large language models has revolutionized code intelligence in software development. However, the predominance of closed-source models has restricted extensive research and development. To address this, we introduce the DeepSeek-Coder series, a range of open-source code models with sizes from 1.3B to 33B, trained from scratch on 2 trillion tokens. These models are pre-trained on a high-quality project-level code corpus and employ a fill-in-the-blank task with a 16K window to enhance code generation and infilling. Our extensive evaluations demonstrate that DeepSeek-Coder not only achieves state-of-the-art performance among open-source code models across multiple benchmarks but also surpasses existing closed-source models like Codex and GPT-3.5. Furthermore, DeepSeek-Coder models are under a permissive license that allows for both research and unrestricted commercial use.",https://arxiv.org/abs/2401.14196,DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence
DeepSeek Coder 33B,Language,China,Peking University,2024-01-25,Industry,Code generation,Likely,2048.0,Open weights (restricted use),Not-defined,Unreleased,33000000000.0,2000000000000.0,960.0,3.96e+23,1877.1621580193864,38473.85279999992,700.0,1532450000000000.0,1979000000000000.0,80000000000.0,NVIDIA H800 SXM5,Training cost,1,36864000,788306.6236983632,78794450.53439984,3.869823232323232e-08,1433600.0,756774358.7504287,1376256000.0,"The rapid development of large language models has revolutionized code intelligence in software development. However, the predominance of closed-source models has restricted extensive research and development. To address this, we introduce the DeepSeek-Coder series, a range of open-source code models with sizes from 1.3B to 33B, trained from scratch on 2 trillion tokens. These models are pre-trained on a high-quality project-level code corpus and employ a fill-in-the-blank task with a 16K window to enhance code generation and infilling. Our extensive evaluations demonstrate that DeepSeek-Coder not only achieves state-of-the-art performance among open-source code models across multiple benchmarks but also surpasses existing closed-source models like Codex and GPT-3.5. Furthermore, DeepSeek-Coder models are under a permissive license that allows for both research and unrestricted commercial use.",https://arxiv.org/abs/2401.14196,DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence
DeepSeek LLM 67B,Language,China,DeepSeek,2024-01-05,Industry,Chat,Confident,512.0,Open weights (restricted use),Unspecified unreleased,Unreleased,67000000000.0,2000000000000.0,500.0,8.04e+23,1877.1621580193864,38473.85279999992,700.0,1532450000000000.0,1979000000000000.0,80000000000.0,NVIDIA H800 SXM5,Training cost,1,36864000,788306.6236983632,19698612.63359996,1.9060323383084577e-08,358400.0,394153311.8491816,179200000.0,"The rapid development of open-source large language models (LLMs) has been truly remarkable. However, the scaling law described in previous literature presents varying conclusions, which casts a dark cloud over scaling LLMs. We delve into the study of scaling laws and present our distinctive findings that facilitate scaling of large scale models in two commonly used open-source configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek LLM, a project dedicated to advancing open-source language models with a long-term perspective. To support the pre-training phase, we have developed a dataset that currently consists of 2 trillion tokens and is continuously expanding. We further conduct supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) on DeepSeek LLM Base models, resulting in the creation of DeepSeek Chat models. Our evaluation results demonstrate that DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in the domains of code, mathematics, and reasoning. Furthermore, open-ended evaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance compared to GPT-3.5.","https://arxiv.org/abs/2401.02954, https://github.com/deepseek-ai/DeepSeek-LLM",DeepSeek LLM: Scaling Open-Source Language Models with Longtermism
DeepSeek LLM 67B,Language,China,DeepSeek,2024-01-05,Industry,Language modeling/generation,Confident,512.0,Open weights (restricted use),Unspecified unreleased,Unreleased,67000000000.0,2000000000000.0,500.0,8.04e+23,1877.1621580193864,38473.85279999992,700.0,1532450000000000.0,1979000000000000.0,80000000000.0,NVIDIA H800 SXM5,Training cost,1,36864000,788306.6236983632,19698612.63359996,1.9060323383084577e-08,358400.0,394153311.8491816,179200000.0,"The rapid development of open-source large language models (LLMs) has been truly remarkable. However, the scaling law described in previous literature presents varying conclusions, which casts a dark cloud over scaling LLMs. We delve into the study of scaling laws and present our distinctive findings that facilitate scaling of large scale models in two commonly used open-source configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek LLM, a project dedicated to advancing open-source language models with a long-term perspective. To support the pre-training phase, we have developed a dataset that currently consists of 2 trillion tokens and is continuously expanding. We further conduct supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) on DeepSeek LLM Base models, resulting in the creation of DeepSeek Chat models. Our evaluation results demonstrate that DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in the domains of code, mathematics, and reasoning. Furthermore, open-ended evaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance compared to GPT-3.5.","https://arxiv.org/abs/2401.02954, https://github.com/deepseek-ai/DeepSeek-LLM",DeepSeek LLM: Scaling Open-Source Language Models with Longtermism
DeepSeek LLM 67B,Language,China,DeepSeek,2024-01-05,Industry,Question answering,Confident,512.0,Open weights (restricted use),Unspecified unreleased,Unreleased,67000000000.0,2000000000000.0,500.0,8.04e+23,1877.1621580193864,38473.85279999992,700.0,1532450000000000.0,1979000000000000.0,80000000000.0,NVIDIA H800 SXM5,Training cost,1,36864000,788306.6236983632,19698612.63359996,1.9060323383084577e-08,358400.0,394153311.8491816,179200000.0,"The rapid development of open-source large language models (LLMs) has been truly remarkable. However, the scaling law described in previous literature presents varying conclusions, which casts a dark cloud over scaling LLMs. We delve into the study of scaling laws and present our distinctive findings that facilitate scaling of large scale models in two commonly used open-source configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek LLM, a project dedicated to advancing open-source language models with a long-term perspective. To support the pre-training phase, we have developed a dataset that currently consists of 2 trillion tokens and is continuously expanding. We further conduct supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) on DeepSeek LLM Base models, resulting in the creation of DeepSeek Chat models. Our evaluation results demonstrate that DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in the domains of code, mathematics, and reasoning. Furthermore, open-ended evaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance compared to GPT-3.5.","https://arxiv.org/abs/2401.02954, https://github.com/deepseek-ai/DeepSeek-LLM",DeepSeek LLM: Scaling Open-Source Language Models with Longtermism
DeepSeek-V2 (MoE-236B),Language,China,DeepSeek,2024-05-07,Industry,Chat,Confident,2048.0,Open weights (restricted use),Unspecified unreleased,Unreleased,236000000000.0,8100000000000.0,1282.4326530612243,1.02e+24,1877.1621580193864,38473.85279999992,700.0,1532450000000000.0,1979000000000000.0,80000000000.0,NVIDIA H800 SXM5,Training cost,1,36864000,3155569.9410964646,78794450.53439984,1.5024019607843137e-08,1433600.0,4046805931.4805903,1838495451.4285712,"We present DeepSeek-V2, a strong Mixture-of-Experts (MoE) language model characterized by economical training and efficient inference. It comprises 236B total parameters, of which 21B are activated for each token, and supports a context length of 128K tokens. DeepSeek-V2 adopts innovative architectures including Multi-head Latent Attention (MLA) and DeepSeekMoE. MLA guarantees efficient inference through significantly compressing the Key-Value (KV) cache into a latent vector, while DeepSeekMoE enables training strong models at an economical cost through sparse computation. Compared with DeepSeek 67B, DeepSeek-V2 achieves significantly stronger performance, and meanwhile saves 42.5% of training costs, reduces the KV cache by 93.3%, and boosts the maximum generation throughput to 5.76 times. We pretrain DeepSeek-V2 on a high-quality and multi-source corpus consisting of 8.1T tokens, and further perform Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to fully unlock its potential. Evaluation results show that, even with only 21B activated parameters, DeepSeek-V2 and its chat versions still achieve top-tier performance among open-source models.","https://arxiv.org/abs/2405.04434 
https://github.com/deepseek-ai/DeepSeek-V2 ","DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model"
DeepSeek-V2 (MoE-236B),Language,China,DeepSeek,2024-05-07,Industry,Code generation,Confident,2048.0,Open weights (restricted use),Unspecified unreleased,Unreleased,236000000000.0,8100000000000.0,1282.4326530612243,1.02e+24,1877.1621580193864,38473.85279999992,700.0,1532450000000000.0,1979000000000000.0,80000000000.0,NVIDIA H800 SXM5,Training cost,1,36864000,3155569.9410964646,78794450.53439984,1.5024019607843137e-08,1433600.0,4046805931.4805903,1838495451.4285712,"We present DeepSeek-V2, a strong Mixture-of-Experts (MoE) language model characterized by economical training and efficient inference. It comprises 236B total parameters, of which 21B are activated for each token, and supports a context length of 128K tokens. DeepSeek-V2 adopts innovative architectures including Multi-head Latent Attention (MLA) and DeepSeekMoE. MLA guarantees efficient inference through significantly compressing the Key-Value (KV) cache into a latent vector, while DeepSeekMoE enables training strong models at an economical cost through sparse computation. Compared with DeepSeek 67B, DeepSeek-V2 achieves significantly stronger performance, and meanwhile saves 42.5% of training costs, reduces the KV cache by 93.3%, and boosts the maximum generation throughput to 5.76 times. We pretrain DeepSeek-V2 on a high-quality and multi-source corpus consisting of 8.1T tokens, and further perform Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to fully unlock its potential. Evaluation results show that, even with only 21B activated parameters, DeepSeek-V2 and its chat versions still achieve top-tier performance among open-source models.","https://arxiv.org/abs/2405.04434 
https://github.com/deepseek-ai/DeepSeek-V2 ","DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model"
DeepSeek-V2 (MoE-236B),Language,China,DeepSeek,2024-05-07,Industry,Language modeling/generation,Confident,2048.0,Open weights (restricted use),Unspecified unreleased,Unreleased,236000000000.0,8100000000000.0,1282.4326530612243,1.02e+24,1877.1621580193864,38473.85279999992,700.0,1532450000000000.0,1979000000000000.0,80000000000.0,NVIDIA H800 SXM5,Training cost,1,36864000,3155569.9410964646,78794450.53439984,1.5024019607843137e-08,1433600.0,4046805931.4805903,1838495451.4285712,"We present DeepSeek-V2, a strong Mixture-of-Experts (MoE) language model characterized by economical training and efficient inference. It comprises 236B total parameters, of which 21B are activated for each token, and supports a context length of 128K tokens. DeepSeek-V2 adopts innovative architectures including Multi-head Latent Attention (MLA) and DeepSeekMoE. MLA guarantees efficient inference through significantly compressing the Key-Value (KV) cache into a latent vector, while DeepSeekMoE enables training strong models at an economical cost through sparse computation. Compared with DeepSeek 67B, DeepSeek-V2 achieves significantly stronger performance, and meanwhile saves 42.5% of training costs, reduces the KV cache by 93.3%, and boosts the maximum generation throughput to 5.76 times. We pretrain DeepSeek-V2 on a high-quality and multi-source corpus consisting of 8.1T tokens, and further perform Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to fully unlock its potential. Evaluation results show that, even with only 21B activated parameters, DeepSeek-V2 and its chat versions still achieve top-tier performance among open-source models.","https://arxiv.org/abs/2405.04434 
https://github.com/deepseek-ai/DeepSeek-V2 ","DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model"
DeepSeek-V2.5,Language,China,DeepSeek,2024-09-06,Industry,Chat,Confident,2048.0,Open weights (restricted use),"GitHub,Common Crawl",Unreleased,236000000000.0,14800000000000.0,1282.4326530612243,1.7892e+24,1877.1621580193864,38473.85279999992,700.0,1532450000000000.0,1979000000000000.0,80000000000.0,NVIDIA H800 SXM5,Training cost,1,36864000,3155569.9410964646,78794450.53439984,8.56500111781802e-09,1433600.0,4046805931.4805903,1838495451.4285712,,https://huggingface.co/deepseek-ai/DeepSeek-V2.5,DeepSeek-V2.5
DeepSeek-V2.5,Language,China,DeepSeek,2024-09-06,Industry,Code generation,Confident,2048.0,Open weights (restricted use),"GitHub,Common Crawl",Unreleased,236000000000.0,14800000000000.0,1282.4326530612243,1.7892e+24,1877.1621580193864,38473.85279999992,700.0,1532450000000000.0,1979000000000000.0,80000000000.0,NVIDIA H800 SXM5,Training cost,1,36864000,3155569.9410964646,78794450.53439984,8.56500111781802e-09,1433600.0,4046805931.4805903,1838495451.4285712,,https://huggingface.co/deepseek-ai/DeepSeek-V2.5,DeepSeek-V2.5
DeepSeek-V2.5,Language,China,DeepSeek,2024-09-06,Industry,Language modeling/generation,Confident,2048.0,Open weights (restricted use),"GitHub,Common Crawl",Unreleased,236000000000.0,14800000000000.0,1282.4326530612243,1.7892e+24,1877.1621580193864,38473.85279999992,700.0,1532450000000000.0,1979000000000000.0,80000000000.0,NVIDIA H800 SXM5,Training cost,1,36864000,3155569.9410964646,78794450.53439984,8.56500111781802e-09,1433600.0,4046805931.4805903,1838495451.4285712,,https://huggingface.co/deepseek-ai/DeepSeek-V2.5,DeepSeek-V2.5
DeepSeek-V3,Language,China,DeepSeek,2024-12-24,Industry,Code generation,Confident,2048.0,Open weights (restricted use),Not-defined,Not-defined,671000000000.0,14800000000000.0,1282.4326530612243,3.4078e+24,1877.1621580193864,38473.85279999992,700.0,1532450000000000.0,1979000000000000.0,80000000000.0,NVIDIA H800 SXM5,Training cost,1,36864000,3155569.9410964646,78794450.53439984,0.2244,1433600.0,4046805931.4805903,1838495451.4285712,"We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. In addition, its training process is remarkably stable. Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks.",https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf,DeepSeek-V3 Technical Report
DeepSeek-V3,Language,China,DeepSeek,2024-12-24,Industry,Language modeling/generation,Confident,2048.0,Open weights (restricted use),Not-defined,Not-defined,671000000000.0,14800000000000.0,1282.4326530612243,3.4078e+24,1877.1621580193864,38473.85279999992,700.0,1532450000000000.0,1979000000000000.0,80000000000.0,NVIDIA H800 SXM5,Training cost,1,36864000,3155569.9410964646,78794450.53439984,0.2244,1433600.0,4046805931.4805903,1838495451.4285712,"We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. In addition, its training process is remarkably stable. Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks.",https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf,DeepSeek-V3 Technical Report
DeepSeek-V3,Language,China,DeepSeek,2024-12-24,Industry,Quantitative reasoning,Confident,2048.0,Open weights (restricted use),Not-defined,Not-defined,671000000000.0,14800000000000.0,1282.4326530612243,3.4078e+24,1877.1621580193864,38473.85279999992,700.0,1532450000000000.0,1979000000000000.0,80000000000.0,NVIDIA H800 SXM5,Training cost,1,36864000,3155569.9410964646,78794450.53439984,0.2244,1433600.0,4046805931.4805903,1838495451.4285712,"We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. In addition, its training process is remarkably stable. Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks.",https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf,DeepSeek-V3 Technical Report
DeepSeek-V3,Language,China,DeepSeek,2024-12-24,Industry,Question answering,Confident,2048.0,Open weights (restricted use),Not-defined,Not-defined,671000000000.0,14800000000000.0,1282.4326530612243,3.4078e+24,1877.1621580193864,38473.85279999992,700.0,1532450000000000.0,1979000000000000.0,80000000000.0,NVIDIA H800 SXM5,Training cost,1,36864000,3155569.9410964646,78794450.53439984,0.2244,1433600.0,4046805931.4805903,1838495451.4285712,"We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. In addition, its training process is remarkably stable. Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks.",https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf,DeepSeek-V3 Technical Report
EGRU (PTB),Language,Germany,Ruhr University Bochum,2022-06-13,Academia,Language modeling,Confident,1.0,Unreleased,Penn TreeBank,Open source,55000000.0,589652798174979.1,1282.4326530612243,3.1183314335135225e+25,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,None,4,6472188,91700208.69015627,10105.69,1.844480012029824e-10,400.0,117599341916.78503,512973.0612244897,"Recurrent neural networks (RNNs) are well suited for solving sequence tasks in resource-constrained systems due to their expressivity and low computational requirements. However, there is still a need to bridge the gap between what RNNs are capable of in terms of efficiency and performance and real-world application requirements. The memory and computational requirements arising from propagating the activations of all the neurons at every time step to every connected neuron, together with the sequential dependence of activations, contribute to the inefficiency of training and using RNNs. We propose a solution inspired by biological neuron dynamics that makes the communication between RNN units sparse and discrete. This makes the backward pass with backpropagation through time (BPTT) computationally sparse and efficient as well. We base our model on the gated recurrent unit (GRU), extending it with units that emit discrete events for communication triggered by a threshold so that no information is communicated to other units in the absence of events. We show theoretically that the communication between units, and hence the computation required for both the forward and backward passes, scales with the number of events in the network. Our model achieves efficiency without compromising task performance, demonstrating competitive performance compared to state-of-the-art recurrent network models in real-world tasks, including language modeling. The dynamic activity sparsity mechanism also makes our model well suited for novel energy-efficient neuromorphic hardware. Code is available at this https URL.",https://arxiv.org/abs/2206.06178v3,Efficient recurrent architectures through activity sparsity and sparse back-propagation through time
EGRU (PTB),Language,Germany,Technische Universität Dresden,2022-06-13,Academia,Language modeling,Confident,1.0,Unreleased,Penn TreeBank,Open source,55000000.0,589652798174979.1,1282.4326530612243,3.1183314335135225e+25,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,None,4,6472188,91700208.69015627,10105.69,1.844480012029824e-10,400.0,117599341916.78503,512973.0612244897,"Recurrent neural networks (RNNs) are well suited for solving sequence tasks in resource-constrained systems due to their expressivity and low computational requirements. However, there is still a need to bridge the gap between what RNNs are capable of in terms of efficiency and performance and real-world application requirements. The memory and computational requirements arising from propagating the activations of all the neurons at every time step to every connected neuron, together with the sequential dependence of activations, contribute to the inefficiency of training and using RNNs. We propose a solution inspired by biological neuron dynamics that makes the communication between RNN units sparse and discrete. This makes the backward pass with backpropagation through time (BPTT) computationally sparse and efficient as well. We base our model on the gated recurrent unit (GRU), extending it with units that emit discrete events for communication triggered by a threshold so that no information is communicated to other units in the absence of events. We show theoretically that the communication between units, and hence the computation required for both the forward and backward passes, scales with the number of events in the network. Our model achieves efficiency without compromising task performance, demonstrating competitive performance compared to state-of-the-art recurrent network models in real-world tasks, including language modeling. The dynamic activity sparsity mechanism also makes our model well suited for novel energy-efficient neuromorphic hardware. Code is available at this https URL.",https://arxiv.org/abs/2206.06178v3,Efficient recurrent architectures through activity sparsity and sparse back-propagation through time
EGRU (PTB),Language,Germany,University of London,2022-06-13,Academia,Language modeling,Confident,1.0,Unreleased,Penn TreeBank,Open source,55000000.0,589652798174979.1,1282.4326530612243,3.1183314335135225e+25,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,None,4,6472188,91700208.69015627,10105.69,1.844480012029824e-10,400.0,117599341916.78503,512973.0612244897,"Recurrent neural networks (RNNs) are well suited for solving sequence tasks in resource-constrained systems due to their expressivity and low computational requirements. However, there is still a need to bridge the gap between what RNNs are capable of in terms of efficiency and performance and real-world application requirements. The memory and computational requirements arising from propagating the activations of all the neurons at every time step to every connected neuron, together with the sequential dependence of activations, contribute to the inefficiency of training and using RNNs. We propose a solution inspired by biological neuron dynamics that makes the communication between RNN units sparse and discrete. This makes the backward pass with backpropagation through time (BPTT) computationally sparse and efficient as well. We base our model on the gated recurrent unit (GRU), extending it with units that emit discrete events for communication triggered by a threshold so that no information is communicated to other units in the absence of events. We show theoretically that the communication between units, and hence the computation required for both the forward and backward passes, scales with the number of events in the network. Our model achieves efficiency without compromising task performance, demonstrating competitive performance compared to state-of-the-art recurrent network models in real-world tasks, including language modeling. The dynamic activity sparsity mechanism also makes our model well suited for novel energy-efficient neuromorphic hardware. Code is available at this https URL.",https://arxiv.org/abs/2206.06178v3,Efficient recurrent architectures through activity sparsity and sparse back-propagation through time
EGRU (PTB),Language,UK,Ruhr University Bochum,2022-06-13,Academia,Language modeling,Confident,1.0,Unreleased,Penn TreeBank,Open source,55000000.0,589652798174979.1,1282.4326530612243,3.1183314335135225e+25,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,None,4,6472188,91700208.69015627,10105.69,1.844480012029824e-10,400.0,117599341916.78503,512973.0612244897,"Recurrent neural networks (RNNs) are well suited for solving sequence tasks in resource-constrained systems due to their expressivity and low computational requirements. However, there is still a need to bridge the gap between what RNNs are capable of in terms of efficiency and performance and real-world application requirements. The memory and computational requirements arising from propagating the activations of all the neurons at every time step to every connected neuron, together with the sequential dependence of activations, contribute to the inefficiency of training and using RNNs. We propose a solution inspired by biological neuron dynamics that makes the communication between RNN units sparse and discrete. This makes the backward pass with backpropagation through time (BPTT) computationally sparse and efficient as well. We base our model on the gated recurrent unit (GRU), extending it with units that emit discrete events for communication triggered by a threshold so that no information is communicated to other units in the absence of events. We show theoretically that the communication between units, and hence the computation required for both the forward and backward passes, scales with the number of events in the network. Our model achieves efficiency without compromising task performance, demonstrating competitive performance compared to state-of-the-art recurrent network models in real-world tasks, including language modeling. The dynamic activity sparsity mechanism also makes our model well suited for novel energy-efficient neuromorphic hardware. Code is available at this https URL.",https://arxiv.org/abs/2206.06178v3,Efficient recurrent architectures through activity sparsity and sparse back-propagation through time
EGRU (PTB),Language,UK,Technische Universität Dresden,2022-06-13,Academia,Language modeling,Confident,1.0,Unreleased,Penn TreeBank,Open source,55000000.0,589652798174979.1,1282.4326530612243,3.1183314335135225e+25,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,None,4,6472188,91700208.69015627,10105.69,1.844480012029824e-10,400.0,117599341916.78503,512973.0612244897,"Recurrent neural networks (RNNs) are well suited for solving sequence tasks in resource-constrained systems due to their expressivity and low computational requirements. However, there is still a need to bridge the gap between what RNNs are capable of in terms of efficiency and performance and real-world application requirements. The memory and computational requirements arising from propagating the activations of all the neurons at every time step to every connected neuron, together with the sequential dependence of activations, contribute to the inefficiency of training and using RNNs. We propose a solution inspired by biological neuron dynamics that makes the communication between RNN units sparse and discrete. This makes the backward pass with backpropagation through time (BPTT) computationally sparse and efficient as well. We base our model on the gated recurrent unit (GRU), extending it with units that emit discrete events for communication triggered by a threshold so that no information is communicated to other units in the absence of events. We show theoretically that the communication between units, and hence the computation required for both the forward and backward passes, scales with the number of events in the network. Our model achieves efficiency without compromising task performance, demonstrating competitive performance compared to state-of-the-art recurrent network models in real-world tasks, including language modeling. The dynamic activity sparsity mechanism also makes our model well suited for novel energy-efficient neuromorphic hardware. Code is available at this https URL.",https://arxiv.org/abs/2206.06178v3,Efficient recurrent architectures through activity sparsity and sparse back-propagation through time
EGRU (PTB),Language,UK,University of London,2022-06-13,Academia,Language modeling,Confident,1.0,Unreleased,Penn TreeBank,Open source,55000000.0,589652798174979.1,1282.4326530612243,3.1183314335135225e+25,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,None,4,6472188,91700208.69015627,10105.69,1.844480012029824e-10,400.0,117599341916.78503,512973.0612244897,"Recurrent neural networks (RNNs) are well suited for solving sequence tasks in resource-constrained systems due to their expressivity and low computational requirements. However, there is still a need to bridge the gap between what RNNs are capable of in terms of efficiency and performance and real-world application requirements. The memory and computational requirements arising from propagating the activations of all the neurons at every time step to every connected neuron, together with the sequential dependence of activations, contribute to the inefficiency of training and using RNNs. We propose a solution inspired by biological neuron dynamics that makes the communication between RNN units sparse and discrete. This makes the backward pass with backpropagation through time (BPTT) computationally sparse and efficient as well. We base our model on the gated recurrent unit (GRU), extending it with units that emit discrete events for communication triggered by a threshold so that no information is communicated to other units in the absence of events. We show theoretically that the communication between units, and hence the computation required for both the forward and backward passes, scales with the number of events in the network. Our model achieves efficiency without compromising task performance, demonstrating competitive performance compared to state-of-the-art recurrent network models in real-world tasks, including language modeling. The dynamic activity sparsity mechanism also makes our model well suited for novel energy-efficient neuromorphic hardware. Code is available at this https URL.",https://arxiv.org/abs/2206.06178v3,Efficient recurrent architectures through activity sparsity and sparse back-propagation through time
ERNIE 3.0 Titan,Language,China,Baidu,2021-12-23,Academia,Language modeling,Confident,1920.0,Hosted access (no API),ERNIE 3.0 Corpus,Unreleased,260000000000.0,668000000000.0,1282.4326530612243,1.0421e+24,1877.1621580193864,10230.01,280.0,217917000000000.0,374371000000000.0,43123092676.96655,"NVIDIA Tesla V100 DGXS 32 GB,Huawei Ascend 910",SOTA improvement,4,1048576,2106.480893731142,19641619.2,2.0911332885519624e-09,537600.0,2701419.881170407,689435794.2857141,"Pre-trained language models have achieved state-of-the-art results in various Natural Language Processing (NLP) tasks. GPT-3 has shown that scaling up pre-trained language models can further exploit their enormous potential. A unified framework named ERNIE 3.0 was recently proposed for pre-training large-scale knowledge enhanced models and trained a model with 10 billion parameters. ERNIE 3.0 outperformed the state-of-the-art models on various NLP tasks. In order to explore the performance of scaling up ERNIE 3.0, we train a hundred-billion-parameter model called ERNIE 3.0 Titan with up to 260 billion parameters on the PaddlePaddle platform. Furthermore, we design a self-supervised adversarial loss and a controllable language modeling loss to make ERNIE 3.0 Titan generate credible and controllable texts. To reduce the computation overhead and carbon emission, we propose an online distillation framework for ERNIE 3.0 Titan, where the teacher model will teach students and train itself simultaneously. ERNIE 3.0 Titan is the largest Chinese dense pre-trained model so far. Empirical results show that the ERNIE 3.0 Titan outperforms the state-of-the-art models on 68 NLP datasets.",https://arxiv.org/abs/2112.12731,ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation
ERNIE 3.0 Titan,Language,China,Baidu,2021-12-23,Academia,Language modeling/generation,Confident,1920.0,Hosted access (no API),ERNIE 3.0 Corpus,Unreleased,260000000000.0,668000000000.0,1282.4326530612243,1.0421e+24,1877.1621580193864,10230.01,280.0,217917000000000.0,374371000000000.0,43123092676.96655,"NVIDIA Tesla V100 DGXS 32 GB,Huawei Ascend 910",SOTA improvement,4,1048576,2106.480893731142,19641619.2,2.0911332885519624e-09,537600.0,2701419.881170407,689435794.2857141,"Pre-trained language models have achieved state-of-the-art results in various Natural Language Processing (NLP) tasks. GPT-3 has shown that scaling up pre-trained language models can further exploit their enormous potential. A unified framework named ERNIE 3.0 was recently proposed for pre-training large-scale knowledge enhanced models and trained a model with 10 billion parameters. ERNIE 3.0 outperformed the state-of-the-art models on various NLP tasks. In order to explore the performance of scaling up ERNIE 3.0, we train a hundred-billion-parameter model called ERNIE 3.0 Titan with up to 260 billion parameters on the PaddlePaddle platform. Furthermore, we design a self-supervised adversarial loss and a controllable language modeling loss to make ERNIE 3.0 Titan generate credible and controllable texts. To reduce the computation overhead and carbon emission, we propose an online distillation framework for ERNIE 3.0 Titan, where the teacher model will teach students and train itself simultaneously. ERNIE 3.0 Titan is the largest Chinese dense pre-trained model so far. Empirical results show that the ERNIE 3.0 Titan outperforms the state-of-the-art models on 68 NLP datasets.",https://arxiv.org/abs/2112.12731,ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation
ERNIE 3.0 Titan,Language,China,Baidu,2021-12-23,Industry,Language modeling,Confident,1920.0,Hosted access (no API),ERNIE 3.0 Corpus,Unreleased,260000000000.0,668000000000.0,1282.4326530612243,1.0421e+24,1877.1621580193864,10230.01,280.0,217917000000000.0,374371000000000.0,43123092676.96655,"NVIDIA Tesla V100 DGXS 32 GB,Huawei Ascend 910",SOTA improvement,4,1048576,2106.480893731142,19641619.2,2.0911332885519624e-09,537600.0,2701419.881170407,689435794.2857141,"Pre-trained language models have achieved state-of-the-art results in various Natural Language Processing (NLP) tasks. GPT-3 has shown that scaling up pre-trained language models can further exploit their enormous potential. A unified framework named ERNIE 3.0 was recently proposed for pre-training large-scale knowledge enhanced models and trained a model with 10 billion parameters. ERNIE 3.0 outperformed the state-of-the-art models on various NLP tasks. In order to explore the performance of scaling up ERNIE 3.0, we train a hundred-billion-parameter model called ERNIE 3.0 Titan with up to 260 billion parameters on the PaddlePaddle platform. Furthermore, we design a self-supervised adversarial loss and a controllable language modeling loss to make ERNIE 3.0 Titan generate credible and controllable texts. To reduce the computation overhead and carbon emission, we propose an online distillation framework for ERNIE 3.0 Titan, where the teacher model will teach students and train itself simultaneously. ERNIE 3.0 Titan is the largest Chinese dense pre-trained model so far. Empirical results show that the ERNIE 3.0 Titan outperforms the state-of-the-art models on 68 NLP datasets.",https://arxiv.org/abs/2112.12731,ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation
ERNIE 3.0 Titan,Language,China,Baidu,2021-12-23,Industry,Language modeling/generation,Confident,1920.0,Hosted access (no API),ERNIE 3.0 Corpus,Unreleased,260000000000.0,668000000000.0,1282.4326530612243,1.0421e+24,1877.1621580193864,10230.01,280.0,217917000000000.0,374371000000000.0,43123092676.96655,"NVIDIA Tesla V100 DGXS 32 GB,Huawei Ascend 910",SOTA improvement,4,1048576,2106.480893731142,19641619.2,2.0911332885519624e-09,537600.0,2701419.881170407,689435794.2857141,"Pre-trained language models have achieved state-of-the-art results in various Natural Language Processing (NLP) tasks. GPT-3 has shown that scaling up pre-trained language models can further exploit their enormous potential. A unified framework named ERNIE 3.0 was recently proposed for pre-training large-scale knowledge enhanced models and trained a model with 10 billion parameters. ERNIE 3.0 outperformed the state-of-the-art models on various NLP tasks. In order to explore the performance of scaling up ERNIE 3.0, we train a hundred-billion-parameter model called ERNIE 3.0 Titan with up to 260 billion parameters on the PaddlePaddle platform. Furthermore, we design a self-supervised adversarial loss and a controllable language modeling loss to make ERNIE 3.0 Titan generate credible and controllable texts. To reduce the computation overhead and carbon emission, we propose an online distillation framework for ERNIE 3.0 Titan, where the teacher model will teach students and train itself simultaneously. ERNIE 3.0 Titan is the largest Chinese dense pre-trained model so far. Empirical results show that the ERNIE 3.0 Titan outperforms the state-of-the-art models on 68 NLP datasets.",https://arxiv.org/abs/2112.12731,ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation
ERNIE 3.0 Titan,Language,China,Peng Cheng Laboratory,2021-12-23,Academia,Language modeling,Confident,1920.0,Hosted access (no API),ERNIE 3.0 Corpus,Unreleased,260000000000.0,668000000000.0,1282.4326530612243,1.0421e+24,1877.1621580193864,10230.01,280.0,217917000000000.0,374371000000000.0,43123092676.96655,"NVIDIA Tesla V100 DGXS 32 GB,Huawei Ascend 910",SOTA improvement,4,1048576,2106.480893731142,19641619.2,2.0911332885519624e-09,537600.0,2701419.881170407,689435794.2857141,"Pre-trained language models have achieved state-of-the-art results in various Natural Language Processing (NLP) tasks. GPT-3 has shown that scaling up pre-trained language models can further exploit their enormous potential. A unified framework named ERNIE 3.0 was recently proposed for pre-training large-scale knowledge enhanced models and trained a model with 10 billion parameters. ERNIE 3.0 outperformed the state-of-the-art models on various NLP tasks. In order to explore the performance of scaling up ERNIE 3.0, we train a hundred-billion-parameter model called ERNIE 3.0 Titan with up to 260 billion parameters on the PaddlePaddle platform. Furthermore, we design a self-supervised adversarial loss and a controllable language modeling loss to make ERNIE 3.0 Titan generate credible and controllable texts. To reduce the computation overhead and carbon emission, we propose an online distillation framework for ERNIE 3.0 Titan, where the teacher model will teach students and train itself simultaneously. ERNIE 3.0 Titan is the largest Chinese dense pre-trained model so far. Empirical results show that the ERNIE 3.0 Titan outperforms the state-of-the-art models on 68 NLP datasets.",https://arxiv.org/abs/2112.12731,ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation
ERNIE 3.0 Titan,Language,China,Peng Cheng Laboratory,2021-12-23,Academia,Language modeling/generation,Confident,1920.0,Hosted access (no API),ERNIE 3.0 Corpus,Unreleased,260000000000.0,668000000000.0,1282.4326530612243,1.0421e+24,1877.1621580193864,10230.01,280.0,217917000000000.0,374371000000000.0,43123092676.96655,"NVIDIA Tesla V100 DGXS 32 GB,Huawei Ascend 910",SOTA improvement,4,1048576,2106.480893731142,19641619.2,2.0911332885519624e-09,537600.0,2701419.881170407,689435794.2857141,"Pre-trained language models have achieved state-of-the-art results in various Natural Language Processing (NLP) tasks. GPT-3 has shown that scaling up pre-trained language models can further exploit their enormous potential. A unified framework named ERNIE 3.0 was recently proposed for pre-training large-scale knowledge enhanced models and trained a model with 10 billion parameters. ERNIE 3.0 outperformed the state-of-the-art models on various NLP tasks. In order to explore the performance of scaling up ERNIE 3.0, we train a hundred-billion-parameter model called ERNIE 3.0 Titan with up to 260 billion parameters on the PaddlePaddle platform. Furthermore, we design a self-supervised adversarial loss and a controllable language modeling loss to make ERNIE 3.0 Titan generate credible and controllable texts. To reduce the computation overhead and carbon emission, we propose an online distillation framework for ERNIE 3.0 Titan, where the teacher model will teach students and train itself simultaneously. ERNIE 3.0 Titan is the largest Chinese dense pre-trained model so far. Empirical results show that the ERNIE 3.0 Titan outperforms the state-of-the-art models on 68 NLP datasets.",https://arxiv.org/abs/2112.12731,ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation
ERNIE 3.0 Titan,Language,China,Peng Cheng Laboratory,2021-12-23,Industry,Language modeling,Confident,1920.0,Hosted access (no API),ERNIE 3.0 Corpus,Unreleased,260000000000.0,668000000000.0,1282.4326530612243,1.0421e+24,1877.1621580193864,10230.01,280.0,217917000000000.0,374371000000000.0,43123092676.96655,"NVIDIA Tesla V100 DGXS 32 GB,Huawei Ascend 910",SOTA improvement,4,1048576,2106.480893731142,19641619.2,2.0911332885519624e-09,537600.0,2701419.881170407,689435794.2857141,"Pre-trained language models have achieved state-of-the-art results in various Natural Language Processing (NLP) tasks. GPT-3 has shown that scaling up pre-trained language models can further exploit their enormous potential. A unified framework named ERNIE 3.0 was recently proposed for pre-training large-scale knowledge enhanced models and trained a model with 10 billion parameters. ERNIE 3.0 outperformed the state-of-the-art models on various NLP tasks. In order to explore the performance of scaling up ERNIE 3.0, we train a hundred-billion-parameter model called ERNIE 3.0 Titan with up to 260 billion parameters on the PaddlePaddle platform. Furthermore, we design a self-supervised adversarial loss and a controllable language modeling loss to make ERNIE 3.0 Titan generate credible and controllable texts. To reduce the computation overhead and carbon emission, we propose an online distillation framework for ERNIE 3.0 Titan, where the teacher model will teach students and train itself simultaneously. ERNIE 3.0 Titan is the largest Chinese dense pre-trained model so far. Empirical results show that the ERNIE 3.0 Titan outperforms the state-of-the-art models on 68 NLP datasets.",https://arxiv.org/abs/2112.12731,ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation
ERNIE 3.0 Titan,Language,China,Peng Cheng Laboratory,2021-12-23,Industry,Language modeling/generation,Confident,1920.0,Hosted access (no API),ERNIE 3.0 Corpus,Unreleased,260000000000.0,668000000000.0,1282.4326530612243,1.0421e+24,1877.1621580193864,10230.01,280.0,217917000000000.0,374371000000000.0,43123092676.96655,"NVIDIA Tesla V100 DGXS 32 GB,Huawei Ascend 910",SOTA improvement,4,1048576,2106.480893731142,19641619.2,2.0911332885519624e-09,537600.0,2701419.881170407,689435794.2857141,"Pre-trained language models have achieved state-of-the-art results in various Natural Language Processing (NLP) tasks. GPT-3 has shown that scaling up pre-trained language models can further exploit their enormous potential. A unified framework named ERNIE 3.0 was recently proposed for pre-training large-scale knowledge enhanced models and trained a model with 10 billion parameters. ERNIE 3.0 outperformed the state-of-the-art models on various NLP tasks. In order to explore the performance of scaling up ERNIE 3.0, we train a hundred-billion-parameter model called ERNIE 3.0 Titan with up to 260 billion parameters on the PaddlePaddle platform. Furthermore, we design a self-supervised adversarial loss and a controllable language modeling loss to make ERNIE 3.0 Titan generate credible and controllable texts. To reduce the computation overhead and carbon emission, we propose an online distillation framework for ERNIE 3.0 Titan, where the teacher model will teach students and train itself simultaneously. ERNIE 3.0 Titan is the largest Chinese dense pre-trained model so far. Empirical results show that the ERNIE 3.0 Titan outperforms the state-of-the-art models on 68 NLP datasets.",https://arxiv.org/abs/2112.12731,ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation
ERNIE 3.5,Language,China,Baidu,2023-06-27,Industry,Language modeling,Unknown,1920.0,Not-defined,Not-defined,Not-defined,260000000000.0,668000000000.0,1282.4326530612243,1.0421e+24,1877.1621580193864,10230.01,280.0,217917000000000.0,374371000000000.0,43123092676.96655,"NVIDIA Tesla V100 DGXS 32 GB,Huawei Ascend 910",SOTA improvement,4,1048576,788306.6236983632,19641619.2,2.0911332885519624e-09,537600.0,1010950154.8552281,689435794.2857141,,http://research.baidu.com/Blog/index-view?id=185,Introducing ERNIE 3.5: Baidu’s Knowledge-Enhanced Foundation Model Takes a Giant Leap Forward
ERNIE 3.5,Language,China,Baidu,2023-06-27,Industry,Language modeling/generation,Unknown,1920.0,Not-defined,Not-defined,Not-defined,260000000000.0,668000000000.0,1282.4326530612243,1.0421e+24,1877.1621580193864,10230.01,280.0,217917000000000.0,374371000000000.0,43123092676.96655,"NVIDIA Tesla V100 DGXS 32 GB,Huawei Ascend 910",SOTA improvement,4,1048576,788306.6236983632,19641619.2,2.0911332885519624e-09,537600.0,1010950154.8552281,689435794.2857141,,http://research.baidu.com/Blog/index-view?id=185,Introducing ERNIE 3.5: Baidu’s Knowledge-Enhanced Foundation Model Takes a Giant Leap Forward
ERNIE 4.0,Image generation,China,Baidu,2023-10-17,Industry,Chat,Unknown,1920.0,Not-defined,Not-defined,Not-defined,260000000000.0,668000000000.0,1282.4326530612243,1.0421e+24,1877.1621580193864,10230.01,280.0,217917000000000.0,374371000000000.0,43123092676.96655,"NVIDIA Tesla V100 DGXS 32 GB,Huawei Ascend 910",Significant use,4,1048576,788306.6236983632,19641619.2,2.0911332885519624e-09,537600.0,1010950154.8552281,689435794.2857141,"Baidu, Inc. (NASDAQ: BIDU and HKEX: 9888), a leading AI company with strong Internet foundation, today hosted its annual flagship technology conference Baidu World 2023 in Beijing, marking the conference's return to an offline format after four years. With the theme ""Prompt the World,"" this year's Baidu World conference saw Baidu launch ERNIE 4.0, Baidu's next-generation and most powerful foundation model offering drastically enhanced core AI capabilities. Baidu also showcased some of its most popular applications, solutions, and products re-built around the company's state-of-the-art generative AI.                                                                               

Robin Li, Co-founder, Chairman and CEO of Baidu, announced ERNIE 4.0 at Baidu World 2023
""ERNIE 4.0 has achieved a full upgrade with drastically improved performance in understanding, generation, reasoning, and memory,"" Robin Li, Co-founder, Chairman and CEO of Baidu, said at the event. ""These four core capabilities form the foundation of AI-native applications and have now unleashed unlimited opportunities for new innovations.""

",https://www.prnewswire.com/news-releases/baidu-launches-ernie-4-0-foundation-model-leading-a-new-wave-of-ai-native-applications-301958681.html,"Baidu Launches ERNIE 4.0 Foundation Model, Leading a New Wave of AI-Native Applications"
ERNIE 4.0,Image generation,China,Baidu,2023-10-17,Industry,Image generation,Unknown,1920.0,Not-defined,Not-defined,Not-defined,260000000000.0,668000000000.0,1282.4326530612243,1.0421e+24,1877.1621580193864,10230.01,280.0,217917000000000.0,374371000000000.0,43123092676.96655,"NVIDIA Tesla V100 DGXS 32 GB,Huawei Ascend 910",Significant use,4,1048576,788306.6236983632,19641619.2,2.0911332885519624e-09,537600.0,1010950154.8552281,689435794.2857141,"Baidu, Inc. (NASDAQ: BIDU and HKEX: 9888), a leading AI company with strong Internet foundation, today hosted its annual flagship technology conference Baidu World 2023 in Beijing, marking the conference's return to an offline format after four years. With the theme ""Prompt the World,"" this year's Baidu World conference saw Baidu launch ERNIE 4.0, Baidu's next-generation and most powerful foundation model offering drastically enhanced core AI capabilities. Baidu also showcased some of its most popular applications, solutions, and products re-built around the company's state-of-the-art generative AI.                                                                               

Robin Li, Co-founder, Chairman and CEO of Baidu, announced ERNIE 4.0 at Baidu World 2023
""ERNIE 4.0 has achieved a full upgrade with drastically improved performance in understanding, generation, reasoning, and memory,"" Robin Li, Co-founder, Chairman and CEO of Baidu, said at the event. ""These four core capabilities form the foundation of AI-native applications and have now unleashed unlimited opportunities for new innovations.""

",https://www.prnewswire.com/news-releases/baidu-launches-ernie-4-0-foundation-model-leading-a-new-wave-of-ai-native-applications-301958681.html,"Baidu Launches ERNIE 4.0 Foundation Model, Leading a New Wave of AI-Native Applications"
ERNIE 4.0,Image generation,China,Baidu,2023-10-17,Industry,Language modeling/generation,Unknown,1920.0,Not-defined,Not-defined,Not-defined,260000000000.0,668000000000.0,1282.4326530612243,1.0421e+24,1877.1621580193864,10230.01,280.0,217917000000000.0,374371000000000.0,43123092676.96655,"NVIDIA Tesla V100 DGXS 32 GB,Huawei Ascend 910",Significant use,4,1048576,788306.6236983632,19641619.2,2.0911332885519624e-09,537600.0,1010950154.8552281,689435794.2857141,"Baidu, Inc. (NASDAQ: BIDU and HKEX: 9888), a leading AI company with strong Internet foundation, today hosted its annual flagship technology conference Baidu World 2023 in Beijing, marking the conference's return to an offline format after four years. With the theme ""Prompt the World,"" this year's Baidu World conference saw Baidu launch ERNIE 4.0, Baidu's next-generation and most powerful foundation model offering drastically enhanced core AI capabilities. Baidu also showcased some of its most popular applications, solutions, and products re-built around the company's state-of-the-art generative AI.                                                                               

Robin Li, Co-founder, Chairman and CEO of Baidu, announced ERNIE 4.0 at Baidu World 2023
""ERNIE 4.0 has achieved a full upgrade with drastically improved performance in understanding, generation, reasoning, and memory,"" Robin Li, Co-founder, Chairman and CEO of Baidu, said at the event. ""These four core capabilities form the foundation of AI-native applications and have now unleashed unlimited opportunities for new innovations.""

",https://www.prnewswire.com/news-releases/baidu-launches-ernie-4-0-foundation-model-leading-a-new-wave-of-ai-native-applications-301958681.html,"Baidu Launches ERNIE 4.0 Foundation Model, Leading a New Wave of AI-Native Applications"
ERNIE 4.0,Image generation,China,Baidu,2023-10-17,Industry,Video generation,Unknown,1920.0,Not-defined,Not-defined,Not-defined,260000000000.0,668000000000.0,1282.4326530612243,1.0421e+24,1877.1621580193864,10230.01,280.0,217917000000000.0,374371000000000.0,43123092676.96655,"NVIDIA Tesla V100 DGXS 32 GB,Huawei Ascend 910",Significant use,4,1048576,788306.6236983632,19641619.2,2.0911332885519624e-09,537600.0,1010950154.8552281,689435794.2857141,"Baidu, Inc. (NASDAQ: BIDU and HKEX: 9888), a leading AI company with strong Internet foundation, today hosted its annual flagship technology conference Baidu World 2023 in Beijing, marking the conference's return to an offline format after four years. With the theme ""Prompt the World,"" this year's Baidu World conference saw Baidu launch ERNIE 4.0, Baidu's next-generation and most powerful foundation model offering drastically enhanced core AI capabilities. Baidu also showcased some of its most popular applications, solutions, and products re-built around the company's state-of-the-art generative AI.                                                                               

Robin Li, Co-founder, Chairman and CEO of Baidu, announced ERNIE 4.0 at Baidu World 2023
""ERNIE 4.0 has achieved a full upgrade with drastically improved performance in understanding, generation, reasoning, and memory,"" Robin Li, Co-founder, Chairman and CEO of Baidu, said at the event. ""These four core capabilities form the foundation of AI-native applications and have now unleashed unlimited opportunities for new innovations.""

",https://www.prnewswire.com/news-releases/baidu-launches-ernie-4-0-foundation-model-leading-a-new-wave-of-ai-native-applications-301958681.html,"Baidu Launches ERNIE 4.0 Foundation Model, Leading a New Wave of AI-Native Applications"
ERNIE 4.0,Language,China,Baidu,2023-10-17,Industry,Chat,Unknown,1920.0,Not-defined,Not-defined,Not-defined,260000000000.0,668000000000.0,1282.4326530612243,1.0421e+24,1877.1621580193864,10230.01,280.0,217917000000000.0,374371000000000.0,43123092676.96655,"NVIDIA Tesla V100 DGXS 32 GB,Huawei Ascend 910",Significant use,4,1048576,788306.6236983632,19641619.2,2.0911332885519624e-09,537600.0,1010950154.8552281,689435794.2857141,"Baidu, Inc. (NASDAQ: BIDU and HKEX: 9888), a leading AI company with strong Internet foundation, today hosted its annual flagship technology conference Baidu World 2023 in Beijing, marking the conference's return to an offline format after four years. With the theme ""Prompt the World,"" this year's Baidu World conference saw Baidu launch ERNIE 4.0, Baidu's next-generation and most powerful foundation model offering drastically enhanced core AI capabilities. Baidu also showcased some of its most popular applications, solutions, and products re-built around the company's state-of-the-art generative AI.                                                                               

Robin Li, Co-founder, Chairman and CEO of Baidu, announced ERNIE 4.0 at Baidu World 2023
""ERNIE 4.0 has achieved a full upgrade with drastically improved performance in understanding, generation, reasoning, and memory,"" Robin Li, Co-founder, Chairman and CEO of Baidu, said at the event. ""These four core capabilities form the foundation of AI-native applications and have now unleashed unlimited opportunities for new innovations.""

",https://www.prnewswire.com/news-releases/baidu-launches-ernie-4-0-foundation-model-leading-a-new-wave-of-ai-native-applications-301958681.html,"Baidu Launches ERNIE 4.0 Foundation Model, Leading a New Wave of AI-Native Applications"
ERNIE 4.0,Language,China,Baidu,2023-10-17,Industry,Image generation,Unknown,1920.0,Not-defined,Not-defined,Not-defined,260000000000.0,668000000000.0,1282.4326530612243,1.0421e+24,1877.1621580193864,10230.01,280.0,217917000000000.0,374371000000000.0,43123092676.96655,"NVIDIA Tesla V100 DGXS 32 GB,Huawei Ascend 910",Significant use,4,1048576,788306.6236983632,19641619.2,2.0911332885519624e-09,537600.0,1010950154.8552281,689435794.2857141,"Baidu, Inc. (NASDAQ: BIDU and HKEX: 9888), a leading AI company with strong Internet foundation, today hosted its annual flagship technology conference Baidu World 2023 in Beijing, marking the conference's return to an offline format after four years. With the theme ""Prompt the World,"" this year's Baidu World conference saw Baidu launch ERNIE 4.0, Baidu's next-generation and most powerful foundation model offering drastically enhanced core AI capabilities. Baidu also showcased some of its most popular applications, solutions, and products re-built around the company's state-of-the-art generative AI.                                                                               

Robin Li, Co-founder, Chairman and CEO of Baidu, announced ERNIE 4.0 at Baidu World 2023
""ERNIE 4.0 has achieved a full upgrade with drastically improved performance in understanding, generation, reasoning, and memory,"" Robin Li, Co-founder, Chairman and CEO of Baidu, said at the event. ""These four core capabilities form the foundation of AI-native applications and have now unleashed unlimited opportunities for new innovations.""

",https://www.prnewswire.com/news-releases/baidu-launches-ernie-4-0-foundation-model-leading-a-new-wave-of-ai-native-applications-301958681.html,"Baidu Launches ERNIE 4.0 Foundation Model, Leading a New Wave of AI-Native Applications"
ERNIE 4.0,Language,China,Baidu,2023-10-17,Industry,Language modeling/generation,Unknown,1920.0,Not-defined,Not-defined,Not-defined,260000000000.0,668000000000.0,1282.4326530612243,1.0421e+24,1877.1621580193864,10230.01,280.0,217917000000000.0,374371000000000.0,43123092676.96655,"NVIDIA Tesla V100 DGXS 32 GB,Huawei Ascend 910",Significant use,4,1048576,788306.6236983632,19641619.2,2.0911332885519624e-09,537600.0,1010950154.8552281,689435794.2857141,"Baidu, Inc. (NASDAQ: BIDU and HKEX: 9888), a leading AI company with strong Internet foundation, today hosted its annual flagship technology conference Baidu World 2023 in Beijing, marking the conference's return to an offline format after four years. With the theme ""Prompt the World,"" this year's Baidu World conference saw Baidu launch ERNIE 4.0, Baidu's next-generation and most powerful foundation model offering drastically enhanced core AI capabilities. Baidu also showcased some of its most popular applications, solutions, and products re-built around the company's state-of-the-art generative AI.                                                                               

Robin Li, Co-founder, Chairman and CEO of Baidu, announced ERNIE 4.0 at Baidu World 2023
""ERNIE 4.0 has achieved a full upgrade with drastically improved performance in understanding, generation, reasoning, and memory,"" Robin Li, Co-founder, Chairman and CEO of Baidu, said at the event. ""These four core capabilities form the foundation of AI-native applications and have now unleashed unlimited opportunities for new innovations.""

",https://www.prnewswire.com/news-releases/baidu-launches-ernie-4-0-foundation-model-leading-a-new-wave-of-ai-native-applications-301958681.html,"Baidu Launches ERNIE 4.0 Foundation Model, Leading a New Wave of AI-Native Applications"
ERNIE 4.0,Language,China,Baidu,2023-10-17,Industry,Video generation,Unknown,1920.0,Not-defined,Not-defined,Not-defined,260000000000.0,668000000000.0,1282.4326530612243,1.0421e+24,1877.1621580193864,10230.01,280.0,217917000000000.0,374371000000000.0,43123092676.96655,"NVIDIA Tesla V100 DGXS 32 GB,Huawei Ascend 910",Significant use,4,1048576,788306.6236983632,19641619.2,2.0911332885519624e-09,537600.0,1010950154.8552281,689435794.2857141,"Baidu, Inc. (NASDAQ: BIDU and HKEX: 9888), a leading AI company with strong Internet foundation, today hosted its annual flagship technology conference Baidu World 2023 in Beijing, marking the conference's return to an offline format after four years. With the theme ""Prompt the World,"" this year's Baidu World conference saw Baidu launch ERNIE 4.0, Baidu's next-generation and most powerful foundation model offering drastically enhanced core AI capabilities. Baidu also showcased some of its most popular applications, solutions, and products re-built around the company's state-of-the-art generative AI.                                                                               

Robin Li, Co-founder, Chairman and CEO of Baidu, announced ERNIE 4.0 at Baidu World 2023
""ERNIE 4.0 has achieved a full upgrade with drastically improved performance in understanding, generation, reasoning, and memory,"" Robin Li, Co-founder, Chairman and CEO of Baidu, said at the event. ""These four core capabilities form the foundation of AI-native applications and have now unleashed unlimited opportunities for new innovations.""

",https://www.prnewswire.com/news-releases/baidu-launches-ernie-4-0-foundation-model-leading-a-new-wave-of-ai-native-applications-301958681.html,"Baidu Launches ERNIE 4.0 Foundation Model, Leading a New Wave of AI-Native Applications"
ERNIE 4.0,Multimodal,China,Baidu,2023-10-17,Industry,Chat,Unknown,1920.0,Not-defined,Not-defined,Not-defined,260000000000.0,668000000000.0,1282.4326530612243,1.0421e+24,1877.1621580193864,10230.01,280.0,217917000000000.0,374371000000000.0,43123092676.96655,"NVIDIA Tesla V100 DGXS 32 GB,Huawei Ascend 910",Significant use,4,1048576,788306.6236983632,19641619.2,2.0911332885519624e-09,537600.0,1010950154.8552281,689435794.2857141,"Baidu, Inc. (NASDAQ: BIDU and HKEX: 9888), a leading AI company with strong Internet foundation, today hosted its annual flagship technology conference Baidu World 2023 in Beijing, marking the conference's return to an offline format after four years. With the theme ""Prompt the World,"" this year's Baidu World conference saw Baidu launch ERNIE 4.0, Baidu's next-generation and most powerful foundation model offering drastically enhanced core AI capabilities. Baidu also showcased some of its most popular applications, solutions, and products re-built around the company's state-of-the-art generative AI.                                                                               

Robin Li, Co-founder, Chairman and CEO of Baidu, announced ERNIE 4.0 at Baidu World 2023
""ERNIE 4.0 has achieved a full upgrade with drastically improved performance in understanding, generation, reasoning, and memory,"" Robin Li, Co-founder, Chairman and CEO of Baidu, said at the event. ""These four core capabilities form the foundation of AI-native applications and have now unleashed unlimited opportunities for new innovations.""

",https://www.prnewswire.com/news-releases/baidu-launches-ernie-4-0-foundation-model-leading-a-new-wave-of-ai-native-applications-301958681.html,"Baidu Launches ERNIE 4.0 Foundation Model, Leading a New Wave of AI-Native Applications"
ERNIE 4.0,Multimodal,China,Baidu,2023-10-17,Industry,Image generation,Unknown,1920.0,Not-defined,Not-defined,Not-defined,260000000000.0,668000000000.0,1282.4326530612243,1.0421e+24,1877.1621580193864,10230.01,280.0,217917000000000.0,374371000000000.0,43123092676.96655,"NVIDIA Tesla V100 DGXS 32 GB,Huawei Ascend 910",Significant use,4,1048576,788306.6236983632,19641619.2,2.0911332885519624e-09,537600.0,1010950154.8552281,689435794.2857141,"Baidu, Inc. (NASDAQ: BIDU and HKEX: 9888), a leading AI company with strong Internet foundation, today hosted its annual flagship technology conference Baidu World 2023 in Beijing, marking the conference's return to an offline format after four years. With the theme ""Prompt the World,"" this year's Baidu World conference saw Baidu launch ERNIE 4.0, Baidu's next-generation and most powerful foundation model offering drastically enhanced core AI capabilities. Baidu also showcased some of its most popular applications, solutions, and products re-built around the company's state-of-the-art generative AI.                                                                               

Robin Li, Co-founder, Chairman and CEO of Baidu, announced ERNIE 4.0 at Baidu World 2023
""ERNIE 4.0 has achieved a full upgrade with drastically improved performance in understanding, generation, reasoning, and memory,"" Robin Li, Co-founder, Chairman and CEO of Baidu, said at the event. ""These four core capabilities form the foundation of AI-native applications and have now unleashed unlimited opportunities for new innovations.""

",https://www.prnewswire.com/news-releases/baidu-launches-ernie-4-0-foundation-model-leading-a-new-wave-of-ai-native-applications-301958681.html,"Baidu Launches ERNIE 4.0 Foundation Model, Leading a New Wave of AI-Native Applications"
ERNIE 4.0,Multimodal,China,Baidu,2023-10-17,Industry,Language modeling/generation,Unknown,1920.0,Not-defined,Not-defined,Not-defined,260000000000.0,668000000000.0,1282.4326530612243,1.0421e+24,1877.1621580193864,10230.01,280.0,217917000000000.0,374371000000000.0,43123092676.96655,"NVIDIA Tesla V100 DGXS 32 GB,Huawei Ascend 910",Significant use,4,1048576,788306.6236983632,19641619.2,2.0911332885519624e-09,537600.0,1010950154.8552281,689435794.2857141,"Baidu, Inc. (NASDAQ: BIDU and HKEX: 9888), a leading AI company with strong Internet foundation, today hosted its annual flagship technology conference Baidu World 2023 in Beijing, marking the conference's return to an offline format after four years. With the theme ""Prompt the World,"" this year's Baidu World conference saw Baidu launch ERNIE 4.0, Baidu's next-generation and most powerful foundation model offering drastically enhanced core AI capabilities. Baidu also showcased some of its most popular applications, solutions, and products re-built around the company's state-of-the-art generative AI.                                                                               

Robin Li, Co-founder, Chairman and CEO of Baidu, announced ERNIE 4.0 at Baidu World 2023
""ERNIE 4.0 has achieved a full upgrade with drastically improved performance in understanding, generation, reasoning, and memory,"" Robin Li, Co-founder, Chairman and CEO of Baidu, said at the event. ""These four core capabilities form the foundation of AI-native applications and have now unleashed unlimited opportunities for new innovations.""

",https://www.prnewswire.com/news-releases/baidu-launches-ernie-4-0-foundation-model-leading-a-new-wave-of-ai-native-applications-301958681.html,"Baidu Launches ERNIE 4.0 Foundation Model, Leading a New Wave of AI-Native Applications"
ERNIE 4.0,Multimodal,China,Baidu,2023-10-17,Industry,Video generation,Unknown,1920.0,Not-defined,Not-defined,Not-defined,260000000000.0,668000000000.0,1282.4326530612243,1.0421e+24,1877.1621580193864,10230.01,280.0,217917000000000.0,374371000000000.0,43123092676.96655,"NVIDIA Tesla V100 DGXS 32 GB,Huawei Ascend 910",Significant use,4,1048576,788306.6236983632,19641619.2,2.0911332885519624e-09,537600.0,1010950154.8552281,689435794.2857141,"Baidu, Inc. (NASDAQ: BIDU and HKEX: 9888), a leading AI company with strong Internet foundation, today hosted its annual flagship technology conference Baidu World 2023 in Beijing, marking the conference's return to an offline format after four years. With the theme ""Prompt the World,"" this year's Baidu World conference saw Baidu launch ERNIE 4.0, Baidu's next-generation and most powerful foundation model offering drastically enhanced core AI capabilities. Baidu also showcased some of its most popular applications, solutions, and products re-built around the company's state-of-the-art generative AI.                                                                               

Robin Li, Co-founder, Chairman and CEO of Baidu, announced ERNIE 4.0 at Baidu World 2023
""ERNIE 4.0 has achieved a full upgrade with drastically improved performance in understanding, generation, reasoning, and memory,"" Robin Li, Co-founder, Chairman and CEO of Baidu, said at the event. ""These four core capabilities form the foundation of AI-native applications and have now unleashed unlimited opportunities for new innovations.""

",https://www.prnewswire.com/news-releases/baidu-launches-ernie-4-0-foundation-model-leading-a-new-wave-of-ai-native-applications-301958681.html,"Baidu Launches ERNIE 4.0 Foundation Model, Leading a New Wave of AI-Native Applications"
ERNIE 4.0,Video,China,Baidu,2023-10-17,Industry,Chat,Unknown,1920.0,Not-defined,Not-defined,Not-defined,260000000000.0,668000000000.0,1282.4326530612243,1.0421e+24,1877.1621580193864,10230.01,280.0,217917000000000.0,374371000000000.0,43123092676.96655,"NVIDIA Tesla V100 DGXS 32 GB,Huawei Ascend 910",Significant use,4,1048576,788306.6236983632,19641619.2,2.0911332885519624e-09,537600.0,1010950154.8552281,689435794.2857141,"Baidu, Inc. (NASDAQ: BIDU and HKEX: 9888), a leading AI company with strong Internet foundation, today hosted its annual flagship technology conference Baidu World 2023 in Beijing, marking the conference's return to an offline format after four years. With the theme ""Prompt the World,"" this year's Baidu World conference saw Baidu launch ERNIE 4.0, Baidu's next-generation and most powerful foundation model offering drastically enhanced core AI capabilities. Baidu also showcased some of its most popular applications, solutions, and products re-built around the company's state-of-the-art generative AI.                                                                               

Robin Li, Co-founder, Chairman and CEO of Baidu, announced ERNIE 4.0 at Baidu World 2023
""ERNIE 4.0 has achieved a full upgrade with drastically improved performance in understanding, generation, reasoning, and memory,"" Robin Li, Co-founder, Chairman and CEO of Baidu, said at the event. ""These four core capabilities form the foundation of AI-native applications and have now unleashed unlimited opportunities for new innovations.""

",https://www.prnewswire.com/news-releases/baidu-launches-ernie-4-0-foundation-model-leading-a-new-wave-of-ai-native-applications-301958681.html,"Baidu Launches ERNIE 4.0 Foundation Model, Leading a New Wave of AI-Native Applications"
ERNIE 4.0,Video,China,Baidu,2023-10-17,Industry,Image generation,Unknown,1920.0,Not-defined,Not-defined,Not-defined,260000000000.0,668000000000.0,1282.4326530612243,1.0421e+24,1877.1621580193864,10230.01,280.0,217917000000000.0,374371000000000.0,43123092676.96655,"NVIDIA Tesla V100 DGXS 32 GB,Huawei Ascend 910",Significant use,4,1048576,788306.6236983632,19641619.2,2.0911332885519624e-09,537600.0,1010950154.8552281,689435794.2857141,"Baidu, Inc. (NASDAQ: BIDU and HKEX: 9888), a leading AI company with strong Internet foundation, today hosted its annual flagship technology conference Baidu World 2023 in Beijing, marking the conference's return to an offline format after four years. With the theme ""Prompt the World,"" this year's Baidu World conference saw Baidu launch ERNIE 4.0, Baidu's next-generation and most powerful foundation model offering drastically enhanced core AI capabilities. Baidu also showcased some of its most popular applications, solutions, and products re-built around the company's state-of-the-art generative AI.                                                                               

Robin Li, Co-founder, Chairman and CEO of Baidu, announced ERNIE 4.0 at Baidu World 2023
""ERNIE 4.0 has achieved a full upgrade with drastically improved performance in understanding, generation, reasoning, and memory,"" Robin Li, Co-founder, Chairman and CEO of Baidu, said at the event. ""These four core capabilities form the foundation of AI-native applications and have now unleashed unlimited opportunities for new innovations.""

",https://www.prnewswire.com/news-releases/baidu-launches-ernie-4-0-foundation-model-leading-a-new-wave-of-ai-native-applications-301958681.html,"Baidu Launches ERNIE 4.0 Foundation Model, Leading a New Wave of AI-Native Applications"
ERNIE 4.0,Video,China,Baidu,2023-10-17,Industry,Language modeling/generation,Unknown,1920.0,Not-defined,Not-defined,Not-defined,260000000000.0,668000000000.0,1282.4326530612243,1.0421e+24,1877.1621580193864,10230.01,280.0,217917000000000.0,374371000000000.0,43123092676.96655,"NVIDIA Tesla V100 DGXS 32 GB,Huawei Ascend 910",Significant use,4,1048576,788306.6236983632,19641619.2,2.0911332885519624e-09,537600.0,1010950154.8552281,689435794.2857141,"Baidu, Inc. (NASDAQ: BIDU and HKEX: 9888), a leading AI company with strong Internet foundation, today hosted its annual flagship technology conference Baidu World 2023 in Beijing, marking the conference's return to an offline format after four years. With the theme ""Prompt the World,"" this year's Baidu World conference saw Baidu launch ERNIE 4.0, Baidu's next-generation and most powerful foundation model offering drastically enhanced core AI capabilities. Baidu also showcased some of its most popular applications, solutions, and products re-built around the company's state-of-the-art generative AI.                                                                               

Robin Li, Co-founder, Chairman and CEO of Baidu, announced ERNIE 4.0 at Baidu World 2023
""ERNIE 4.0 has achieved a full upgrade with drastically improved performance in understanding, generation, reasoning, and memory,"" Robin Li, Co-founder, Chairman and CEO of Baidu, said at the event. ""These four core capabilities form the foundation of AI-native applications and have now unleashed unlimited opportunities for new innovations.""

",https://www.prnewswire.com/news-releases/baidu-launches-ernie-4-0-foundation-model-leading-a-new-wave-of-ai-native-applications-301958681.html,"Baidu Launches ERNIE 4.0 Foundation Model, Leading a New Wave of AI-Native Applications"
ERNIE 4.0,Video,China,Baidu,2023-10-17,Industry,Video generation,Unknown,1920.0,Not-defined,Not-defined,Not-defined,260000000000.0,668000000000.0,1282.4326530612243,1.0421e+24,1877.1621580193864,10230.01,280.0,217917000000000.0,374371000000000.0,43123092676.96655,"NVIDIA Tesla V100 DGXS 32 GB,Huawei Ascend 910",Significant use,4,1048576,788306.6236983632,19641619.2,2.0911332885519624e-09,537600.0,1010950154.8552281,689435794.2857141,"Baidu, Inc. (NASDAQ: BIDU and HKEX: 9888), a leading AI company with strong Internet foundation, today hosted its annual flagship technology conference Baidu World 2023 in Beijing, marking the conference's return to an offline format after four years. With the theme ""Prompt the World,"" this year's Baidu World conference saw Baidu launch ERNIE 4.0, Baidu's next-generation and most powerful foundation model offering drastically enhanced core AI capabilities. Baidu also showcased some of its most popular applications, solutions, and products re-built around the company's state-of-the-art generative AI.                                                                               

Robin Li, Co-founder, Chairman and CEO of Baidu, announced ERNIE 4.0 at Baidu World 2023
""ERNIE 4.0 has achieved a full upgrade with drastically improved performance in understanding, generation, reasoning, and memory,"" Robin Li, Co-founder, Chairman and CEO of Baidu, said at the event. ""These four core capabilities form the foundation of AI-native applications and have now unleashed unlimited opportunities for new innovations.""

",https://www.prnewswire.com/news-releases/baidu-launches-ernie-4-0-foundation-model-leading-a-new-wave-of-ai-native-applications-301958681.html,"Baidu Launches ERNIE 4.0 Foundation Model, Leading a New Wave of AI-Native Applications"
Evo 2 40B,Biology,USA,Arc Institute,2025-02-19,Academia,Protein or nucleotide language model (pLM/nLM),Unverified,128.0,Open weights (unrestricted),OpenGenome 2,Not-defined,40300000000.0,9300000000000.0,2880.0,2.25e+24,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,SOTA improvement,1,2000000,113643.12380745166,1293528.32,2.5563111111111108e-09,51200.0,327292196.5654608,147456000.0,"All of life encodes information with DNA. While tools for sequencing, synthesis, and editing of genomic code have transformed biological research, intelligently composing new biological systems would also require a deep understanding of the immense complexity encoded by genomes. We introduce Evo 2, a biological foundation model trained on 9.3 trillion DNA base pairs from a highly curated genomic atlas spanning all domains of life. We train Evo 2 with 7B and 40B parameters to have an unprecedented 1 million token context window with single-nucleotide resolution. Evo 2 learns from DNA sequence alone to accurately predict the functional impacts of genetic variation—from noncoding pathogenic mutations to clinically significant BRCA1 variants—without task-specific finetuning. Applying mechanistic interpretability analyses, we reveal that Evo 2 autonomously learns a breadth of biological features, including exon–intron boundaries, transcription factor binding sites, protein structural elements, and prophage genomic regions. Beyond its predictive capabilities, Evo 2 generates mitochondrial, prokaryotic, and eukaryotic sequences at genome scale with greater naturalness and coherence than previous methods. Guiding Evo 2 via inference-time search enables controllable generation of epigenomic structure, for which we demonstrate the first inference-time scaling results in biology. We make Evo 2 fully open, including model parameters, training code, inference code, and the OpenGenome2 dataset, to accelerate the exploration and design of biological complexity.",https://arcinstitute.org/manuscripts/Evo2,Genome modeling and design across all domains of life with Evo 2
Evo 2 40B,Biology,USA,Arc Institute,2025-02-19,Industry,Protein or nucleotide language model (pLM/nLM),Unverified,128.0,Open weights (unrestricted),OpenGenome 2,Not-defined,40300000000.0,9300000000000.0,2880.0,2.25e+24,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,SOTA improvement,1,2000000,113643.12380745166,1293528.32,2.5563111111111108e-09,51200.0,327292196.5654608,147456000.0,"All of life encodes information with DNA. While tools for sequencing, synthesis, and editing of genomic code have transformed biological research, intelligently composing new biological systems would also require a deep understanding of the immense complexity encoded by genomes. We introduce Evo 2, a biological foundation model trained on 9.3 trillion DNA base pairs from a highly curated genomic atlas spanning all domains of life. We train Evo 2 with 7B and 40B parameters to have an unprecedented 1 million token context window with single-nucleotide resolution. Evo 2 learns from DNA sequence alone to accurately predict the functional impacts of genetic variation—from noncoding pathogenic mutations to clinically significant BRCA1 variants—without task-specific finetuning. Applying mechanistic interpretability analyses, we reveal that Evo 2 autonomously learns a breadth of biological features, including exon–intron boundaries, transcription factor binding sites, protein structural elements, and prophage genomic regions. Beyond its predictive capabilities, Evo 2 generates mitochondrial, prokaryotic, and eukaryotic sequences at genome scale with greater naturalness and coherence than previous methods. Guiding Evo 2 via inference-time search enables controllable generation of epigenomic structure, for which we demonstrate the first inference-time scaling results in biology. We make Evo 2 fully open, including model parameters, training code, inference code, and the OpenGenome2 dataset, to accelerate the exploration and design of biological complexity.",https://arcinstitute.org/manuscripts/Evo2,Genome modeling and design across all domains of life with Evo 2
Evo 2 40B,Biology,USA,Columbia University,2025-02-19,Academia,Protein or nucleotide language model (pLM/nLM),Unverified,128.0,Open weights (unrestricted),OpenGenome 2,Not-defined,40300000000.0,9300000000000.0,2880.0,2.25e+24,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,SOTA improvement,1,2000000,113643.12380745166,1293528.32,2.5563111111111108e-09,51200.0,327292196.5654608,147456000.0,"All of life encodes information with DNA. While tools for sequencing, synthesis, and editing of genomic code have transformed biological research, intelligently composing new biological systems would also require a deep understanding of the immense complexity encoded by genomes. We introduce Evo 2, a biological foundation model trained on 9.3 trillion DNA base pairs from a highly curated genomic atlas spanning all domains of life. We train Evo 2 with 7B and 40B parameters to have an unprecedented 1 million token context window with single-nucleotide resolution. Evo 2 learns from DNA sequence alone to accurately predict the functional impacts of genetic variation—from noncoding pathogenic mutations to clinically significant BRCA1 variants—without task-specific finetuning. Applying mechanistic interpretability analyses, we reveal that Evo 2 autonomously learns a breadth of biological features, including exon–intron boundaries, transcription factor binding sites, protein structural elements, and prophage genomic regions. Beyond its predictive capabilities, Evo 2 generates mitochondrial, prokaryotic, and eukaryotic sequences at genome scale with greater naturalness and coherence than previous methods. Guiding Evo 2 via inference-time search enables controllable generation of epigenomic structure, for which we demonstrate the first inference-time scaling results in biology. We make Evo 2 fully open, including model parameters, training code, inference code, and the OpenGenome2 dataset, to accelerate the exploration and design of biological complexity.",https://arcinstitute.org/manuscripts/Evo2,Genome modeling and design across all domains of life with Evo 2
Evo 2 40B,Biology,USA,Columbia University,2025-02-19,Industry,Protein or nucleotide language model (pLM/nLM),Unverified,128.0,Open weights (unrestricted),OpenGenome 2,Not-defined,40300000000.0,9300000000000.0,2880.0,2.25e+24,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,SOTA improvement,1,2000000,113643.12380745166,1293528.32,2.5563111111111108e-09,51200.0,327292196.5654608,147456000.0,"All of life encodes information with DNA. While tools for sequencing, synthesis, and editing of genomic code have transformed biological research, intelligently composing new biological systems would also require a deep understanding of the immense complexity encoded by genomes. We introduce Evo 2, a biological foundation model trained on 9.3 trillion DNA base pairs from a highly curated genomic atlas spanning all domains of life. We train Evo 2 with 7B and 40B parameters to have an unprecedented 1 million token context window with single-nucleotide resolution. Evo 2 learns from DNA sequence alone to accurately predict the functional impacts of genetic variation—from noncoding pathogenic mutations to clinically significant BRCA1 variants—without task-specific finetuning. Applying mechanistic interpretability analyses, we reveal that Evo 2 autonomously learns a breadth of biological features, including exon–intron boundaries, transcription factor binding sites, protein structural elements, and prophage genomic regions. Beyond its predictive capabilities, Evo 2 generates mitochondrial, prokaryotic, and eukaryotic sequences at genome scale with greater naturalness and coherence than previous methods. Guiding Evo 2 via inference-time search enables controllable generation of epigenomic structure, for which we demonstrate the first inference-time scaling results in biology. We make Evo 2 fully open, including model parameters, training code, inference code, and the OpenGenome2 dataset, to accelerate the exploration and design of biological complexity.",https://arcinstitute.org/manuscripts/Evo2,Genome modeling and design across all domains of life with Evo 2
Evo 2 40B,Biology,USA,Goodfire,2025-02-19,Academia,Protein or nucleotide language model (pLM/nLM),Unverified,128.0,Open weights (unrestricted),OpenGenome 2,Not-defined,40300000000.0,9300000000000.0,2880.0,2.25e+24,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,SOTA improvement,1,2000000,113643.12380745166,1293528.32,2.5563111111111108e-09,51200.0,327292196.5654608,147456000.0,"All of life encodes information with DNA. While tools for sequencing, synthesis, and editing of genomic code have transformed biological research, intelligently composing new biological systems would also require a deep understanding of the immense complexity encoded by genomes. We introduce Evo 2, a biological foundation model trained on 9.3 trillion DNA base pairs from a highly curated genomic atlas spanning all domains of life. We train Evo 2 with 7B and 40B parameters to have an unprecedented 1 million token context window with single-nucleotide resolution. Evo 2 learns from DNA sequence alone to accurately predict the functional impacts of genetic variation—from noncoding pathogenic mutations to clinically significant BRCA1 variants—without task-specific finetuning. Applying mechanistic interpretability analyses, we reveal that Evo 2 autonomously learns a breadth of biological features, including exon–intron boundaries, transcription factor binding sites, protein structural elements, and prophage genomic regions. Beyond its predictive capabilities, Evo 2 generates mitochondrial, prokaryotic, and eukaryotic sequences at genome scale with greater naturalness and coherence than previous methods. Guiding Evo 2 via inference-time search enables controllable generation of epigenomic structure, for which we demonstrate the first inference-time scaling results in biology. We make Evo 2 fully open, including model parameters, training code, inference code, and the OpenGenome2 dataset, to accelerate the exploration and design of biological complexity.",https://arcinstitute.org/manuscripts/Evo2,Genome modeling and design across all domains of life with Evo 2
Evo 2 40B,Biology,USA,Goodfire,2025-02-19,Industry,Protein or nucleotide language model (pLM/nLM),Unverified,128.0,Open weights (unrestricted),OpenGenome 2,Not-defined,40300000000.0,9300000000000.0,2880.0,2.25e+24,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,SOTA improvement,1,2000000,113643.12380745166,1293528.32,2.5563111111111108e-09,51200.0,327292196.5654608,147456000.0,"All of life encodes information with DNA. While tools for sequencing, synthesis, and editing of genomic code have transformed biological research, intelligently composing new biological systems would also require a deep understanding of the immense complexity encoded by genomes. We introduce Evo 2, a biological foundation model trained on 9.3 trillion DNA base pairs from a highly curated genomic atlas spanning all domains of life. We train Evo 2 with 7B and 40B parameters to have an unprecedented 1 million token context window with single-nucleotide resolution. Evo 2 learns from DNA sequence alone to accurately predict the functional impacts of genetic variation—from noncoding pathogenic mutations to clinically significant BRCA1 variants—without task-specific finetuning. Applying mechanistic interpretability analyses, we reveal that Evo 2 autonomously learns a breadth of biological features, including exon–intron boundaries, transcription factor binding sites, protein structural elements, and prophage genomic regions. Beyond its predictive capabilities, Evo 2 generates mitochondrial, prokaryotic, and eukaryotic sequences at genome scale with greater naturalness and coherence than previous methods. Guiding Evo 2 via inference-time search enables controllable generation of epigenomic structure, for which we demonstrate the first inference-time scaling results in biology. We make Evo 2 fully open, including model parameters, training code, inference code, and the OpenGenome2 dataset, to accelerate the exploration and design of biological complexity.",https://arcinstitute.org/manuscripts/Evo2,Genome modeling and design across all domains of life with Evo 2
Evo 2 40B,Biology,USA,Liquid,2025-02-19,Academia,Protein or nucleotide language model (pLM/nLM),Unverified,128.0,Open weights (unrestricted),OpenGenome 2,Not-defined,40300000000.0,9300000000000.0,2880.0,2.25e+24,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,SOTA improvement,1,2000000,113643.12380745166,1293528.32,2.5563111111111108e-09,51200.0,327292196.5654608,147456000.0,"All of life encodes information with DNA. While tools for sequencing, synthesis, and editing of genomic code have transformed biological research, intelligently composing new biological systems would also require a deep understanding of the immense complexity encoded by genomes. We introduce Evo 2, a biological foundation model trained on 9.3 trillion DNA base pairs from a highly curated genomic atlas spanning all domains of life. We train Evo 2 with 7B and 40B parameters to have an unprecedented 1 million token context window with single-nucleotide resolution. Evo 2 learns from DNA sequence alone to accurately predict the functional impacts of genetic variation—from noncoding pathogenic mutations to clinically significant BRCA1 variants—without task-specific finetuning. Applying mechanistic interpretability analyses, we reveal that Evo 2 autonomously learns a breadth of biological features, including exon–intron boundaries, transcription factor binding sites, protein structural elements, and prophage genomic regions. Beyond its predictive capabilities, Evo 2 generates mitochondrial, prokaryotic, and eukaryotic sequences at genome scale with greater naturalness and coherence than previous methods. Guiding Evo 2 via inference-time search enables controllable generation of epigenomic structure, for which we demonstrate the first inference-time scaling results in biology. We make Evo 2 fully open, including model parameters, training code, inference code, and the OpenGenome2 dataset, to accelerate the exploration and design of biological complexity.",https://arcinstitute.org/manuscripts/Evo2,Genome modeling and design across all domains of life with Evo 2
Evo 2 40B,Biology,USA,Liquid,2025-02-19,Industry,Protein or nucleotide language model (pLM/nLM),Unverified,128.0,Open weights (unrestricted),OpenGenome 2,Not-defined,40300000000.0,9300000000000.0,2880.0,2.25e+24,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,SOTA improvement,1,2000000,113643.12380745166,1293528.32,2.5563111111111108e-09,51200.0,327292196.5654608,147456000.0,"All of life encodes information with DNA. While tools for sequencing, synthesis, and editing of genomic code have transformed biological research, intelligently composing new biological systems would also require a deep understanding of the immense complexity encoded by genomes. We introduce Evo 2, a biological foundation model trained on 9.3 trillion DNA base pairs from a highly curated genomic atlas spanning all domains of life. We train Evo 2 with 7B and 40B parameters to have an unprecedented 1 million token context window with single-nucleotide resolution. Evo 2 learns from DNA sequence alone to accurately predict the functional impacts of genetic variation—from noncoding pathogenic mutations to clinically significant BRCA1 variants—without task-specific finetuning. Applying mechanistic interpretability analyses, we reveal that Evo 2 autonomously learns a breadth of biological features, including exon–intron boundaries, transcription factor binding sites, protein structural elements, and prophage genomic regions. Beyond its predictive capabilities, Evo 2 generates mitochondrial, prokaryotic, and eukaryotic sequences at genome scale with greater naturalness and coherence than previous methods. Guiding Evo 2 via inference-time search enables controllable generation of epigenomic structure, for which we demonstrate the first inference-time scaling results in biology. We make Evo 2 fully open, including model parameters, training code, inference code, and the OpenGenome2 dataset, to accelerate the exploration and design of biological complexity.",https://arcinstitute.org/manuscripts/Evo2,Genome modeling and design across all domains of life with Evo 2
Evo 2 40B,Biology,USA,NVIDIA,2025-02-19,Academia,Protein or nucleotide language model (pLM/nLM),Unverified,128.0,Open weights (unrestricted),OpenGenome 2,Not-defined,40300000000.0,9300000000000.0,2880.0,2.25e+24,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,SOTA improvement,1,2000000,113643.12380745166,1293528.32,2.5563111111111108e-09,51200.0,327292196.5654608,147456000.0,"All of life encodes information with DNA. While tools for sequencing, synthesis, and editing of genomic code have transformed biological research, intelligently composing new biological systems would also require a deep understanding of the immense complexity encoded by genomes. We introduce Evo 2, a biological foundation model trained on 9.3 trillion DNA base pairs from a highly curated genomic atlas spanning all domains of life. We train Evo 2 with 7B and 40B parameters to have an unprecedented 1 million token context window with single-nucleotide resolution. Evo 2 learns from DNA sequence alone to accurately predict the functional impacts of genetic variation—from noncoding pathogenic mutations to clinically significant BRCA1 variants—without task-specific finetuning. Applying mechanistic interpretability analyses, we reveal that Evo 2 autonomously learns a breadth of biological features, including exon–intron boundaries, transcription factor binding sites, protein structural elements, and prophage genomic regions. Beyond its predictive capabilities, Evo 2 generates mitochondrial, prokaryotic, and eukaryotic sequences at genome scale with greater naturalness and coherence than previous methods. Guiding Evo 2 via inference-time search enables controllable generation of epigenomic structure, for which we demonstrate the first inference-time scaling results in biology. We make Evo 2 fully open, including model parameters, training code, inference code, and the OpenGenome2 dataset, to accelerate the exploration and design of biological complexity.",https://arcinstitute.org/manuscripts/Evo2,Genome modeling and design across all domains of life with Evo 2
Evo 2 40B,Biology,USA,NVIDIA,2025-02-19,Industry,Protein or nucleotide language model (pLM/nLM),Unverified,128.0,Open weights (unrestricted),OpenGenome 2,Not-defined,40300000000.0,9300000000000.0,2880.0,2.25e+24,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,SOTA improvement,1,2000000,113643.12380745166,1293528.32,2.5563111111111108e-09,51200.0,327292196.5654608,147456000.0,"All of life encodes information with DNA. While tools for sequencing, synthesis, and editing of genomic code have transformed biological research, intelligently composing new biological systems would also require a deep understanding of the immense complexity encoded by genomes. We introduce Evo 2, a biological foundation model trained on 9.3 trillion DNA base pairs from a highly curated genomic atlas spanning all domains of life. We train Evo 2 with 7B and 40B parameters to have an unprecedented 1 million token context window with single-nucleotide resolution. Evo 2 learns from DNA sequence alone to accurately predict the functional impacts of genetic variation—from noncoding pathogenic mutations to clinically significant BRCA1 variants—without task-specific finetuning. Applying mechanistic interpretability analyses, we reveal that Evo 2 autonomously learns a breadth of biological features, including exon–intron boundaries, transcription factor binding sites, protein structural elements, and prophage genomic regions. Beyond its predictive capabilities, Evo 2 generates mitochondrial, prokaryotic, and eukaryotic sequences at genome scale with greater naturalness and coherence than previous methods. Guiding Evo 2 via inference-time search enables controllable generation of epigenomic structure, for which we demonstrate the first inference-time scaling results in biology. We make Evo 2 fully open, including model parameters, training code, inference code, and the OpenGenome2 dataset, to accelerate the exploration and design of biological complexity.",https://arcinstitute.org/manuscripts/Evo2,Genome modeling and design across all domains of life with Evo 2
Evo 2 40B,Biology,USA,Stanford University,2025-02-19,Academia,Protein or nucleotide language model (pLM/nLM),Unverified,128.0,Open weights (unrestricted),OpenGenome 2,Not-defined,40300000000.0,9300000000000.0,2880.0,2.25e+24,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,SOTA improvement,1,2000000,113643.12380745166,1293528.32,2.5563111111111108e-09,51200.0,327292196.5654608,147456000.0,"All of life encodes information with DNA. While tools for sequencing, synthesis, and editing of genomic code have transformed biological research, intelligently composing new biological systems would also require a deep understanding of the immense complexity encoded by genomes. We introduce Evo 2, a biological foundation model trained on 9.3 trillion DNA base pairs from a highly curated genomic atlas spanning all domains of life. We train Evo 2 with 7B and 40B parameters to have an unprecedented 1 million token context window with single-nucleotide resolution. Evo 2 learns from DNA sequence alone to accurately predict the functional impacts of genetic variation—from noncoding pathogenic mutations to clinically significant BRCA1 variants—without task-specific finetuning. Applying mechanistic interpretability analyses, we reveal that Evo 2 autonomously learns a breadth of biological features, including exon–intron boundaries, transcription factor binding sites, protein structural elements, and prophage genomic regions. Beyond its predictive capabilities, Evo 2 generates mitochondrial, prokaryotic, and eukaryotic sequences at genome scale with greater naturalness and coherence than previous methods. Guiding Evo 2 via inference-time search enables controllable generation of epigenomic structure, for which we demonstrate the first inference-time scaling results in biology. We make Evo 2 fully open, including model parameters, training code, inference code, and the OpenGenome2 dataset, to accelerate the exploration and design of biological complexity.",https://arcinstitute.org/manuscripts/Evo2,Genome modeling and design across all domains of life with Evo 2
Evo 2 40B,Biology,USA,Stanford University,2025-02-19,Industry,Protein or nucleotide language model (pLM/nLM),Unverified,128.0,Open weights (unrestricted),OpenGenome 2,Not-defined,40300000000.0,9300000000000.0,2880.0,2.25e+24,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,SOTA improvement,1,2000000,113643.12380745166,1293528.32,2.5563111111111108e-09,51200.0,327292196.5654608,147456000.0,"All of life encodes information with DNA. While tools for sequencing, synthesis, and editing of genomic code have transformed biological research, intelligently composing new biological systems would also require a deep understanding of the immense complexity encoded by genomes. We introduce Evo 2, a biological foundation model trained on 9.3 trillion DNA base pairs from a highly curated genomic atlas spanning all domains of life. We train Evo 2 with 7B and 40B parameters to have an unprecedented 1 million token context window with single-nucleotide resolution. Evo 2 learns from DNA sequence alone to accurately predict the functional impacts of genetic variation—from noncoding pathogenic mutations to clinically significant BRCA1 variants—without task-specific finetuning. Applying mechanistic interpretability analyses, we reveal that Evo 2 autonomously learns a breadth of biological features, including exon–intron boundaries, transcription factor binding sites, protein structural elements, and prophage genomic regions. Beyond its predictive capabilities, Evo 2 generates mitochondrial, prokaryotic, and eukaryotic sequences at genome scale with greater naturalness and coherence than previous methods. Guiding Evo 2 via inference-time search enables controllable generation of epigenomic structure, for which we demonstrate the first inference-time scaling results in biology. We make Evo 2 fully open, including model parameters, training code, inference code, and the OpenGenome2 dataset, to accelerate the exploration and design of biological complexity.",https://arcinstitute.org/manuscripts/Evo2,Genome modeling and design across all domains of life with Evo 2
Evo 2 40B,Biology,USA,University of California (UC) Berkeley,2025-02-19,Academia,Protein or nucleotide language model (pLM/nLM),Unverified,128.0,Open weights (unrestricted),OpenGenome 2,Not-defined,40300000000.0,9300000000000.0,2880.0,2.25e+24,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,SOTA improvement,1,2000000,113643.12380745166,1293528.32,2.5563111111111108e-09,51200.0,327292196.5654608,147456000.0,"All of life encodes information with DNA. While tools for sequencing, synthesis, and editing of genomic code have transformed biological research, intelligently composing new biological systems would also require a deep understanding of the immense complexity encoded by genomes. We introduce Evo 2, a biological foundation model trained on 9.3 trillion DNA base pairs from a highly curated genomic atlas spanning all domains of life. We train Evo 2 with 7B and 40B parameters to have an unprecedented 1 million token context window with single-nucleotide resolution. Evo 2 learns from DNA sequence alone to accurately predict the functional impacts of genetic variation—from noncoding pathogenic mutations to clinically significant BRCA1 variants—without task-specific finetuning. Applying mechanistic interpretability analyses, we reveal that Evo 2 autonomously learns a breadth of biological features, including exon–intron boundaries, transcription factor binding sites, protein structural elements, and prophage genomic regions. Beyond its predictive capabilities, Evo 2 generates mitochondrial, prokaryotic, and eukaryotic sequences at genome scale with greater naturalness and coherence than previous methods. Guiding Evo 2 via inference-time search enables controllable generation of epigenomic structure, for which we demonstrate the first inference-time scaling results in biology. We make Evo 2 fully open, including model parameters, training code, inference code, and the OpenGenome2 dataset, to accelerate the exploration and design of biological complexity.",https://arcinstitute.org/manuscripts/Evo2,Genome modeling and design across all domains of life with Evo 2
Evo 2 40B,Biology,USA,University of California (UC) Berkeley,2025-02-19,Industry,Protein or nucleotide language model (pLM/nLM),Unverified,128.0,Open weights (unrestricted),OpenGenome 2,Not-defined,40300000000.0,9300000000000.0,2880.0,2.25e+24,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,SOTA improvement,1,2000000,113643.12380745166,1293528.32,2.5563111111111108e-09,51200.0,327292196.5654608,147456000.0,"All of life encodes information with DNA. While tools for sequencing, synthesis, and editing of genomic code have transformed biological research, intelligently composing new biological systems would also require a deep understanding of the immense complexity encoded by genomes. We introduce Evo 2, a biological foundation model trained on 9.3 trillion DNA base pairs from a highly curated genomic atlas spanning all domains of life. We train Evo 2 with 7B and 40B parameters to have an unprecedented 1 million token context window with single-nucleotide resolution. Evo 2 learns from DNA sequence alone to accurately predict the functional impacts of genetic variation—from noncoding pathogenic mutations to clinically significant BRCA1 variants—without task-specific finetuning. Applying mechanistic interpretability analyses, we reveal that Evo 2 autonomously learns a breadth of biological features, including exon–intron boundaries, transcription factor binding sites, protein structural elements, and prophage genomic regions. Beyond its predictive capabilities, Evo 2 generates mitochondrial, prokaryotic, and eukaryotic sequences at genome scale with greater naturalness and coherence than previous methods. Guiding Evo 2 via inference-time search enables controllable generation of epigenomic structure, for which we demonstrate the first inference-time scaling results in biology. We make Evo 2 fully open, including model parameters, training code, inference code, and the OpenGenome2 dataset, to accelerate the exploration and design of biological complexity.",https://arcinstitute.org/manuscripts/Evo2,Genome modeling and design across all domains of life with Evo 2
Evo 2 40B,Biology,USA,University of California San Francisco,2025-02-19,Academia,Protein or nucleotide language model (pLM/nLM),Unverified,128.0,Open weights (unrestricted),OpenGenome 2,Not-defined,40300000000.0,9300000000000.0,2880.0,2.25e+24,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,SOTA improvement,1,2000000,113643.12380745166,1293528.32,2.5563111111111108e-09,51200.0,327292196.5654608,147456000.0,"All of life encodes information with DNA. While tools for sequencing, synthesis, and editing of genomic code have transformed biological research, intelligently composing new biological systems would also require a deep understanding of the immense complexity encoded by genomes. We introduce Evo 2, a biological foundation model trained on 9.3 trillion DNA base pairs from a highly curated genomic atlas spanning all domains of life. We train Evo 2 with 7B and 40B parameters to have an unprecedented 1 million token context window with single-nucleotide resolution. Evo 2 learns from DNA sequence alone to accurately predict the functional impacts of genetic variation—from noncoding pathogenic mutations to clinically significant BRCA1 variants—without task-specific finetuning. Applying mechanistic interpretability analyses, we reveal that Evo 2 autonomously learns a breadth of biological features, including exon–intron boundaries, transcription factor binding sites, protein structural elements, and prophage genomic regions. Beyond its predictive capabilities, Evo 2 generates mitochondrial, prokaryotic, and eukaryotic sequences at genome scale with greater naturalness and coherence than previous methods. Guiding Evo 2 via inference-time search enables controllable generation of epigenomic structure, for which we demonstrate the first inference-time scaling results in biology. We make Evo 2 fully open, including model parameters, training code, inference code, and the OpenGenome2 dataset, to accelerate the exploration and design of biological complexity.",https://arcinstitute.org/manuscripts/Evo2,Genome modeling and design across all domains of life with Evo 2
Evo 2 40B,Biology,USA,University of California San Francisco,2025-02-19,Industry,Protein or nucleotide language model (pLM/nLM),Unverified,128.0,Open weights (unrestricted),OpenGenome 2,Not-defined,40300000000.0,9300000000000.0,2880.0,2.25e+24,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,SOTA improvement,1,2000000,113643.12380745166,1293528.32,2.5563111111111108e-09,51200.0,327292196.5654608,147456000.0,"All of life encodes information with DNA. While tools for sequencing, synthesis, and editing of genomic code have transformed biological research, intelligently composing new biological systems would also require a deep understanding of the immense complexity encoded by genomes. We introduce Evo 2, a biological foundation model trained on 9.3 trillion DNA base pairs from a highly curated genomic atlas spanning all domains of life. We train Evo 2 with 7B and 40B parameters to have an unprecedented 1 million token context window with single-nucleotide resolution. Evo 2 learns from DNA sequence alone to accurately predict the functional impacts of genetic variation—from noncoding pathogenic mutations to clinically significant BRCA1 variants—without task-specific finetuning. Applying mechanistic interpretability analyses, we reveal that Evo 2 autonomously learns a breadth of biological features, including exon–intron boundaries, transcription factor binding sites, protein structural elements, and prophage genomic regions. Beyond its predictive capabilities, Evo 2 generates mitochondrial, prokaryotic, and eukaryotic sequences at genome scale with greater naturalness and coherence than previous methods. Guiding Evo 2 via inference-time search enables controllable generation of epigenomic structure, for which we demonstrate the first inference-time scaling results in biology. We make Evo 2 fully open, including model parameters, training code, inference code, and the OpenGenome2 dataset, to accelerate the exploration and design of biological complexity.",https://arcinstitute.org/manuscripts/Evo2,Genome modeling and design across all domains of life with Evo 2
Falcon 2 11B,Language,UAE,Technology Innovation Institute,2024-05-09,Government,Language modeling/generation,Confident,384.0,Open weights (restricted use),RefinedWeb,Not-defined,11000000000.0,5500000000000.0,1400.0,3.6e+23,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,1,2359296,340173.4453754254,5760000.0,0.3864,153600.0,476242823.52559555,215040000.0,"Falcon2-11B is an 11B parameters causal decoder-only model built by TII and trained on over 5,000B tokens of RefinedWeb enhanced with curated corpora. The model is made available under the TII Falcon License 2.0, the permissive Apache 2.0-based software license which includes an acceptable use policy that promotes the responsible use of AI.",https://huggingface.co/tiiuae/falcon-11B ,Falcon2-11B
Falcon-180B,Language,UAE,Technology Innovation Institute,2023-09-06,Government,Language modeling,Confident,4096.0,Open weights (restricted use),RefinedWeb,Unreleased,180000000000.0,2625000000000.0,4320.0,3.76e+24,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,4194304,3622388.5616717767,61440000.0,0.1892,1638400.0,15648718586.422075,7077888000.0,"Falcon 180B is a super-powerful language model with 180 billion parameters, trained on 3.5 trillion tokens. It's currently at the top of the Hugging Face Leaderboard for pre-trained Open Large Language Models and is available for both research and commercial use.

This model performs exceptionally well in various tasks like reasoning, coding, proficiency, and knowledge tests, even beating competitors like Meta's LLaMA 2.

Among closed source models, it ranks just behind OpenAI's GPT 4, and performs on par with Google's PaLM 2 Large, which powers Bard, despite being half the size of the model.",https://falconllm.tii.ae/falcon-180b.html; https://arxiv.org/abs/2311.16867,The Falcon Series of Open Language Models
Falcon-40B,Language,UAE,Technology Innovation Institute,2023-03-15,Government,Language modeling,Confident,384.0,Open weights (unrestricted),RefinedWeb,Unreleased,40000000000.0,1000000000000.0,1440.0,2.4e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Historical significance,1,2359296,340173.4453754254,3880584.96,0.3864,153600.0,489849761.3406126,221184000.0,,https://arxiv.org/abs/2311.16867; https://www.tii.ae/news/abu-dhabi-based-technology-innovation-institute-introduces-falcon-llm-foundational-large,Abu Dhabi-based Technology Innovation Institute Introduces Falcon LLM: Foundational Large Language Model (LLM) outperforms GPT-3 with 40 Billion Parameters
Firefly,Image generation,USA,Adobe,2023-03-21,Industry,Image generation,Unknown,128.0,Not-defined,Adobe Stock,Not-defined,120000000000.0,106000000000.0,1282.4326530612243,3.24e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Significant use,4,2000000,788306.6236983632,1453512.96,1.7753271604938272e-08,51200.0,1010950154.8552281,65660551.83673468,"Today, Adobe (Nasdaq:ADBE) introduced Adobe Firefly, a new family of creative generative AI models, first focused on the generation of images and text effects. Adobe Firefly will bring even more precision, power, speed and ease directly into Creative Cloud, Document Cloud, Experience Cloud and Adobe Express workflows where content is created and modified. Adobe Firefly will be part of a series of new Adobe Sensei generative AI services across Adobe’s clouds.",https://news.adobe.com/news/news-details/2023/Adobe-Unveils-Firefly-a-Family-of-new-Creative-Generative-AI/default.aspx,"Adobe Unveils Firefly, a Family of new Creative Generative AI"
Fuyu-8B,Language,USA,Adept,2023-10-17,Industry,Chat,Confident,128.0,Open weights (non-commercial),Not-defined,Unreleased,8000000000.0,2500000000000.0,1440.0,2.4e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Significant use,1,4000000,113138.0969599182,1293528.32,2.3965416666666668e-08,51200.0,162918859.6222822,73728000.0,"We’re releasing Fuyu-8B, a small version of the multimodal1 model that powers our product. The model is available on HuggingFace. We think Fuyu-8B is exciting because:

It has a much simpler architecture and training procedure than other multi-modal models, which makes it easier to understand, scale, and deploy.
It’s designed from the ground up for digital agents, so it can support arbitrary image resolutions, answer questions about graphs and diagrams, answer UI-based questions, and do fine-grained localization on screen images.
It’s fast - we can get responses for large images in less than 100 milliseconds.
Despite being optimized for our use-case, it performs well at standard image understanding benchmarks such as visual question-answering and natural-image-captioning.","https://www.adept.ai/blog/fuyu-8b, www.huggingface.co/adept/fuyu-8b",Fuyu-8B: A Multimodal Architecture for AI Agents
Fuyu-8B,Language,USA,Adept,2023-10-17,Industry,Image captioning,Confident,128.0,Open weights (non-commercial),Not-defined,Unreleased,8000000000.0,2500000000000.0,1440.0,2.4e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Significant use,1,4000000,113138.0969599182,1293528.32,2.3965416666666668e-08,51200.0,162918859.6222822,73728000.0,"We’re releasing Fuyu-8B, a small version of the multimodal1 model that powers our product. The model is available on HuggingFace. We think Fuyu-8B is exciting because:

It has a much simpler architecture and training procedure than other multi-modal models, which makes it easier to understand, scale, and deploy.
It’s designed from the ground up for digital agents, so it can support arbitrary image resolutions, answer questions about graphs and diagrams, answer UI-based questions, and do fine-grained localization on screen images.
It’s fast - we can get responses for large images in less than 100 milliseconds.
Despite being optimized for our use-case, it performs well at standard image understanding benchmarks such as visual question-answering and natural-image-captioning.","https://www.adept.ai/blog/fuyu-8b, www.huggingface.co/adept/fuyu-8b",Fuyu-8B: A Multimodal Architecture for AI Agents
Fuyu-8B,Language,USA,Adept,2023-10-17,Industry,Image classification,Confident,128.0,Open weights (non-commercial),Not-defined,Unreleased,8000000000.0,2500000000000.0,1440.0,2.4e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Significant use,1,4000000,113138.0969599182,1293528.32,2.3965416666666668e-08,51200.0,162918859.6222822,73728000.0,"We’re releasing Fuyu-8B, a small version of the multimodal1 model that powers our product. The model is available on HuggingFace. We think Fuyu-8B is exciting because:

It has a much simpler architecture and training procedure than other multi-modal models, which makes it easier to understand, scale, and deploy.
It’s designed from the ground up for digital agents, so it can support arbitrary image resolutions, answer questions about graphs and diagrams, answer UI-based questions, and do fine-grained localization on screen images.
It’s fast - we can get responses for large images in less than 100 milliseconds.
Despite being optimized for our use-case, it performs well at standard image understanding benchmarks such as visual question-answering and natural-image-captioning.","https://www.adept.ai/blog/fuyu-8b, www.huggingface.co/adept/fuyu-8b",Fuyu-8B: A Multimodal Architecture for AI Agents
Fuyu-8B,Language,USA,Adept,2023-10-17,Industry,Visual question answering,Confident,128.0,Open weights (non-commercial),Not-defined,Unreleased,8000000000.0,2500000000000.0,1440.0,2.4e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Significant use,1,4000000,113138.0969599182,1293528.32,2.3965416666666668e-08,51200.0,162918859.6222822,73728000.0,"We’re releasing Fuyu-8B, a small version of the multimodal1 model that powers our product. The model is available on HuggingFace. We think Fuyu-8B is exciting because:

It has a much simpler architecture and training procedure than other multi-modal models, which makes it easier to understand, scale, and deploy.
It’s designed from the ground up for digital agents, so it can support arbitrary image resolutions, answer questions about graphs and diagrams, answer UI-based questions, and do fine-grained localization on screen images.
It’s fast - we can get responses for large images in less than 100 milliseconds.
Despite being optimized for our use-case, it performs well at standard image understanding benchmarks such as visual question-answering and natural-image-captioning.","https://www.adept.ai/blog/fuyu-8b, www.huggingface.co/adept/fuyu-8b",Fuyu-8B: A Multimodal Architecture for AI Agents
Fuyu-8B,Multimodal,USA,Adept,2023-10-17,Industry,Chat,Confident,128.0,Open weights (non-commercial),Not-defined,Unreleased,8000000000.0,2500000000000.0,1440.0,2.4e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Significant use,1,4000000,113138.0969599182,1293528.32,2.3965416666666668e-08,51200.0,162918859.6222822,73728000.0,"We’re releasing Fuyu-8B, a small version of the multimodal1 model that powers our product. The model is available on HuggingFace. We think Fuyu-8B is exciting because:

It has a much simpler architecture and training procedure than other multi-modal models, which makes it easier to understand, scale, and deploy.
It’s designed from the ground up for digital agents, so it can support arbitrary image resolutions, answer questions about graphs and diagrams, answer UI-based questions, and do fine-grained localization on screen images.
It’s fast - we can get responses for large images in less than 100 milliseconds.
Despite being optimized for our use-case, it performs well at standard image understanding benchmarks such as visual question-answering and natural-image-captioning.","https://www.adept.ai/blog/fuyu-8b, www.huggingface.co/adept/fuyu-8b",Fuyu-8B: A Multimodal Architecture for AI Agents
Fuyu-8B,Multimodal,USA,Adept,2023-10-17,Industry,Image captioning,Confident,128.0,Open weights (non-commercial),Not-defined,Unreleased,8000000000.0,2500000000000.0,1440.0,2.4e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Significant use,1,4000000,113138.0969599182,1293528.32,2.3965416666666668e-08,51200.0,162918859.6222822,73728000.0,"We’re releasing Fuyu-8B, a small version of the multimodal1 model that powers our product. The model is available on HuggingFace. We think Fuyu-8B is exciting because:

It has a much simpler architecture and training procedure than other multi-modal models, which makes it easier to understand, scale, and deploy.
It’s designed from the ground up for digital agents, so it can support arbitrary image resolutions, answer questions about graphs and diagrams, answer UI-based questions, and do fine-grained localization on screen images.
It’s fast - we can get responses for large images in less than 100 milliseconds.
Despite being optimized for our use-case, it performs well at standard image understanding benchmarks such as visual question-answering and natural-image-captioning.","https://www.adept.ai/blog/fuyu-8b, www.huggingface.co/adept/fuyu-8b",Fuyu-8B: A Multimodal Architecture for AI Agents
Fuyu-8B,Multimodal,USA,Adept,2023-10-17,Industry,Image classification,Confident,128.0,Open weights (non-commercial),Not-defined,Unreleased,8000000000.0,2500000000000.0,1440.0,2.4e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Significant use,1,4000000,113138.0969599182,1293528.32,2.3965416666666668e-08,51200.0,162918859.6222822,73728000.0,"We’re releasing Fuyu-8B, a small version of the multimodal1 model that powers our product. The model is available on HuggingFace. We think Fuyu-8B is exciting because:

It has a much simpler architecture and training procedure than other multi-modal models, which makes it easier to understand, scale, and deploy.
It’s designed from the ground up for digital agents, so it can support arbitrary image resolutions, answer questions about graphs and diagrams, answer UI-based questions, and do fine-grained localization on screen images.
It’s fast - we can get responses for large images in less than 100 milliseconds.
Despite being optimized for our use-case, it performs well at standard image understanding benchmarks such as visual question-answering and natural-image-captioning.","https://www.adept.ai/blog/fuyu-8b, www.huggingface.co/adept/fuyu-8b",Fuyu-8B: A Multimodal Architecture for AI Agents
Fuyu-8B,Multimodal,USA,Adept,2023-10-17,Industry,Visual question answering,Confident,128.0,Open weights (non-commercial),Not-defined,Unreleased,8000000000.0,2500000000000.0,1440.0,2.4e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Significant use,1,4000000,113138.0969599182,1293528.32,2.3965416666666668e-08,51200.0,162918859.6222822,73728000.0,"We’re releasing Fuyu-8B, a small version of the multimodal1 model that powers our product. The model is available on HuggingFace. We think Fuyu-8B is exciting because:

It has a much simpler architecture and training procedure than other multi-modal models, which makes it easier to understand, scale, and deploy.
It’s designed from the ground up for digital agents, so it can support arbitrary image resolutions, answer questions about graphs and diagrams, answer UI-based questions, and do fine-grained localization on screen images.
It’s fast - we can get responses for large images in less than 100 milliseconds.
Despite being optimized for our use-case, it performs well at standard image understanding benchmarks such as visual question-answering and natural-image-captioning.","https://www.adept.ai/blog/fuyu-8b, www.huggingface.co/adept/fuyu-8b",Fuyu-8B: A Multimodal Architecture for AI Agents
Fuyu-8B,Vision,USA,Adept,2023-10-17,Industry,Chat,Confident,128.0,Open weights (non-commercial),Not-defined,Unreleased,8000000000.0,2500000000000.0,1440.0,2.4e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Significant use,1,4000000,113138.0969599182,1293528.32,2.3965416666666668e-08,51200.0,162918859.6222822,73728000.0,"We’re releasing Fuyu-8B, a small version of the multimodal1 model that powers our product. The model is available on HuggingFace. We think Fuyu-8B is exciting because:

It has a much simpler architecture and training procedure than other multi-modal models, which makes it easier to understand, scale, and deploy.
It’s designed from the ground up for digital agents, so it can support arbitrary image resolutions, answer questions about graphs and diagrams, answer UI-based questions, and do fine-grained localization on screen images.
It’s fast - we can get responses for large images in less than 100 milliseconds.
Despite being optimized for our use-case, it performs well at standard image understanding benchmarks such as visual question-answering and natural-image-captioning.","https://www.adept.ai/blog/fuyu-8b, www.huggingface.co/adept/fuyu-8b",Fuyu-8B: A Multimodal Architecture for AI Agents
Fuyu-8B,Vision,USA,Adept,2023-10-17,Industry,Image captioning,Confident,128.0,Open weights (non-commercial),Not-defined,Unreleased,8000000000.0,2500000000000.0,1440.0,2.4e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Significant use,1,4000000,113138.0969599182,1293528.32,2.3965416666666668e-08,51200.0,162918859.6222822,73728000.0,"We’re releasing Fuyu-8B, a small version of the multimodal1 model that powers our product. The model is available on HuggingFace. We think Fuyu-8B is exciting because:

It has a much simpler architecture and training procedure than other multi-modal models, which makes it easier to understand, scale, and deploy.
It’s designed from the ground up for digital agents, so it can support arbitrary image resolutions, answer questions about graphs and diagrams, answer UI-based questions, and do fine-grained localization on screen images.
It’s fast - we can get responses for large images in less than 100 milliseconds.
Despite being optimized for our use-case, it performs well at standard image understanding benchmarks such as visual question-answering and natural-image-captioning.","https://www.adept.ai/blog/fuyu-8b, www.huggingface.co/adept/fuyu-8b",Fuyu-8B: A Multimodal Architecture for AI Agents
Fuyu-8B,Vision,USA,Adept,2023-10-17,Industry,Image classification,Confident,128.0,Open weights (non-commercial),Not-defined,Unreleased,8000000000.0,2500000000000.0,1440.0,2.4e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Significant use,1,4000000,113138.0969599182,1293528.32,2.3965416666666668e-08,51200.0,162918859.6222822,73728000.0,"We’re releasing Fuyu-8B, a small version of the multimodal1 model that powers our product. The model is available on HuggingFace. We think Fuyu-8B is exciting because:

It has a much simpler architecture and training procedure than other multi-modal models, which makes it easier to understand, scale, and deploy.
It’s designed from the ground up for digital agents, so it can support arbitrary image resolutions, answer questions about graphs and diagrams, answer UI-based questions, and do fine-grained localization on screen images.
It’s fast - we can get responses for large images in less than 100 milliseconds.
Despite being optimized for our use-case, it performs well at standard image understanding benchmarks such as visual question-answering and natural-image-captioning.","https://www.adept.ai/blog/fuyu-8b, www.huggingface.co/adept/fuyu-8b",Fuyu-8B: A Multimodal Architecture for AI Agents
Fuyu-8B,Vision,USA,Adept,2023-10-17,Industry,Visual question answering,Confident,128.0,Open weights (non-commercial),Not-defined,Unreleased,8000000000.0,2500000000000.0,1440.0,2.4e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Significant use,1,4000000,113138.0969599182,1293528.32,2.3965416666666668e-08,51200.0,162918859.6222822,73728000.0,"We’re releasing Fuyu-8B, a small version of the multimodal1 model that powers our product. The model is available on HuggingFace. We think Fuyu-8B is exciting because:

It has a much simpler architecture and training procedure than other multi-modal models, which makes it easier to understand, scale, and deploy.
It’s designed from the ground up for digital agents, so it can support arbitrary image resolutions, answer questions about graphs and diagrams, answer UI-based questions, and do fine-grained localization on screen images.
It’s fast - we can get responses for large images in less than 100 milliseconds.
Despite being optimized for our use-case, it performs well at standard image understanding benchmarks such as visual question-answering and natural-image-captioning.","https://www.adept.ai/blog/fuyu-8b, www.huggingface.co/adept/fuyu-8b",Fuyu-8B: A Multimodal Architecture for AI Agents
GLM-130B,Language,China,Tsinghua University,2022-08-04,Academia,Language modeling/generation,Confident,768.0,Open weights (non-commercial),"The Pile,WuDao Corpora",Unreleased,130000000000.0,400000000000.0,1440.0,3.5490054945e+23,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,1,8650752,681845.1304541394,11520000.0,0.325,307200.0,981856987.8539608,442368000.0,"GLM-130B (ICLR 2023) is an open bilingual (English & Chinese) bidirectional dense model with 130 billion parameters, pre-trained using the General Language Model (GLM) algorithm1. It is designed to support inference tasks with the 130B parameters on a single A100 (40G * 8) or V100 (32G * 8) server. As of July 3rd, 2022, GLM-130B has been trained on over 400 billion text tokens (200B each for Chinese and English) ",https://keg.cs.tsinghua.edu.cn/glm-130b/posts/glm-130b/,GLM-130B: An Open Bilingual Pre-trained Model
GLM-130B,Language,China,Tsinghua University,2022-08-04,Academia,Translation,Confident,768.0,Open weights (non-commercial),"The Pile,WuDao Corpora",Unreleased,130000000000.0,400000000000.0,1440.0,3.5490054945e+23,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,1,8650752,681845.1304541394,11520000.0,0.325,307200.0,981856987.8539608,442368000.0,"GLM-130B (ICLR 2023) is an open bilingual (English & Chinese) bidirectional dense model with 130 billion parameters, pre-trained using the General Language Model (GLM) algorithm1. It is designed to support inference tasks with the 130B parameters on a single A100 (40G * 8) or V100 (32G * 8) server. As of July 3rd, 2022, GLM-130B has been trained on over 400 billion text tokens (200B each for Chinese and English) ",https://keg.cs.tsinghua.edu.cn/glm-130b/posts/glm-130b/,GLM-130B: An Open Bilingual Pre-trained Model
GLM-4,Image generation,China,Zhipu AI,2024-01-17,Industry,Code generation,Confident,768.0,Hosted access (no API),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4,Image generation,China,Zhipu AI,2024-01-17,Industry,Image generation,Confident,768.0,Hosted access (no API),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4,Image generation,China,Zhipu AI,2024-01-17,Industry,Language modeling/generation,Confident,768.0,Hosted access (no API),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4,Image generation,China,Zhipu AI,2024-01-17,Industry,Question answering,Confident,768.0,Hosted access (no API),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4,Image generation,China,Zhipu AI,2024-01-17,Industry,Text-to-image,Confident,768.0,Hosted access (no API),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4,Language,China,Zhipu AI,2024-01-17,Industry,Code generation,Confident,768.0,Hosted access (no API),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4,Language,China,Zhipu AI,2024-01-17,Industry,Image generation,Confident,768.0,Hosted access (no API),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4,Language,China,Zhipu AI,2024-01-17,Industry,Language modeling/generation,Confident,768.0,Hosted access (no API),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4,Language,China,Zhipu AI,2024-01-17,Industry,Question answering,Confident,768.0,Hosted access (no API),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4,Language,China,Zhipu AI,2024-01-17,Industry,Text-to-image,Confident,768.0,Hosted access (no API),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4,Multimodal,China,Zhipu AI,2024-01-17,Industry,Code generation,Confident,768.0,Hosted access (no API),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4,Multimodal,China,Zhipu AI,2024-01-17,Industry,Image generation,Confident,768.0,Hosted access (no API),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4,Multimodal,China,Zhipu AI,2024-01-17,Industry,Language modeling/generation,Confident,768.0,Hosted access (no API),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4,Multimodal,China,Zhipu AI,2024-01-17,Industry,Question answering,Confident,768.0,Hosted access (no API),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4,Multimodal,China,Zhipu AI,2024-01-17,Industry,Text-to-image,Confident,768.0,Hosted access (no API),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4 (0116),Language,China,Zhipu AI,2024-01-17,Industry,Code generation,Likely,768.0,API access,Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,1.2e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,1,8650752,788306.6236983632,11520000.0,4.793383333333333e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.","https://arxiv.org/abs/2406.12793
https://zhipuai.cn/en/devday",ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4 (0116),Language,China,Zhipu AI,2024-01-17,Industry,Language modeling/generation,Likely,768.0,API access,Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,1.2e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,1,8650752,788306.6236983632,11520000.0,4.793383333333333e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.","https://arxiv.org/abs/2406.12793
https://zhipuai.cn/en/devday",ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4 (0116),Language,China,Zhipu AI,2024-01-17,Industry,Quantitative reasoning,Likely,768.0,API access,Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,1.2e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,1,8650752,788306.6236983632,11520000.0,4.793383333333333e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.","https://arxiv.org/abs/2406.12793
https://zhipuai.cn/en/devday",ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4 (0116),Language,China,Zhipu AI,2024-01-17,Industry,Question answering,Likely,768.0,API access,Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,1.2e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,1,8650752,788306.6236983632,11520000.0,4.793383333333333e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.","https://arxiv.org/abs/2406.12793
https://zhipuai.cn/en/devday",ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4 (0116),Language,China,Zhipu AI,2024-01-17,Industry,Translation,Likely,768.0,API access,Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,1.2e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,1,8650752,788306.6236983632,11520000.0,4.793383333333333e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.","https://arxiv.org/abs/2406.12793
https://zhipuai.cn/en/devday",ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4-Plus,Language,China,Zhipu AI,2024-08-29,Industry,Language modeling,Unknown,768.0,API access,Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"At the KDD International Conference on Data Mining and Knowledge Discovery, the Zhipu GLM team unveiled the new generation of base large model—GLM-4-Plus. As the latest version of Zhipu’s fully self-developed GLM large model, GLM-4-Plus signifies Zhipu AI’s continuous dedication in the field of general artificial intelligence, advancing the independent and autonomous innovation of large model technology.",https://bigmodel.cn/dev/howuse/glm-4,GLM-4-Plus
GLM-4V-9B,Language,China,Tsinghua University,2024-06-18,Academia,Code generation,Likely,768.0,Open weights (non-commercial),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4V-9B,Language,China,Tsinghua University,2024-06-18,Academia,Language modeling/generation,Likely,768.0,Open weights (non-commercial),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4V-9B,Language,China,Tsinghua University,2024-06-18,Academia,Question answering,Likely,768.0,Open weights (non-commercial),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4V-9B,Language,China,Tsinghua University,2024-06-18,Academia,Translation,Likely,768.0,Open weights (non-commercial),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4V-9B,Language,China,Tsinghua University,2024-06-18,Academia,Visual question answering,Likely,768.0,Open weights (non-commercial),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4V-9B,Language,China,Tsinghua University,2024-06-18,Industry,Code generation,Likely,768.0,Open weights (non-commercial),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4V-9B,Language,China,Tsinghua University,2024-06-18,Industry,Language modeling/generation,Likely,768.0,Open weights (non-commercial),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4V-9B,Language,China,Tsinghua University,2024-06-18,Industry,Question answering,Likely,768.0,Open weights (non-commercial),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4V-9B,Language,China,Tsinghua University,2024-06-18,Industry,Translation,Likely,768.0,Open weights (non-commercial),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4V-9B,Language,China,Tsinghua University,2024-06-18,Industry,Visual question answering,Likely,768.0,Open weights (non-commercial),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4V-9B,Language,China,Zhipu AI,2024-06-18,Academia,Code generation,Likely,768.0,Open weights (non-commercial),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4V-9B,Language,China,Zhipu AI,2024-06-18,Academia,Language modeling/generation,Likely,768.0,Open weights (non-commercial),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4V-9B,Language,China,Zhipu AI,2024-06-18,Academia,Question answering,Likely,768.0,Open weights (non-commercial),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4V-9B,Language,China,Zhipu AI,2024-06-18,Academia,Translation,Likely,768.0,Open weights (non-commercial),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4V-9B,Language,China,Zhipu AI,2024-06-18,Academia,Visual question answering,Likely,768.0,Open weights (non-commercial),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4V-9B,Language,China,Zhipu AI,2024-06-18,Industry,Code generation,Likely,768.0,Open weights (non-commercial),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4V-9B,Language,China,Zhipu AI,2024-06-18,Industry,Language modeling/generation,Likely,768.0,Open weights (non-commercial),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4V-9B,Language,China,Zhipu AI,2024-06-18,Industry,Question answering,Likely,768.0,Open weights (non-commercial),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4V-9B,Language,China,Zhipu AI,2024-06-18,Industry,Translation,Likely,768.0,Open weights (non-commercial),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4V-9B,Language,China,Zhipu AI,2024-06-18,Industry,Visual question answering,Likely,768.0,Open weights (non-commercial),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4V-9B,Multimodal,China,Tsinghua University,2024-06-18,Academia,Code generation,Likely,768.0,Open weights (non-commercial),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4V-9B,Multimodal,China,Tsinghua University,2024-06-18,Academia,Language modeling/generation,Likely,768.0,Open weights (non-commercial),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4V-9B,Multimodal,China,Tsinghua University,2024-06-18,Academia,Question answering,Likely,768.0,Open weights (non-commercial),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4V-9B,Multimodal,China,Tsinghua University,2024-06-18,Academia,Translation,Likely,768.0,Open weights (non-commercial),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4V-9B,Multimodal,China,Tsinghua University,2024-06-18,Academia,Visual question answering,Likely,768.0,Open weights (non-commercial),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4V-9B,Multimodal,China,Tsinghua University,2024-06-18,Industry,Code generation,Likely,768.0,Open weights (non-commercial),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4V-9B,Multimodal,China,Tsinghua University,2024-06-18,Industry,Language modeling/generation,Likely,768.0,Open weights (non-commercial),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4V-9B,Multimodal,China,Tsinghua University,2024-06-18,Industry,Question answering,Likely,768.0,Open weights (non-commercial),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4V-9B,Multimodal,China,Tsinghua University,2024-06-18,Industry,Translation,Likely,768.0,Open weights (non-commercial),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4V-9B,Multimodal,China,Tsinghua University,2024-06-18,Industry,Visual question answering,Likely,768.0,Open weights (non-commercial),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4V-9B,Multimodal,China,Zhipu AI,2024-06-18,Academia,Code generation,Likely,768.0,Open weights (non-commercial),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4V-9B,Multimodal,China,Zhipu AI,2024-06-18,Academia,Language modeling/generation,Likely,768.0,Open weights (non-commercial),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4V-9B,Multimodal,China,Zhipu AI,2024-06-18,Academia,Question answering,Likely,768.0,Open weights (non-commercial),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4V-9B,Multimodal,China,Zhipu AI,2024-06-18,Academia,Translation,Likely,768.0,Open weights (non-commercial),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4V-9B,Multimodal,China,Zhipu AI,2024-06-18,Academia,Visual question answering,Likely,768.0,Open weights (non-commercial),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4V-9B,Multimodal,China,Zhipu AI,2024-06-18,Industry,Code generation,Likely,768.0,Open weights (non-commercial),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4V-9B,Multimodal,China,Zhipu AI,2024-06-18,Industry,Language modeling/generation,Likely,768.0,Open weights (non-commercial),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4V-9B,Multimodal,China,Zhipu AI,2024-06-18,Industry,Question answering,Likely,768.0,Open weights (non-commercial),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4V-9B,Multimodal,China,Zhipu AI,2024-06-18,Industry,Translation,Likely,768.0,Open weights (non-commercial),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4V-9B,Multimodal,China,Zhipu AI,2024-06-18,Industry,Visual question answering,Likely,768.0,Open weights (non-commercial),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4V-9B,Vision,China,Tsinghua University,2024-06-18,Academia,Code generation,Likely,768.0,Open weights (non-commercial),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4V-9B,Vision,China,Tsinghua University,2024-06-18,Academia,Language modeling/generation,Likely,768.0,Open weights (non-commercial),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4V-9B,Vision,China,Tsinghua University,2024-06-18,Academia,Question answering,Likely,768.0,Open weights (non-commercial),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4V-9B,Vision,China,Tsinghua University,2024-06-18,Academia,Translation,Likely,768.0,Open weights (non-commercial),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4V-9B,Vision,China,Tsinghua University,2024-06-18,Academia,Visual question answering,Likely,768.0,Open weights (non-commercial),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4V-9B,Vision,China,Tsinghua University,2024-06-18,Industry,Code generation,Likely,768.0,Open weights (non-commercial),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4V-9B,Vision,China,Tsinghua University,2024-06-18,Industry,Language modeling/generation,Likely,768.0,Open weights (non-commercial),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4V-9B,Vision,China,Tsinghua University,2024-06-18,Industry,Question answering,Likely,768.0,Open weights (non-commercial),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4V-9B,Vision,China,Tsinghua University,2024-06-18,Industry,Translation,Likely,768.0,Open weights (non-commercial),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4V-9B,Vision,China,Tsinghua University,2024-06-18,Industry,Visual question answering,Likely,768.0,Open weights (non-commercial),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4V-9B,Vision,China,Zhipu AI,2024-06-18,Academia,Code generation,Likely,768.0,Open weights (non-commercial),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4V-9B,Vision,China,Zhipu AI,2024-06-18,Academia,Language modeling/generation,Likely,768.0,Open weights (non-commercial),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4V-9B,Vision,China,Zhipu AI,2024-06-18,Academia,Question answering,Likely,768.0,Open weights (non-commercial),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4V-9B,Vision,China,Zhipu AI,2024-06-18,Academia,Translation,Likely,768.0,Open weights (non-commercial),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4V-9B,Vision,China,Zhipu AI,2024-06-18,Academia,Visual question answering,Likely,768.0,Open weights (non-commercial),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4V-9B,Vision,China,Zhipu AI,2024-06-18,Industry,Code generation,Likely,768.0,Open weights (non-commercial),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4V-9B,Vision,China,Zhipu AI,2024-06-18,Industry,Language modeling/generation,Likely,768.0,Open weights (non-commercial),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4V-9B,Vision,China,Zhipu AI,2024-06-18,Industry,Question answering,Likely,768.0,Open weights (non-commercial),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4V-9B,Vision,China,Zhipu AI,2024-06-18,Industry,Translation,Likely,768.0,Open weights (non-commercial),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLM-4V-9B,Vision,China,Zhipu AI,2024-06-18,Industry,Visual question answering,Likely,768.0,Open weights (non-commercial),Not-defined,Not-defined,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",https://arxiv.org/abs/2406.12793,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
GLaM,Language,USA,Google,2021-12-13,Industry,Language modeling/generation,Confident,1024.0,Unreleased,"Wikipedia,GLaM dataset",Unreleased,1200000000000.0,600000000000.0,1366.0,3.6363112434e+23,1877.1621580193864,6796.58,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,SOTA improvement,4,1000000,437413.9513787687,6959697.92,0.286963696,196608.0,597507457.5833981,268566528.0,"Scaling language models with more data, compute and parameters has driven significant progress in natural language processing. For example, thanks to scaling, GPT-3 was able to achieve strong results on in-context learning tasks. However, training these large dense models requires significant amounts of computing resources. In this paper, we propose and develop a family of language models named GLaM (Generalist Language Model), which uses a sparsely activated mixture-of-experts architecture to scale the model capacity while also incurring substantially less training cost compared to dense variants. The largest GLaM has 1.2 trillion parameters, which is approximately 7x larger than GPT-3. It consumes only 1/3 of the energy used to train GPT-3 and requires half of the computation flops for inference, while still achieving better overall zero-shot and one-shot performance across 29 NLP tasks.",https://arxiv.org/abs/2112.06905,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts
GLaM,Language,USA,Google,2021-12-13,Industry,Question answering,Confident,1024.0,Unreleased,"Wikipedia,GLaM dataset",Unreleased,1200000000000.0,600000000000.0,1366.0,3.6363112434e+23,1877.1621580193864,6796.58,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,SOTA improvement,4,1000000,437413.9513787687,6959697.92,0.286963696,196608.0,597507457.5833981,268566528.0,"Scaling language models with more data, compute and parameters has driven significant progress in natural language processing. For example, thanks to scaling, GPT-3 was able to achieve strong results on in-context learning tasks. However, training these large dense models requires significant amounts of computing resources. In this paper, we propose and develop a family of language models named GLaM (Generalist Language Model), which uses a sparsely activated mixture-of-experts architecture to scale the model capacity while also incurring substantially less training cost compared to dense variants. The largest GLaM has 1.2 trillion parameters, which is approximately 7x larger than GPT-3. It consumes only 1/3 of the energy used to train GPT-3 and requires half of the computation flops for inference, while still achieving better overall zero-shot and one-shot performance across 29 NLP tasks.",https://arxiv.org/abs/2112.06905,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts
GPT-3 175B (davinci),Language,USA,OpenAI,2020-05-28,Industry,Language modeling/generation,Confident,10000.0,API access,"Common Crawl,WebText2,Wikipedia,Books1,Books2",Unreleased,175000000000.0,374000000000.0,355.2,3.14e+23,1877.1621580193864,11373.83,250.0,179834000000000.0,236742000000000.0,32000000000.0,NVIDIA Tesla V100 DGXS 32 GB,Highly cited,0,3200000,5595164.712836602,113738300.0,0.1968,2500000.0,1987402505.9995608,888000000.0,"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",https://arxiv.org/abs/2005.14165,Language Models are Few-Shot Learners
GPT-3 175B (davinci),Language,USA,OpenAI,2020-05-28,Industry,Language modeling/generation,Confident,10000.0,API access,"Common Crawl,WebText2,Wikipedia,Books1,Books2",Unreleased,175000000000.0,374000000000.0,355.2,3.14e+23,1877.1621580193864,11373.83,250.0,179834000000000.0,236742000000000.0,32000000000.0,NVIDIA Tesla V100 DGXS 32 GB,Training cost,0,3200000,5595164.712836602,113738300.0,0.1968,2500000.0,1987402505.9995608,888000000.0,"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",https://arxiv.org/abs/2005.14165,Language Models are Few-Shot Learners
GPT-3 175B (davinci),Language,USA,OpenAI,2020-05-28,Industry,Text autocompletion,Confident,10000.0,API access,"Common Crawl,WebText2,Wikipedia,Books1,Books2",Unreleased,175000000000.0,374000000000.0,355.2,3.14e+23,1877.1621580193864,11373.83,250.0,179834000000000.0,236742000000000.0,32000000000.0,NVIDIA Tesla V100 DGXS 32 GB,Highly cited,0,3200000,5595164.712836602,113738300.0,0.1968,2500000.0,1987402505.9995608,888000000.0,"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",https://arxiv.org/abs/2005.14165,Language Models are Few-Shot Learners
GPT-3 175B (davinci),Language,USA,OpenAI,2020-05-28,Industry,Text autocompletion,Confident,10000.0,API access,"Common Crawl,WebText2,Wikipedia,Books1,Books2",Unreleased,175000000000.0,374000000000.0,355.2,3.14e+23,1877.1621580193864,11373.83,250.0,179834000000000.0,236742000000000.0,32000000000.0,NVIDIA Tesla V100 DGXS 32 GB,Training cost,0,3200000,5595164.712836602,113738300.0,0.1968,2500000.0,1987402505.9995608,888000000.0,"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",https://arxiv.org/abs/2005.14165,Language Models are Few-Shot Learners
GPT-3.5,Language,USA,OpenAI,2022-11-28,Industry,Language modeling,Speculative,25000.0,API access,Not-defined,Unreleased,20000000000.0,4900000000000.0,2280.0,2.578e+24,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Historical significance,2,3200000,788306.6236983632,375000000.0,2.231210240496509e-09,10000000.0,1797339102.032268,22800000000.0,,https://platform.openai.com/docs/models/gpt-3-5,
GPT-3.5,Language,USA,OpenAI,2022-11-28,Industry,Language modeling,Speculative,25000.0,API access,Not-defined,Unreleased,20000000000.0,4900000000000.0,2280.0,2.578e+24,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Significant use,2,3200000,788306.6236983632,375000000.0,2.231210240496509e-09,10000000.0,1797339102.032268,22800000000.0,,https://platform.openai.com/docs/models/gpt-3-5,
GPT-3.5,Language,USA,OpenAI,2022-11-28,Industry,Language modeling,Speculative,25000.0,API access,Not-defined,Unreleased,20000000000.0,4900000000000.0,2280.0,2.578e+24,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,788306.6236983632,375000000.0,2.231210240496509e-09,10000000.0,1797339102.032268,22800000000.0,,https://platform.openai.com/docs/models/gpt-3-5,
GPT-3.5,Language,USA,OpenAI,2022-11-28,Industry,Language modeling,Speculative,25000.0,API access,Not-defined,Unreleased,20000000000.0,4900000000000.0,2280.0,2.578e+24,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,2,3200000,788306.6236983632,375000000.0,2.231210240496509e-09,10000000.0,1797339102.032268,22800000000.0,,https://platform.openai.com/docs/models/gpt-3-5,
GPT-3.5 Turbo,Language,USA,OpenAI,2022-11-30,Industry,Language modeling/generation,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,2.2e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Historical significance,2,3200000,788306.6236983632,375000000.0,2.6145727272727273e-10,10000000.0,1797339102.032268,22800000000.0,,https://platform.openai.com/docs/models,"A fast, inexpensive model for simple tasks"
GPT-3.5 Turbo,Language,USA,OpenAI,2022-11-30,Industry,Language modeling/generation,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,2.2e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Significant use,2,3200000,788306.6236983632,375000000.0,2.6145727272727273e-10,10000000.0,1797339102.032268,22800000000.0,,https://platform.openai.com/docs/models,"A fast, inexpensive model for simple tasks"
GPT-3.5 Turbo,Language,USA,OpenAI,2022-11-30,Industry,Question answering,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,2.2e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Historical significance,2,3200000,788306.6236983632,375000000.0,2.6145727272727273e-10,10000000.0,1797339102.032268,22800000000.0,,https://platform.openai.com/docs/models,"A fast, inexpensive model for simple tasks"
GPT-3.5 Turbo,Language,USA,OpenAI,2022-11-30,Industry,Question answering,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,2.2e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Significant use,2,3200000,788306.6236983632,375000000.0,2.6145727272727273e-10,10000000.0,1797339102.032268,22800000000.0,,https://platform.openai.com/docs/models,"A fast, inexpensive model for simple tasks"
GPT-4,Image generation,USA,OpenAI,2023-03-15,Industry,Language modeling,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,2.1e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Highly cited,2,3200000,22146708.683295924,375000000.0,0.34,10000000.0,50494495797.91471,22800000000.0,"We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",https://arxiv.org/abs/2303.08774,GPT-4 Technical Report
GPT-4,Image generation,USA,OpenAI,2023-03-15,Industry,Language modeling,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,2.1e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,22146708.683295924,375000000.0,0.34,10000000.0,50494495797.91471,22800000000.0,"We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",https://arxiv.org/abs/2303.08774,GPT-4 Technical Report
GPT-4,Image generation,USA,OpenAI,2023-03-15,Industry,Language modeling,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,2.1e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,2,3200000,22146708.683295924,375000000.0,0.34,10000000.0,50494495797.91471,22800000000.0,"We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",https://arxiv.org/abs/2303.08774,GPT-4 Technical Report
GPT-4,Language,USA,OpenAI,2023-03-15,Industry,Language modeling,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,2.1e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Highly cited,2,3200000,22146708.683295924,375000000.0,0.34,10000000.0,50494495797.91471,22800000000.0,"We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",https://arxiv.org/abs/2303.08774,GPT-4 Technical Report
GPT-4,Language,USA,OpenAI,2023-03-15,Industry,Language modeling,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,2.1e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,22146708.683295924,375000000.0,0.34,10000000.0,50494495797.91471,22800000000.0,"We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",https://arxiv.org/abs/2303.08774,GPT-4 Technical Report
GPT-4,Language,USA,OpenAI,2023-03-15,Industry,Language modeling,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,2.1e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,2,3200000,22146708.683295924,375000000.0,0.34,10000000.0,50494495797.91471,22800000000.0,"We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",https://arxiv.org/abs/2303.08774,GPT-4 Technical Report
GPT-4,Multimodal,USA,OpenAI,2023-03-15,Industry,Language modeling,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,2.1e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Highly cited,2,3200000,22146708.683295924,375000000.0,0.34,10000000.0,50494495797.91471,22800000000.0,"We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",https://arxiv.org/abs/2303.08774,GPT-4 Technical Report
GPT-4,Multimodal,USA,OpenAI,2023-03-15,Industry,Language modeling,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,2.1e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,22146708.683295924,375000000.0,0.34,10000000.0,50494495797.91471,22800000000.0,"We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",https://arxiv.org/abs/2303.08774,GPT-4 Technical Report
GPT-4,Multimodal,USA,OpenAI,2023-03-15,Industry,Language modeling,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,2.1e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,2,3200000,22146708.683295924,375000000.0,0.34,10000000.0,50494495797.91471,22800000000.0,"We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",https://arxiv.org/abs/2303.08774,GPT-4 Technical Report
GPT-4,Vision,USA,OpenAI,2023-03-15,Industry,Language modeling,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,2.1e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Highly cited,2,3200000,22146708.683295924,375000000.0,0.34,10000000.0,50494495797.91471,22800000000.0,"We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",https://arxiv.org/abs/2303.08774,GPT-4 Technical Report
GPT-4,Vision,USA,OpenAI,2023-03-15,Industry,Language modeling,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,2.1e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,22146708.683295924,375000000.0,0.34,10000000.0,50494495797.91471,22800000000.0,"We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",https://arxiv.org/abs/2303.08774,GPT-4 Technical Report
GPT-4,Vision,USA,OpenAI,2023-03-15,Industry,Language modeling,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,2.1e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,2,3200000,22146708.683295924,375000000.0,0.34,10000000.0,50494495797.91471,22800000000.0,"We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",https://arxiv.org/abs/2303.08774,GPT-4 Technical Report
GPT-4 Turbo,Image generation,USA,OpenAI,2023-11-06,Industry,Chat,Unknown,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,2.2e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,788306.6236983632,375000000.0,2.6145727272727273e-10,10000000.0,1797339102.032268,22800000000.0,"Today, we shared dozens of new additions and improvements, and reduced pricing across many parts of our platform. These include:

New GPT-4 Turbo model that is more capable, cheaper and supports a 128K context window",https://openai.com/blog/new-models-and-developer-products-announced-at-devday,New models and developer products announced at DevDay
GPT-4 Turbo,Image generation,USA,OpenAI,2023-11-06,Industry,Image captioning,Unknown,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,2.2e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,788306.6236983632,375000000.0,2.6145727272727273e-10,10000000.0,1797339102.032268,22800000000.0,"Today, we shared dozens of new additions and improvements, and reduced pricing across many parts of our platform. These include:

New GPT-4 Turbo model that is more capable, cheaper and supports a 128K context window",https://openai.com/blog/new-models-and-developer-products-announced-at-devday,New models and developer products announced at DevDay
GPT-4 Turbo,Image generation,USA,OpenAI,2023-11-06,Industry,Image generation,Unknown,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,2.2e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,788306.6236983632,375000000.0,2.6145727272727273e-10,10000000.0,1797339102.032268,22800000000.0,"Today, we shared dozens of new additions and improvements, and reduced pricing across many parts of our platform. These include:

New GPT-4 Turbo model that is more capable, cheaper and supports a 128K context window",https://openai.com/blog/new-models-and-developer-products-announced-at-devday,New models and developer products announced at DevDay
GPT-4 Turbo,Image generation,USA,OpenAI,2023-11-06,Industry,Language modeling/generation,Unknown,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,2.2e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,788306.6236983632,375000000.0,2.6145727272727273e-10,10000000.0,1797339102.032268,22800000000.0,"Today, we shared dozens of new additions and improvements, and reduced pricing across many parts of our platform. These include:

New GPT-4 Turbo model that is more capable, cheaper and supports a 128K context window",https://openai.com/blog/new-models-and-developer-products-announced-at-devday,New models and developer products announced at DevDay
GPT-4 Turbo,Image generation,USA,OpenAI,2023-11-06,Industry,Speech synthesis,Unknown,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,2.2e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,788306.6236983632,375000000.0,2.6145727272727273e-10,10000000.0,1797339102.032268,22800000000.0,"Today, we shared dozens of new additions and improvements, and reduced pricing across many parts of our platform. These include:

New GPT-4 Turbo model that is more capable, cheaper and supports a 128K context window",https://openai.com/blog/new-models-and-developer-products-announced-at-devday,New models and developer products announced at DevDay
GPT-4 Turbo,Image generation,USA,OpenAI,2023-11-06,Industry,Table tasks,Unknown,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,2.2e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,788306.6236983632,375000000.0,2.6145727272727273e-10,10000000.0,1797339102.032268,22800000000.0,"Today, we shared dozens of new additions and improvements, and reduced pricing across many parts of our platform. These include:

New GPT-4 Turbo model that is more capable, cheaper and supports a 128K context window",https://openai.com/blog/new-models-and-developer-products-announced-at-devday,New models and developer products announced at DevDay
GPT-4 Turbo,Image generation,USA,OpenAI,2023-11-06,Industry,Visual question answering,Unknown,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,2.2e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,788306.6236983632,375000000.0,2.6145727272727273e-10,10000000.0,1797339102.032268,22800000000.0,"Today, we shared dozens of new additions and improvements, and reduced pricing across many parts of our platform. These include:

New GPT-4 Turbo model that is more capable, cheaper and supports a 128K context window",https://openai.com/blog/new-models-and-developer-products-announced-at-devday,New models and developer products announced at DevDay
GPT-4 Turbo,Language,USA,OpenAI,2023-11-06,Industry,Chat,Unknown,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,2.2e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,788306.6236983632,375000000.0,2.6145727272727273e-10,10000000.0,1797339102.032268,22800000000.0,"Today, we shared dozens of new additions and improvements, and reduced pricing across many parts of our platform. These include:

New GPT-4 Turbo model that is more capable, cheaper and supports a 128K context window",https://openai.com/blog/new-models-and-developer-products-announced-at-devday,New models and developer products announced at DevDay
GPT-4 Turbo,Language,USA,OpenAI,2023-11-06,Industry,Image captioning,Unknown,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,2.2e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,788306.6236983632,375000000.0,2.6145727272727273e-10,10000000.0,1797339102.032268,22800000000.0,"Today, we shared dozens of new additions and improvements, and reduced pricing across many parts of our platform. These include:

New GPT-4 Turbo model that is more capable, cheaper and supports a 128K context window",https://openai.com/blog/new-models-and-developer-products-announced-at-devday,New models and developer products announced at DevDay
GPT-4 Turbo,Language,USA,OpenAI,2023-11-06,Industry,Image generation,Unknown,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,2.2e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,788306.6236983632,375000000.0,2.6145727272727273e-10,10000000.0,1797339102.032268,22800000000.0,"Today, we shared dozens of new additions and improvements, and reduced pricing across many parts of our platform. These include:

New GPT-4 Turbo model that is more capable, cheaper and supports a 128K context window",https://openai.com/blog/new-models-and-developer-products-announced-at-devday,New models and developer products announced at DevDay
GPT-4 Turbo,Language,USA,OpenAI,2023-11-06,Industry,Language modeling/generation,Unknown,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,2.2e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,788306.6236983632,375000000.0,2.6145727272727273e-10,10000000.0,1797339102.032268,22800000000.0,"Today, we shared dozens of new additions and improvements, and reduced pricing across many parts of our platform. These include:

New GPT-4 Turbo model that is more capable, cheaper and supports a 128K context window",https://openai.com/blog/new-models-and-developer-products-announced-at-devday,New models and developer products announced at DevDay
GPT-4 Turbo,Language,USA,OpenAI,2023-11-06,Industry,Speech synthesis,Unknown,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,2.2e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,788306.6236983632,375000000.0,2.6145727272727273e-10,10000000.0,1797339102.032268,22800000000.0,"Today, we shared dozens of new additions and improvements, and reduced pricing across many parts of our platform. These include:

New GPT-4 Turbo model that is more capable, cheaper and supports a 128K context window",https://openai.com/blog/new-models-and-developer-products-announced-at-devday,New models and developer products announced at DevDay
GPT-4 Turbo,Language,USA,OpenAI,2023-11-06,Industry,Table tasks,Unknown,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,2.2e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,788306.6236983632,375000000.0,2.6145727272727273e-10,10000000.0,1797339102.032268,22800000000.0,"Today, we shared dozens of new additions and improvements, and reduced pricing across many parts of our platform. These include:

New GPT-4 Turbo model that is more capable, cheaper and supports a 128K context window",https://openai.com/blog/new-models-and-developer-products-announced-at-devday,New models and developer products announced at DevDay
GPT-4 Turbo,Language,USA,OpenAI,2023-11-06,Industry,Visual question answering,Unknown,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,2.2e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,788306.6236983632,375000000.0,2.6145727272727273e-10,10000000.0,1797339102.032268,22800000000.0,"Today, we shared dozens of new additions and improvements, and reduced pricing across many parts of our platform. These include:

New GPT-4 Turbo model that is more capable, cheaper and supports a 128K context window",https://openai.com/blog/new-models-and-developer-products-announced-at-devday,New models and developer products announced at DevDay
GPT-4 Turbo,Multimodal,USA,OpenAI,2023-11-06,Industry,Chat,Unknown,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,2.2e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,788306.6236983632,375000000.0,2.6145727272727273e-10,10000000.0,1797339102.032268,22800000000.0,"Today, we shared dozens of new additions and improvements, and reduced pricing across many parts of our platform. These include:

New GPT-4 Turbo model that is more capable, cheaper and supports a 128K context window",https://openai.com/blog/new-models-and-developer-products-announced-at-devday,New models and developer products announced at DevDay
GPT-4 Turbo,Multimodal,USA,OpenAI,2023-11-06,Industry,Image captioning,Unknown,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,2.2e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,788306.6236983632,375000000.0,2.6145727272727273e-10,10000000.0,1797339102.032268,22800000000.0,"Today, we shared dozens of new additions and improvements, and reduced pricing across many parts of our platform. These include:

New GPT-4 Turbo model that is more capable, cheaper and supports a 128K context window",https://openai.com/blog/new-models-and-developer-products-announced-at-devday,New models and developer products announced at DevDay
GPT-4 Turbo,Multimodal,USA,OpenAI,2023-11-06,Industry,Image generation,Unknown,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,2.2e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,788306.6236983632,375000000.0,2.6145727272727273e-10,10000000.0,1797339102.032268,22800000000.0,"Today, we shared dozens of new additions and improvements, and reduced pricing across many parts of our platform. These include:

New GPT-4 Turbo model that is more capable, cheaper and supports a 128K context window",https://openai.com/blog/new-models-and-developer-products-announced-at-devday,New models and developer products announced at DevDay
GPT-4 Turbo,Multimodal,USA,OpenAI,2023-11-06,Industry,Language modeling/generation,Unknown,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,2.2e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,788306.6236983632,375000000.0,2.6145727272727273e-10,10000000.0,1797339102.032268,22800000000.0,"Today, we shared dozens of new additions and improvements, and reduced pricing across many parts of our platform. These include:

New GPT-4 Turbo model that is more capable, cheaper and supports a 128K context window",https://openai.com/blog/new-models-and-developer-products-announced-at-devday,New models and developer products announced at DevDay
GPT-4 Turbo,Multimodal,USA,OpenAI,2023-11-06,Industry,Speech synthesis,Unknown,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,2.2e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,788306.6236983632,375000000.0,2.6145727272727273e-10,10000000.0,1797339102.032268,22800000000.0,"Today, we shared dozens of new additions and improvements, and reduced pricing across many parts of our platform. These include:

New GPT-4 Turbo model that is more capable, cheaper and supports a 128K context window",https://openai.com/blog/new-models-and-developer-products-announced-at-devday,New models and developer products announced at DevDay
GPT-4 Turbo,Multimodal,USA,OpenAI,2023-11-06,Industry,Table tasks,Unknown,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,2.2e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,788306.6236983632,375000000.0,2.6145727272727273e-10,10000000.0,1797339102.032268,22800000000.0,"Today, we shared dozens of new additions and improvements, and reduced pricing across many parts of our platform. These include:

New GPT-4 Turbo model that is more capable, cheaper and supports a 128K context window",https://openai.com/blog/new-models-and-developer-products-announced-at-devday,New models and developer products announced at DevDay
GPT-4 Turbo,Multimodal,USA,OpenAI,2023-11-06,Industry,Visual question answering,Unknown,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,2.2e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,788306.6236983632,375000000.0,2.6145727272727273e-10,10000000.0,1797339102.032268,22800000000.0,"Today, we shared dozens of new additions and improvements, and reduced pricing across many parts of our platform. These include:

New GPT-4 Turbo model that is more capable, cheaper and supports a 128K context window",https://openai.com/blog/new-models-and-developer-products-announced-at-devday,New models and developer products announced at DevDay
GPT-4 Turbo,Vision,USA,OpenAI,2023-11-06,Industry,Chat,Unknown,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,2.2e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,788306.6236983632,375000000.0,2.6145727272727273e-10,10000000.0,1797339102.032268,22800000000.0,"Today, we shared dozens of new additions and improvements, and reduced pricing across many parts of our platform. These include:

New GPT-4 Turbo model that is more capable, cheaper and supports a 128K context window",https://openai.com/blog/new-models-and-developer-products-announced-at-devday,New models and developer products announced at DevDay
GPT-4 Turbo,Vision,USA,OpenAI,2023-11-06,Industry,Image captioning,Unknown,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,2.2e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,788306.6236983632,375000000.0,2.6145727272727273e-10,10000000.0,1797339102.032268,22800000000.0,"Today, we shared dozens of new additions and improvements, and reduced pricing across many parts of our platform. These include:

New GPT-4 Turbo model that is more capable, cheaper and supports a 128K context window",https://openai.com/blog/new-models-and-developer-products-announced-at-devday,New models and developer products announced at DevDay
GPT-4 Turbo,Vision,USA,OpenAI,2023-11-06,Industry,Image generation,Unknown,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,2.2e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,788306.6236983632,375000000.0,2.6145727272727273e-10,10000000.0,1797339102.032268,22800000000.0,"Today, we shared dozens of new additions and improvements, and reduced pricing across many parts of our platform. These include:

New GPT-4 Turbo model that is more capable, cheaper and supports a 128K context window",https://openai.com/blog/new-models-and-developer-products-announced-at-devday,New models and developer products announced at DevDay
GPT-4 Turbo,Vision,USA,OpenAI,2023-11-06,Industry,Language modeling/generation,Unknown,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,2.2e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,788306.6236983632,375000000.0,2.6145727272727273e-10,10000000.0,1797339102.032268,22800000000.0,"Today, we shared dozens of new additions and improvements, and reduced pricing across many parts of our platform. These include:

New GPT-4 Turbo model that is more capable, cheaper and supports a 128K context window",https://openai.com/blog/new-models-and-developer-products-announced-at-devday,New models and developer products announced at DevDay
GPT-4 Turbo,Vision,USA,OpenAI,2023-11-06,Industry,Speech synthesis,Unknown,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,2.2e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,788306.6236983632,375000000.0,2.6145727272727273e-10,10000000.0,1797339102.032268,22800000000.0,"Today, we shared dozens of new additions and improvements, and reduced pricing across many parts of our platform. These include:

New GPT-4 Turbo model that is more capable, cheaper and supports a 128K context window",https://openai.com/blog/new-models-and-developer-products-announced-at-devday,New models and developer products announced at DevDay
GPT-4 Turbo,Vision,USA,OpenAI,2023-11-06,Industry,Table tasks,Unknown,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,2.2e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,788306.6236983632,375000000.0,2.6145727272727273e-10,10000000.0,1797339102.032268,22800000000.0,"Today, we shared dozens of new additions and improvements, and reduced pricing across many parts of our platform. These include:

New GPT-4 Turbo model that is more capable, cheaper and supports a 128K context window",https://openai.com/blog/new-models-and-developer-products-announced-at-devday,New models and developer products announced at DevDay
GPT-4 Turbo,Vision,USA,OpenAI,2023-11-06,Industry,Visual question answering,Unknown,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,2.2e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,788306.6236983632,375000000.0,2.6145727272727273e-10,10000000.0,1797339102.032268,22800000000.0,"Today, we shared dozens of new additions and improvements, and reduced pricing across many parts of our platform. These include:

New GPT-4 Turbo model that is more capable, cheaper and supports a 128K context window",https://openai.com/blog/new-models-and-developer-products-announced-at-devday,New models and developer products announced at DevDay
GPT-4.5,Language,USA,OpenAI,2025-02-27,Industry,Code generation,Unknown,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,2.578e+24,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,2,3200000,788306.6236983632,375000000.0,2.231210240496509e-09,10000000.0,1797339102.032268,22800000000.0,"We advance AI capabilities by scaling two complementary paradigms: unsupervised learning and reasoning. These represent two axes of intelligence.

Unsupervised learning increases world model accuracy and intuition. Models like GPT‑3.5, GPT‑4, and GPT‑4.5 advance this paradigm.
Scaling reasoning⁠, on the other hand, teaches models to think and produce a chain of thought before they respond, allowing them to tackle complex STEM or logic problems. Models like OpenAI o1 and OpenAI o3‑mini advance this paradigm.
GPT‑4.5 is an example of scaling unsupervised learning by scaling up compute and data, along with architecture and optimization innovations. GPT‑4.5 was trained on Microsoft Azure AI supercomputers. The result is a model that has broader knowledge and a deeper understanding of the world, leading to reduced hallucinations and more reliability across a wide range of topics.",https://openai.com/index/introducing-gpt-4-5/,Introducing GPT-4.5
GPT-4.5,Language,USA,OpenAI,2025-02-27,Industry,Instruction interpretation,Unknown,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,2.578e+24,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,2,3200000,788306.6236983632,375000000.0,2.231210240496509e-09,10000000.0,1797339102.032268,22800000000.0,"We advance AI capabilities by scaling two complementary paradigms: unsupervised learning and reasoning. These represent two axes of intelligence.

Unsupervised learning increases world model accuracy and intuition. Models like GPT‑3.5, GPT‑4, and GPT‑4.5 advance this paradigm.
Scaling reasoning⁠, on the other hand, teaches models to think and produce a chain of thought before they respond, allowing them to tackle complex STEM or logic problems. Models like OpenAI o1 and OpenAI o3‑mini advance this paradigm.
GPT‑4.5 is an example of scaling unsupervised learning by scaling up compute and data, along with architecture and optimization innovations. GPT‑4.5 was trained on Microsoft Azure AI supercomputers. The result is a model that has broader knowledge and a deeper understanding of the world, leading to reduced hallucinations and more reliability across a wide range of topics.",https://openai.com/index/introducing-gpt-4-5/,Introducing GPT-4.5
GPT-4.5,Language,USA,OpenAI,2025-02-27,Industry,Language modeling/generation,Unknown,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,2.578e+24,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,2,3200000,788306.6236983632,375000000.0,2.231210240496509e-09,10000000.0,1797339102.032268,22800000000.0,"We advance AI capabilities by scaling two complementary paradigms: unsupervised learning and reasoning. These represent two axes of intelligence.

Unsupervised learning increases world model accuracy and intuition. Models like GPT‑3.5, GPT‑4, and GPT‑4.5 advance this paradigm.
Scaling reasoning⁠, on the other hand, teaches models to think and produce a chain of thought before they respond, allowing them to tackle complex STEM or logic problems. Models like OpenAI o1 and OpenAI o3‑mini advance this paradigm.
GPT‑4.5 is an example of scaling unsupervised learning by scaling up compute and data, along with architecture and optimization innovations. GPT‑4.5 was trained on Microsoft Azure AI supercomputers. The result is a model that has broader knowledge and a deeper understanding of the world, leading to reduced hallucinations and more reliability across a wide range of topics.",https://openai.com/index/introducing-gpt-4-5/,Introducing GPT-4.5
GPT-4.5,Language,USA,OpenAI,2025-02-27,Industry,Quantitative reasoning,Unknown,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,2.578e+24,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,2,3200000,788306.6236983632,375000000.0,2.231210240496509e-09,10000000.0,1797339102.032268,22800000000.0,"We advance AI capabilities by scaling two complementary paradigms: unsupervised learning and reasoning. These represent two axes of intelligence.

Unsupervised learning increases world model accuracy and intuition. Models like GPT‑3.5, GPT‑4, and GPT‑4.5 advance this paradigm.
Scaling reasoning⁠, on the other hand, teaches models to think and produce a chain of thought before they respond, allowing them to tackle complex STEM or logic problems. Models like OpenAI o1 and OpenAI o3‑mini advance this paradigm.
GPT‑4.5 is an example of scaling unsupervised learning by scaling up compute and data, along with architecture and optimization innovations. GPT‑4.5 was trained on Microsoft Azure AI supercomputers. The result is a model that has broader knowledge and a deeper understanding of the world, leading to reduced hallucinations and more reliability across a wide range of topics.",https://openai.com/index/introducing-gpt-4-5/,Introducing GPT-4.5
GPT-4.5,Language,USA,OpenAI,2025-02-27,Industry,Question answering,Unknown,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,2.578e+24,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,2,3200000,788306.6236983632,375000000.0,2.231210240496509e-09,10000000.0,1797339102.032268,22800000000.0,"We advance AI capabilities by scaling two complementary paradigms: unsupervised learning and reasoning. These represent two axes of intelligence.

Unsupervised learning increases world model accuracy and intuition. Models like GPT‑3.5, GPT‑4, and GPT‑4.5 advance this paradigm.
Scaling reasoning⁠, on the other hand, teaches models to think and produce a chain of thought before they respond, allowing them to tackle complex STEM or logic problems. Models like OpenAI o1 and OpenAI o3‑mini advance this paradigm.
GPT‑4.5 is an example of scaling unsupervised learning by scaling up compute and data, along with architecture and optimization innovations. GPT‑4.5 was trained on Microsoft Azure AI supercomputers. The result is a model that has broader knowledge and a deeper understanding of the world, leading to reduced hallucinations and more reliability across a wide range of topics.",https://openai.com/index/introducing-gpt-4-5/,Introducing GPT-4.5
GPT-4.5,Language,USA,OpenAI,2025-02-27,Industry,Translation,Unknown,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,2.578e+24,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,2,3200000,788306.6236983632,375000000.0,2.231210240496509e-09,10000000.0,1797339102.032268,22800000000.0,"We advance AI capabilities by scaling two complementary paradigms: unsupervised learning and reasoning. These represent two axes of intelligence.

Unsupervised learning increases world model accuracy and intuition. Models like GPT‑3.5, GPT‑4, and GPT‑4.5 advance this paradigm.
Scaling reasoning⁠, on the other hand, teaches models to think and produce a chain of thought before they respond, allowing them to tackle complex STEM or logic problems. Models like OpenAI o1 and OpenAI o3‑mini advance this paradigm.
GPT‑4.5 is an example of scaling unsupervised learning by scaling up compute and data, along with architecture and optimization innovations. GPT‑4.5 was trained on Microsoft Azure AI supercomputers. The result is a model that has broader knowledge and a deeper understanding of the world, leading to reduced hallucinations and more reliability across a wide range of topics.",https://openai.com/index/introducing-gpt-4-5/,Introducing GPT-4.5
GPT-4.5,Language,USA,OpenAI,2025-02-27,Industry,Visual question answering,Unknown,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,2.578e+24,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,2,3200000,788306.6236983632,375000000.0,2.231210240496509e-09,10000000.0,1797339102.032268,22800000000.0,"We advance AI capabilities by scaling two complementary paradigms: unsupervised learning and reasoning. These represent two axes of intelligence.

Unsupervised learning increases world model accuracy and intuition. Models like GPT‑3.5, GPT‑4, and GPT‑4.5 advance this paradigm.
Scaling reasoning⁠, on the other hand, teaches models to think and produce a chain of thought before they respond, allowing them to tackle complex STEM or logic problems. Models like OpenAI o1 and OpenAI o3‑mini advance this paradigm.
GPT‑4.5 is an example of scaling unsupervised learning by scaling up compute and data, along with architecture and optimization innovations. GPT‑4.5 was trained on Microsoft Azure AI supercomputers. The result is a model that has broader knowledge and a deeper understanding of the world, leading to reduced hallucinations and more reliability across a wide range of topics.",https://openai.com/index/introducing-gpt-4-5/,Introducing GPT-4.5
GPT-4.5,Multimodal,USA,OpenAI,2025-02-27,Industry,Code generation,Unknown,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,2.578e+24,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,2,3200000,788306.6236983632,375000000.0,2.231210240496509e-09,10000000.0,1797339102.032268,22800000000.0,"We advance AI capabilities by scaling two complementary paradigms: unsupervised learning and reasoning. These represent two axes of intelligence.

Unsupervised learning increases world model accuracy and intuition. Models like GPT‑3.5, GPT‑4, and GPT‑4.5 advance this paradigm.
Scaling reasoning⁠, on the other hand, teaches models to think and produce a chain of thought before they respond, allowing them to tackle complex STEM or logic problems. Models like OpenAI o1 and OpenAI o3‑mini advance this paradigm.
GPT‑4.5 is an example of scaling unsupervised learning by scaling up compute and data, along with architecture and optimization innovations. GPT‑4.5 was trained on Microsoft Azure AI supercomputers. The result is a model that has broader knowledge and a deeper understanding of the world, leading to reduced hallucinations and more reliability across a wide range of topics.",https://openai.com/index/introducing-gpt-4-5/,Introducing GPT-4.5
GPT-4.5,Multimodal,USA,OpenAI,2025-02-27,Industry,Instruction interpretation,Unknown,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,2.578e+24,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,2,3200000,788306.6236983632,375000000.0,2.231210240496509e-09,10000000.0,1797339102.032268,22800000000.0,"We advance AI capabilities by scaling two complementary paradigms: unsupervised learning and reasoning. These represent two axes of intelligence.

Unsupervised learning increases world model accuracy and intuition. Models like GPT‑3.5, GPT‑4, and GPT‑4.5 advance this paradigm.
Scaling reasoning⁠, on the other hand, teaches models to think and produce a chain of thought before they respond, allowing them to tackle complex STEM or logic problems. Models like OpenAI o1 and OpenAI o3‑mini advance this paradigm.
GPT‑4.5 is an example of scaling unsupervised learning by scaling up compute and data, along with architecture and optimization innovations. GPT‑4.5 was trained on Microsoft Azure AI supercomputers. The result is a model that has broader knowledge and a deeper understanding of the world, leading to reduced hallucinations and more reliability across a wide range of topics.",https://openai.com/index/introducing-gpt-4-5/,Introducing GPT-4.5
GPT-4.5,Multimodal,USA,OpenAI,2025-02-27,Industry,Language modeling/generation,Unknown,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,2.578e+24,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,2,3200000,788306.6236983632,375000000.0,2.231210240496509e-09,10000000.0,1797339102.032268,22800000000.0,"We advance AI capabilities by scaling two complementary paradigms: unsupervised learning and reasoning. These represent two axes of intelligence.

Unsupervised learning increases world model accuracy and intuition. Models like GPT‑3.5, GPT‑4, and GPT‑4.5 advance this paradigm.
Scaling reasoning⁠, on the other hand, teaches models to think and produce a chain of thought before they respond, allowing them to tackle complex STEM or logic problems. Models like OpenAI o1 and OpenAI o3‑mini advance this paradigm.
GPT‑4.5 is an example of scaling unsupervised learning by scaling up compute and data, along with architecture and optimization innovations. GPT‑4.5 was trained on Microsoft Azure AI supercomputers. The result is a model that has broader knowledge and a deeper understanding of the world, leading to reduced hallucinations and more reliability across a wide range of topics.",https://openai.com/index/introducing-gpt-4-5/,Introducing GPT-4.5
GPT-4.5,Multimodal,USA,OpenAI,2025-02-27,Industry,Quantitative reasoning,Unknown,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,2.578e+24,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,2,3200000,788306.6236983632,375000000.0,2.231210240496509e-09,10000000.0,1797339102.032268,22800000000.0,"We advance AI capabilities by scaling two complementary paradigms: unsupervised learning and reasoning. These represent two axes of intelligence.

Unsupervised learning increases world model accuracy and intuition. Models like GPT‑3.5, GPT‑4, and GPT‑4.5 advance this paradigm.
Scaling reasoning⁠, on the other hand, teaches models to think and produce a chain of thought before they respond, allowing them to tackle complex STEM or logic problems. Models like OpenAI o1 and OpenAI o3‑mini advance this paradigm.
GPT‑4.5 is an example of scaling unsupervised learning by scaling up compute and data, along with architecture and optimization innovations. GPT‑4.5 was trained on Microsoft Azure AI supercomputers. The result is a model that has broader knowledge and a deeper understanding of the world, leading to reduced hallucinations and more reliability across a wide range of topics.",https://openai.com/index/introducing-gpt-4-5/,Introducing GPT-4.5
GPT-4.5,Multimodal,USA,OpenAI,2025-02-27,Industry,Question answering,Unknown,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,2.578e+24,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,2,3200000,788306.6236983632,375000000.0,2.231210240496509e-09,10000000.0,1797339102.032268,22800000000.0,"We advance AI capabilities by scaling two complementary paradigms: unsupervised learning and reasoning. These represent two axes of intelligence.

Unsupervised learning increases world model accuracy and intuition. Models like GPT‑3.5, GPT‑4, and GPT‑4.5 advance this paradigm.
Scaling reasoning⁠, on the other hand, teaches models to think and produce a chain of thought before they respond, allowing them to tackle complex STEM or logic problems. Models like OpenAI o1 and OpenAI o3‑mini advance this paradigm.
GPT‑4.5 is an example of scaling unsupervised learning by scaling up compute and data, along with architecture and optimization innovations. GPT‑4.5 was trained on Microsoft Azure AI supercomputers. The result is a model that has broader knowledge and a deeper understanding of the world, leading to reduced hallucinations and more reliability across a wide range of topics.",https://openai.com/index/introducing-gpt-4-5/,Introducing GPT-4.5
GPT-4.5,Multimodal,USA,OpenAI,2025-02-27,Industry,Translation,Unknown,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,2.578e+24,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,2,3200000,788306.6236983632,375000000.0,2.231210240496509e-09,10000000.0,1797339102.032268,22800000000.0,"We advance AI capabilities by scaling two complementary paradigms: unsupervised learning and reasoning. These represent two axes of intelligence.

Unsupervised learning increases world model accuracy and intuition. Models like GPT‑3.5, GPT‑4, and GPT‑4.5 advance this paradigm.
Scaling reasoning⁠, on the other hand, teaches models to think and produce a chain of thought before they respond, allowing them to tackle complex STEM or logic problems. Models like OpenAI o1 and OpenAI o3‑mini advance this paradigm.
GPT‑4.5 is an example of scaling unsupervised learning by scaling up compute and data, along with architecture and optimization innovations. GPT‑4.5 was trained on Microsoft Azure AI supercomputers. The result is a model that has broader knowledge and a deeper understanding of the world, leading to reduced hallucinations and more reliability across a wide range of topics.",https://openai.com/index/introducing-gpt-4-5/,Introducing GPT-4.5
GPT-4.5,Multimodal,USA,OpenAI,2025-02-27,Industry,Visual question answering,Unknown,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,2.578e+24,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,2,3200000,788306.6236983632,375000000.0,2.231210240496509e-09,10000000.0,1797339102.032268,22800000000.0,"We advance AI capabilities by scaling two complementary paradigms: unsupervised learning and reasoning. These represent two axes of intelligence.

Unsupervised learning increases world model accuracy and intuition. Models like GPT‑3.5, GPT‑4, and GPT‑4.5 advance this paradigm.
Scaling reasoning⁠, on the other hand, teaches models to think and produce a chain of thought before they respond, allowing them to tackle complex STEM or logic problems. Models like OpenAI o1 and OpenAI o3‑mini advance this paradigm.
GPT‑4.5 is an example of scaling unsupervised learning by scaling up compute and data, along with architecture and optimization innovations. GPT‑4.5 was trained on Microsoft Azure AI supercomputers. The result is a model that has broader knowledge and a deeper understanding of the world, leading to reduced hallucinations and more reliability across a wide range of topics.",https://openai.com/index/introducing-gpt-4-5/,Introducing GPT-4.5
GPT-4.5,Vision,USA,OpenAI,2025-02-27,Industry,Code generation,Unknown,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,2.578e+24,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,2,3200000,788306.6236983632,375000000.0,2.231210240496509e-09,10000000.0,1797339102.032268,22800000000.0,"We advance AI capabilities by scaling two complementary paradigms: unsupervised learning and reasoning. These represent two axes of intelligence.

Unsupervised learning increases world model accuracy and intuition. Models like GPT‑3.5, GPT‑4, and GPT‑4.5 advance this paradigm.
Scaling reasoning⁠, on the other hand, teaches models to think and produce a chain of thought before they respond, allowing them to tackle complex STEM or logic problems. Models like OpenAI o1 and OpenAI o3‑mini advance this paradigm.
GPT‑4.5 is an example of scaling unsupervised learning by scaling up compute and data, along with architecture and optimization innovations. GPT‑4.5 was trained on Microsoft Azure AI supercomputers. The result is a model that has broader knowledge and a deeper understanding of the world, leading to reduced hallucinations and more reliability across a wide range of topics.",https://openai.com/index/introducing-gpt-4-5/,Introducing GPT-4.5
GPT-4.5,Vision,USA,OpenAI,2025-02-27,Industry,Instruction interpretation,Unknown,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,2.578e+24,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,2,3200000,788306.6236983632,375000000.0,2.231210240496509e-09,10000000.0,1797339102.032268,22800000000.0,"We advance AI capabilities by scaling two complementary paradigms: unsupervised learning and reasoning. These represent two axes of intelligence.

Unsupervised learning increases world model accuracy and intuition. Models like GPT‑3.5, GPT‑4, and GPT‑4.5 advance this paradigm.
Scaling reasoning⁠, on the other hand, teaches models to think and produce a chain of thought before they respond, allowing them to tackle complex STEM or logic problems. Models like OpenAI o1 and OpenAI o3‑mini advance this paradigm.
GPT‑4.5 is an example of scaling unsupervised learning by scaling up compute and data, along with architecture and optimization innovations. GPT‑4.5 was trained on Microsoft Azure AI supercomputers. The result is a model that has broader knowledge and a deeper understanding of the world, leading to reduced hallucinations and more reliability across a wide range of topics.",https://openai.com/index/introducing-gpt-4-5/,Introducing GPT-4.5
GPT-4.5,Vision,USA,OpenAI,2025-02-27,Industry,Language modeling/generation,Unknown,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,2.578e+24,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,2,3200000,788306.6236983632,375000000.0,2.231210240496509e-09,10000000.0,1797339102.032268,22800000000.0,"We advance AI capabilities by scaling two complementary paradigms: unsupervised learning and reasoning. These represent two axes of intelligence.

Unsupervised learning increases world model accuracy and intuition. Models like GPT‑3.5, GPT‑4, and GPT‑4.5 advance this paradigm.
Scaling reasoning⁠, on the other hand, teaches models to think and produce a chain of thought before they respond, allowing them to tackle complex STEM or logic problems. Models like OpenAI o1 and OpenAI o3‑mini advance this paradigm.
GPT‑4.5 is an example of scaling unsupervised learning by scaling up compute and data, along with architecture and optimization innovations. GPT‑4.5 was trained on Microsoft Azure AI supercomputers. The result is a model that has broader knowledge and a deeper understanding of the world, leading to reduced hallucinations and more reliability across a wide range of topics.",https://openai.com/index/introducing-gpt-4-5/,Introducing GPT-4.5
GPT-4.5,Vision,USA,OpenAI,2025-02-27,Industry,Quantitative reasoning,Unknown,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,2.578e+24,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,2,3200000,788306.6236983632,375000000.0,2.231210240496509e-09,10000000.0,1797339102.032268,22800000000.0,"We advance AI capabilities by scaling two complementary paradigms: unsupervised learning and reasoning. These represent two axes of intelligence.

Unsupervised learning increases world model accuracy and intuition. Models like GPT‑3.5, GPT‑4, and GPT‑4.5 advance this paradigm.
Scaling reasoning⁠, on the other hand, teaches models to think and produce a chain of thought before they respond, allowing them to tackle complex STEM or logic problems. Models like OpenAI o1 and OpenAI o3‑mini advance this paradigm.
GPT‑4.5 is an example of scaling unsupervised learning by scaling up compute and data, along with architecture and optimization innovations. GPT‑4.5 was trained on Microsoft Azure AI supercomputers. The result is a model that has broader knowledge and a deeper understanding of the world, leading to reduced hallucinations and more reliability across a wide range of topics.",https://openai.com/index/introducing-gpt-4-5/,Introducing GPT-4.5
GPT-4.5,Vision,USA,OpenAI,2025-02-27,Industry,Question answering,Unknown,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,2.578e+24,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,2,3200000,788306.6236983632,375000000.0,2.231210240496509e-09,10000000.0,1797339102.032268,22800000000.0,"We advance AI capabilities by scaling two complementary paradigms: unsupervised learning and reasoning. These represent two axes of intelligence.

Unsupervised learning increases world model accuracy and intuition. Models like GPT‑3.5, GPT‑4, and GPT‑4.5 advance this paradigm.
Scaling reasoning⁠, on the other hand, teaches models to think and produce a chain of thought before they respond, allowing them to tackle complex STEM or logic problems. Models like OpenAI o1 and OpenAI o3‑mini advance this paradigm.
GPT‑4.5 is an example of scaling unsupervised learning by scaling up compute and data, along with architecture and optimization innovations. GPT‑4.5 was trained on Microsoft Azure AI supercomputers. The result is a model that has broader knowledge and a deeper understanding of the world, leading to reduced hallucinations and more reliability across a wide range of topics.",https://openai.com/index/introducing-gpt-4-5/,Introducing GPT-4.5
GPT-4.5,Vision,USA,OpenAI,2025-02-27,Industry,Translation,Unknown,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,2.578e+24,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,2,3200000,788306.6236983632,375000000.0,2.231210240496509e-09,10000000.0,1797339102.032268,22800000000.0,"We advance AI capabilities by scaling two complementary paradigms: unsupervised learning and reasoning. These represent two axes of intelligence.

Unsupervised learning increases world model accuracy and intuition. Models like GPT‑3.5, GPT‑4, and GPT‑4.5 advance this paradigm.
Scaling reasoning⁠, on the other hand, teaches models to think and produce a chain of thought before they respond, allowing them to tackle complex STEM or logic problems. Models like OpenAI o1 and OpenAI o3‑mini advance this paradigm.
GPT‑4.5 is an example of scaling unsupervised learning by scaling up compute and data, along with architecture and optimization innovations. GPT‑4.5 was trained on Microsoft Azure AI supercomputers. The result is a model that has broader knowledge and a deeper understanding of the world, leading to reduced hallucinations and more reliability across a wide range of topics.",https://openai.com/index/introducing-gpt-4-5/,Introducing GPT-4.5
GPT-4.5,Vision,USA,OpenAI,2025-02-27,Industry,Visual question answering,Unknown,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,2.578e+24,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,2,3200000,788306.6236983632,375000000.0,2.231210240496509e-09,10000000.0,1797339102.032268,22800000000.0,"We advance AI capabilities by scaling two complementary paradigms: unsupervised learning and reasoning. These represent two axes of intelligence.

Unsupervised learning increases world model accuracy and intuition. Models like GPT‑3.5, GPT‑4, and GPT‑4.5 advance this paradigm.
Scaling reasoning⁠, on the other hand, teaches models to think and produce a chain of thought before they respond, allowing them to tackle complex STEM or logic problems. Models like OpenAI o1 and OpenAI o3‑mini advance this paradigm.
GPT‑4.5 is an example of scaling unsupervised learning by scaling up compute and data, along with architecture and optimization innovations. GPT‑4.5 was trained on Microsoft Azure AI supercomputers. The result is a model that has broader knowledge and a deeper understanding of the world, leading to reduced hallucinations and more reliability across a wide range of topics.",https://openai.com/index/introducing-gpt-4-5/,Introducing GPT-4.5
GPT-4o,Audio,USA,OpenAI,2024-05-13,Industry,Audio generation,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Audio,USA,OpenAI,2024-05-13,Industry,Audio generation,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Significant use,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Audio,USA,OpenAI,2024-05-13,Industry,Chat,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Audio,USA,OpenAI,2024-05-13,Industry,Chat,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Significant use,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Audio,USA,OpenAI,2024-05-13,Industry,Image generation,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Audio,USA,OpenAI,2024-05-13,Industry,Image generation,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Significant use,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Audio,USA,OpenAI,2024-05-13,Industry,Language modeling/generation,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Audio,USA,OpenAI,2024-05-13,Industry,Language modeling/generation,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Significant use,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Audio,USA,OpenAI,2024-05-13,Industry,Question answering,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Audio,USA,OpenAI,2024-05-13,Industry,Question answering,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Significant use,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Audio,USA,OpenAI,2024-05-13,Industry,Speech recognition,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Audio,USA,OpenAI,2024-05-13,Industry,Speech recognition,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Significant use,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Audio,USA,OpenAI,2024-05-13,Industry,Table tasks,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Audio,USA,OpenAI,2024-05-13,Industry,Table tasks,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Significant use,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Audio,USA,OpenAI,2024-05-13,Industry,Vision-language generation,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Audio,USA,OpenAI,2024-05-13,Industry,Vision-language generation,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Significant use,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Language,USA,OpenAI,2024-05-13,Industry,Audio generation,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Language,USA,OpenAI,2024-05-13,Industry,Audio generation,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Significant use,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Language,USA,OpenAI,2024-05-13,Industry,Chat,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Language,USA,OpenAI,2024-05-13,Industry,Chat,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Significant use,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Language,USA,OpenAI,2024-05-13,Industry,Image generation,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Language,USA,OpenAI,2024-05-13,Industry,Image generation,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Significant use,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Language,USA,OpenAI,2024-05-13,Industry,Language modeling/generation,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Language,USA,OpenAI,2024-05-13,Industry,Language modeling/generation,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Significant use,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Language,USA,OpenAI,2024-05-13,Industry,Question answering,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Language,USA,OpenAI,2024-05-13,Industry,Question answering,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Significant use,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Language,USA,OpenAI,2024-05-13,Industry,Speech recognition,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Language,USA,OpenAI,2024-05-13,Industry,Speech recognition,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Significant use,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Language,USA,OpenAI,2024-05-13,Industry,Table tasks,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Language,USA,OpenAI,2024-05-13,Industry,Table tasks,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Significant use,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Language,USA,OpenAI,2024-05-13,Industry,Vision-language generation,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Language,USA,OpenAI,2024-05-13,Industry,Vision-language generation,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Significant use,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Multimodal,USA,OpenAI,2024-05-13,Industry,Audio generation,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Multimodal,USA,OpenAI,2024-05-13,Industry,Audio generation,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Significant use,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Multimodal,USA,OpenAI,2024-05-13,Industry,Chat,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Multimodal,USA,OpenAI,2024-05-13,Industry,Chat,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Significant use,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Multimodal,USA,OpenAI,2024-05-13,Industry,Image generation,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Multimodal,USA,OpenAI,2024-05-13,Industry,Image generation,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Significant use,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Multimodal,USA,OpenAI,2024-05-13,Industry,Language modeling/generation,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Multimodal,USA,OpenAI,2024-05-13,Industry,Language modeling/generation,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Significant use,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Multimodal,USA,OpenAI,2024-05-13,Industry,Question answering,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Multimodal,USA,OpenAI,2024-05-13,Industry,Question answering,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Significant use,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Multimodal,USA,OpenAI,2024-05-13,Industry,Speech recognition,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Multimodal,USA,OpenAI,2024-05-13,Industry,Speech recognition,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Significant use,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Multimodal,USA,OpenAI,2024-05-13,Industry,Table tasks,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Multimodal,USA,OpenAI,2024-05-13,Industry,Table tasks,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Significant use,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Multimodal,USA,OpenAI,2024-05-13,Industry,Vision-language generation,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Multimodal,USA,OpenAI,2024-05-13,Industry,Vision-language generation,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Significant use,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Speech,USA,OpenAI,2024-05-13,Industry,Audio generation,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Speech,USA,OpenAI,2024-05-13,Industry,Audio generation,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Significant use,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Speech,USA,OpenAI,2024-05-13,Industry,Chat,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Speech,USA,OpenAI,2024-05-13,Industry,Chat,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Significant use,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Speech,USA,OpenAI,2024-05-13,Industry,Image generation,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Speech,USA,OpenAI,2024-05-13,Industry,Image generation,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Significant use,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Speech,USA,OpenAI,2024-05-13,Industry,Language modeling/generation,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Speech,USA,OpenAI,2024-05-13,Industry,Language modeling/generation,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Significant use,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Speech,USA,OpenAI,2024-05-13,Industry,Question answering,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Speech,USA,OpenAI,2024-05-13,Industry,Question answering,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Significant use,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Speech,USA,OpenAI,2024-05-13,Industry,Speech recognition,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Speech,USA,OpenAI,2024-05-13,Industry,Speech recognition,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Significant use,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Speech,USA,OpenAI,2024-05-13,Industry,Table tasks,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Speech,USA,OpenAI,2024-05-13,Industry,Table tasks,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Significant use,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Speech,USA,OpenAI,2024-05-13,Industry,Vision-language generation,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Speech,USA,OpenAI,2024-05-13,Industry,Vision-language generation,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Significant use,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Vision,USA,OpenAI,2024-05-13,Industry,Audio generation,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Vision,USA,OpenAI,2024-05-13,Industry,Audio generation,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Significant use,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Vision,USA,OpenAI,2024-05-13,Industry,Chat,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Vision,USA,OpenAI,2024-05-13,Industry,Chat,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Significant use,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Vision,USA,OpenAI,2024-05-13,Industry,Image generation,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Vision,USA,OpenAI,2024-05-13,Industry,Image generation,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Significant use,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Vision,USA,OpenAI,2024-05-13,Industry,Language modeling/generation,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Vision,USA,OpenAI,2024-05-13,Industry,Language modeling/generation,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Significant use,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Vision,USA,OpenAI,2024-05-13,Industry,Question answering,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Vision,USA,OpenAI,2024-05-13,Industry,Question answering,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Significant use,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Vision,USA,OpenAI,2024-05-13,Industry,Speech recognition,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Vision,USA,OpenAI,2024-05-13,Industry,Speech recognition,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Significant use,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Vision,USA,OpenAI,2024-05-13,Industry,Table tasks,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Vision,USA,OpenAI,2024-05-13,Industry,Table tasks,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Significant use,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Vision,USA,OpenAI,2024-05-13,Industry,Vision-language generation,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o,Vision,USA,OpenAI,2024-05-13,Industry,Vision-language generation,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,3.810001e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Significant use,2,3200000,22146708.683295924,375000000.0,1.5097266378670244e-10,10000000.0,50494495797.91471,22800000000.0,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.","https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",Hello GPT-4o
GPT-4o mini,Language,USA,OpenAI,2024-07-18,Industry,Chat,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,7.360010000000001e+24,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Significant use,2,3200000,788306.6236983632,375000000.0,7.815288294445251e-10,10000000.0,1797339102.032268,22800000000.0,"OpenAI is committed to making intelligence as broadly accessible as possible. Today, we're announcing GPT-4o mini, our most cost-efficient small model. We expect GPT-4o mini will significantly expand the range of applications built with AI by making intelligence much more affordable. GPT-4o mini scores 82% on MMLU and currently outperforms GPT-41 on chat preferences in LMSYS leaderboard(opens in a new window). It is priced at 15 cents per million input tokens and 60 cents per million output tokens, an order of magnitude more affordable than previous frontier models and more than 60% cheaper than GPT-3.5 Turbo.",https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/,GPT-4o mini: advancing cost-efficient intelligence
GPT-4o mini,Language,USA,OpenAI,2024-07-18,Industry,Code generation,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,7.360010000000001e+24,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Significant use,2,3200000,788306.6236983632,375000000.0,7.815288294445251e-10,10000000.0,1797339102.032268,22800000000.0,"OpenAI is committed to making intelligence as broadly accessible as possible. Today, we're announcing GPT-4o mini, our most cost-efficient small model. We expect GPT-4o mini will significantly expand the range of applications built with AI by making intelligence much more affordable. GPT-4o mini scores 82% on MMLU and currently outperforms GPT-41 on chat preferences in LMSYS leaderboard(opens in a new window). It is priced at 15 cents per million input tokens and 60 cents per million output tokens, an order of magnitude more affordable than previous frontier models and more than 60% cheaper than GPT-3.5 Turbo.",https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/,GPT-4o mini: advancing cost-efficient intelligence
GPT-4o mini,Language,USA,OpenAI,2024-07-18,Industry,Language modeling/generation,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,7.360010000000001e+24,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Significant use,2,3200000,788306.6236983632,375000000.0,7.815288294445251e-10,10000000.0,1797339102.032268,22800000000.0,"OpenAI is committed to making intelligence as broadly accessible as possible. Today, we're announcing GPT-4o mini, our most cost-efficient small model. We expect GPT-4o mini will significantly expand the range of applications built with AI by making intelligence much more affordable. GPT-4o mini scores 82% on MMLU and currently outperforms GPT-41 on chat preferences in LMSYS leaderboard(opens in a new window). It is priced at 15 cents per million input tokens and 60 cents per million output tokens, an order of magnitude more affordable than previous frontier models and more than 60% cheaper than GPT-3.5 Turbo.",https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/,GPT-4o mini: advancing cost-efficient intelligence
GPT-4o mini,Language,USA,OpenAI,2024-07-18,Industry,Visual question answering,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,7.360010000000001e+24,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Significant use,2,3200000,788306.6236983632,375000000.0,7.815288294445251e-10,10000000.0,1797339102.032268,22800000000.0,"OpenAI is committed to making intelligence as broadly accessible as possible. Today, we're announcing GPT-4o mini, our most cost-efficient small model. We expect GPT-4o mini will significantly expand the range of applications built with AI by making intelligence much more affordable. GPT-4o mini scores 82% on MMLU and currently outperforms GPT-41 on chat preferences in LMSYS leaderboard(opens in a new window). It is priced at 15 cents per million input tokens and 60 cents per million output tokens, an order of magnitude more affordable than previous frontier models and more than 60% cheaper than GPT-3.5 Turbo.",https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/,GPT-4o mini: advancing cost-efficient intelligence
GPT-4o mini,Multimodal,USA,OpenAI,2024-07-18,Industry,Chat,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,7.360010000000001e+24,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Significant use,2,3200000,788306.6236983632,375000000.0,7.815288294445251e-10,10000000.0,1797339102.032268,22800000000.0,"OpenAI is committed to making intelligence as broadly accessible as possible. Today, we're announcing GPT-4o mini, our most cost-efficient small model. We expect GPT-4o mini will significantly expand the range of applications built with AI by making intelligence much more affordable. GPT-4o mini scores 82% on MMLU and currently outperforms GPT-41 on chat preferences in LMSYS leaderboard(opens in a new window). It is priced at 15 cents per million input tokens and 60 cents per million output tokens, an order of magnitude more affordable than previous frontier models and more than 60% cheaper than GPT-3.5 Turbo.",https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/,GPT-4o mini: advancing cost-efficient intelligence
GPT-4o mini,Multimodal,USA,OpenAI,2024-07-18,Industry,Code generation,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,7.360010000000001e+24,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Significant use,2,3200000,788306.6236983632,375000000.0,7.815288294445251e-10,10000000.0,1797339102.032268,22800000000.0,"OpenAI is committed to making intelligence as broadly accessible as possible. Today, we're announcing GPT-4o mini, our most cost-efficient small model. We expect GPT-4o mini will significantly expand the range of applications built with AI by making intelligence much more affordable. GPT-4o mini scores 82% on MMLU and currently outperforms GPT-41 on chat preferences in LMSYS leaderboard(opens in a new window). It is priced at 15 cents per million input tokens and 60 cents per million output tokens, an order of magnitude more affordable than previous frontier models and more than 60% cheaper than GPT-3.5 Turbo.",https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/,GPT-4o mini: advancing cost-efficient intelligence
GPT-4o mini,Multimodal,USA,OpenAI,2024-07-18,Industry,Language modeling/generation,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,7.360010000000001e+24,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Significant use,2,3200000,788306.6236983632,375000000.0,7.815288294445251e-10,10000000.0,1797339102.032268,22800000000.0,"OpenAI is committed to making intelligence as broadly accessible as possible. Today, we're announcing GPT-4o mini, our most cost-efficient small model. We expect GPT-4o mini will significantly expand the range of applications built with AI by making intelligence much more affordable. GPT-4o mini scores 82% on MMLU and currently outperforms GPT-41 on chat preferences in LMSYS leaderboard(opens in a new window). It is priced at 15 cents per million input tokens and 60 cents per million output tokens, an order of magnitude more affordable than previous frontier models and more than 60% cheaper than GPT-3.5 Turbo.",https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/,GPT-4o mini: advancing cost-efficient intelligence
GPT-4o mini,Multimodal,USA,OpenAI,2024-07-18,Industry,Visual question answering,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,7.360010000000001e+24,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Significant use,2,3200000,788306.6236983632,375000000.0,7.815288294445251e-10,10000000.0,1797339102.032268,22800000000.0,"OpenAI is committed to making intelligence as broadly accessible as possible. Today, we're announcing GPT-4o mini, our most cost-efficient small model. We expect GPT-4o mini will significantly expand the range of applications built with AI by making intelligence much more affordable. GPT-4o mini scores 82% on MMLU and currently outperforms GPT-41 on chat preferences in LMSYS leaderboard(opens in a new window). It is priced at 15 cents per million input tokens and 60 cents per million output tokens, an order of magnitude more affordable than previous frontier models and more than 60% cheaper than GPT-3.5 Turbo.",https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/,GPT-4o mini: advancing cost-efficient intelligence
GPT-4o mini,Vision,USA,OpenAI,2024-07-18,Industry,Chat,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,7.360010000000001e+24,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Significant use,2,3200000,788306.6236983632,375000000.0,7.815288294445251e-10,10000000.0,1797339102.032268,22800000000.0,"OpenAI is committed to making intelligence as broadly accessible as possible. Today, we're announcing GPT-4o mini, our most cost-efficient small model. We expect GPT-4o mini will significantly expand the range of applications built with AI by making intelligence much more affordable. GPT-4o mini scores 82% on MMLU and currently outperforms GPT-41 on chat preferences in LMSYS leaderboard(opens in a new window). It is priced at 15 cents per million input tokens and 60 cents per million output tokens, an order of magnitude more affordable than previous frontier models and more than 60% cheaper than GPT-3.5 Turbo.",https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/,GPT-4o mini: advancing cost-efficient intelligence
GPT-4o mini,Vision,USA,OpenAI,2024-07-18,Industry,Code generation,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,7.360010000000001e+24,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Significant use,2,3200000,788306.6236983632,375000000.0,7.815288294445251e-10,10000000.0,1797339102.032268,22800000000.0,"OpenAI is committed to making intelligence as broadly accessible as possible. Today, we're announcing GPT-4o mini, our most cost-efficient small model. We expect GPT-4o mini will significantly expand the range of applications built with AI by making intelligence much more affordable. GPT-4o mini scores 82% on MMLU and currently outperforms GPT-41 on chat preferences in LMSYS leaderboard(opens in a new window). It is priced at 15 cents per million input tokens and 60 cents per million output tokens, an order of magnitude more affordable than previous frontier models and more than 60% cheaper than GPT-3.5 Turbo.",https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/,GPT-4o mini: advancing cost-efficient intelligence
GPT-4o mini,Vision,USA,OpenAI,2024-07-18,Industry,Language modeling/generation,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,7.360010000000001e+24,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Significant use,2,3200000,788306.6236983632,375000000.0,7.815288294445251e-10,10000000.0,1797339102.032268,22800000000.0,"OpenAI is committed to making intelligence as broadly accessible as possible. Today, we're announcing GPT-4o mini, our most cost-efficient small model. We expect GPT-4o mini will significantly expand the range of applications built with AI by making intelligence much more affordable. GPT-4o mini scores 82% on MMLU and currently outperforms GPT-41 on chat preferences in LMSYS leaderboard(opens in a new window). It is priced at 15 cents per million input tokens and 60 cents per million output tokens, an order of magnitude more affordable than previous frontier models and more than 60% cheaper than GPT-3.5 Turbo.",https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/,GPT-4o mini: advancing cost-efficient intelligence
GPT-4o mini,Vision,USA,OpenAI,2024-07-18,Industry,Visual question answering,Speculative,25000.0,API access,Unspecified unreleased,Unreleased,20000000000.0,4900000000000.0,2280.0,7.360010000000001e+24,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Significant use,2,3200000,788306.6236983632,375000000.0,7.815288294445251e-10,10000000.0,1797339102.032268,22800000000.0,"OpenAI is committed to making intelligence as broadly accessible as possible. Today, we're announcing GPT-4o mini, our most cost-efficient small model. We expect GPT-4o mini will significantly expand the range of applications built with AI by making intelligence much more affordable. GPT-4o mini scores 82% on MMLU and currently outperforms GPT-41 on chat preferences in LMSYS leaderboard(opens in a new window). It is priced at 15 cents per million input tokens and 60 cents per million output tokens, an order of magnitude more affordable than previous frontier models and more than 60% cheaper than GPT-3.5 Turbo.",https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/,GPT-4o mini: advancing cost-efficient intelligence
Galactica,Biology,USA,Meta AI,2022-11-16,Industry,Language modeling,Likely,128.0,Open weights (non-commercial),Galactica Corpus,Unreleased,120000000000.0,106000000000.0,1282.4326530612243,3.24e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,SOTA improvement,4,2000000,113523.59989094557,1453512.96,1.7753271604938272e-08,51200.0,145586371.39320624,65660551.83673468,"Information overload is a major obstacle to scientific progress. The explosive growth in scientific literature and data has made it ever harder to discover useful insights in a large mass of information. Today scientific knowledge is accessed through search engines, but they are unable to organize scientific knowledge alone. In this paper we introduce Galactica: a large language model that can store, combine and reason about scientific knowledge. We train on a large scientific corpus of papers, reference material, knowledge bases and many other sources. We outperform existing models on a range of scientific tasks. On technical knowledge probes such as LaTeX equations, Galactica outperforms the latest GPT-3 by 68.2% versus 49.0%. Galactica also performs well on reasoning, outperforming Chinchilla on mathematical MMLU by 41.3% to 35.7%, and PaLM 540B on MATH with a score of 20.4% versus 8.8%. It also sets a new state-of-the-art on downstream tasks such as PubMedQA and MedMCQA dev of 77.6% and 52.9%. And despite not being trained on a general corpus, Galactica outperforms BLOOM and OPT-175B on BIG-bench. We believe these results demonstrate the potential for language models as a new interface for science. We open source the model for the benefit of the scientific community.",https://arxiv.org/abs/2211.09085,Galactica: A Large Language Model for Science
Galactica,Biology,USA,Meta AI,2022-11-16,Industry,Question answering,Likely,128.0,Open weights (non-commercial),Galactica Corpus,Unreleased,120000000000.0,106000000000.0,1282.4326530612243,3.24e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,SOTA improvement,4,2000000,113523.59989094557,1453512.96,1.7753271604938272e-08,51200.0,145586371.39320624,65660551.83673468,"Information overload is a major obstacle to scientific progress. The explosive growth in scientific literature and data has made it ever harder to discover useful insights in a large mass of information. Today scientific knowledge is accessed through search engines, but they are unable to organize scientific knowledge alone. In this paper we introduce Galactica: a large language model that can store, combine and reason about scientific knowledge. We train on a large scientific corpus of papers, reference material, knowledge bases and many other sources. We outperform existing models on a range of scientific tasks. On technical knowledge probes such as LaTeX equations, Galactica outperforms the latest GPT-3 by 68.2% versus 49.0%. Galactica also performs well on reasoning, outperforming Chinchilla on mathematical MMLU by 41.3% to 35.7%, and PaLM 540B on MATH with a score of 20.4% versus 8.8%. It also sets a new state-of-the-art on downstream tasks such as PubMedQA and MedMCQA dev of 77.6% and 52.9%. And despite not being trained on a general corpus, Galactica outperforms BLOOM and OPT-175B on BIG-bench. We believe these results demonstrate the potential for language models as a new interface for science. We open source the model for the benefit of the scientific community.",https://arxiv.org/abs/2211.09085,Galactica: A Large Language Model for Science
Galactica,Language,USA,Meta AI,2022-11-16,Industry,Language modeling,Likely,128.0,Open weights (non-commercial),Galactica Corpus,Unreleased,120000000000.0,106000000000.0,1282.4326530612243,3.24e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,SOTA improvement,4,2000000,113523.59989094557,1453512.96,1.7753271604938272e-08,51200.0,145586371.39320624,65660551.83673468,"Information overload is a major obstacle to scientific progress. The explosive growth in scientific literature and data has made it ever harder to discover useful insights in a large mass of information. Today scientific knowledge is accessed through search engines, but they are unable to organize scientific knowledge alone. In this paper we introduce Galactica: a large language model that can store, combine and reason about scientific knowledge. We train on a large scientific corpus of papers, reference material, knowledge bases and many other sources. We outperform existing models on a range of scientific tasks. On technical knowledge probes such as LaTeX equations, Galactica outperforms the latest GPT-3 by 68.2% versus 49.0%. Galactica also performs well on reasoning, outperforming Chinchilla on mathematical MMLU by 41.3% to 35.7%, and PaLM 540B on MATH with a score of 20.4% versus 8.8%. It also sets a new state-of-the-art on downstream tasks such as PubMedQA and MedMCQA dev of 77.6% and 52.9%. And despite not being trained on a general corpus, Galactica outperforms BLOOM and OPT-175B on BIG-bench. We believe these results demonstrate the potential for language models as a new interface for science. We open source the model for the benefit of the scientific community.",https://arxiv.org/abs/2211.09085,Galactica: A Large Language Model for Science
Galactica,Language,USA,Meta AI,2022-11-16,Industry,Question answering,Likely,128.0,Open weights (non-commercial),Galactica Corpus,Unreleased,120000000000.0,106000000000.0,1282.4326530612243,3.24e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,SOTA improvement,4,2000000,113523.59989094557,1453512.96,1.7753271604938272e-08,51200.0,145586371.39320624,65660551.83673468,"Information overload is a major obstacle to scientific progress. The explosive growth in scientific literature and data has made it ever harder to discover useful insights in a large mass of information. Today scientific knowledge is accessed through search engines, but they are unable to organize scientific knowledge alone. In this paper we introduce Galactica: a large language model that can store, combine and reason about scientific knowledge. We train on a large scientific corpus of papers, reference material, knowledge bases and many other sources. We outperform existing models on a range of scientific tasks. On technical knowledge probes such as LaTeX equations, Galactica outperforms the latest GPT-3 by 68.2% versus 49.0%. Galactica also performs well on reasoning, outperforming Chinchilla on mathematical MMLU by 41.3% to 35.7%, and PaLM 540B on MATH with a score of 20.4% versus 8.8%. It also sets a new state-of-the-art on downstream tasks such as PubMedQA and MedMCQA dev of 77.6% and 52.9%. And despite not being trained on a general corpus, Galactica outperforms BLOOM and OPT-175B on BIG-bench. We believe these results demonstrate the potential for language models as a new interface for science. We open source the model for the benefit of the scientific community.",https://arxiv.org/abs/2211.09085,Galactica: A Large Language Model for Science
Gemini 1.0 Pro,Language,Multinational,Google DeepMind,2023-12-06,Industry,Chat,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Significant use,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Pro,Language,Multinational,Google DeepMind,2023-12-06,Industry,Language modeling,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Significant use,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Pro,Language,Multinational,Google DeepMind,2023-12-06,Industry,Translation,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Significant use,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Pro,Language,Multinational,Google DeepMind,2023-12-06,Industry,Visual question answering,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Significant use,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Pro,Language,UK,Google DeepMind,2023-12-06,Industry,Chat,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Significant use,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Pro,Language,UK,Google DeepMind,2023-12-06,Industry,Language modeling,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Significant use,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Pro,Language,UK,Google DeepMind,2023-12-06,Industry,Translation,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Significant use,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Pro,Language,UK,Google DeepMind,2023-12-06,Industry,Visual question answering,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Significant use,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Pro,Language,USA,Google DeepMind,2023-12-06,Industry,Chat,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Significant use,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Pro,Language,USA,Google DeepMind,2023-12-06,Industry,Language modeling,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Significant use,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Pro,Language,USA,Google DeepMind,2023-12-06,Industry,Translation,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Significant use,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Pro,Language,USA,Google DeepMind,2023-12-06,Industry,Visual question answering,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Significant use,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Pro,Multimodal,Multinational,Google DeepMind,2023-12-06,Industry,Chat,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Significant use,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Pro,Multimodal,Multinational,Google DeepMind,2023-12-06,Industry,Language modeling,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Significant use,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Pro,Multimodal,Multinational,Google DeepMind,2023-12-06,Industry,Translation,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Significant use,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Pro,Multimodal,Multinational,Google DeepMind,2023-12-06,Industry,Visual question answering,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Significant use,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Pro,Multimodal,UK,Google DeepMind,2023-12-06,Industry,Chat,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Significant use,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Pro,Multimodal,UK,Google DeepMind,2023-12-06,Industry,Language modeling,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Significant use,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Pro,Multimodal,UK,Google DeepMind,2023-12-06,Industry,Translation,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Significant use,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Pro,Multimodal,UK,Google DeepMind,2023-12-06,Industry,Visual question answering,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Significant use,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Pro,Multimodal,USA,Google DeepMind,2023-12-06,Industry,Chat,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Significant use,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Pro,Multimodal,USA,Google DeepMind,2023-12-06,Industry,Language modeling,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Significant use,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Pro,Multimodal,USA,Google DeepMind,2023-12-06,Industry,Translation,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Significant use,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Pro,Multimodal,USA,Google DeepMind,2023-12-06,Industry,Visual question answering,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Significant use,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Pro,Vision,Multinational,Google DeepMind,2023-12-06,Industry,Chat,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Significant use,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Pro,Vision,Multinational,Google DeepMind,2023-12-06,Industry,Language modeling,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Significant use,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Pro,Vision,Multinational,Google DeepMind,2023-12-06,Industry,Translation,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Significant use,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Pro,Vision,Multinational,Google DeepMind,2023-12-06,Industry,Visual question answering,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Significant use,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Pro,Vision,UK,Google DeepMind,2023-12-06,Industry,Chat,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Significant use,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Pro,Vision,UK,Google DeepMind,2023-12-06,Industry,Language modeling,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Significant use,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Pro,Vision,UK,Google DeepMind,2023-12-06,Industry,Translation,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Significant use,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Pro,Vision,UK,Google DeepMind,2023-12-06,Industry,Visual question answering,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Significant use,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Pro,Vision,USA,Google DeepMind,2023-12-06,Industry,Chat,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Significant use,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Pro,Vision,USA,Google DeepMind,2023-12-06,Industry,Language modeling,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Significant use,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Pro,Vision,USA,Google DeepMind,2023-12-06,Industry,Translation,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Significant use,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Pro,Vision,USA,Google DeepMind,2023-12-06,Industry,Visual question answering,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Significant use,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Pro Vision,Language,Multinational,Google DeepMind,2024-02-15,Industry,Chat,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"Gemini 1.0 Pro Vision supports text, image, and video as inputs.

Max input tokens: 16,384
Max output tokens: 2,048
Max images per prompt: 16
Max video length: 2 minutes
Max videos per prompt: 1
Training data: Up to February 2023",,The best performing image and video understanding model to handle a broad range of applications. 
Gemini 1.0 Pro Vision,Language,Multinational,Google DeepMind,2024-02-15,Industry,Language modeling,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"Gemini 1.0 Pro Vision supports text, image, and video as inputs.

Max input tokens: 16,384
Max output tokens: 2,048
Max images per prompt: 16
Max video length: 2 minutes
Max videos per prompt: 1
Training data: Up to February 2023",,The best performing image and video understanding model to handle a broad range of applications. 
Gemini 1.0 Pro Vision,Language,Multinational,Google DeepMind,2024-02-15,Industry,Question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"Gemini 1.0 Pro Vision supports text, image, and video as inputs.

Max input tokens: 16,384
Max output tokens: 2,048
Max images per prompt: 16
Max video length: 2 minutes
Max videos per prompt: 1
Training data: Up to February 2023",,The best performing image and video understanding model to handle a broad range of applications. 
Gemini 1.0 Pro Vision,Language,Multinational,Google DeepMind,2024-02-15,Industry,Translation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"Gemini 1.0 Pro Vision supports text, image, and video as inputs.

Max input tokens: 16,384
Max output tokens: 2,048
Max images per prompt: 16
Max video length: 2 minutes
Max videos per prompt: 1
Training data: Up to February 2023",,The best performing image and video understanding model to handle a broad range of applications. 
Gemini 1.0 Pro Vision,Language,Multinational,Google DeepMind,2024-02-15,Industry,Video description,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"Gemini 1.0 Pro Vision supports text, image, and video as inputs.

Max input tokens: 16,384
Max output tokens: 2,048
Max images per prompt: 16
Max video length: 2 minutes
Max videos per prompt: 1
Training data: Up to February 2023",,The best performing image and video understanding model to handle a broad range of applications. 
Gemini 1.0 Pro Vision,Language,Multinational,Google DeepMind,2024-02-15,Industry,Visual question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"Gemini 1.0 Pro Vision supports text, image, and video as inputs.

Max input tokens: 16,384
Max output tokens: 2,048
Max images per prompt: 16
Max video length: 2 minutes
Max videos per prompt: 1
Training data: Up to February 2023",,The best performing image and video understanding model to handle a broad range of applications. 
Gemini 1.0 Pro Vision,Language,UK,Google DeepMind,2024-02-15,Industry,Chat,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"Gemini 1.0 Pro Vision supports text, image, and video as inputs.

Max input tokens: 16,384
Max output tokens: 2,048
Max images per prompt: 16
Max video length: 2 minutes
Max videos per prompt: 1
Training data: Up to February 2023",,The best performing image and video understanding model to handle a broad range of applications. 
Gemini 1.0 Pro Vision,Language,UK,Google DeepMind,2024-02-15,Industry,Language modeling,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"Gemini 1.0 Pro Vision supports text, image, and video as inputs.

Max input tokens: 16,384
Max output tokens: 2,048
Max images per prompt: 16
Max video length: 2 minutes
Max videos per prompt: 1
Training data: Up to February 2023",,The best performing image and video understanding model to handle a broad range of applications. 
Gemini 1.0 Pro Vision,Language,UK,Google DeepMind,2024-02-15,Industry,Question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"Gemini 1.0 Pro Vision supports text, image, and video as inputs.

Max input tokens: 16,384
Max output tokens: 2,048
Max images per prompt: 16
Max video length: 2 minutes
Max videos per prompt: 1
Training data: Up to February 2023",,The best performing image and video understanding model to handle a broad range of applications. 
Gemini 1.0 Pro Vision,Language,UK,Google DeepMind,2024-02-15,Industry,Translation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"Gemini 1.0 Pro Vision supports text, image, and video as inputs.

Max input tokens: 16,384
Max output tokens: 2,048
Max images per prompt: 16
Max video length: 2 minutes
Max videos per prompt: 1
Training data: Up to February 2023",,The best performing image and video understanding model to handle a broad range of applications. 
Gemini 1.0 Pro Vision,Language,UK,Google DeepMind,2024-02-15,Industry,Video description,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"Gemini 1.0 Pro Vision supports text, image, and video as inputs.

Max input tokens: 16,384
Max output tokens: 2,048
Max images per prompt: 16
Max video length: 2 minutes
Max videos per prompt: 1
Training data: Up to February 2023",,The best performing image and video understanding model to handle a broad range of applications. 
Gemini 1.0 Pro Vision,Language,UK,Google DeepMind,2024-02-15,Industry,Visual question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"Gemini 1.0 Pro Vision supports text, image, and video as inputs.

Max input tokens: 16,384
Max output tokens: 2,048
Max images per prompt: 16
Max video length: 2 minutes
Max videos per prompt: 1
Training data: Up to February 2023",,The best performing image and video understanding model to handle a broad range of applications. 
Gemini 1.0 Pro Vision,Language,USA,Google DeepMind,2024-02-15,Industry,Chat,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"Gemini 1.0 Pro Vision supports text, image, and video as inputs.

Max input tokens: 16,384
Max output tokens: 2,048
Max images per prompt: 16
Max video length: 2 minutes
Max videos per prompt: 1
Training data: Up to February 2023",,The best performing image and video understanding model to handle a broad range of applications. 
Gemini 1.0 Pro Vision,Language,USA,Google DeepMind,2024-02-15,Industry,Language modeling,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"Gemini 1.0 Pro Vision supports text, image, and video as inputs.

Max input tokens: 16,384
Max output tokens: 2,048
Max images per prompt: 16
Max video length: 2 minutes
Max videos per prompt: 1
Training data: Up to February 2023",,The best performing image and video understanding model to handle a broad range of applications. 
Gemini 1.0 Pro Vision,Language,USA,Google DeepMind,2024-02-15,Industry,Question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"Gemini 1.0 Pro Vision supports text, image, and video as inputs.

Max input tokens: 16,384
Max output tokens: 2,048
Max images per prompt: 16
Max video length: 2 minutes
Max videos per prompt: 1
Training data: Up to February 2023",,The best performing image and video understanding model to handle a broad range of applications. 
Gemini 1.0 Pro Vision,Language,USA,Google DeepMind,2024-02-15,Industry,Translation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"Gemini 1.0 Pro Vision supports text, image, and video as inputs.

Max input tokens: 16,384
Max output tokens: 2,048
Max images per prompt: 16
Max video length: 2 minutes
Max videos per prompt: 1
Training data: Up to February 2023",,The best performing image and video understanding model to handle a broad range of applications. 
Gemini 1.0 Pro Vision,Language,USA,Google DeepMind,2024-02-15,Industry,Video description,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"Gemini 1.0 Pro Vision supports text, image, and video as inputs.

Max input tokens: 16,384
Max output tokens: 2,048
Max images per prompt: 16
Max video length: 2 minutes
Max videos per prompt: 1
Training data: Up to February 2023",,The best performing image and video understanding model to handle a broad range of applications. 
Gemini 1.0 Pro Vision,Language,USA,Google DeepMind,2024-02-15,Industry,Visual question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"Gemini 1.0 Pro Vision supports text, image, and video as inputs.

Max input tokens: 16,384
Max output tokens: 2,048
Max images per prompt: 16
Max video length: 2 minutes
Max videos per prompt: 1
Training data: Up to February 2023",,The best performing image and video understanding model to handle a broad range of applications. 
Gemini 1.0 Pro Vision,Video,Multinational,Google DeepMind,2024-02-15,Industry,Chat,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"Gemini 1.0 Pro Vision supports text, image, and video as inputs.

Max input tokens: 16,384
Max output tokens: 2,048
Max images per prompt: 16
Max video length: 2 minutes
Max videos per prompt: 1
Training data: Up to February 2023",,The best performing image and video understanding model to handle a broad range of applications. 
Gemini 1.0 Pro Vision,Video,Multinational,Google DeepMind,2024-02-15,Industry,Language modeling,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"Gemini 1.0 Pro Vision supports text, image, and video as inputs.

Max input tokens: 16,384
Max output tokens: 2,048
Max images per prompt: 16
Max video length: 2 minutes
Max videos per prompt: 1
Training data: Up to February 2023",,The best performing image and video understanding model to handle a broad range of applications. 
Gemini 1.0 Pro Vision,Video,Multinational,Google DeepMind,2024-02-15,Industry,Question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"Gemini 1.0 Pro Vision supports text, image, and video as inputs.

Max input tokens: 16,384
Max output tokens: 2,048
Max images per prompt: 16
Max video length: 2 minutes
Max videos per prompt: 1
Training data: Up to February 2023",,The best performing image and video understanding model to handle a broad range of applications. 
Gemini 1.0 Pro Vision,Video,Multinational,Google DeepMind,2024-02-15,Industry,Translation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"Gemini 1.0 Pro Vision supports text, image, and video as inputs.

Max input tokens: 16,384
Max output tokens: 2,048
Max images per prompt: 16
Max video length: 2 minutes
Max videos per prompt: 1
Training data: Up to February 2023",,The best performing image and video understanding model to handle a broad range of applications. 
Gemini 1.0 Pro Vision,Video,Multinational,Google DeepMind,2024-02-15,Industry,Video description,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"Gemini 1.0 Pro Vision supports text, image, and video as inputs.

Max input tokens: 16,384
Max output tokens: 2,048
Max images per prompt: 16
Max video length: 2 minutes
Max videos per prompt: 1
Training data: Up to February 2023",,The best performing image and video understanding model to handle a broad range of applications. 
Gemini 1.0 Pro Vision,Video,Multinational,Google DeepMind,2024-02-15,Industry,Visual question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"Gemini 1.0 Pro Vision supports text, image, and video as inputs.

Max input tokens: 16,384
Max output tokens: 2,048
Max images per prompt: 16
Max video length: 2 minutes
Max videos per prompt: 1
Training data: Up to February 2023",,The best performing image and video understanding model to handle a broad range of applications. 
Gemini 1.0 Pro Vision,Video,UK,Google DeepMind,2024-02-15,Industry,Chat,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"Gemini 1.0 Pro Vision supports text, image, and video as inputs.

Max input tokens: 16,384
Max output tokens: 2,048
Max images per prompt: 16
Max video length: 2 minutes
Max videos per prompt: 1
Training data: Up to February 2023",,The best performing image and video understanding model to handle a broad range of applications. 
Gemini 1.0 Pro Vision,Video,UK,Google DeepMind,2024-02-15,Industry,Language modeling,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"Gemini 1.0 Pro Vision supports text, image, and video as inputs.

Max input tokens: 16,384
Max output tokens: 2,048
Max images per prompt: 16
Max video length: 2 minutes
Max videos per prompt: 1
Training data: Up to February 2023",,The best performing image and video understanding model to handle a broad range of applications. 
Gemini 1.0 Pro Vision,Video,UK,Google DeepMind,2024-02-15,Industry,Question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"Gemini 1.0 Pro Vision supports text, image, and video as inputs.

Max input tokens: 16,384
Max output tokens: 2,048
Max images per prompt: 16
Max video length: 2 minutes
Max videos per prompt: 1
Training data: Up to February 2023",,The best performing image and video understanding model to handle a broad range of applications. 
Gemini 1.0 Pro Vision,Video,UK,Google DeepMind,2024-02-15,Industry,Translation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"Gemini 1.0 Pro Vision supports text, image, and video as inputs.

Max input tokens: 16,384
Max output tokens: 2,048
Max images per prompt: 16
Max video length: 2 minutes
Max videos per prompt: 1
Training data: Up to February 2023",,The best performing image and video understanding model to handle a broad range of applications. 
Gemini 1.0 Pro Vision,Video,UK,Google DeepMind,2024-02-15,Industry,Video description,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"Gemini 1.0 Pro Vision supports text, image, and video as inputs.

Max input tokens: 16,384
Max output tokens: 2,048
Max images per prompt: 16
Max video length: 2 minutes
Max videos per prompt: 1
Training data: Up to February 2023",,The best performing image and video understanding model to handle a broad range of applications. 
Gemini 1.0 Pro Vision,Video,UK,Google DeepMind,2024-02-15,Industry,Visual question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"Gemini 1.0 Pro Vision supports text, image, and video as inputs.

Max input tokens: 16,384
Max output tokens: 2,048
Max images per prompt: 16
Max video length: 2 minutes
Max videos per prompt: 1
Training data: Up to February 2023",,The best performing image and video understanding model to handle a broad range of applications. 
Gemini 1.0 Pro Vision,Video,USA,Google DeepMind,2024-02-15,Industry,Chat,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"Gemini 1.0 Pro Vision supports text, image, and video as inputs.

Max input tokens: 16,384
Max output tokens: 2,048
Max images per prompt: 16
Max video length: 2 minutes
Max videos per prompt: 1
Training data: Up to February 2023",,The best performing image and video understanding model to handle a broad range of applications. 
Gemini 1.0 Pro Vision,Video,USA,Google DeepMind,2024-02-15,Industry,Language modeling,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"Gemini 1.0 Pro Vision supports text, image, and video as inputs.

Max input tokens: 16,384
Max output tokens: 2,048
Max images per prompt: 16
Max video length: 2 minutes
Max videos per prompt: 1
Training data: Up to February 2023",,The best performing image and video understanding model to handle a broad range of applications. 
Gemini 1.0 Pro Vision,Video,USA,Google DeepMind,2024-02-15,Industry,Question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"Gemini 1.0 Pro Vision supports text, image, and video as inputs.

Max input tokens: 16,384
Max output tokens: 2,048
Max images per prompt: 16
Max video length: 2 minutes
Max videos per prompt: 1
Training data: Up to February 2023",,The best performing image and video understanding model to handle a broad range of applications. 
Gemini 1.0 Pro Vision,Video,USA,Google DeepMind,2024-02-15,Industry,Translation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"Gemini 1.0 Pro Vision supports text, image, and video as inputs.

Max input tokens: 16,384
Max output tokens: 2,048
Max images per prompt: 16
Max video length: 2 minutes
Max videos per prompt: 1
Training data: Up to February 2023",,The best performing image and video understanding model to handle a broad range of applications. 
Gemini 1.0 Pro Vision,Video,USA,Google DeepMind,2024-02-15,Industry,Video description,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"Gemini 1.0 Pro Vision supports text, image, and video as inputs.

Max input tokens: 16,384
Max output tokens: 2,048
Max images per prompt: 16
Max video length: 2 minutes
Max videos per prompt: 1
Training data: Up to February 2023",,The best performing image and video understanding model to handle a broad range of applications. 
Gemini 1.0 Pro Vision,Video,USA,Google DeepMind,2024-02-15,Industry,Visual question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"Gemini 1.0 Pro Vision supports text, image, and video as inputs.

Max input tokens: 16,384
Max output tokens: 2,048
Max images per prompt: 16
Max video length: 2 minutes
Max videos per prompt: 1
Training data: Up to February 2023",,The best performing image and video understanding model to handle a broad range of applications. 
Gemini 1.0 Pro Vision,Vision,Multinational,Google DeepMind,2024-02-15,Industry,Chat,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"Gemini 1.0 Pro Vision supports text, image, and video as inputs.

Max input tokens: 16,384
Max output tokens: 2,048
Max images per prompt: 16
Max video length: 2 minutes
Max videos per prompt: 1
Training data: Up to February 2023",,The best performing image and video understanding model to handle a broad range of applications. 
Gemini 1.0 Pro Vision,Vision,Multinational,Google DeepMind,2024-02-15,Industry,Language modeling,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"Gemini 1.0 Pro Vision supports text, image, and video as inputs.

Max input tokens: 16,384
Max output tokens: 2,048
Max images per prompt: 16
Max video length: 2 minutes
Max videos per prompt: 1
Training data: Up to February 2023",,The best performing image and video understanding model to handle a broad range of applications. 
Gemini 1.0 Pro Vision,Vision,Multinational,Google DeepMind,2024-02-15,Industry,Question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"Gemini 1.0 Pro Vision supports text, image, and video as inputs.

Max input tokens: 16,384
Max output tokens: 2,048
Max images per prompt: 16
Max video length: 2 minutes
Max videos per prompt: 1
Training data: Up to February 2023",,The best performing image and video understanding model to handle a broad range of applications. 
Gemini 1.0 Pro Vision,Vision,Multinational,Google DeepMind,2024-02-15,Industry,Translation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"Gemini 1.0 Pro Vision supports text, image, and video as inputs.

Max input tokens: 16,384
Max output tokens: 2,048
Max images per prompt: 16
Max video length: 2 minutes
Max videos per prompt: 1
Training data: Up to February 2023",,The best performing image and video understanding model to handle a broad range of applications. 
Gemini 1.0 Pro Vision,Vision,Multinational,Google DeepMind,2024-02-15,Industry,Video description,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"Gemini 1.0 Pro Vision supports text, image, and video as inputs.

Max input tokens: 16,384
Max output tokens: 2,048
Max images per prompt: 16
Max video length: 2 minutes
Max videos per prompt: 1
Training data: Up to February 2023",,The best performing image and video understanding model to handle a broad range of applications. 
Gemini 1.0 Pro Vision,Vision,Multinational,Google DeepMind,2024-02-15,Industry,Visual question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"Gemini 1.0 Pro Vision supports text, image, and video as inputs.

Max input tokens: 16,384
Max output tokens: 2,048
Max images per prompt: 16
Max video length: 2 minutes
Max videos per prompt: 1
Training data: Up to February 2023",,The best performing image and video understanding model to handle a broad range of applications. 
Gemini 1.0 Pro Vision,Vision,UK,Google DeepMind,2024-02-15,Industry,Chat,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"Gemini 1.0 Pro Vision supports text, image, and video as inputs.

Max input tokens: 16,384
Max output tokens: 2,048
Max images per prompt: 16
Max video length: 2 minutes
Max videos per prompt: 1
Training data: Up to February 2023",,The best performing image and video understanding model to handle a broad range of applications. 
Gemini 1.0 Pro Vision,Vision,UK,Google DeepMind,2024-02-15,Industry,Language modeling,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"Gemini 1.0 Pro Vision supports text, image, and video as inputs.

Max input tokens: 16,384
Max output tokens: 2,048
Max images per prompt: 16
Max video length: 2 minutes
Max videos per prompt: 1
Training data: Up to February 2023",,The best performing image and video understanding model to handle a broad range of applications. 
Gemini 1.0 Pro Vision,Vision,UK,Google DeepMind,2024-02-15,Industry,Question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"Gemini 1.0 Pro Vision supports text, image, and video as inputs.

Max input tokens: 16,384
Max output tokens: 2,048
Max images per prompt: 16
Max video length: 2 minutes
Max videos per prompt: 1
Training data: Up to February 2023",,The best performing image and video understanding model to handle a broad range of applications. 
Gemini 1.0 Pro Vision,Vision,UK,Google DeepMind,2024-02-15,Industry,Translation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"Gemini 1.0 Pro Vision supports text, image, and video as inputs.

Max input tokens: 16,384
Max output tokens: 2,048
Max images per prompt: 16
Max video length: 2 minutes
Max videos per prompt: 1
Training data: Up to February 2023",,The best performing image and video understanding model to handle a broad range of applications. 
Gemini 1.0 Pro Vision,Vision,UK,Google DeepMind,2024-02-15,Industry,Video description,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"Gemini 1.0 Pro Vision supports text, image, and video as inputs.

Max input tokens: 16,384
Max output tokens: 2,048
Max images per prompt: 16
Max video length: 2 minutes
Max videos per prompt: 1
Training data: Up to February 2023",,The best performing image and video understanding model to handle a broad range of applications. 
Gemini 1.0 Pro Vision,Vision,UK,Google DeepMind,2024-02-15,Industry,Visual question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"Gemini 1.0 Pro Vision supports text, image, and video as inputs.

Max input tokens: 16,384
Max output tokens: 2,048
Max images per prompt: 16
Max video length: 2 minutes
Max videos per prompt: 1
Training data: Up to February 2023",,The best performing image and video understanding model to handle a broad range of applications. 
Gemini 1.0 Pro Vision,Vision,USA,Google DeepMind,2024-02-15,Industry,Chat,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"Gemini 1.0 Pro Vision supports text, image, and video as inputs.

Max input tokens: 16,384
Max output tokens: 2,048
Max images per prompt: 16
Max video length: 2 minutes
Max videos per prompt: 1
Training data: Up to February 2023",,The best performing image and video understanding model to handle a broad range of applications. 
Gemini 1.0 Pro Vision,Vision,USA,Google DeepMind,2024-02-15,Industry,Language modeling,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"Gemini 1.0 Pro Vision supports text, image, and video as inputs.

Max input tokens: 16,384
Max output tokens: 2,048
Max images per prompt: 16
Max video length: 2 minutes
Max videos per prompt: 1
Training data: Up to February 2023",,The best performing image and video understanding model to handle a broad range of applications. 
Gemini 1.0 Pro Vision,Vision,USA,Google DeepMind,2024-02-15,Industry,Question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"Gemini 1.0 Pro Vision supports text, image, and video as inputs.

Max input tokens: 16,384
Max output tokens: 2,048
Max images per prompt: 16
Max video length: 2 minutes
Max videos per prompt: 1
Training data: Up to February 2023",,The best performing image and video understanding model to handle a broad range of applications. 
Gemini 1.0 Pro Vision,Vision,USA,Google DeepMind,2024-02-15,Industry,Translation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"Gemini 1.0 Pro Vision supports text, image, and video as inputs.

Max input tokens: 16,384
Max output tokens: 2,048
Max images per prompt: 16
Max video length: 2 minutes
Max videos per prompt: 1
Training data: Up to February 2023",,The best performing image and video understanding model to handle a broad range of applications. 
Gemini 1.0 Pro Vision,Vision,USA,Google DeepMind,2024-02-15,Industry,Video description,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"Gemini 1.0 Pro Vision supports text, image, and video as inputs.

Max input tokens: 16,384
Max output tokens: 2,048
Max images per prompt: 16
Max video length: 2 minutes
Max videos per prompt: 1
Training data: Up to February 2023",,The best performing image and video understanding model to handle a broad range of applications. 
Gemini 1.0 Pro Vision,Vision,USA,Google DeepMind,2024-02-15,Industry,Visual question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,154003694.78442368,387405059.99999994,1.5027314192724485e-09,10944000.0,369608867482.6168,26265600000.0,"Gemini 1.0 Pro Vision supports text, image, and video as inputs.

Max input tokens: 16,384
Max output tokens: 2,048
Max images per prompt: 16
Max video length: 2 minutes
Max videos per prompt: 1
Training data: Up to February 2023",,The best performing image and video understanding model to handle a broad range of applications. 
Gemini 1.0 Ultra,Language,Multinational,Google DeepMind,2023-12-06,Industry,Chat,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,SOTA improvement,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Language,Multinational,Google DeepMind,2023-12-06,Industry,Chat,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Language,Multinational,Google DeepMind,2023-12-06,Industry,Language modeling,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,SOTA improvement,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Language,Multinational,Google DeepMind,2023-12-06,Industry,Language modeling,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Language,Multinational,Google DeepMind,2023-12-06,Industry,Translation,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,SOTA improvement,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Language,Multinational,Google DeepMind,2023-12-06,Industry,Translation,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Language,Multinational,Google DeepMind,2023-12-06,Industry,Visual question answering,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,SOTA improvement,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Language,Multinational,Google DeepMind,2023-12-06,Industry,Visual question answering,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Language,UK,Google DeepMind,2023-12-06,Industry,Chat,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,SOTA improvement,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Language,UK,Google DeepMind,2023-12-06,Industry,Chat,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Language,UK,Google DeepMind,2023-12-06,Industry,Language modeling,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,SOTA improvement,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Language,UK,Google DeepMind,2023-12-06,Industry,Language modeling,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Language,UK,Google DeepMind,2023-12-06,Industry,Translation,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,SOTA improvement,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Language,UK,Google DeepMind,2023-12-06,Industry,Translation,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Language,UK,Google DeepMind,2023-12-06,Industry,Visual question answering,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,SOTA improvement,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Language,UK,Google DeepMind,2023-12-06,Industry,Visual question answering,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Language,USA,Google DeepMind,2023-12-06,Industry,Chat,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,SOTA improvement,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Language,USA,Google DeepMind,2023-12-06,Industry,Chat,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Language,USA,Google DeepMind,2023-12-06,Industry,Language modeling,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,SOTA improvement,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Language,USA,Google DeepMind,2023-12-06,Industry,Language modeling,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Language,USA,Google DeepMind,2023-12-06,Industry,Translation,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,SOTA improvement,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Language,USA,Google DeepMind,2023-12-06,Industry,Translation,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Language,USA,Google DeepMind,2023-12-06,Industry,Visual question answering,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,SOTA improvement,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Language,USA,Google DeepMind,2023-12-06,Industry,Visual question answering,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Multimodal,Multinational,Google DeepMind,2023-12-06,Industry,Chat,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,SOTA improvement,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Multimodal,Multinational,Google DeepMind,2023-12-06,Industry,Chat,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Multimodal,Multinational,Google DeepMind,2023-12-06,Industry,Language modeling,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,SOTA improvement,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Multimodal,Multinational,Google DeepMind,2023-12-06,Industry,Language modeling,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Multimodal,Multinational,Google DeepMind,2023-12-06,Industry,Translation,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,SOTA improvement,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Multimodal,Multinational,Google DeepMind,2023-12-06,Industry,Translation,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Multimodal,Multinational,Google DeepMind,2023-12-06,Industry,Visual question answering,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,SOTA improvement,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Multimodal,Multinational,Google DeepMind,2023-12-06,Industry,Visual question answering,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Multimodal,UK,Google DeepMind,2023-12-06,Industry,Chat,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,SOTA improvement,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Multimodal,UK,Google DeepMind,2023-12-06,Industry,Chat,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Multimodal,UK,Google DeepMind,2023-12-06,Industry,Language modeling,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,SOTA improvement,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Multimodal,UK,Google DeepMind,2023-12-06,Industry,Language modeling,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Multimodal,UK,Google DeepMind,2023-12-06,Industry,Translation,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,SOTA improvement,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Multimodal,UK,Google DeepMind,2023-12-06,Industry,Translation,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Multimodal,UK,Google DeepMind,2023-12-06,Industry,Visual question answering,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,SOTA improvement,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Multimodal,UK,Google DeepMind,2023-12-06,Industry,Visual question answering,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Multimodal,USA,Google DeepMind,2023-12-06,Industry,Chat,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,SOTA improvement,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Multimodal,USA,Google DeepMind,2023-12-06,Industry,Chat,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Multimodal,USA,Google DeepMind,2023-12-06,Industry,Language modeling,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,SOTA improvement,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Multimodal,USA,Google DeepMind,2023-12-06,Industry,Language modeling,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Multimodal,USA,Google DeepMind,2023-12-06,Industry,Translation,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,SOTA improvement,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Multimodal,USA,Google DeepMind,2023-12-06,Industry,Translation,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Multimodal,USA,Google DeepMind,2023-12-06,Industry,Visual question answering,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,SOTA improvement,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Multimodal,USA,Google DeepMind,2023-12-06,Industry,Visual question answering,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Vision,Multinational,Google DeepMind,2023-12-06,Industry,Chat,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,SOTA improvement,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Vision,Multinational,Google DeepMind,2023-12-06,Industry,Chat,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Vision,Multinational,Google DeepMind,2023-12-06,Industry,Language modeling,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,SOTA improvement,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Vision,Multinational,Google DeepMind,2023-12-06,Industry,Language modeling,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Vision,Multinational,Google DeepMind,2023-12-06,Industry,Translation,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,SOTA improvement,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Vision,Multinational,Google DeepMind,2023-12-06,Industry,Translation,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Vision,Multinational,Google DeepMind,2023-12-06,Industry,Visual question answering,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,SOTA improvement,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Vision,Multinational,Google DeepMind,2023-12-06,Industry,Visual question answering,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Vision,UK,Google DeepMind,2023-12-06,Industry,Chat,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,SOTA improvement,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Vision,UK,Google DeepMind,2023-12-06,Industry,Chat,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Vision,UK,Google DeepMind,2023-12-06,Industry,Language modeling,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,SOTA improvement,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Vision,UK,Google DeepMind,2023-12-06,Industry,Language modeling,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Vision,UK,Google DeepMind,2023-12-06,Industry,Translation,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,SOTA improvement,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Vision,UK,Google DeepMind,2023-12-06,Industry,Translation,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Vision,UK,Google DeepMind,2023-12-06,Industry,Visual question answering,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,SOTA improvement,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Vision,UK,Google DeepMind,2023-12-06,Industry,Visual question answering,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Vision,USA,Google DeepMind,2023-12-06,Industry,Chat,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,SOTA improvement,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Vision,USA,Google DeepMind,2023-12-06,Industry,Chat,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Vision,USA,Google DeepMind,2023-12-06,Industry,Language modeling,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,SOTA improvement,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Vision,USA,Google DeepMind,2023-12-06,Industry,Language modeling,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Vision,USA,Google DeepMind,2023-12-06,Industry,Translation,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,SOTA improvement,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Vision,USA,Google DeepMind,2023-12-06,Industry,Translation,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Vision,USA,Google DeepMind,2023-12-06,Industry,Visual question answering,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,SOTA improvement,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.0 Ultra,Vision,USA,Google DeepMind,2023-12-06,Industry,Visual question answering,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,5.0000000001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4194304,24175462.27839436,387405059.99999994,5.4999999998899996e-11,10944000.0,58021109468.14646,26265600000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Gemini: A Family of Highly Capable Multimodal Models
Gemini 1.5 Pro,Language,Multinational,Google DeepMind,2024-02-15,Industry,Language modeling,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.580001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Significant use,1,4194304,154003694.78442368,387405059.99999994,1.7405052275283368e-10,10944000.0,369608867482.6168,26265600000.0,,https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf,Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context
Gemini 1.5 Pro,Language,Multinational,Google DeepMind,2024-02-15,Industry,Visual question answering,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.580001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Significant use,1,4194304,154003694.78442368,387405059.99999994,1.7405052275283368e-10,10944000.0,369608867482.6168,26265600000.0,,https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf,Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context
Gemini 1.5 Pro,Language,UK,Google DeepMind,2024-02-15,Industry,Language modeling,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.580001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Significant use,1,4194304,154003694.78442368,387405059.99999994,1.7405052275283368e-10,10944000.0,369608867482.6168,26265600000.0,,https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf,Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context
Gemini 1.5 Pro,Language,UK,Google DeepMind,2024-02-15,Industry,Visual question answering,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.580001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Significant use,1,4194304,154003694.78442368,387405059.99999994,1.7405052275283368e-10,10944000.0,369608867482.6168,26265600000.0,,https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf,Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context
Gemini 1.5 Pro,Language,USA,Google DeepMind,2024-02-15,Industry,Language modeling,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.580001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Significant use,1,4194304,154003694.78442368,387405059.99999994,1.7405052275283368e-10,10944000.0,369608867482.6168,26265600000.0,,https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf,Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context
Gemini 1.5 Pro,Language,USA,Google DeepMind,2024-02-15,Industry,Visual question answering,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.580001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Significant use,1,4194304,154003694.78442368,387405059.99999994,1.7405052275283368e-10,10944000.0,369608867482.6168,26265600000.0,,https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf,Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context
Gemini 1.5 Pro,Multimodal,Multinational,Google DeepMind,2024-02-15,Industry,Language modeling,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.580001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Significant use,1,4194304,154003694.78442368,387405059.99999994,1.7405052275283368e-10,10944000.0,369608867482.6168,26265600000.0,,https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf,Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context
Gemini 1.5 Pro,Multimodal,Multinational,Google DeepMind,2024-02-15,Industry,Visual question answering,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.580001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Significant use,1,4194304,154003694.78442368,387405059.99999994,1.7405052275283368e-10,10944000.0,369608867482.6168,26265600000.0,,https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf,Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context
Gemini 1.5 Pro,Multimodal,UK,Google DeepMind,2024-02-15,Industry,Language modeling,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.580001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Significant use,1,4194304,154003694.78442368,387405059.99999994,1.7405052275283368e-10,10944000.0,369608867482.6168,26265600000.0,,https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf,Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context
Gemini 1.5 Pro,Multimodal,UK,Google DeepMind,2024-02-15,Industry,Visual question answering,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.580001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Significant use,1,4194304,154003694.78442368,387405059.99999994,1.7405052275283368e-10,10944000.0,369608867482.6168,26265600000.0,,https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf,Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context
Gemini 1.5 Pro,Multimodal,USA,Google DeepMind,2024-02-15,Industry,Language modeling,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.580001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Significant use,1,4194304,154003694.78442368,387405059.99999994,1.7405052275283368e-10,10944000.0,369608867482.6168,26265600000.0,,https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf,Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context
Gemini 1.5 Pro,Multimodal,USA,Google DeepMind,2024-02-15,Industry,Visual question answering,Speculative,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.580001e+25,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Significant use,1,4194304,154003694.78442368,387405059.99999994,1.7405052275283368e-10,10944000.0,369608867482.6168,26265600000.0,,https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf,Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context
Gemini 2.0 Flash,Audio,Multinational,Google,2024-12-11,Industry,Chat,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,Multinational,Google,2024-12-11,Industry,Code generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,Multinational,Google,2024-12-11,Industry,Language modeling/generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,Multinational,Google,2024-12-11,Industry,Quantitative reasoning,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,Multinational,Google,2024-12-11,Industry,Question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,Multinational,Google,2024-12-11,Industry,Speech recognition,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,Multinational,Google,2024-12-11,Industry,Speech synthesis,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,Multinational,Google,2024-12-11,Industry,Table tasks,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,Multinational,Google,2024-12-11,Industry,Text-to-image,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,Multinational,Google,2024-12-11,Industry,Translation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,Multinational,Google,2024-12-11,Industry,Video description,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,Multinational,Google,2024-12-11,Industry,Visual question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,Multinational,Google DeepMind,2024-12-11,Industry,Chat,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,Multinational,Google DeepMind,2024-12-11,Industry,Code generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,Multinational,Google DeepMind,2024-12-11,Industry,Language modeling/generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,Multinational,Google DeepMind,2024-12-11,Industry,Quantitative reasoning,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,Multinational,Google DeepMind,2024-12-11,Industry,Question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,Multinational,Google DeepMind,2024-12-11,Industry,Speech recognition,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,Multinational,Google DeepMind,2024-12-11,Industry,Speech synthesis,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,Multinational,Google DeepMind,2024-12-11,Industry,Table tasks,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,Multinational,Google DeepMind,2024-12-11,Industry,Text-to-image,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,Multinational,Google DeepMind,2024-12-11,Industry,Translation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,Multinational,Google DeepMind,2024-12-11,Industry,Video description,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,Multinational,Google DeepMind,2024-12-11,Industry,Visual question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,UK,Google,2024-12-11,Industry,Chat,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,UK,Google,2024-12-11,Industry,Code generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,UK,Google,2024-12-11,Industry,Language modeling/generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,UK,Google,2024-12-11,Industry,Quantitative reasoning,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,UK,Google,2024-12-11,Industry,Question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,UK,Google,2024-12-11,Industry,Speech recognition,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,UK,Google,2024-12-11,Industry,Speech synthesis,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,UK,Google,2024-12-11,Industry,Table tasks,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,UK,Google,2024-12-11,Industry,Text-to-image,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,UK,Google,2024-12-11,Industry,Translation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,UK,Google,2024-12-11,Industry,Video description,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,UK,Google,2024-12-11,Industry,Visual question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,UK,Google DeepMind,2024-12-11,Industry,Chat,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,UK,Google DeepMind,2024-12-11,Industry,Code generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,UK,Google DeepMind,2024-12-11,Industry,Language modeling/generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,UK,Google DeepMind,2024-12-11,Industry,Quantitative reasoning,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,UK,Google DeepMind,2024-12-11,Industry,Question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,UK,Google DeepMind,2024-12-11,Industry,Speech recognition,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,UK,Google DeepMind,2024-12-11,Industry,Speech synthesis,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,UK,Google DeepMind,2024-12-11,Industry,Table tasks,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,UK,Google DeepMind,2024-12-11,Industry,Text-to-image,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,UK,Google DeepMind,2024-12-11,Industry,Translation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,UK,Google DeepMind,2024-12-11,Industry,Video description,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,UK,Google DeepMind,2024-12-11,Industry,Visual question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,USA,Google,2024-12-11,Industry,Chat,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,USA,Google,2024-12-11,Industry,Code generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,USA,Google,2024-12-11,Industry,Language modeling/generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,USA,Google,2024-12-11,Industry,Quantitative reasoning,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,USA,Google,2024-12-11,Industry,Question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,USA,Google,2024-12-11,Industry,Speech recognition,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,USA,Google,2024-12-11,Industry,Speech synthesis,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,USA,Google,2024-12-11,Industry,Table tasks,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,USA,Google,2024-12-11,Industry,Text-to-image,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,USA,Google,2024-12-11,Industry,Translation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,USA,Google,2024-12-11,Industry,Video description,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,USA,Google,2024-12-11,Industry,Visual question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,USA,Google DeepMind,2024-12-11,Industry,Chat,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,USA,Google DeepMind,2024-12-11,Industry,Code generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,USA,Google DeepMind,2024-12-11,Industry,Language modeling/generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,USA,Google DeepMind,2024-12-11,Industry,Quantitative reasoning,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,USA,Google DeepMind,2024-12-11,Industry,Question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,USA,Google DeepMind,2024-12-11,Industry,Speech recognition,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,USA,Google DeepMind,2024-12-11,Industry,Speech synthesis,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,USA,Google DeepMind,2024-12-11,Industry,Table tasks,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,USA,Google DeepMind,2024-12-11,Industry,Text-to-image,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,USA,Google DeepMind,2024-12-11,Industry,Translation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,USA,Google DeepMind,2024-12-11,Industry,Video description,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Audio,USA,Google DeepMind,2024-12-11,Industry,Visual question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,Multinational,Google,2024-12-11,Industry,Chat,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,Multinational,Google,2024-12-11,Industry,Code generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,Multinational,Google,2024-12-11,Industry,Language modeling/generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,Multinational,Google,2024-12-11,Industry,Quantitative reasoning,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,Multinational,Google,2024-12-11,Industry,Question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,Multinational,Google,2024-12-11,Industry,Speech recognition,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,Multinational,Google,2024-12-11,Industry,Speech synthesis,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,Multinational,Google,2024-12-11,Industry,Table tasks,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,Multinational,Google,2024-12-11,Industry,Text-to-image,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,Multinational,Google,2024-12-11,Industry,Translation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,Multinational,Google,2024-12-11,Industry,Video description,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,Multinational,Google,2024-12-11,Industry,Visual question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,Multinational,Google DeepMind,2024-12-11,Industry,Chat,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,Multinational,Google DeepMind,2024-12-11,Industry,Code generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,Multinational,Google DeepMind,2024-12-11,Industry,Language modeling/generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,Multinational,Google DeepMind,2024-12-11,Industry,Quantitative reasoning,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,Multinational,Google DeepMind,2024-12-11,Industry,Question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,Multinational,Google DeepMind,2024-12-11,Industry,Speech recognition,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,Multinational,Google DeepMind,2024-12-11,Industry,Speech synthesis,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,Multinational,Google DeepMind,2024-12-11,Industry,Table tasks,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,Multinational,Google DeepMind,2024-12-11,Industry,Text-to-image,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,Multinational,Google DeepMind,2024-12-11,Industry,Translation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,Multinational,Google DeepMind,2024-12-11,Industry,Video description,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,Multinational,Google DeepMind,2024-12-11,Industry,Visual question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,UK,Google,2024-12-11,Industry,Chat,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,UK,Google,2024-12-11,Industry,Code generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,UK,Google,2024-12-11,Industry,Language modeling/generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,UK,Google,2024-12-11,Industry,Quantitative reasoning,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,UK,Google,2024-12-11,Industry,Question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,UK,Google,2024-12-11,Industry,Speech recognition,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,UK,Google,2024-12-11,Industry,Speech synthesis,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,UK,Google,2024-12-11,Industry,Table tasks,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,UK,Google,2024-12-11,Industry,Text-to-image,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,UK,Google,2024-12-11,Industry,Translation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,UK,Google,2024-12-11,Industry,Video description,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,UK,Google,2024-12-11,Industry,Visual question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,UK,Google DeepMind,2024-12-11,Industry,Chat,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,UK,Google DeepMind,2024-12-11,Industry,Code generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,UK,Google DeepMind,2024-12-11,Industry,Language modeling/generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,UK,Google DeepMind,2024-12-11,Industry,Quantitative reasoning,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,UK,Google DeepMind,2024-12-11,Industry,Question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,UK,Google DeepMind,2024-12-11,Industry,Speech recognition,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,UK,Google DeepMind,2024-12-11,Industry,Speech synthesis,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,UK,Google DeepMind,2024-12-11,Industry,Table tasks,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,UK,Google DeepMind,2024-12-11,Industry,Text-to-image,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,UK,Google DeepMind,2024-12-11,Industry,Translation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,UK,Google DeepMind,2024-12-11,Industry,Video description,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,UK,Google DeepMind,2024-12-11,Industry,Visual question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,USA,Google,2024-12-11,Industry,Chat,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,USA,Google,2024-12-11,Industry,Code generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,USA,Google,2024-12-11,Industry,Language modeling/generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,USA,Google,2024-12-11,Industry,Quantitative reasoning,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,USA,Google,2024-12-11,Industry,Question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,USA,Google,2024-12-11,Industry,Speech recognition,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,USA,Google,2024-12-11,Industry,Speech synthesis,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,USA,Google,2024-12-11,Industry,Table tasks,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,USA,Google,2024-12-11,Industry,Text-to-image,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,USA,Google,2024-12-11,Industry,Translation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,USA,Google,2024-12-11,Industry,Video description,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,USA,Google,2024-12-11,Industry,Visual question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,USA,Google DeepMind,2024-12-11,Industry,Chat,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,USA,Google DeepMind,2024-12-11,Industry,Code generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,USA,Google DeepMind,2024-12-11,Industry,Language modeling/generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,USA,Google DeepMind,2024-12-11,Industry,Quantitative reasoning,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,USA,Google DeepMind,2024-12-11,Industry,Question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,USA,Google DeepMind,2024-12-11,Industry,Speech recognition,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,USA,Google DeepMind,2024-12-11,Industry,Speech synthesis,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,USA,Google DeepMind,2024-12-11,Industry,Table tasks,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,USA,Google DeepMind,2024-12-11,Industry,Text-to-image,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,USA,Google DeepMind,2024-12-11,Industry,Translation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,USA,Google DeepMind,2024-12-11,Industry,Video description,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Language,USA,Google DeepMind,2024-12-11,Industry,Visual question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,Multinational,Google,2024-12-11,Industry,Chat,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,Multinational,Google,2024-12-11,Industry,Code generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,Multinational,Google,2024-12-11,Industry,Language modeling/generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,Multinational,Google,2024-12-11,Industry,Quantitative reasoning,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,Multinational,Google,2024-12-11,Industry,Question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,Multinational,Google,2024-12-11,Industry,Speech recognition,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,Multinational,Google,2024-12-11,Industry,Speech synthesis,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,Multinational,Google,2024-12-11,Industry,Table tasks,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,Multinational,Google,2024-12-11,Industry,Text-to-image,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,Multinational,Google,2024-12-11,Industry,Translation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,Multinational,Google,2024-12-11,Industry,Video description,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,Multinational,Google,2024-12-11,Industry,Visual question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,Multinational,Google DeepMind,2024-12-11,Industry,Chat,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,Multinational,Google DeepMind,2024-12-11,Industry,Code generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,Multinational,Google DeepMind,2024-12-11,Industry,Language modeling/generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,Multinational,Google DeepMind,2024-12-11,Industry,Quantitative reasoning,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,Multinational,Google DeepMind,2024-12-11,Industry,Question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,Multinational,Google DeepMind,2024-12-11,Industry,Speech recognition,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,Multinational,Google DeepMind,2024-12-11,Industry,Speech synthesis,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,Multinational,Google DeepMind,2024-12-11,Industry,Table tasks,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,Multinational,Google DeepMind,2024-12-11,Industry,Text-to-image,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,Multinational,Google DeepMind,2024-12-11,Industry,Translation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,Multinational,Google DeepMind,2024-12-11,Industry,Video description,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,Multinational,Google DeepMind,2024-12-11,Industry,Visual question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,UK,Google,2024-12-11,Industry,Chat,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,UK,Google,2024-12-11,Industry,Code generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,UK,Google,2024-12-11,Industry,Language modeling/generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,UK,Google,2024-12-11,Industry,Quantitative reasoning,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,UK,Google,2024-12-11,Industry,Question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,UK,Google,2024-12-11,Industry,Speech recognition,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,UK,Google,2024-12-11,Industry,Speech synthesis,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,UK,Google,2024-12-11,Industry,Table tasks,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,UK,Google,2024-12-11,Industry,Text-to-image,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,UK,Google,2024-12-11,Industry,Translation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,UK,Google,2024-12-11,Industry,Video description,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,UK,Google,2024-12-11,Industry,Visual question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,UK,Google DeepMind,2024-12-11,Industry,Chat,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,UK,Google DeepMind,2024-12-11,Industry,Code generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,UK,Google DeepMind,2024-12-11,Industry,Language modeling/generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,UK,Google DeepMind,2024-12-11,Industry,Quantitative reasoning,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,UK,Google DeepMind,2024-12-11,Industry,Question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,UK,Google DeepMind,2024-12-11,Industry,Speech recognition,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,UK,Google DeepMind,2024-12-11,Industry,Speech synthesis,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,UK,Google DeepMind,2024-12-11,Industry,Table tasks,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,UK,Google DeepMind,2024-12-11,Industry,Text-to-image,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,UK,Google DeepMind,2024-12-11,Industry,Translation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,UK,Google DeepMind,2024-12-11,Industry,Video description,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,UK,Google DeepMind,2024-12-11,Industry,Visual question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,USA,Google,2024-12-11,Industry,Chat,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,USA,Google,2024-12-11,Industry,Code generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,USA,Google,2024-12-11,Industry,Language modeling/generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,USA,Google,2024-12-11,Industry,Quantitative reasoning,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,USA,Google,2024-12-11,Industry,Question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,USA,Google,2024-12-11,Industry,Speech recognition,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,USA,Google,2024-12-11,Industry,Speech synthesis,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,USA,Google,2024-12-11,Industry,Table tasks,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,USA,Google,2024-12-11,Industry,Text-to-image,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,USA,Google,2024-12-11,Industry,Translation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,USA,Google,2024-12-11,Industry,Video description,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,USA,Google,2024-12-11,Industry,Visual question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,USA,Google DeepMind,2024-12-11,Industry,Chat,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,USA,Google DeepMind,2024-12-11,Industry,Code generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,USA,Google DeepMind,2024-12-11,Industry,Language modeling/generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,USA,Google DeepMind,2024-12-11,Industry,Quantitative reasoning,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,USA,Google DeepMind,2024-12-11,Industry,Question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,USA,Google DeepMind,2024-12-11,Industry,Speech recognition,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,USA,Google DeepMind,2024-12-11,Industry,Speech synthesis,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,USA,Google DeepMind,2024-12-11,Industry,Table tasks,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,USA,Google DeepMind,2024-12-11,Industry,Text-to-image,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,USA,Google DeepMind,2024-12-11,Industry,Translation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,USA,Google DeepMind,2024-12-11,Industry,Video description,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Multimodal,USA,Google DeepMind,2024-12-11,Industry,Visual question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,Multinational,Google,2024-12-11,Industry,Chat,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,Multinational,Google,2024-12-11,Industry,Code generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,Multinational,Google,2024-12-11,Industry,Language modeling/generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,Multinational,Google,2024-12-11,Industry,Quantitative reasoning,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,Multinational,Google,2024-12-11,Industry,Question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,Multinational,Google,2024-12-11,Industry,Speech recognition,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,Multinational,Google,2024-12-11,Industry,Speech synthesis,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,Multinational,Google,2024-12-11,Industry,Table tasks,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,Multinational,Google,2024-12-11,Industry,Text-to-image,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,Multinational,Google,2024-12-11,Industry,Translation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,Multinational,Google,2024-12-11,Industry,Video description,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,Multinational,Google,2024-12-11,Industry,Visual question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,Multinational,Google DeepMind,2024-12-11,Industry,Chat,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,Multinational,Google DeepMind,2024-12-11,Industry,Code generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,Multinational,Google DeepMind,2024-12-11,Industry,Language modeling/generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,Multinational,Google DeepMind,2024-12-11,Industry,Quantitative reasoning,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,Multinational,Google DeepMind,2024-12-11,Industry,Question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,Multinational,Google DeepMind,2024-12-11,Industry,Speech recognition,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,Multinational,Google DeepMind,2024-12-11,Industry,Speech synthesis,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,Multinational,Google DeepMind,2024-12-11,Industry,Table tasks,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,Multinational,Google DeepMind,2024-12-11,Industry,Text-to-image,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,Multinational,Google DeepMind,2024-12-11,Industry,Translation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,Multinational,Google DeepMind,2024-12-11,Industry,Video description,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,Multinational,Google DeepMind,2024-12-11,Industry,Visual question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,UK,Google,2024-12-11,Industry,Chat,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,UK,Google,2024-12-11,Industry,Code generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,UK,Google,2024-12-11,Industry,Language modeling/generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,UK,Google,2024-12-11,Industry,Quantitative reasoning,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,UK,Google,2024-12-11,Industry,Question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,UK,Google,2024-12-11,Industry,Speech recognition,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,UK,Google,2024-12-11,Industry,Speech synthesis,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,UK,Google,2024-12-11,Industry,Table tasks,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,UK,Google,2024-12-11,Industry,Text-to-image,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,UK,Google,2024-12-11,Industry,Translation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,UK,Google,2024-12-11,Industry,Video description,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,UK,Google,2024-12-11,Industry,Visual question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,UK,Google DeepMind,2024-12-11,Industry,Chat,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,UK,Google DeepMind,2024-12-11,Industry,Code generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,UK,Google DeepMind,2024-12-11,Industry,Language modeling/generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,UK,Google DeepMind,2024-12-11,Industry,Quantitative reasoning,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,UK,Google DeepMind,2024-12-11,Industry,Question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,UK,Google DeepMind,2024-12-11,Industry,Speech recognition,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,UK,Google DeepMind,2024-12-11,Industry,Speech synthesis,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,UK,Google DeepMind,2024-12-11,Industry,Table tasks,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,UK,Google DeepMind,2024-12-11,Industry,Text-to-image,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,UK,Google DeepMind,2024-12-11,Industry,Translation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,UK,Google DeepMind,2024-12-11,Industry,Video description,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,UK,Google DeepMind,2024-12-11,Industry,Visual question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,USA,Google,2024-12-11,Industry,Chat,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,USA,Google,2024-12-11,Industry,Code generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,USA,Google,2024-12-11,Industry,Language modeling/generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,USA,Google,2024-12-11,Industry,Quantitative reasoning,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,USA,Google,2024-12-11,Industry,Question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,USA,Google,2024-12-11,Industry,Speech recognition,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,USA,Google,2024-12-11,Industry,Speech synthesis,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,USA,Google,2024-12-11,Industry,Table tasks,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,USA,Google,2024-12-11,Industry,Text-to-image,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,USA,Google,2024-12-11,Industry,Translation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,USA,Google,2024-12-11,Industry,Video description,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,USA,Google,2024-12-11,Industry,Visual question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,USA,Google DeepMind,2024-12-11,Industry,Chat,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,USA,Google DeepMind,2024-12-11,Industry,Code generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,USA,Google DeepMind,2024-12-11,Industry,Language modeling/generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,USA,Google DeepMind,2024-12-11,Industry,Quantitative reasoning,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,USA,Google DeepMind,2024-12-11,Industry,Question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,USA,Google DeepMind,2024-12-11,Industry,Speech recognition,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,USA,Google DeepMind,2024-12-11,Industry,Speech synthesis,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,USA,Google DeepMind,2024-12-11,Industry,Table tasks,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,USA,Google DeepMind,2024-12-11,Industry,Text-to-image,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,USA,Google DeepMind,2024-12-11,Industry,Translation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,USA,Google DeepMind,2024-12-11,Industry,Video description,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Speech,USA,Google DeepMind,2024-12-11,Industry,Visual question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,Multinational,Google,2024-12-11,Industry,Chat,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,Multinational,Google,2024-12-11,Industry,Code generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,Multinational,Google,2024-12-11,Industry,Language modeling/generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,Multinational,Google,2024-12-11,Industry,Quantitative reasoning,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,Multinational,Google,2024-12-11,Industry,Question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,Multinational,Google,2024-12-11,Industry,Speech recognition,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,Multinational,Google,2024-12-11,Industry,Speech synthesis,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,Multinational,Google,2024-12-11,Industry,Table tasks,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,Multinational,Google,2024-12-11,Industry,Text-to-image,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,Multinational,Google,2024-12-11,Industry,Translation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,Multinational,Google,2024-12-11,Industry,Video description,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,Multinational,Google,2024-12-11,Industry,Visual question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,Multinational,Google DeepMind,2024-12-11,Industry,Chat,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,Multinational,Google DeepMind,2024-12-11,Industry,Code generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,Multinational,Google DeepMind,2024-12-11,Industry,Language modeling/generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,Multinational,Google DeepMind,2024-12-11,Industry,Quantitative reasoning,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,Multinational,Google DeepMind,2024-12-11,Industry,Question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,Multinational,Google DeepMind,2024-12-11,Industry,Speech recognition,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,Multinational,Google DeepMind,2024-12-11,Industry,Speech synthesis,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,Multinational,Google DeepMind,2024-12-11,Industry,Table tasks,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,Multinational,Google DeepMind,2024-12-11,Industry,Text-to-image,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,Multinational,Google DeepMind,2024-12-11,Industry,Translation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,Multinational,Google DeepMind,2024-12-11,Industry,Video description,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,Multinational,Google DeepMind,2024-12-11,Industry,Visual question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,UK,Google,2024-12-11,Industry,Chat,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,UK,Google,2024-12-11,Industry,Code generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,UK,Google,2024-12-11,Industry,Language modeling/generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,UK,Google,2024-12-11,Industry,Quantitative reasoning,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,UK,Google,2024-12-11,Industry,Question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,UK,Google,2024-12-11,Industry,Speech recognition,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,UK,Google,2024-12-11,Industry,Speech synthesis,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,UK,Google,2024-12-11,Industry,Table tasks,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,UK,Google,2024-12-11,Industry,Text-to-image,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,UK,Google,2024-12-11,Industry,Translation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,UK,Google,2024-12-11,Industry,Video description,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,UK,Google,2024-12-11,Industry,Visual question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,UK,Google DeepMind,2024-12-11,Industry,Chat,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,UK,Google DeepMind,2024-12-11,Industry,Code generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,UK,Google DeepMind,2024-12-11,Industry,Language modeling/generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,UK,Google DeepMind,2024-12-11,Industry,Quantitative reasoning,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,UK,Google DeepMind,2024-12-11,Industry,Question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,UK,Google DeepMind,2024-12-11,Industry,Speech recognition,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,UK,Google DeepMind,2024-12-11,Industry,Speech synthesis,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,UK,Google DeepMind,2024-12-11,Industry,Table tasks,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,UK,Google DeepMind,2024-12-11,Industry,Text-to-image,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,UK,Google DeepMind,2024-12-11,Industry,Translation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,UK,Google DeepMind,2024-12-11,Industry,Video description,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,UK,Google DeepMind,2024-12-11,Industry,Visual question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,USA,Google,2024-12-11,Industry,Chat,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,USA,Google,2024-12-11,Industry,Code generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,USA,Google,2024-12-11,Industry,Language modeling/generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,USA,Google,2024-12-11,Industry,Quantitative reasoning,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,USA,Google,2024-12-11,Industry,Question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,USA,Google,2024-12-11,Industry,Speech recognition,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,USA,Google,2024-12-11,Industry,Speech synthesis,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,USA,Google,2024-12-11,Industry,Table tasks,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,USA,Google,2024-12-11,Industry,Text-to-image,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,USA,Google,2024-12-11,Industry,Translation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,USA,Google,2024-12-11,Industry,Video description,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,USA,Google,2024-12-11,Industry,Visual question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,USA,Google DeepMind,2024-12-11,Industry,Chat,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,USA,Google DeepMind,2024-12-11,Industry,Code generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,USA,Google DeepMind,2024-12-11,Industry,Language modeling/generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,USA,Google DeepMind,2024-12-11,Industry,Quantitative reasoning,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,USA,Google DeepMind,2024-12-11,Industry,Question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,USA,Google DeepMind,2024-12-11,Industry,Speech recognition,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,USA,Google DeepMind,2024-12-11,Industry,Speech synthesis,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,USA,Google DeepMind,2024-12-11,Industry,Table tasks,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,USA,Google DeepMind,2024-12-11,Industry,Text-to-image,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,USA,Google DeepMind,2024-12-11,Industry,Translation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,USA,Google DeepMind,2024-12-11,Industry,Video description,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Video,USA,Google DeepMind,2024-12-11,Industry,Visual question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,Multinational,Google,2024-12-11,Industry,Chat,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,Multinational,Google,2024-12-11,Industry,Code generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,Multinational,Google,2024-12-11,Industry,Language modeling/generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,Multinational,Google,2024-12-11,Industry,Quantitative reasoning,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,Multinational,Google,2024-12-11,Industry,Question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,Multinational,Google,2024-12-11,Industry,Speech recognition,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,Multinational,Google,2024-12-11,Industry,Speech synthesis,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,Multinational,Google,2024-12-11,Industry,Table tasks,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,Multinational,Google,2024-12-11,Industry,Text-to-image,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,Multinational,Google,2024-12-11,Industry,Translation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,Multinational,Google,2024-12-11,Industry,Video description,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,Multinational,Google,2024-12-11,Industry,Visual question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,Multinational,Google DeepMind,2024-12-11,Industry,Chat,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,Multinational,Google DeepMind,2024-12-11,Industry,Code generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,Multinational,Google DeepMind,2024-12-11,Industry,Language modeling/generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,Multinational,Google DeepMind,2024-12-11,Industry,Quantitative reasoning,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,Multinational,Google DeepMind,2024-12-11,Industry,Question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,Multinational,Google DeepMind,2024-12-11,Industry,Speech recognition,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,Multinational,Google DeepMind,2024-12-11,Industry,Speech synthesis,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,Multinational,Google DeepMind,2024-12-11,Industry,Table tasks,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,Multinational,Google DeepMind,2024-12-11,Industry,Text-to-image,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,Multinational,Google DeepMind,2024-12-11,Industry,Translation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,Multinational,Google DeepMind,2024-12-11,Industry,Video description,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,Multinational,Google DeepMind,2024-12-11,Industry,Visual question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,UK,Google,2024-12-11,Industry,Chat,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,UK,Google,2024-12-11,Industry,Code generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,UK,Google,2024-12-11,Industry,Language modeling/generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,UK,Google,2024-12-11,Industry,Quantitative reasoning,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,UK,Google,2024-12-11,Industry,Question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,UK,Google,2024-12-11,Industry,Speech recognition,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,UK,Google,2024-12-11,Industry,Speech synthesis,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,UK,Google,2024-12-11,Industry,Table tasks,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,UK,Google,2024-12-11,Industry,Text-to-image,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,UK,Google,2024-12-11,Industry,Translation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,UK,Google,2024-12-11,Industry,Video description,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,UK,Google,2024-12-11,Industry,Visual question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,UK,Google DeepMind,2024-12-11,Industry,Chat,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,UK,Google DeepMind,2024-12-11,Industry,Code generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,UK,Google DeepMind,2024-12-11,Industry,Language modeling/generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,UK,Google DeepMind,2024-12-11,Industry,Quantitative reasoning,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,UK,Google DeepMind,2024-12-11,Industry,Question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,UK,Google DeepMind,2024-12-11,Industry,Speech recognition,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,UK,Google DeepMind,2024-12-11,Industry,Speech synthesis,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,UK,Google DeepMind,2024-12-11,Industry,Table tasks,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,UK,Google DeepMind,2024-12-11,Industry,Text-to-image,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,UK,Google DeepMind,2024-12-11,Industry,Translation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,UK,Google DeepMind,2024-12-11,Industry,Video description,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,UK,Google DeepMind,2024-12-11,Industry,Visual question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,USA,Google,2024-12-11,Industry,Chat,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,USA,Google,2024-12-11,Industry,Code generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,USA,Google,2024-12-11,Industry,Language modeling/generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,USA,Google,2024-12-11,Industry,Quantitative reasoning,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,USA,Google,2024-12-11,Industry,Question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,USA,Google,2024-12-11,Industry,Speech recognition,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,USA,Google,2024-12-11,Industry,Speech synthesis,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,USA,Google,2024-12-11,Industry,Table tasks,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,USA,Google,2024-12-11,Industry,Text-to-image,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,USA,Google,2024-12-11,Industry,Translation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,USA,Google,2024-12-11,Industry,Video description,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,USA,Google,2024-12-11,Industry,Visual question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,USA,Google DeepMind,2024-12-11,Industry,Chat,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,USA,Google DeepMind,2024-12-11,Industry,Code generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,USA,Google DeepMind,2024-12-11,Industry,Language modeling/generation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,USA,Google DeepMind,2024-12-11,Industry,Quantitative reasoning,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,USA,Google DeepMind,2024-12-11,Industry,Question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,USA,Google DeepMind,2024-12-11,Industry,Speech recognition,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,USA,Google DeepMind,2024-12-11,Industry,Speech synthesis,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,USA,Google DeepMind,2024-12-11,Industry,Table tasks,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,USA,Google DeepMind,2024-12-11,Industry,Text-to-image,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,USA,Google DeepMind,2024-12-11,Industry,Translation,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,USA,Google DeepMind,2024-12-11,Industry,Video description,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini 2.0 Flash,Vision,USA,Google DeepMind,2024-12-11,Industry,Visual question answering,Unknown,57000.0,API access,Unspecified unreleased,Unreleased,3250000000.0,589652798174979.1,2400.0,1.830001e+24,1877.1621580193864,12056.170666666652,180.3,489333333333333.3,795333333333333.4,32000000000.0,Google TPU v6e,Significant use,1,4194304,154003694.78442368,687201727.9999992,2.6739511799902477e-09,10277100.0,369608867482.6168,24665040000.0,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,Introducing Gemini 2.0: our new AI model for the agentic era
Gemini Nano-2,Audio,Multinational,Google DeepMind,2023-12-19,Industry,Chat,Confident,57000.0,Unreleased,Unspecified unreleased,Not-defined,3250000000.0,2625000000000.0,2400.0,5.0000000001e+25,1877.1621580193864,2689.98,140.0,197000000000000.0,393000000000000.0,16000000000.0,Google TPU v5e,Significant use,1,4194304,24175462.27839436,153328860.0,3.9399999999212e-11,7980000.0,58021109468.14646,19152000000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.",https://arxiv.org/abs/2312.11805,Gemini: A Family of Highly Capable Multimodal Models
Gemini Nano-2,Audio,Multinational,Google DeepMind,2023-12-19,Industry,Image captioning,Confident,57000.0,Unreleased,Unspecified unreleased,Not-defined,3250000000.0,2625000000000.0,2400.0,5.0000000001e+25,1877.1621580193864,2689.98,140.0,197000000000000.0,393000000000000.0,16000000000.0,Google TPU v5e,Significant use,1,4194304,24175462.27839436,153328860.0,3.9399999999212e-11,7980000.0,58021109468.14646,19152000000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.",https://arxiv.org/abs/2312.11805,Gemini: A Family of Highly Capable Multimodal Models
Gemini Nano-2,Audio,Multinational,Google DeepMind,2023-12-19,Industry,Speech recognition,Confident,57000.0,Unreleased,Unspecified unreleased,Not-defined,3250000000.0,2625000000000.0,2400.0,5.0000000001e+25,1877.1621580193864,2689.98,140.0,197000000000000.0,393000000000000.0,16000000000.0,Google TPU v5e,Significant use,1,4194304,24175462.27839436,153328860.0,3.9399999999212e-11,7980000.0,58021109468.14646,19152000000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.",https://arxiv.org/abs/2312.11805,Gemini: A Family of Highly Capable Multimodal Models
Gemini Nano-2,Audio,UK,Google DeepMind,2023-12-19,Industry,Chat,Confident,57000.0,Unreleased,Unspecified unreleased,Not-defined,3250000000.0,2625000000000.0,2400.0,5.0000000001e+25,1877.1621580193864,2689.98,140.0,197000000000000.0,393000000000000.0,16000000000.0,Google TPU v5e,Significant use,1,4194304,24175462.27839436,153328860.0,3.9399999999212e-11,7980000.0,58021109468.14646,19152000000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.",https://arxiv.org/abs/2312.11805,Gemini: A Family of Highly Capable Multimodal Models
Gemini Nano-2,Audio,UK,Google DeepMind,2023-12-19,Industry,Image captioning,Confident,57000.0,Unreleased,Unspecified unreleased,Not-defined,3250000000.0,2625000000000.0,2400.0,5.0000000001e+25,1877.1621580193864,2689.98,140.0,197000000000000.0,393000000000000.0,16000000000.0,Google TPU v5e,Significant use,1,4194304,24175462.27839436,153328860.0,3.9399999999212e-11,7980000.0,58021109468.14646,19152000000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.",https://arxiv.org/abs/2312.11805,Gemini: A Family of Highly Capable Multimodal Models
Gemini Nano-2,Audio,UK,Google DeepMind,2023-12-19,Industry,Speech recognition,Confident,57000.0,Unreleased,Unspecified unreleased,Not-defined,3250000000.0,2625000000000.0,2400.0,5.0000000001e+25,1877.1621580193864,2689.98,140.0,197000000000000.0,393000000000000.0,16000000000.0,Google TPU v5e,Significant use,1,4194304,24175462.27839436,153328860.0,3.9399999999212e-11,7980000.0,58021109468.14646,19152000000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.",https://arxiv.org/abs/2312.11805,Gemini: A Family of Highly Capable Multimodal Models
Gemini Nano-2,Audio,USA,Google DeepMind,2023-12-19,Industry,Chat,Confident,57000.0,Unreleased,Unspecified unreleased,Not-defined,3250000000.0,2625000000000.0,2400.0,5.0000000001e+25,1877.1621580193864,2689.98,140.0,197000000000000.0,393000000000000.0,16000000000.0,Google TPU v5e,Significant use,1,4194304,24175462.27839436,153328860.0,3.9399999999212e-11,7980000.0,58021109468.14646,19152000000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.",https://arxiv.org/abs/2312.11805,Gemini: A Family of Highly Capable Multimodal Models
Gemini Nano-2,Audio,USA,Google DeepMind,2023-12-19,Industry,Image captioning,Confident,57000.0,Unreleased,Unspecified unreleased,Not-defined,3250000000.0,2625000000000.0,2400.0,5.0000000001e+25,1877.1621580193864,2689.98,140.0,197000000000000.0,393000000000000.0,16000000000.0,Google TPU v5e,Significant use,1,4194304,24175462.27839436,153328860.0,3.9399999999212e-11,7980000.0,58021109468.14646,19152000000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.",https://arxiv.org/abs/2312.11805,Gemini: A Family of Highly Capable Multimodal Models
Gemini Nano-2,Audio,USA,Google DeepMind,2023-12-19,Industry,Speech recognition,Confident,57000.0,Unreleased,Unspecified unreleased,Not-defined,3250000000.0,2625000000000.0,2400.0,5.0000000001e+25,1877.1621580193864,2689.98,140.0,197000000000000.0,393000000000000.0,16000000000.0,Google TPU v5e,Significant use,1,4194304,24175462.27839436,153328860.0,3.9399999999212e-11,7980000.0,58021109468.14646,19152000000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.",https://arxiv.org/abs/2312.11805,Gemini: A Family of Highly Capable Multimodal Models
Gemini Nano-2,Language,Multinational,Google DeepMind,2023-12-19,Industry,Chat,Confident,57000.0,Unreleased,Unspecified unreleased,Not-defined,3250000000.0,2625000000000.0,2400.0,5.0000000001e+25,1877.1621580193864,2689.98,140.0,197000000000000.0,393000000000000.0,16000000000.0,Google TPU v5e,Significant use,1,4194304,24175462.27839436,153328860.0,3.9399999999212e-11,7980000.0,58021109468.14646,19152000000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.",https://arxiv.org/abs/2312.11805,Gemini: A Family of Highly Capable Multimodal Models
Gemini Nano-2,Language,Multinational,Google DeepMind,2023-12-19,Industry,Image captioning,Confident,57000.0,Unreleased,Unspecified unreleased,Not-defined,3250000000.0,2625000000000.0,2400.0,5.0000000001e+25,1877.1621580193864,2689.98,140.0,197000000000000.0,393000000000000.0,16000000000.0,Google TPU v5e,Significant use,1,4194304,24175462.27839436,153328860.0,3.9399999999212e-11,7980000.0,58021109468.14646,19152000000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.",https://arxiv.org/abs/2312.11805,Gemini: A Family of Highly Capable Multimodal Models
Gemini Nano-2,Language,Multinational,Google DeepMind,2023-12-19,Industry,Speech recognition,Confident,57000.0,Unreleased,Unspecified unreleased,Not-defined,3250000000.0,2625000000000.0,2400.0,5.0000000001e+25,1877.1621580193864,2689.98,140.0,197000000000000.0,393000000000000.0,16000000000.0,Google TPU v5e,Significant use,1,4194304,24175462.27839436,153328860.0,3.9399999999212e-11,7980000.0,58021109468.14646,19152000000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.",https://arxiv.org/abs/2312.11805,Gemini: A Family of Highly Capable Multimodal Models
Gemini Nano-2,Language,UK,Google DeepMind,2023-12-19,Industry,Chat,Confident,57000.0,Unreleased,Unspecified unreleased,Not-defined,3250000000.0,2625000000000.0,2400.0,5.0000000001e+25,1877.1621580193864,2689.98,140.0,197000000000000.0,393000000000000.0,16000000000.0,Google TPU v5e,Significant use,1,4194304,24175462.27839436,153328860.0,3.9399999999212e-11,7980000.0,58021109468.14646,19152000000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.",https://arxiv.org/abs/2312.11805,Gemini: A Family of Highly Capable Multimodal Models
Gemini Nano-2,Language,UK,Google DeepMind,2023-12-19,Industry,Image captioning,Confident,57000.0,Unreleased,Unspecified unreleased,Not-defined,3250000000.0,2625000000000.0,2400.0,5.0000000001e+25,1877.1621580193864,2689.98,140.0,197000000000000.0,393000000000000.0,16000000000.0,Google TPU v5e,Significant use,1,4194304,24175462.27839436,153328860.0,3.9399999999212e-11,7980000.0,58021109468.14646,19152000000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.",https://arxiv.org/abs/2312.11805,Gemini: A Family of Highly Capable Multimodal Models
Gemini Nano-2,Language,UK,Google DeepMind,2023-12-19,Industry,Speech recognition,Confident,57000.0,Unreleased,Unspecified unreleased,Not-defined,3250000000.0,2625000000000.0,2400.0,5.0000000001e+25,1877.1621580193864,2689.98,140.0,197000000000000.0,393000000000000.0,16000000000.0,Google TPU v5e,Significant use,1,4194304,24175462.27839436,153328860.0,3.9399999999212e-11,7980000.0,58021109468.14646,19152000000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.",https://arxiv.org/abs/2312.11805,Gemini: A Family of Highly Capable Multimodal Models
Gemini Nano-2,Language,USA,Google DeepMind,2023-12-19,Industry,Chat,Confident,57000.0,Unreleased,Unspecified unreleased,Not-defined,3250000000.0,2625000000000.0,2400.0,5.0000000001e+25,1877.1621580193864,2689.98,140.0,197000000000000.0,393000000000000.0,16000000000.0,Google TPU v5e,Significant use,1,4194304,24175462.27839436,153328860.0,3.9399999999212e-11,7980000.0,58021109468.14646,19152000000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.",https://arxiv.org/abs/2312.11805,Gemini: A Family of Highly Capable Multimodal Models
Gemini Nano-2,Language,USA,Google DeepMind,2023-12-19,Industry,Image captioning,Confident,57000.0,Unreleased,Unspecified unreleased,Not-defined,3250000000.0,2625000000000.0,2400.0,5.0000000001e+25,1877.1621580193864,2689.98,140.0,197000000000000.0,393000000000000.0,16000000000.0,Google TPU v5e,Significant use,1,4194304,24175462.27839436,153328860.0,3.9399999999212e-11,7980000.0,58021109468.14646,19152000000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.",https://arxiv.org/abs/2312.11805,Gemini: A Family of Highly Capable Multimodal Models
Gemini Nano-2,Language,USA,Google DeepMind,2023-12-19,Industry,Speech recognition,Confident,57000.0,Unreleased,Unspecified unreleased,Not-defined,3250000000.0,2625000000000.0,2400.0,5.0000000001e+25,1877.1621580193864,2689.98,140.0,197000000000000.0,393000000000000.0,16000000000.0,Google TPU v5e,Significant use,1,4194304,24175462.27839436,153328860.0,3.9399999999212e-11,7980000.0,58021109468.14646,19152000000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.",https://arxiv.org/abs/2312.11805,Gemini: A Family of Highly Capable Multimodal Models
Gemini Nano-2,Multimodal,Multinational,Google DeepMind,2023-12-19,Industry,Chat,Confident,57000.0,Unreleased,Unspecified unreleased,Not-defined,3250000000.0,2625000000000.0,2400.0,5.0000000001e+25,1877.1621580193864,2689.98,140.0,197000000000000.0,393000000000000.0,16000000000.0,Google TPU v5e,Significant use,1,4194304,24175462.27839436,153328860.0,3.9399999999212e-11,7980000.0,58021109468.14646,19152000000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.",https://arxiv.org/abs/2312.11805,Gemini: A Family of Highly Capable Multimodal Models
Gemini Nano-2,Multimodal,Multinational,Google DeepMind,2023-12-19,Industry,Image captioning,Confident,57000.0,Unreleased,Unspecified unreleased,Not-defined,3250000000.0,2625000000000.0,2400.0,5.0000000001e+25,1877.1621580193864,2689.98,140.0,197000000000000.0,393000000000000.0,16000000000.0,Google TPU v5e,Significant use,1,4194304,24175462.27839436,153328860.0,3.9399999999212e-11,7980000.0,58021109468.14646,19152000000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.",https://arxiv.org/abs/2312.11805,Gemini: A Family of Highly Capable Multimodal Models
Gemini Nano-2,Multimodal,Multinational,Google DeepMind,2023-12-19,Industry,Speech recognition,Confident,57000.0,Unreleased,Unspecified unreleased,Not-defined,3250000000.0,2625000000000.0,2400.0,5.0000000001e+25,1877.1621580193864,2689.98,140.0,197000000000000.0,393000000000000.0,16000000000.0,Google TPU v5e,Significant use,1,4194304,24175462.27839436,153328860.0,3.9399999999212e-11,7980000.0,58021109468.14646,19152000000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.",https://arxiv.org/abs/2312.11805,Gemini: A Family of Highly Capable Multimodal Models
Gemini Nano-2,Multimodal,UK,Google DeepMind,2023-12-19,Industry,Chat,Confident,57000.0,Unreleased,Unspecified unreleased,Not-defined,3250000000.0,2625000000000.0,2400.0,5.0000000001e+25,1877.1621580193864,2689.98,140.0,197000000000000.0,393000000000000.0,16000000000.0,Google TPU v5e,Significant use,1,4194304,24175462.27839436,153328860.0,3.9399999999212e-11,7980000.0,58021109468.14646,19152000000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.",https://arxiv.org/abs/2312.11805,Gemini: A Family of Highly Capable Multimodal Models
Gemini Nano-2,Multimodal,UK,Google DeepMind,2023-12-19,Industry,Image captioning,Confident,57000.0,Unreleased,Unspecified unreleased,Not-defined,3250000000.0,2625000000000.0,2400.0,5.0000000001e+25,1877.1621580193864,2689.98,140.0,197000000000000.0,393000000000000.0,16000000000.0,Google TPU v5e,Significant use,1,4194304,24175462.27839436,153328860.0,3.9399999999212e-11,7980000.0,58021109468.14646,19152000000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.",https://arxiv.org/abs/2312.11805,Gemini: A Family of Highly Capable Multimodal Models
Gemini Nano-2,Multimodal,UK,Google DeepMind,2023-12-19,Industry,Speech recognition,Confident,57000.0,Unreleased,Unspecified unreleased,Not-defined,3250000000.0,2625000000000.0,2400.0,5.0000000001e+25,1877.1621580193864,2689.98,140.0,197000000000000.0,393000000000000.0,16000000000.0,Google TPU v5e,Significant use,1,4194304,24175462.27839436,153328860.0,3.9399999999212e-11,7980000.0,58021109468.14646,19152000000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.",https://arxiv.org/abs/2312.11805,Gemini: A Family of Highly Capable Multimodal Models
Gemini Nano-2,Multimodal,USA,Google DeepMind,2023-12-19,Industry,Chat,Confident,57000.0,Unreleased,Unspecified unreleased,Not-defined,3250000000.0,2625000000000.0,2400.0,5.0000000001e+25,1877.1621580193864,2689.98,140.0,197000000000000.0,393000000000000.0,16000000000.0,Google TPU v5e,Significant use,1,4194304,24175462.27839436,153328860.0,3.9399999999212e-11,7980000.0,58021109468.14646,19152000000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.",https://arxiv.org/abs/2312.11805,Gemini: A Family of Highly Capable Multimodal Models
Gemini Nano-2,Multimodal,USA,Google DeepMind,2023-12-19,Industry,Image captioning,Confident,57000.0,Unreleased,Unspecified unreleased,Not-defined,3250000000.0,2625000000000.0,2400.0,5.0000000001e+25,1877.1621580193864,2689.98,140.0,197000000000000.0,393000000000000.0,16000000000.0,Google TPU v5e,Significant use,1,4194304,24175462.27839436,153328860.0,3.9399999999212e-11,7980000.0,58021109468.14646,19152000000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.",https://arxiv.org/abs/2312.11805,Gemini: A Family of Highly Capable Multimodal Models
Gemini Nano-2,Multimodal,USA,Google DeepMind,2023-12-19,Industry,Speech recognition,Confident,57000.0,Unreleased,Unspecified unreleased,Not-defined,3250000000.0,2625000000000.0,2400.0,5.0000000001e+25,1877.1621580193864,2689.98,140.0,197000000000000.0,393000000000000.0,16000000000.0,Google TPU v5e,Significant use,1,4194304,24175462.27839436,153328860.0,3.9399999999212e-11,7980000.0,58021109468.14646,19152000000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.",https://arxiv.org/abs/2312.11805,Gemini: A Family of Highly Capable Multimodal Models
Gemini Nano-2,Vision,Multinational,Google DeepMind,2023-12-19,Industry,Chat,Confident,57000.0,Unreleased,Unspecified unreleased,Not-defined,3250000000.0,2625000000000.0,2400.0,5.0000000001e+25,1877.1621580193864,2689.98,140.0,197000000000000.0,393000000000000.0,16000000000.0,Google TPU v5e,Significant use,1,4194304,24175462.27839436,153328860.0,3.9399999999212e-11,7980000.0,58021109468.14646,19152000000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.",https://arxiv.org/abs/2312.11805,Gemini: A Family of Highly Capable Multimodal Models
Gemini Nano-2,Vision,Multinational,Google DeepMind,2023-12-19,Industry,Image captioning,Confident,57000.0,Unreleased,Unspecified unreleased,Not-defined,3250000000.0,2625000000000.0,2400.0,5.0000000001e+25,1877.1621580193864,2689.98,140.0,197000000000000.0,393000000000000.0,16000000000.0,Google TPU v5e,Significant use,1,4194304,24175462.27839436,153328860.0,3.9399999999212e-11,7980000.0,58021109468.14646,19152000000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.",https://arxiv.org/abs/2312.11805,Gemini: A Family of Highly Capable Multimodal Models
Gemini Nano-2,Vision,Multinational,Google DeepMind,2023-12-19,Industry,Speech recognition,Confident,57000.0,Unreleased,Unspecified unreleased,Not-defined,3250000000.0,2625000000000.0,2400.0,5.0000000001e+25,1877.1621580193864,2689.98,140.0,197000000000000.0,393000000000000.0,16000000000.0,Google TPU v5e,Significant use,1,4194304,24175462.27839436,153328860.0,3.9399999999212e-11,7980000.0,58021109468.14646,19152000000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.",https://arxiv.org/abs/2312.11805,Gemini: A Family of Highly Capable Multimodal Models
Gemini Nano-2,Vision,UK,Google DeepMind,2023-12-19,Industry,Chat,Confident,57000.0,Unreleased,Unspecified unreleased,Not-defined,3250000000.0,2625000000000.0,2400.0,5.0000000001e+25,1877.1621580193864,2689.98,140.0,197000000000000.0,393000000000000.0,16000000000.0,Google TPU v5e,Significant use,1,4194304,24175462.27839436,153328860.0,3.9399999999212e-11,7980000.0,58021109468.14646,19152000000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.",https://arxiv.org/abs/2312.11805,Gemini: A Family of Highly Capable Multimodal Models
Gemini Nano-2,Vision,UK,Google DeepMind,2023-12-19,Industry,Image captioning,Confident,57000.0,Unreleased,Unspecified unreleased,Not-defined,3250000000.0,2625000000000.0,2400.0,5.0000000001e+25,1877.1621580193864,2689.98,140.0,197000000000000.0,393000000000000.0,16000000000.0,Google TPU v5e,Significant use,1,4194304,24175462.27839436,153328860.0,3.9399999999212e-11,7980000.0,58021109468.14646,19152000000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.",https://arxiv.org/abs/2312.11805,Gemini: A Family of Highly Capable Multimodal Models
Gemini Nano-2,Vision,UK,Google DeepMind,2023-12-19,Industry,Speech recognition,Confident,57000.0,Unreleased,Unspecified unreleased,Not-defined,3250000000.0,2625000000000.0,2400.0,5.0000000001e+25,1877.1621580193864,2689.98,140.0,197000000000000.0,393000000000000.0,16000000000.0,Google TPU v5e,Significant use,1,4194304,24175462.27839436,153328860.0,3.9399999999212e-11,7980000.0,58021109468.14646,19152000000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.",https://arxiv.org/abs/2312.11805,Gemini: A Family of Highly Capable Multimodal Models
Gemini Nano-2,Vision,USA,Google DeepMind,2023-12-19,Industry,Chat,Confident,57000.0,Unreleased,Unspecified unreleased,Not-defined,3250000000.0,2625000000000.0,2400.0,5.0000000001e+25,1877.1621580193864,2689.98,140.0,197000000000000.0,393000000000000.0,16000000000.0,Google TPU v5e,Significant use,1,4194304,24175462.27839436,153328860.0,3.9399999999212e-11,7980000.0,58021109468.14646,19152000000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.",https://arxiv.org/abs/2312.11805,Gemini: A Family of Highly Capable Multimodal Models
Gemini Nano-2,Vision,USA,Google DeepMind,2023-12-19,Industry,Image captioning,Confident,57000.0,Unreleased,Unspecified unreleased,Not-defined,3250000000.0,2625000000000.0,2400.0,5.0000000001e+25,1877.1621580193864,2689.98,140.0,197000000000000.0,393000000000000.0,16000000000.0,Google TPU v5e,Significant use,1,4194304,24175462.27839436,153328860.0,3.9399999999212e-11,7980000.0,58021109468.14646,19152000000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.",https://arxiv.org/abs/2312.11805,Gemini: A Family of Highly Capable Multimodal Models
Gemini Nano-2,Vision,USA,Google DeepMind,2023-12-19,Industry,Speech recognition,Confident,57000.0,Unreleased,Unspecified unreleased,Not-defined,3250000000.0,2625000000000.0,2400.0,5.0000000001e+25,1877.1621580193864,2689.98,140.0,197000000000000.0,393000000000000.0,16000000000.0,Google TPU v5e,Significant use,1,4194304,24175462.27839436,153328860.0,3.9399999999212e-11,7980000.0,58021109468.14646,19152000000.0,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.",https://arxiv.org/abs/2312.11805,Gemini: A Family of Highly Capable Multimodal Models
Gemma 1.1 7B Instruct,Language,USA,Google,2024-02-24,Industry,Language modeling/generation,Confident,6144.0,Open weights (restricted use),Unspecified unreleased,Unreleased,8540000000.0,6000000000000.0,1282.4326530612243,3.0744e+23,1877.1621580193864,2689.98,140.0,197000000000000.0,393000000000000.0,16000000000.0,Google TPU v5e,None,1,6472188,197336.5951439333,16527237.120000001,6.407754358574032e-09,860160.0,253070893.25650308,1103097270.8571427,"This is Gemma 1.1 7B (IT), an update over the original instruction-tuned Gemma release.

Gemma 1.1 was trained using a novel RLHF method, leading to substantial gains on quality, coding capabilities, factuality, instruction following and multi-turn conversation quality. We also fixed a bug in multi-turn conversations, and made sure that model responses don't always start with ""Sure,"".

We believe this release represents an improvement for most use cases, but we encourage users to test in their particular applications. The previous model will continue to be available in the same repo. We appreciate the enthusiastic adoption of Gemma, and we continue to welcome all feedback from the community.",https://huggingface.co/google/gemma-1.1-7b-it,
Gemma 1.1 7B Instruct,Language,USA,Google,2024-02-24,Industry,Question answering,Confident,6144.0,Open weights (restricted use),Unspecified unreleased,Unreleased,8540000000.0,6000000000000.0,1282.4326530612243,3.0744e+23,1877.1621580193864,2689.98,140.0,197000000000000.0,393000000000000.0,16000000000.0,Google TPU v5e,None,1,6472188,197336.5951439333,16527237.120000001,6.407754358574032e-09,860160.0,253070893.25650308,1103097270.8571427,"This is Gemma 1.1 7B (IT), an update over the original instruction-tuned Gemma release.

Gemma 1.1 was trained using a novel RLHF method, leading to substantial gains on quality, coding capabilities, factuality, instruction following and multi-turn conversation quality. We also fixed a bug in multi-turn conversations, and made sure that model responses don't always start with ""Sure,"".

We believe this release represents an improvement for most use cases, but we encourage users to test in their particular applications. The previous model will continue to be available in the same repo. We appreciate the enthusiastic adoption of Gemma, and we continue to welcome all feedback from the community.",https://huggingface.co/google/gemma-1.1-7b-it,
Gemma 2 27B,Language,Multinational,Google DeepMind,2024-06-24,Industry,Chat,Confident,6144.0,Open weights (restricted use),Unspecified unreleased,Unreleased,27000000000.0,13000000000000.0,1282.4326530612243,2.106e+24,1877.1621580193864,6086.016266666666,155.3111111111111,272555555555555.6,483444444444444.5,37111111111.11112,Google TPU v5p,None,1,4000000,197336.5951439333,37392483.94239999,1.2941859238155535e-09,954231.4666666666,253070893.25650308,1223737591.4318364,"Now we’re officially releasing Gemma 2 to researchers and developers globally. Available in both 9 billion (9B) and 27 billion (27B) parameter sizes, Gemma 2 is higher-performing and more efficient at inference than the first generation, with significant safety advancements built in. In fact, at 27B, it offers competitive alternatives to models more than twice its size, delivering the kind of performance that was only possible with proprietary models as recently as December. And that’s now achievable on a single NVIDIA H100 Tensor Core GPU or TPU host, significantly reducing deployment costs.",https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf,"Gemma 2 offers best-in-class performance, runs at incredible speed across different hardware and easily integrates with other AI tools."
Gemma 2 27B,Language,Multinational,Google DeepMind,2024-06-24,Industry,Code generation,Confident,6144.0,Open weights (restricted use),Unspecified unreleased,Unreleased,27000000000.0,13000000000000.0,1282.4326530612243,2.106e+24,1877.1621580193864,6086.016266666666,155.3111111111111,272555555555555.6,483444444444444.5,37111111111.11112,Google TPU v5p,None,1,4000000,197336.5951439333,37392483.94239999,1.2941859238155535e-09,954231.4666666666,253070893.25650308,1223737591.4318364,"Now we’re officially releasing Gemma 2 to researchers and developers globally. Available in both 9 billion (9B) and 27 billion (27B) parameter sizes, Gemma 2 is higher-performing and more efficient at inference than the first generation, with significant safety advancements built in. In fact, at 27B, it offers competitive alternatives to models more than twice its size, delivering the kind of performance that was only possible with proprietary models as recently as December. And that’s now achievable on a single NVIDIA H100 Tensor Core GPU or TPU host, significantly reducing deployment costs.",https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf,"Gemma 2 offers best-in-class performance, runs at incredible speed across different hardware and easily integrates with other AI tools."
Gemma 2 27B,Language,Multinational,Google DeepMind,2024-06-24,Industry,Language modeling/generation,Confident,6144.0,Open weights (restricted use),Unspecified unreleased,Unreleased,27000000000.0,13000000000000.0,1282.4326530612243,2.106e+24,1877.1621580193864,6086.016266666666,155.3111111111111,272555555555555.6,483444444444444.5,37111111111.11112,Google TPU v5p,None,1,4000000,197336.5951439333,37392483.94239999,1.2941859238155535e-09,954231.4666666666,253070893.25650308,1223737591.4318364,"Now we’re officially releasing Gemma 2 to researchers and developers globally. Available in both 9 billion (9B) and 27 billion (27B) parameter sizes, Gemma 2 is higher-performing and more efficient at inference than the first generation, with significant safety advancements built in. In fact, at 27B, it offers competitive alternatives to models more than twice its size, delivering the kind of performance that was only possible with proprietary models as recently as December. And that’s now achievable on a single NVIDIA H100 Tensor Core GPU or TPU host, significantly reducing deployment costs.",https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf,"Gemma 2 offers best-in-class performance, runs at incredible speed across different hardware and easily integrates with other AI tools."
Gemma 2 27B,Language,Multinational,Google DeepMind,2024-06-24,Industry,Quantitative reasoning,Confident,6144.0,Open weights (restricted use),Unspecified unreleased,Unreleased,27000000000.0,13000000000000.0,1282.4326530612243,2.106e+24,1877.1621580193864,6086.016266666666,155.3111111111111,272555555555555.6,483444444444444.5,37111111111.11112,Google TPU v5p,None,1,4000000,197336.5951439333,37392483.94239999,1.2941859238155535e-09,954231.4666666666,253070893.25650308,1223737591.4318364,"Now we’re officially releasing Gemma 2 to researchers and developers globally. Available in both 9 billion (9B) and 27 billion (27B) parameter sizes, Gemma 2 is higher-performing and more efficient at inference than the first generation, with significant safety advancements built in. In fact, at 27B, it offers competitive alternatives to models more than twice its size, delivering the kind of performance that was only possible with proprietary models as recently as December. And that’s now achievable on a single NVIDIA H100 Tensor Core GPU or TPU host, significantly reducing deployment costs.",https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf,"Gemma 2 offers best-in-class performance, runs at incredible speed across different hardware and easily integrates with other AI tools."
Gemma 2 27B,Language,Multinational,Google DeepMind,2024-06-24,Industry,Question answering,Confident,6144.0,Open weights (restricted use),Unspecified unreleased,Unreleased,27000000000.0,13000000000000.0,1282.4326530612243,2.106e+24,1877.1621580193864,6086.016266666666,155.3111111111111,272555555555555.6,483444444444444.5,37111111111.11112,Google TPU v5p,None,1,4000000,197336.5951439333,37392483.94239999,1.2941859238155535e-09,954231.4666666666,253070893.25650308,1223737591.4318364,"Now we’re officially releasing Gemma 2 to researchers and developers globally. Available in both 9 billion (9B) and 27 billion (27B) parameter sizes, Gemma 2 is higher-performing and more efficient at inference than the first generation, with significant safety advancements built in. In fact, at 27B, it offers competitive alternatives to models more than twice its size, delivering the kind of performance that was only possible with proprietary models as recently as December. And that’s now achievable on a single NVIDIA H100 Tensor Core GPU or TPU host, significantly reducing deployment costs.",https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf,"Gemma 2 offers best-in-class performance, runs at incredible speed across different hardware and easily integrates with other AI tools."
Gemma 2 27B,Language,UK,Google DeepMind,2024-06-24,Industry,Chat,Confident,6144.0,Open weights (restricted use),Unspecified unreleased,Unreleased,27000000000.0,13000000000000.0,1282.4326530612243,2.106e+24,1877.1621580193864,6086.016266666666,155.3111111111111,272555555555555.6,483444444444444.5,37111111111.11112,Google TPU v5p,None,1,4000000,197336.5951439333,37392483.94239999,1.2941859238155535e-09,954231.4666666666,253070893.25650308,1223737591.4318364,"Now we’re officially releasing Gemma 2 to researchers and developers globally. Available in both 9 billion (9B) and 27 billion (27B) parameter sizes, Gemma 2 is higher-performing and more efficient at inference than the first generation, with significant safety advancements built in. In fact, at 27B, it offers competitive alternatives to models more than twice its size, delivering the kind of performance that was only possible with proprietary models as recently as December. And that’s now achievable on a single NVIDIA H100 Tensor Core GPU or TPU host, significantly reducing deployment costs.",https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf,"Gemma 2 offers best-in-class performance, runs at incredible speed across different hardware and easily integrates with other AI tools."
Gemma 2 27B,Language,UK,Google DeepMind,2024-06-24,Industry,Code generation,Confident,6144.0,Open weights (restricted use),Unspecified unreleased,Unreleased,27000000000.0,13000000000000.0,1282.4326530612243,2.106e+24,1877.1621580193864,6086.016266666666,155.3111111111111,272555555555555.6,483444444444444.5,37111111111.11112,Google TPU v5p,None,1,4000000,197336.5951439333,37392483.94239999,1.2941859238155535e-09,954231.4666666666,253070893.25650308,1223737591.4318364,"Now we’re officially releasing Gemma 2 to researchers and developers globally. Available in both 9 billion (9B) and 27 billion (27B) parameter sizes, Gemma 2 is higher-performing and more efficient at inference than the first generation, with significant safety advancements built in. In fact, at 27B, it offers competitive alternatives to models more than twice its size, delivering the kind of performance that was only possible with proprietary models as recently as December. And that’s now achievable on a single NVIDIA H100 Tensor Core GPU or TPU host, significantly reducing deployment costs.",https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf,"Gemma 2 offers best-in-class performance, runs at incredible speed across different hardware and easily integrates with other AI tools."
Gemma 2 27B,Language,UK,Google DeepMind,2024-06-24,Industry,Language modeling/generation,Confident,6144.0,Open weights (restricted use),Unspecified unreleased,Unreleased,27000000000.0,13000000000000.0,1282.4326530612243,2.106e+24,1877.1621580193864,6086.016266666666,155.3111111111111,272555555555555.6,483444444444444.5,37111111111.11112,Google TPU v5p,None,1,4000000,197336.5951439333,37392483.94239999,1.2941859238155535e-09,954231.4666666666,253070893.25650308,1223737591.4318364,"Now we’re officially releasing Gemma 2 to researchers and developers globally. Available in both 9 billion (9B) and 27 billion (27B) parameter sizes, Gemma 2 is higher-performing and more efficient at inference than the first generation, with significant safety advancements built in. In fact, at 27B, it offers competitive alternatives to models more than twice its size, delivering the kind of performance that was only possible with proprietary models as recently as December. And that’s now achievable on a single NVIDIA H100 Tensor Core GPU or TPU host, significantly reducing deployment costs.",https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf,"Gemma 2 offers best-in-class performance, runs at incredible speed across different hardware and easily integrates with other AI tools."
Gemma 2 27B,Language,UK,Google DeepMind,2024-06-24,Industry,Quantitative reasoning,Confident,6144.0,Open weights (restricted use),Unspecified unreleased,Unreleased,27000000000.0,13000000000000.0,1282.4326530612243,2.106e+24,1877.1621580193864,6086.016266666666,155.3111111111111,272555555555555.6,483444444444444.5,37111111111.11112,Google TPU v5p,None,1,4000000,197336.5951439333,37392483.94239999,1.2941859238155535e-09,954231.4666666666,253070893.25650308,1223737591.4318364,"Now we’re officially releasing Gemma 2 to researchers and developers globally. Available in both 9 billion (9B) and 27 billion (27B) parameter sizes, Gemma 2 is higher-performing and more efficient at inference than the first generation, with significant safety advancements built in. In fact, at 27B, it offers competitive alternatives to models more than twice its size, delivering the kind of performance that was only possible with proprietary models as recently as December. And that’s now achievable on a single NVIDIA H100 Tensor Core GPU or TPU host, significantly reducing deployment costs.",https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf,"Gemma 2 offers best-in-class performance, runs at incredible speed across different hardware and easily integrates with other AI tools."
Gemma 2 27B,Language,UK,Google DeepMind,2024-06-24,Industry,Question answering,Confident,6144.0,Open weights (restricted use),Unspecified unreleased,Unreleased,27000000000.0,13000000000000.0,1282.4326530612243,2.106e+24,1877.1621580193864,6086.016266666666,155.3111111111111,272555555555555.6,483444444444444.5,37111111111.11112,Google TPU v5p,None,1,4000000,197336.5951439333,37392483.94239999,1.2941859238155535e-09,954231.4666666666,253070893.25650308,1223737591.4318364,"Now we’re officially releasing Gemma 2 to researchers and developers globally. Available in both 9 billion (9B) and 27 billion (27B) parameter sizes, Gemma 2 is higher-performing and more efficient at inference than the first generation, with significant safety advancements built in. In fact, at 27B, it offers competitive alternatives to models more than twice its size, delivering the kind of performance that was only possible with proprietary models as recently as December. And that’s now achievable on a single NVIDIA H100 Tensor Core GPU or TPU host, significantly reducing deployment costs.",https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf,"Gemma 2 offers best-in-class performance, runs at incredible speed across different hardware and easily integrates with other AI tools."
Gemma 2 27B,Language,USA,Google DeepMind,2024-06-24,Industry,Chat,Confident,6144.0,Open weights (restricted use),Unspecified unreleased,Unreleased,27000000000.0,13000000000000.0,1282.4326530612243,2.106e+24,1877.1621580193864,6086.016266666666,155.3111111111111,272555555555555.6,483444444444444.5,37111111111.11112,Google TPU v5p,None,1,4000000,197336.5951439333,37392483.94239999,1.2941859238155535e-09,954231.4666666666,253070893.25650308,1223737591.4318364,"Now we’re officially releasing Gemma 2 to researchers and developers globally. Available in both 9 billion (9B) and 27 billion (27B) parameter sizes, Gemma 2 is higher-performing and more efficient at inference than the first generation, with significant safety advancements built in. In fact, at 27B, it offers competitive alternatives to models more than twice its size, delivering the kind of performance that was only possible with proprietary models as recently as December. And that’s now achievable on a single NVIDIA H100 Tensor Core GPU or TPU host, significantly reducing deployment costs.",https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf,"Gemma 2 offers best-in-class performance, runs at incredible speed across different hardware and easily integrates with other AI tools."
Gemma 2 27B,Language,USA,Google DeepMind,2024-06-24,Industry,Code generation,Confident,6144.0,Open weights (restricted use),Unspecified unreleased,Unreleased,27000000000.0,13000000000000.0,1282.4326530612243,2.106e+24,1877.1621580193864,6086.016266666666,155.3111111111111,272555555555555.6,483444444444444.5,37111111111.11112,Google TPU v5p,None,1,4000000,197336.5951439333,37392483.94239999,1.2941859238155535e-09,954231.4666666666,253070893.25650308,1223737591.4318364,"Now we’re officially releasing Gemma 2 to researchers and developers globally. Available in both 9 billion (9B) and 27 billion (27B) parameter sizes, Gemma 2 is higher-performing and more efficient at inference than the first generation, with significant safety advancements built in. In fact, at 27B, it offers competitive alternatives to models more than twice its size, delivering the kind of performance that was only possible with proprietary models as recently as December. And that’s now achievable on a single NVIDIA H100 Tensor Core GPU or TPU host, significantly reducing deployment costs.",https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf,"Gemma 2 offers best-in-class performance, runs at incredible speed across different hardware and easily integrates with other AI tools."
Gemma 2 27B,Language,USA,Google DeepMind,2024-06-24,Industry,Language modeling/generation,Confident,6144.0,Open weights (restricted use),Unspecified unreleased,Unreleased,27000000000.0,13000000000000.0,1282.4326530612243,2.106e+24,1877.1621580193864,6086.016266666666,155.3111111111111,272555555555555.6,483444444444444.5,37111111111.11112,Google TPU v5p,None,1,4000000,197336.5951439333,37392483.94239999,1.2941859238155535e-09,954231.4666666666,253070893.25650308,1223737591.4318364,"Now we’re officially releasing Gemma 2 to researchers and developers globally. Available in both 9 billion (9B) and 27 billion (27B) parameter sizes, Gemma 2 is higher-performing and more efficient at inference than the first generation, with significant safety advancements built in. In fact, at 27B, it offers competitive alternatives to models more than twice its size, delivering the kind of performance that was only possible with proprietary models as recently as December. And that’s now achievable on a single NVIDIA H100 Tensor Core GPU or TPU host, significantly reducing deployment costs.",https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf,"Gemma 2 offers best-in-class performance, runs at incredible speed across different hardware and easily integrates with other AI tools."
Gemma 2 27B,Language,USA,Google DeepMind,2024-06-24,Industry,Quantitative reasoning,Confident,6144.0,Open weights (restricted use),Unspecified unreleased,Unreleased,27000000000.0,13000000000000.0,1282.4326530612243,2.106e+24,1877.1621580193864,6086.016266666666,155.3111111111111,272555555555555.6,483444444444444.5,37111111111.11112,Google TPU v5p,None,1,4000000,197336.5951439333,37392483.94239999,1.2941859238155535e-09,954231.4666666666,253070893.25650308,1223737591.4318364,"Now we’re officially releasing Gemma 2 to researchers and developers globally. Available in both 9 billion (9B) and 27 billion (27B) parameter sizes, Gemma 2 is higher-performing and more efficient at inference than the first generation, with significant safety advancements built in. In fact, at 27B, it offers competitive alternatives to models more than twice its size, delivering the kind of performance that was only possible with proprietary models as recently as December. And that’s now achievable on a single NVIDIA H100 Tensor Core GPU or TPU host, significantly reducing deployment costs.",https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf,"Gemma 2 offers best-in-class performance, runs at incredible speed across different hardware and easily integrates with other AI tools."
Gemma 2 27B,Language,USA,Google DeepMind,2024-06-24,Industry,Question answering,Confident,6144.0,Open weights (restricted use),Unspecified unreleased,Unreleased,27000000000.0,13000000000000.0,1282.4326530612243,2.106e+24,1877.1621580193864,6086.016266666666,155.3111111111111,272555555555555.6,483444444444444.5,37111111111.11112,Google TPU v5p,None,1,4000000,197336.5951439333,37392483.94239999,1.2941859238155535e-09,954231.4666666666,253070893.25650308,1223737591.4318364,"Now we’re officially releasing Gemma 2 to researchers and developers globally. Available in both 9 billion (9B) and 27 billion (27B) parameter sizes, Gemma 2 is higher-performing and more efficient at inference than the first generation, with significant safety advancements built in. In fact, at 27B, it offers competitive alternatives to models more than twice its size, delivering the kind of performance that was only possible with proprietary models as recently as December. And that’s now achievable on a single NVIDIA H100 Tensor Core GPU or TPU host, significantly reducing deployment costs.",https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf,"Gemma 2 offers best-in-class performance, runs at incredible speed across different hardware and easily integrates with other AI tools."
Gemma 2 9B,Language,Multinational,Google DeepMind,2024-06-24,Industry,Chat,Confident,4096.0,Open weights (restricted use),Unspecified unreleased,Unreleased,9000000000.0,8000000000000.0,1282.4326530612243,4.32e+23,1877.1621580193864,6086.016266666666,155.3111111111111,272555555555555.6,483444444444444.5,37111111111.11112,Google TPU v4,None,1,6472188,197336.5951439333,24928322.628266662,6.309156378600823e-09,636154.3111111111,253070893.25650308,815825060.9545577,"Now we’re officially releasing Gemma 2 to researchers and developers globally. Available in both 9 billion (9B) and 27 billion (27B) parameter sizes, Gemma 2 is higher-performing and more efficient at inference than the first generation, with significant safety advancements built in. In fact, at 27B, it offers competitive alternatives to models more than twice its size, delivering the kind of performance that was only possible with proprietary models as recently as December. And that’s now achievable on a single NVIDIA H100 Tensor Core GPU or TPU host, significantly reducing deployment costs.",https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf,"Gemma 2 offers best-in-class performance, runs at incredible speed across different hardware and easily integrates with other AI tools."
Gemma 2 9B,Language,Multinational,Google DeepMind,2024-06-24,Industry,Code generation,Confident,4096.0,Open weights (restricted use),Unspecified unreleased,Unreleased,9000000000.0,8000000000000.0,1282.4326530612243,4.32e+23,1877.1621580193864,6086.016266666666,155.3111111111111,272555555555555.6,483444444444444.5,37111111111.11112,Google TPU v4,None,1,6472188,197336.5951439333,24928322.628266662,6.309156378600823e-09,636154.3111111111,253070893.25650308,815825060.9545577,"Now we’re officially releasing Gemma 2 to researchers and developers globally. Available in both 9 billion (9B) and 27 billion (27B) parameter sizes, Gemma 2 is higher-performing and more efficient at inference than the first generation, with significant safety advancements built in. In fact, at 27B, it offers competitive alternatives to models more than twice its size, delivering the kind of performance that was only possible with proprietary models as recently as December. And that’s now achievable on a single NVIDIA H100 Tensor Core GPU or TPU host, significantly reducing deployment costs.",https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf,"Gemma 2 offers best-in-class performance, runs at incredible speed across different hardware and easily integrates with other AI tools."
Gemma 2 9B,Language,Multinational,Google DeepMind,2024-06-24,Industry,Language modeling/generation,Confident,4096.0,Open weights (restricted use),Unspecified unreleased,Unreleased,9000000000.0,8000000000000.0,1282.4326530612243,4.32e+23,1877.1621580193864,6086.016266666666,155.3111111111111,272555555555555.6,483444444444444.5,37111111111.11112,Google TPU v4,None,1,6472188,197336.5951439333,24928322.628266662,6.309156378600823e-09,636154.3111111111,253070893.25650308,815825060.9545577,"Now we’re officially releasing Gemma 2 to researchers and developers globally. Available in both 9 billion (9B) and 27 billion (27B) parameter sizes, Gemma 2 is higher-performing and more efficient at inference than the first generation, with significant safety advancements built in. In fact, at 27B, it offers competitive alternatives to models more than twice its size, delivering the kind of performance that was only possible with proprietary models as recently as December. And that’s now achievable on a single NVIDIA H100 Tensor Core GPU or TPU host, significantly reducing deployment costs.",https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf,"Gemma 2 offers best-in-class performance, runs at incredible speed across different hardware and easily integrates with other AI tools."
Gemma 2 9B,Language,Multinational,Google DeepMind,2024-06-24,Industry,Quantitative reasoning,Confident,4096.0,Open weights (restricted use),Unspecified unreleased,Unreleased,9000000000.0,8000000000000.0,1282.4326530612243,4.32e+23,1877.1621580193864,6086.016266666666,155.3111111111111,272555555555555.6,483444444444444.5,37111111111.11112,Google TPU v4,None,1,6472188,197336.5951439333,24928322.628266662,6.309156378600823e-09,636154.3111111111,253070893.25650308,815825060.9545577,"Now we’re officially releasing Gemma 2 to researchers and developers globally. Available in both 9 billion (9B) and 27 billion (27B) parameter sizes, Gemma 2 is higher-performing and more efficient at inference than the first generation, with significant safety advancements built in. In fact, at 27B, it offers competitive alternatives to models more than twice its size, delivering the kind of performance that was only possible with proprietary models as recently as December. And that’s now achievable on a single NVIDIA H100 Tensor Core GPU or TPU host, significantly reducing deployment costs.",https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf,"Gemma 2 offers best-in-class performance, runs at incredible speed across different hardware and easily integrates with other AI tools."
Gemma 2 9B,Language,Multinational,Google DeepMind,2024-06-24,Industry,Question answering,Confident,4096.0,Open weights (restricted use),Unspecified unreleased,Unreleased,9000000000.0,8000000000000.0,1282.4326530612243,4.32e+23,1877.1621580193864,6086.016266666666,155.3111111111111,272555555555555.6,483444444444444.5,37111111111.11112,Google TPU v4,None,1,6472188,197336.5951439333,24928322.628266662,6.309156378600823e-09,636154.3111111111,253070893.25650308,815825060.9545577,"Now we’re officially releasing Gemma 2 to researchers and developers globally. Available in both 9 billion (9B) and 27 billion (27B) parameter sizes, Gemma 2 is higher-performing and more efficient at inference than the first generation, with significant safety advancements built in. In fact, at 27B, it offers competitive alternatives to models more than twice its size, delivering the kind of performance that was only possible with proprietary models as recently as December. And that’s now achievable on a single NVIDIA H100 Tensor Core GPU or TPU host, significantly reducing deployment costs.",https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf,"Gemma 2 offers best-in-class performance, runs at incredible speed across different hardware and easily integrates with other AI tools."
Gemma 2 9B,Language,UK,Google DeepMind,2024-06-24,Industry,Chat,Confident,4096.0,Open weights (restricted use),Unspecified unreleased,Unreleased,9000000000.0,8000000000000.0,1282.4326530612243,4.32e+23,1877.1621580193864,6086.016266666666,155.3111111111111,272555555555555.6,483444444444444.5,37111111111.11112,Google TPU v4,None,1,6472188,197336.5951439333,24928322.628266662,6.309156378600823e-09,636154.3111111111,253070893.25650308,815825060.9545577,"Now we’re officially releasing Gemma 2 to researchers and developers globally. Available in both 9 billion (9B) and 27 billion (27B) parameter sizes, Gemma 2 is higher-performing and more efficient at inference than the first generation, with significant safety advancements built in. In fact, at 27B, it offers competitive alternatives to models more than twice its size, delivering the kind of performance that was only possible with proprietary models as recently as December. And that’s now achievable on a single NVIDIA H100 Tensor Core GPU or TPU host, significantly reducing deployment costs.",https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf,"Gemma 2 offers best-in-class performance, runs at incredible speed across different hardware and easily integrates with other AI tools."
Gemma 2 9B,Language,UK,Google DeepMind,2024-06-24,Industry,Code generation,Confident,4096.0,Open weights (restricted use),Unspecified unreleased,Unreleased,9000000000.0,8000000000000.0,1282.4326530612243,4.32e+23,1877.1621580193864,6086.016266666666,155.3111111111111,272555555555555.6,483444444444444.5,37111111111.11112,Google TPU v4,None,1,6472188,197336.5951439333,24928322.628266662,6.309156378600823e-09,636154.3111111111,253070893.25650308,815825060.9545577,"Now we’re officially releasing Gemma 2 to researchers and developers globally. Available in both 9 billion (9B) and 27 billion (27B) parameter sizes, Gemma 2 is higher-performing and more efficient at inference than the first generation, with significant safety advancements built in. In fact, at 27B, it offers competitive alternatives to models more than twice its size, delivering the kind of performance that was only possible with proprietary models as recently as December. And that’s now achievable on a single NVIDIA H100 Tensor Core GPU or TPU host, significantly reducing deployment costs.",https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf,"Gemma 2 offers best-in-class performance, runs at incredible speed across different hardware and easily integrates with other AI tools."
Gemma 2 9B,Language,UK,Google DeepMind,2024-06-24,Industry,Language modeling/generation,Confident,4096.0,Open weights (restricted use),Unspecified unreleased,Unreleased,9000000000.0,8000000000000.0,1282.4326530612243,4.32e+23,1877.1621580193864,6086.016266666666,155.3111111111111,272555555555555.6,483444444444444.5,37111111111.11112,Google TPU v4,None,1,6472188,197336.5951439333,24928322.628266662,6.309156378600823e-09,636154.3111111111,253070893.25650308,815825060.9545577,"Now we’re officially releasing Gemma 2 to researchers and developers globally. Available in both 9 billion (9B) and 27 billion (27B) parameter sizes, Gemma 2 is higher-performing and more efficient at inference than the first generation, with significant safety advancements built in. In fact, at 27B, it offers competitive alternatives to models more than twice its size, delivering the kind of performance that was only possible with proprietary models as recently as December. And that’s now achievable on a single NVIDIA H100 Tensor Core GPU or TPU host, significantly reducing deployment costs.",https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf,"Gemma 2 offers best-in-class performance, runs at incredible speed across different hardware and easily integrates with other AI tools."
Gemma 2 9B,Language,UK,Google DeepMind,2024-06-24,Industry,Quantitative reasoning,Confident,4096.0,Open weights (restricted use),Unspecified unreleased,Unreleased,9000000000.0,8000000000000.0,1282.4326530612243,4.32e+23,1877.1621580193864,6086.016266666666,155.3111111111111,272555555555555.6,483444444444444.5,37111111111.11112,Google TPU v4,None,1,6472188,197336.5951439333,24928322.628266662,6.309156378600823e-09,636154.3111111111,253070893.25650308,815825060.9545577,"Now we’re officially releasing Gemma 2 to researchers and developers globally. Available in both 9 billion (9B) and 27 billion (27B) parameter sizes, Gemma 2 is higher-performing and more efficient at inference than the first generation, with significant safety advancements built in. In fact, at 27B, it offers competitive alternatives to models more than twice its size, delivering the kind of performance that was only possible with proprietary models as recently as December. And that’s now achievable on a single NVIDIA H100 Tensor Core GPU or TPU host, significantly reducing deployment costs.",https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf,"Gemma 2 offers best-in-class performance, runs at incredible speed across different hardware and easily integrates with other AI tools."
Gemma 2 9B,Language,UK,Google DeepMind,2024-06-24,Industry,Question answering,Confident,4096.0,Open weights (restricted use),Unspecified unreleased,Unreleased,9000000000.0,8000000000000.0,1282.4326530612243,4.32e+23,1877.1621580193864,6086.016266666666,155.3111111111111,272555555555555.6,483444444444444.5,37111111111.11112,Google TPU v4,None,1,6472188,197336.5951439333,24928322.628266662,6.309156378600823e-09,636154.3111111111,253070893.25650308,815825060.9545577,"Now we’re officially releasing Gemma 2 to researchers and developers globally. Available in both 9 billion (9B) and 27 billion (27B) parameter sizes, Gemma 2 is higher-performing and more efficient at inference than the first generation, with significant safety advancements built in. In fact, at 27B, it offers competitive alternatives to models more than twice its size, delivering the kind of performance that was only possible with proprietary models as recently as December. And that’s now achievable on a single NVIDIA H100 Tensor Core GPU or TPU host, significantly reducing deployment costs.",https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf,"Gemma 2 offers best-in-class performance, runs at incredible speed across different hardware and easily integrates with other AI tools."
Gemma 2 9B,Language,USA,Google DeepMind,2024-06-24,Industry,Chat,Confident,4096.0,Open weights (restricted use),Unspecified unreleased,Unreleased,9000000000.0,8000000000000.0,1282.4326530612243,4.32e+23,1877.1621580193864,6086.016266666666,155.3111111111111,272555555555555.6,483444444444444.5,37111111111.11112,Google TPU v4,None,1,6472188,197336.5951439333,24928322.628266662,6.309156378600823e-09,636154.3111111111,253070893.25650308,815825060.9545577,"Now we’re officially releasing Gemma 2 to researchers and developers globally. Available in both 9 billion (9B) and 27 billion (27B) parameter sizes, Gemma 2 is higher-performing and more efficient at inference than the first generation, with significant safety advancements built in. In fact, at 27B, it offers competitive alternatives to models more than twice its size, delivering the kind of performance that was only possible with proprietary models as recently as December. And that’s now achievable on a single NVIDIA H100 Tensor Core GPU or TPU host, significantly reducing deployment costs.",https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf,"Gemma 2 offers best-in-class performance, runs at incredible speed across different hardware and easily integrates with other AI tools."
Gemma 2 9B,Language,USA,Google DeepMind,2024-06-24,Industry,Code generation,Confident,4096.0,Open weights (restricted use),Unspecified unreleased,Unreleased,9000000000.0,8000000000000.0,1282.4326530612243,4.32e+23,1877.1621580193864,6086.016266666666,155.3111111111111,272555555555555.6,483444444444444.5,37111111111.11112,Google TPU v4,None,1,6472188,197336.5951439333,24928322.628266662,6.309156378600823e-09,636154.3111111111,253070893.25650308,815825060.9545577,"Now we’re officially releasing Gemma 2 to researchers and developers globally. Available in both 9 billion (9B) and 27 billion (27B) parameter sizes, Gemma 2 is higher-performing and more efficient at inference than the first generation, with significant safety advancements built in. In fact, at 27B, it offers competitive alternatives to models more than twice its size, delivering the kind of performance that was only possible with proprietary models as recently as December. And that’s now achievable on a single NVIDIA H100 Tensor Core GPU or TPU host, significantly reducing deployment costs.",https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf,"Gemma 2 offers best-in-class performance, runs at incredible speed across different hardware and easily integrates with other AI tools."
Gemma 2 9B,Language,USA,Google DeepMind,2024-06-24,Industry,Language modeling/generation,Confident,4096.0,Open weights (restricted use),Unspecified unreleased,Unreleased,9000000000.0,8000000000000.0,1282.4326530612243,4.32e+23,1877.1621580193864,6086.016266666666,155.3111111111111,272555555555555.6,483444444444444.5,37111111111.11112,Google TPU v4,None,1,6472188,197336.5951439333,24928322.628266662,6.309156378600823e-09,636154.3111111111,253070893.25650308,815825060.9545577,"Now we’re officially releasing Gemma 2 to researchers and developers globally. Available in both 9 billion (9B) and 27 billion (27B) parameter sizes, Gemma 2 is higher-performing and more efficient at inference than the first generation, with significant safety advancements built in. In fact, at 27B, it offers competitive alternatives to models more than twice its size, delivering the kind of performance that was only possible with proprietary models as recently as December. And that’s now achievable on a single NVIDIA H100 Tensor Core GPU or TPU host, significantly reducing deployment costs.",https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf,"Gemma 2 offers best-in-class performance, runs at incredible speed across different hardware and easily integrates with other AI tools."
Gemma 2 9B,Language,USA,Google DeepMind,2024-06-24,Industry,Quantitative reasoning,Confident,4096.0,Open weights (restricted use),Unspecified unreleased,Unreleased,9000000000.0,8000000000000.0,1282.4326530612243,4.32e+23,1877.1621580193864,6086.016266666666,155.3111111111111,272555555555555.6,483444444444444.5,37111111111.11112,Google TPU v4,None,1,6472188,197336.5951439333,24928322.628266662,6.309156378600823e-09,636154.3111111111,253070893.25650308,815825060.9545577,"Now we’re officially releasing Gemma 2 to researchers and developers globally. Available in both 9 billion (9B) and 27 billion (27B) parameter sizes, Gemma 2 is higher-performing and more efficient at inference than the first generation, with significant safety advancements built in. In fact, at 27B, it offers competitive alternatives to models more than twice its size, delivering the kind of performance that was only possible with proprietary models as recently as December. And that’s now achievable on a single NVIDIA H100 Tensor Core GPU or TPU host, significantly reducing deployment costs.",https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf,"Gemma 2 offers best-in-class performance, runs at incredible speed across different hardware and easily integrates with other AI tools."
Gemma 2 9B,Language,USA,Google DeepMind,2024-06-24,Industry,Question answering,Confident,4096.0,Open weights (restricted use),Unspecified unreleased,Unreleased,9000000000.0,8000000000000.0,1282.4326530612243,4.32e+23,1877.1621580193864,6086.016266666666,155.3111111111111,272555555555555.6,483444444444444.5,37111111111.11112,Google TPU v4,None,1,6472188,197336.5951439333,24928322.628266662,6.309156378600823e-09,636154.3111111111,253070893.25650308,815825060.9545577,"Now we’re officially releasing Gemma 2 to researchers and developers globally. Available in both 9 billion (9B) and 27 billion (27B) parameter sizes, Gemma 2 is higher-performing and more efficient at inference than the first generation, with significant safety advancements built in. In fact, at 27B, it offers competitive alternatives to models more than twice its size, delivering the kind of performance that was only possible with proprietary models as recently as December. And that’s now achievable on a single NVIDIA H100 Tensor Core GPU or TPU host, significantly reducing deployment costs.",https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf,"Gemma 2 offers best-in-class performance, runs at incredible speed across different hardware and easily integrates with other AI tools."
Gemma 7B,Language,Multinational,Google DeepMind,2024-02-21,Industry,Chat,Confident,4096.0,Open weights (restricted use),Unspecified unreleased,Unreleased,8538074112.0,6000000000000.0,1282.4326530612243,3.07e+23,1877.1621580193864,6086.016266666666,155.3111111111111,272555555555555.6,483444444444444.5,37111111111.11112,Google TPU v5e,None,1,6472188,197336.5951439333,24928322.628266662,8.878031125588131e-09,636154.3111111111,253070893.25650308,815825060.9545577,,https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf,Gemma: Open Models Based on Gemini Research and Technology
Gemma 7B,Language,Multinational,Google DeepMind,2024-02-21,Industry,Code generation,Confident,4096.0,Open weights (restricted use),Unspecified unreleased,Unreleased,8538074112.0,6000000000000.0,1282.4326530612243,3.07e+23,1877.1621580193864,6086.016266666666,155.3111111111111,272555555555555.6,483444444444444.5,37111111111.11112,Google TPU v5e,None,1,6472188,197336.5951439333,24928322.628266662,8.878031125588131e-09,636154.3111111111,253070893.25650308,815825060.9545577,,https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf,Gemma: Open Models Based on Gemini Research and Technology
Gemma 7B,Language,Multinational,Google DeepMind,2024-02-21,Industry,Language modeling/generation,Confident,4096.0,Open weights (restricted use),Unspecified unreleased,Unreleased,8538074112.0,6000000000000.0,1282.4326530612243,3.07e+23,1877.1621580193864,6086.016266666666,155.3111111111111,272555555555555.6,483444444444444.5,37111111111.11112,Google TPU v5e,None,1,6472188,197336.5951439333,24928322.628266662,8.878031125588131e-09,636154.3111111111,253070893.25650308,815825060.9545577,,https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf,Gemma: Open Models Based on Gemini Research and Technology
Gemma 7B,Language,Multinational,Google DeepMind,2024-02-21,Industry,Quantitative reasoning,Confident,4096.0,Open weights (restricted use),Unspecified unreleased,Unreleased,8538074112.0,6000000000000.0,1282.4326530612243,3.07e+23,1877.1621580193864,6086.016266666666,155.3111111111111,272555555555555.6,483444444444444.5,37111111111.11112,Google TPU v5e,None,1,6472188,197336.5951439333,24928322.628266662,8.878031125588131e-09,636154.3111111111,253070893.25650308,815825060.9545577,,https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf,Gemma: Open Models Based on Gemini Research and Technology
Gemma 7B,Language,Multinational,Google DeepMind,2024-02-21,Industry,Question answering,Confident,4096.0,Open weights (restricted use),Unspecified unreleased,Unreleased,8538074112.0,6000000000000.0,1282.4326530612243,3.07e+23,1877.1621580193864,6086.016266666666,155.3111111111111,272555555555555.6,483444444444444.5,37111111111.11112,Google TPU v5e,None,1,6472188,197336.5951439333,24928322.628266662,8.878031125588131e-09,636154.3111111111,253070893.25650308,815825060.9545577,,https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf,Gemma: Open Models Based on Gemini Research and Technology
Gemma 7B,Language,UK,Google DeepMind,2024-02-21,Industry,Chat,Confident,4096.0,Open weights (restricted use),Unspecified unreleased,Unreleased,8538074112.0,6000000000000.0,1282.4326530612243,3.07e+23,1877.1621580193864,6086.016266666666,155.3111111111111,272555555555555.6,483444444444444.5,37111111111.11112,Google TPU v5e,None,1,6472188,197336.5951439333,24928322.628266662,8.878031125588131e-09,636154.3111111111,253070893.25650308,815825060.9545577,,https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf,Gemma: Open Models Based on Gemini Research and Technology
Gemma 7B,Language,UK,Google DeepMind,2024-02-21,Industry,Code generation,Confident,4096.0,Open weights (restricted use),Unspecified unreleased,Unreleased,8538074112.0,6000000000000.0,1282.4326530612243,3.07e+23,1877.1621580193864,6086.016266666666,155.3111111111111,272555555555555.6,483444444444444.5,37111111111.11112,Google TPU v5e,None,1,6472188,197336.5951439333,24928322.628266662,8.878031125588131e-09,636154.3111111111,253070893.25650308,815825060.9545577,,https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf,Gemma: Open Models Based on Gemini Research and Technology
Gemma 7B,Language,UK,Google DeepMind,2024-02-21,Industry,Language modeling/generation,Confident,4096.0,Open weights (restricted use),Unspecified unreleased,Unreleased,8538074112.0,6000000000000.0,1282.4326530612243,3.07e+23,1877.1621580193864,6086.016266666666,155.3111111111111,272555555555555.6,483444444444444.5,37111111111.11112,Google TPU v5e,None,1,6472188,197336.5951439333,24928322.628266662,8.878031125588131e-09,636154.3111111111,253070893.25650308,815825060.9545577,,https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf,Gemma: Open Models Based on Gemini Research and Technology
Gemma 7B,Language,UK,Google DeepMind,2024-02-21,Industry,Quantitative reasoning,Confident,4096.0,Open weights (restricted use),Unspecified unreleased,Unreleased,8538074112.0,6000000000000.0,1282.4326530612243,3.07e+23,1877.1621580193864,6086.016266666666,155.3111111111111,272555555555555.6,483444444444444.5,37111111111.11112,Google TPU v5e,None,1,6472188,197336.5951439333,24928322.628266662,8.878031125588131e-09,636154.3111111111,253070893.25650308,815825060.9545577,,https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf,Gemma: Open Models Based on Gemini Research and Technology
Gemma 7B,Language,UK,Google DeepMind,2024-02-21,Industry,Question answering,Confident,4096.0,Open weights (restricted use),Unspecified unreleased,Unreleased,8538074112.0,6000000000000.0,1282.4326530612243,3.07e+23,1877.1621580193864,6086.016266666666,155.3111111111111,272555555555555.6,483444444444444.5,37111111111.11112,Google TPU v5e,None,1,6472188,197336.5951439333,24928322.628266662,8.878031125588131e-09,636154.3111111111,253070893.25650308,815825060.9545577,,https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf,Gemma: Open Models Based on Gemini Research and Technology
Gemma 7B,Language,USA,Google DeepMind,2024-02-21,Industry,Chat,Confident,4096.0,Open weights (restricted use),Unspecified unreleased,Unreleased,8538074112.0,6000000000000.0,1282.4326530612243,3.07e+23,1877.1621580193864,6086.016266666666,155.3111111111111,272555555555555.6,483444444444444.5,37111111111.11112,Google TPU v5e,None,1,6472188,197336.5951439333,24928322.628266662,8.878031125588131e-09,636154.3111111111,253070893.25650308,815825060.9545577,,https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf,Gemma: Open Models Based on Gemini Research and Technology
Gemma 7B,Language,USA,Google DeepMind,2024-02-21,Industry,Code generation,Confident,4096.0,Open weights (restricted use),Unspecified unreleased,Unreleased,8538074112.0,6000000000000.0,1282.4326530612243,3.07e+23,1877.1621580193864,6086.016266666666,155.3111111111111,272555555555555.6,483444444444444.5,37111111111.11112,Google TPU v5e,None,1,6472188,197336.5951439333,24928322.628266662,8.878031125588131e-09,636154.3111111111,253070893.25650308,815825060.9545577,,https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf,Gemma: Open Models Based on Gemini Research and Technology
Gemma 7B,Language,USA,Google DeepMind,2024-02-21,Industry,Language modeling/generation,Confident,4096.0,Open weights (restricted use),Unspecified unreleased,Unreleased,8538074112.0,6000000000000.0,1282.4326530612243,3.07e+23,1877.1621580193864,6086.016266666666,155.3111111111111,272555555555555.6,483444444444444.5,37111111111.11112,Google TPU v5e,None,1,6472188,197336.5951439333,24928322.628266662,8.878031125588131e-09,636154.3111111111,253070893.25650308,815825060.9545577,,https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf,Gemma: Open Models Based on Gemini Research and Technology
Gemma 7B,Language,USA,Google DeepMind,2024-02-21,Industry,Quantitative reasoning,Confident,4096.0,Open weights (restricted use),Unspecified unreleased,Unreleased,8538074112.0,6000000000000.0,1282.4326530612243,3.07e+23,1877.1621580193864,6086.016266666666,155.3111111111111,272555555555555.6,483444444444444.5,37111111111.11112,Google TPU v5e,None,1,6472188,197336.5951439333,24928322.628266662,8.878031125588131e-09,636154.3111111111,253070893.25650308,815825060.9545577,,https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf,Gemma: Open Models Based on Gemini Research and Technology
Gemma 7B,Language,USA,Google DeepMind,2024-02-21,Industry,Question answering,Confident,4096.0,Open weights (restricted use),Unspecified unreleased,Unreleased,8538074112.0,6000000000000.0,1282.4326530612243,3.07e+23,1877.1621580193864,6086.016266666666,155.3111111111111,272555555555555.6,483444444444444.5,37111111111.11112,Google TPU v5e,None,1,6472188,197336.5951439333,24928322.628266662,8.878031125588131e-09,636154.3111111111,253070893.25650308,815825060.9545577,,https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf,Gemma: Open Models Based on Gemini Research and Technology
Gopher (280B),Language,UK,DeepMind,2021-12-08,Industry,Language modeling,Confident,4096.0,Unreleased,MassiveTex,Unreleased,280000000000.0,300000000000.0,920.0,6.31e+23,1877.1621580193864,6589.05,450.0,123000000000000.0,223459000000000.0,32000000000.0,Google TPU v3,SOTA improvement,1,6000000,4100965.606974364,26988748.8,0.378,1843200.0,3772888358.4164147,1695744000.0,"We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a 2 trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25× fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale.",https://arxiv.org/abs/2112.11446,"""Scaling Language Models: Methods, Analysis & Insights from Training Gopher"""
Gopher (280B),Language,UK,DeepMind,2021-12-08,Industry,Question answering,Confident,4096.0,Unreleased,MassiveTex,Unreleased,280000000000.0,300000000000.0,920.0,6.31e+23,1877.1621580193864,6589.05,450.0,123000000000000.0,223459000000000.0,32000000000.0,Google TPU v3,SOTA improvement,1,6000000,4100965.606974364,26988748.8,0.378,1843200.0,3772888358.4164147,1695744000.0,"We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a 2 trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25× fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale.",https://arxiv.org/abs/2112.11446,"""Scaling Language Models: Methods, Analysis & Insights from Training Gopher"""
Granite 13B,Language,USA,IBM,2023-11-30,Industry,Chat,Likely,768.0,API access,"Unspecified unreleased,Common Crawl,arXiv,OPENWEBTEXT",Unreleased,13000000000.0,2500000000000.0,2208.0,2.44e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,SOTA improvement,1,16000000,788306.6236983632,7761169.92,2.357254098360656e-08,307200.0,1740581025.1259859,678297600.0,"We introduce the Granite series of decoder-only foundation models for generative artificial intelligence (AI) tasks that are ready for enterprise use. We report on the architecture, capabilities, underlying data and data governance, training algorithms, compute infrastructure, energy and carbon footprint,
testing and evaluation, socio-technical harms and mitigations, and usage policies.",https://www.ibm.com/downloads/cas/X9W4O6BM,Granite Foundation Models
Granite 13B,Language,USA,IBM,2023-11-30,Industry,Language modeling/generation,Likely,768.0,API access,"Unspecified unreleased,Common Crawl,arXiv,OPENWEBTEXT",Unreleased,13000000000.0,2500000000000.0,2208.0,2.44e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,SOTA improvement,1,16000000,788306.6236983632,7761169.92,2.357254098360656e-08,307200.0,1740581025.1259859,678297600.0,"We introduce the Granite series of decoder-only foundation models for generative artificial intelligence (AI) tasks that are ready for enterprise use. We report on the architecture, capabilities, underlying data and data governance, training algorithms, compute infrastructure, energy and carbon footprint,
testing and evaluation, socio-technical harms and mitigations, and usage policies.",https://www.ibm.com/downloads/cas/X9W4O6BM,Granite Foundation Models
Granite 13B,Language,USA,IBM,2023-11-30,Industry,Question answering,Likely,768.0,API access,"Unspecified unreleased,Common Crawl,arXiv,OPENWEBTEXT",Unreleased,13000000000.0,2500000000000.0,2208.0,2.44e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,SOTA improvement,1,16000000,788306.6236983632,7761169.92,2.357254098360656e-08,307200.0,1740581025.1259859,678297600.0,"We introduce the Granite series of decoder-only foundation models for generative artificial intelligence (AI) tasks that are ready for enterprise use. We report on the architecture, capabilities, underlying data and data governance, training algorithms, compute infrastructure, energy and carbon footprint,
testing and evaluation, socio-technical harms and mitigations, and usage policies.",https://www.ibm.com/downloads/cas/X9W4O6BM,Granite Foundation Models
Granite 13B,Language,USA,IBM,2023-11-30,Industry,Text summarization,Likely,768.0,API access,"Unspecified unreleased,Common Crawl,arXiv,OPENWEBTEXT",Unreleased,13000000000.0,2500000000000.0,2208.0,2.44e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,SOTA improvement,1,16000000,788306.6236983632,7761169.92,2.357254098360656e-08,307200.0,1740581025.1259859,678297600.0,"We introduce the Granite series of decoder-only foundation models for generative artificial intelligence (AI) tasks that are ready for enterprise use. We report on the architecture, capabilities, underlying data and data governance, training algorithms, compute infrastructure, energy and carbon footprint,
testing and evaluation, socio-technical harms and mitigations, and usage policies.",https://www.ibm.com/downloads/cas/X9W4O6BM,Granite Foundation Models
Granite 20B,Language,Multinational,IBM Research,2024-05-31,Industry,Language modeling/generation,Confident,768.0,Open weights (unrestricted),"Stack Exchange,Common Crawl,Wikimedia",Not-defined,20000000000.0,2500000000000.0,2208.0,3.0000000000001e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,SOTA improvement,0,2000000,113643.12380745166,7761169.92,1.9172333333332692e-08,307200.0,250924017.36685327,678297600.0,"We introduce the Granite series of decoder-only foundation models for generative artificial intelligence (AI) tasks that are ready for enterprise use. We report on the architecture, capabilities, underlying data and data governance, training algorithms, compute infrastructure, energy and carbon footprint, testing and evaluation, socio-technical harms and mitigations, and usage policies.",https://www.ibm.com/downloads/documents/us-en/10a99803c92fdb35,Granite Foundation Models
Granite 20B,Language,USA,IBM Research,2024-05-31,Industry,Language modeling/generation,Confident,768.0,Open weights (unrestricted),"Stack Exchange,Common Crawl,Wikimedia",Not-defined,20000000000.0,2500000000000.0,2208.0,3.0000000000001e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,SOTA improvement,0,2000000,113643.12380745166,7761169.92,1.9172333333332692e-08,307200.0,250924017.36685327,678297600.0,"We introduce the Granite series of decoder-only foundation models for generative artificial intelligence (AI) tasks that are ready for enterprise use. We report on the architecture, capabilities, underlying data and data governance, training algorithms, compute infrastructure, energy and carbon footprint, testing and evaluation, socio-technical harms and mitigations, and usage policies.",https://www.ibm.com/downloads/documents/us-en/10a99803c92fdb35,Granite Foundation Models
Granite 3.0 2B,Language,USA,IBM,2024-10-21,Industry,Code generation,Confident,768.0,Open weights (unrestricted),Unspecified unreleased,Unreleased,2500000000.0,12000000000000.0,2208.0,1.8e+23,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Highly cited,0,3200000,5595164.712836602,34168227.839999996,1.0288666666666667e-07,537600.0,12354123685.943216,1187020800.0,"This report presents Granite 3.0, a new set of lightweight, state-of-the-art, open foundation models ranging in scale from 400 million to 8 billion active parameters.
Equipped with native support of multilingual, coding, function 
calling, and strong safety performance, these models target enterprise use cases, including on-premise and on-device settings. Evaluations on a comprehensive set of tasks demonstrate that our models consistently reach state-of-the-art performance for their size (as shown in Figure 1 and 2). This report also discloses technical details of pre-training and post-training that may help the research community accelerate the collective efforts to develop open foundation models. We publicly release pre-trained and post-trained versions of all our Granite 3.0 models under a standard permissive Apache 2.0 license allowing both research and commercial use. With support from the open source community, the Granite 3.0 models have been integrated with a
range of existing tools for quantization, fine-tuning, and deployment.",https://github.com/ibm-granite/granite-3.0-language-models/tree/main,Granite 3.0 Language Models
Granite 3.0 2B,Language,USA,IBM,2024-10-21,Industry,Language modeling/generation,Confident,768.0,Open weights (unrestricted),Unspecified unreleased,Unreleased,2500000000.0,12000000000000.0,2208.0,1.8e+23,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Highly cited,0,3200000,5595164.712836602,34168227.839999996,1.0288666666666667e-07,537600.0,12354123685.943216,1187020800.0,"This report presents Granite 3.0, a new set of lightweight, state-of-the-art, open foundation models ranging in scale from 400 million to 8 billion active parameters.
Equipped with native support of multilingual, coding, function 
calling, and strong safety performance, these models target enterprise use cases, including on-premise and on-device settings. Evaluations on a comprehensive set of tasks demonstrate that our models consistently reach state-of-the-art performance for their size (as shown in Figure 1 and 2). This report also discloses technical details of pre-training and post-training that may help the research community accelerate the collective efforts to develop open foundation models. We publicly release pre-trained and post-trained versions of all our Granite 3.0 models under a standard permissive Apache 2.0 license allowing both research and commercial use. With support from the open source community, the Granite 3.0 models have been integrated with a
range of existing tools for quantization, fine-tuning, and deployment.",https://github.com/ibm-granite/granite-3.0-language-models/tree/main,Granite 3.0 Language Models
Granite 3.0 2B,Language,USA,IBM,2024-10-21,Industry,Question answering,Confident,768.0,Open weights (unrestricted),Unspecified unreleased,Unreleased,2500000000.0,12000000000000.0,2208.0,1.8e+23,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Highly cited,0,3200000,5595164.712836602,34168227.839999996,1.0288666666666667e-07,537600.0,12354123685.943216,1187020800.0,"This report presents Granite 3.0, a new set of lightweight, state-of-the-art, open foundation models ranging in scale from 400 million to 8 billion active parameters.
Equipped with native support of multilingual, coding, function 
calling, and strong safety performance, these models target enterprise use cases, including on-premise and on-device settings. Evaluations on a comprehensive set of tasks demonstrate that our models consistently reach state-of-the-art performance for their size (as shown in Figure 1 and 2). This report also discloses technical details of pre-training and post-training that may help the research community accelerate the collective efforts to develop open foundation models. We publicly release pre-trained and post-trained versions of all our Granite 3.0 models under a standard permissive Apache 2.0 license allowing both research and commercial use. With support from the open source community, the Granite 3.0 models have been integrated with a
range of existing tools for quantization, fine-tuning, and deployment.",https://github.com/ibm-granite/granite-3.0-language-models/tree/main,Granite 3.0 Language Models
Granite 3.0 2B,Language,USA,IBM,2024-10-21,Industry,Text classification,Confident,768.0,Open weights (unrestricted),Unspecified unreleased,Unreleased,2500000000.0,12000000000000.0,2208.0,1.8e+23,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Highly cited,0,3200000,5595164.712836602,34168227.839999996,1.0288666666666667e-07,537600.0,12354123685.943216,1187020800.0,"This report presents Granite 3.0, a new set of lightweight, state-of-the-art, open foundation models ranging in scale from 400 million to 8 billion active parameters.
Equipped with native support of multilingual, coding, function 
calling, and strong safety performance, these models target enterprise use cases, including on-premise and on-device settings. Evaluations on a comprehensive set of tasks demonstrate that our models consistently reach state-of-the-art performance for their size (as shown in Figure 1 and 2). This report also discloses technical details of pre-training and post-training that may help the research community accelerate the collective efforts to develop open foundation models. We publicly release pre-trained and post-trained versions of all our Granite 3.0 models under a standard permissive Apache 2.0 license allowing both research and commercial use. With support from the open source community, the Granite 3.0 models have been integrated with a
range of existing tools for quantization, fine-tuning, and deployment.",https://github.com/ibm-granite/granite-3.0-language-models/tree/main,Granite 3.0 Language Models
Granite 3.0 2B,Language,USA,IBM,2024-10-21,Industry,Text summarization,Confident,768.0,Open weights (unrestricted),Unspecified unreleased,Unreleased,2500000000.0,12000000000000.0,2208.0,1.8e+23,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Highly cited,0,3200000,5595164.712836602,34168227.839999996,1.0288666666666667e-07,537600.0,12354123685.943216,1187020800.0,"This report presents Granite 3.0, a new set of lightweight, state-of-the-art, open foundation models ranging in scale from 400 million to 8 billion active parameters.
Equipped with native support of multilingual, coding, function 
calling, and strong safety performance, these models target enterprise use cases, including on-premise and on-device settings. Evaluations on a comprehensive set of tasks demonstrate that our models consistently reach state-of-the-art performance for their size (as shown in Figure 1 and 2). This report also discloses technical details of pre-training and post-training that may help the research community accelerate the collective efforts to develop open foundation models. We publicly release pre-trained and post-trained versions of all our Granite 3.0 models under a standard permissive Apache 2.0 license allowing both research and commercial use. With support from the open source community, the Granite 3.0 models have been integrated with a
range of existing tools for quantization, fine-tuning, and deployment.",https://github.com/ibm-granite/granite-3.0-language-models/tree/main,Granite 3.0 Language Models
Granite 3.0 2B,Language,USA,IBM,2024-10-21,Industry,Translation,Confident,768.0,Open weights (unrestricted),Unspecified unreleased,Unreleased,2500000000.0,12000000000000.0,2208.0,1.8e+23,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Highly cited,0,3200000,5595164.712836602,34168227.839999996,1.0288666666666667e-07,537600.0,12354123685.943216,1187020800.0,"This report presents Granite 3.0, a new set of lightweight, state-of-the-art, open foundation models ranging in scale from 400 million to 8 billion active parameters.
Equipped with native support of multilingual, coding, function 
calling, and strong safety performance, these models target enterprise use cases, including on-premise and on-device settings. Evaluations on a comprehensive set of tasks demonstrate that our models consistently reach state-of-the-art performance for their size (as shown in Figure 1 and 2). This report also discloses technical details of pre-training and post-training that may help the research community accelerate the collective efforts to develop open foundation models. We publicly release pre-trained and post-trained versions of all our Granite 3.0 models under a standard permissive Apache 2.0 license allowing both research and commercial use. With support from the open source community, the Granite 3.0 models have been integrated with a
range of existing tools for quantization, fine-tuning, and deployment.",https://github.com/ibm-granite/granite-3.0-language-models/tree/main,Granite 3.0 Language Models
Granite 3.0 8B,Language,USA,IBM,2024-10-21,Industry,Code generation,Confident,256.0,Open weights (unrestricted),Unspecified unreleased,Unreleased,8100000000.0,12000000000000.0,2208.0,5.832e+23,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,None,0,3200000,5595164.712836602,11389409.28,3.175514403292181e-08,179200.0,12354123685.943216,395673600.0,"This report presents Granite 3.0, a new set of lightweight, state-of-the-art, open foundation models ranging in scale from 400 million to 8 billion active parameters.
Equipped with native support of multilingual, coding, function 
calling, and strong safety performance, these models target enterprise use cases, including on-premise and on-device settings. Evaluations on a comprehensive set of tasks demonstrate that our models consistently reach state-of-the-art performance for their size (as shown in Figure 1 and 2). This report also discloses technical details of pre-training and post-training that may help the research community accelerate the collective efforts to develop open foundation models. We publicly release pre-trained and post-trained versions of all our Granite 3.0 models under a standard permissive Apache 2.0 license allowing both research and commercial use. With support from the open source community, the Granite 3.0 models have been integrated with a
range of existing tools for quantization, fine-tuning, and deployment.",https://github.com/ibm-granite/granite-3.0-language-models/tree/main,Granite 3.0 Language Models
Granite 3.0 8B,Language,USA,IBM,2024-10-21,Industry,Language modeling/generation,Confident,256.0,Open weights (unrestricted),Unspecified unreleased,Unreleased,8100000000.0,12000000000000.0,2208.0,5.832e+23,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,None,0,3200000,5595164.712836602,11389409.28,3.175514403292181e-08,179200.0,12354123685.943216,395673600.0,"This report presents Granite 3.0, a new set of lightweight, state-of-the-art, open foundation models ranging in scale from 400 million to 8 billion active parameters.
Equipped with native support of multilingual, coding, function 
calling, and strong safety performance, these models target enterprise use cases, including on-premise and on-device settings. Evaluations on a comprehensive set of tasks demonstrate that our models consistently reach state-of-the-art performance for their size (as shown in Figure 1 and 2). This report also discloses technical details of pre-training and post-training that may help the research community accelerate the collective efforts to develop open foundation models. We publicly release pre-trained and post-trained versions of all our Granite 3.0 models under a standard permissive Apache 2.0 license allowing both research and commercial use. With support from the open source community, the Granite 3.0 models have been integrated with a
range of existing tools for quantization, fine-tuning, and deployment.",https://github.com/ibm-granite/granite-3.0-language-models/tree/main,Granite 3.0 Language Models
Granite 3.0 8B,Language,USA,IBM,2024-10-21,Industry,Question answering,Confident,256.0,Open weights (unrestricted),Unspecified unreleased,Unreleased,8100000000.0,12000000000000.0,2208.0,5.832e+23,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,None,0,3200000,5595164.712836602,11389409.28,3.175514403292181e-08,179200.0,12354123685.943216,395673600.0,"This report presents Granite 3.0, a new set of lightweight, state-of-the-art, open foundation models ranging in scale from 400 million to 8 billion active parameters.
Equipped with native support of multilingual, coding, function 
calling, and strong safety performance, these models target enterprise use cases, including on-premise and on-device settings. Evaluations on a comprehensive set of tasks demonstrate that our models consistently reach state-of-the-art performance for their size (as shown in Figure 1 and 2). This report also discloses technical details of pre-training and post-training that may help the research community accelerate the collective efforts to develop open foundation models. We publicly release pre-trained and post-trained versions of all our Granite 3.0 models under a standard permissive Apache 2.0 license allowing both research and commercial use. With support from the open source community, the Granite 3.0 models have been integrated with a
range of existing tools for quantization, fine-tuning, and deployment.",https://github.com/ibm-granite/granite-3.0-language-models/tree/main,Granite 3.0 Language Models
Granite 3.0 8B,Language,USA,IBM,2024-10-21,Industry,Text classification,Confident,256.0,Open weights (unrestricted),Unspecified unreleased,Unreleased,8100000000.0,12000000000000.0,2208.0,5.832e+23,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,None,0,3200000,5595164.712836602,11389409.28,3.175514403292181e-08,179200.0,12354123685.943216,395673600.0,"This report presents Granite 3.0, a new set of lightweight, state-of-the-art, open foundation models ranging in scale from 400 million to 8 billion active parameters.
Equipped with native support of multilingual, coding, function 
calling, and strong safety performance, these models target enterprise use cases, including on-premise and on-device settings. Evaluations on a comprehensive set of tasks demonstrate that our models consistently reach state-of-the-art performance for their size (as shown in Figure 1 and 2). This report also discloses technical details of pre-training and post-training that may help the research community accelerate the collective efforts to develop open foundation models. We publicly release pre-trained and post-trained versions of all our Granite 3.0 models under a standard permissive Apache 2.0 license allowing both research and commercial use. With support from the open source community, the Granite 3.0 models have been integrated with a
range of existing tools for quantization, fine-tuning, and deployment.",https://github.com/ibm-granite/granite-3.0-language-models/tree/main,Granite 3.0 Language Models
Granite 3.0 8B,Language,USA,IBM,2024-10-21,Industry,Text summarization,Confident,256.0,Open weights (unrestricted),Unspecified unreleased,Unreleased,8100000000.0,12000000000000.0,2208.0,5.832e+23,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,None,0,3200000,5595164.712836602,11389409.28,3.175514403292181e-08,179200.0,12354123685.943216,395673600.0,"This report presents Granite 3.0, a new set of lightweight, state-of-the-art, open foundation models ranging in scale from 400 million to 8 billion active parameters.
Equipped with native support of multilingual, coding, function 
calling, and strong safety performance, these models target enterprise use cases, including on-premise and on-device settings. Evaluations on a comprehensive set of tasks demonstrate that our models consistently reach state-of-the-art performance for their size (as shown in Figure 1 and 2). This report also discloses technical details of pre-training and post-training that may help the research community accelerate the collective efforts to develop open foundation models. We publicly release pre-trained and post-trained versions of all our Granite 3.0 models under a standard permissive Apache 2.0 license allowing both research and commercial use. With support from the open source community, the Granite 3.0 models have been integrated with a
range of existing tools for quantization, fine-tuning, and deployment.",https://github.com/ibm-granite/granite-3.0-language-models/tree/main,Granite 3.0 Language Models
Granite 3.0 8B,Language,USA,IBM,2024-10-21,Industry,Translation,Confident,256.0,Open weights (unrestricted),Unspecified unreleased,Unreleased,8100000000.0,12000000000000.0,2208.0,5.832e+23,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,None,0,3200000,5595164.712836602,11389409.28,3.175514403292181e-08,179200.0,12354123685.943216,395673600.0,"This report presents Granite 3.0, a new set of lightweight, state-of-the-art, open foundation models ranging in scale from 400 million to 8 billion active parameters.
Equipped with native support of multilingual, coding, function 
calling, and strong safety performance, these models target enterprise use cases, including on-premise and on-device settings. Evaluations on a comprehensive set of tasks demonstrate that our models consistently reach state-of-the-art performance for their size (as shown in Figure 1 and 2). This report also discloses technical details of pre-training and post-training that may help the research community accelerate the collective efforts to develop open foundation models. We publicly release pre-trained and post-trained versions of all our Granite 3.0 models under a standard permissive Apache 2.0 license allowing both research and commercial use. With support from the open source community, the Granite 3.0 models have been integrated with a
range of existing tools for quantization, fine-tuning, and deployment.",https://github.com/ibm-granite/granite-3.0-language-models/tree/main,Granite 3.0 Language Models
Grok-0,Language,USA,xAI,2023-11-04,Industry,Chat,Likely,100000.0,Hosted access (no API),Unspecified unreleased,Unreleased,33000000000.0,6200000000000.0,2400.0,2.90000000001e+24,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,4,6472188,154003694.78442368,4448988000.0,6.38606896549522e-09,70000000.0,369608867482.6168,168000000000.0,"""The engine powering Grok is Grok-1, our frontier LLM, which we developed over the last four months. Grok-1 has gone through many iterations over this span of time.

After announcing xAI, we trained a prototype LLM (Grok-0) with 33 billion parameters. This early model approaches LLaMA 2 (70B) capabilities on standard LM benchmarks but uses only half of its training resources.""",https://x.ai/,Announcing Grok
Grok-0,Language,USA,xAI,2023-11-04,Industry,Language modeling/generation,Likely,100000.0,Hosted access (no API),Unspecified unreleased,Unreleased,33000000000.0,6200000000000.0,2400.0,2.90000000001e+24,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,4,6472188,154003694.78442368,4448988000.0,6.38606896549522e-09,70000000.0,369608867482.6168,168000000000.0,"""The engine powering Grok is Grok-1, our frontier LLM, which we developed over the last four months. Grok-1 has gone through many iterations over this span of time.

After announcing xAI, we trained a prototype LLM (Grok-0) with 33 billion parameters. This early model approaches LLaMA 2 (70B) capabilities on standard LM benchmarks but uses only half of its training resources.""",https://x.ai/,Announcing Grok
Grok-1,Language,USA,xAI,2023-11-04,Industry,Chat,Likely,100000.0,Open weights (unrestricted),Unspecified unreleased,Unreleased,314000000000.0,6200000000000.0,2400.0,2.90000000001e+24,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,4,6472188,154003694.78442368,4448988000.0,6.38606896549522e-09,70000000.0,369608867482.6168,168000000000.0,"Grok is an AI modeled after the Hitchhiker’s Guide to the Galaxy, so intended to answer almost anything and, far harder, even suggest what questions to ask!

Grok is designed to answer questions with a bit of wit and has a rebellious streak, so please don’t use it if you hate humor!

A unique and fundamental advantage of Grok is that it has real-time knowledge of the world via the 𝕏 platform. It will also answer spicy questions that are rejected by most other AI systems.

Grok is still a very early beta product – the best we could do with 2 months of training – so expect it to improve rapidly with each passing week with your help.","https://x.ai/model-card/, https://x.ai/blog/grok-os",Announcing Grok
Grok-1,Language,USA,xAI,2023-11-04,Industry,Language modeling,Likely,100000.0,Open weights (unrestricted),Unspecified unreleased,Unreleased,314000000000.0,6200000000000.0,2400.0,2.90000000001e+24,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,4,6472188,154003694.78442368,4448988000.0,6.38606896549522e-09,70000000.0,369608867482.6168,168000000000.0,"Grok is an AI modeled after the Hitchhiker’s Guide to the Galaxy, so intended to answer almost anything and, far harder, even suggest what questions to ask!

Grok is designed to answer questions with a bit of wit and has a rebellious streak, so please don’t use it if you hate humor!

A unique and fundamental advantage of Grok is that it has real-time knowledge of the world via the 𝕏 platform. It will also answer spicy questions that are rejected by most other AI systems.

Grok is still a very early beta product – the best we could do with 2 months of training – so expect it to improve rapidly with each passing week with your help.","https://x.ai/model-card/, https://x.ai/blog/grok-os",Announcing Grok
Grok-1.5,Language,USA,xAI,2024-03-28,Industry,Chat,Speculative,100000.0,Hosted access (no API),Unspecified unreleased,Unreleased,314000000000.0,6200000000000.0,2400.0,9.26e+24,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,SOTA improvement,4,6472188,154003694.78442368,4448988000.0,1.9999568034557236e-09,70000000.0,369608867482.6168,168000000000.0,,https://x.ai/blog/grok-1.5,"Introducing Grok-1.5, our latest model capable of long context understanding and advanced reasoning. Grok-1.5 will be available to our early testers and existing Grok users on the 𝕏 platform in the coming days."
Grok-1.5,Language,USA,xAI,2024-03-28,Industry,Language modeling,Speculative,100000.0,Hosted access (no API),Unspecified unreleased,Unreleased,314000000000.0,6200000000000.0,2400.0,9.26e+24,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,SOTA improvement,4,6472188,154003694.78442368,4448988000.0,1.9999568034557236e-09,70000000.0,369608867482.6168,168000000000.0,,https://x.ai/blog/grok-1.5,"Introducing Grok-1.5, our latest model capable of long context understanding and advanced reasoning. Grok-1.5 will be available to our early testers and existing Grok users on the 𝕏 platform in the coming days."
Grok-1.5V,Language,USA,xAI,2024-03-28,Industry,Chat,Speculative,100000.0,Hosted access (no API),Unspecified unreleased,Unreleased,314000000000.0,6200000000000.0,2400.0,9.26e+24,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,SOTA improvement,4,6472188,154003694.78442368,4448988000.0,1.9999568034557236e-09,70000000.0,369608867482.6168,168000000000.0,,https://x.ai/blog/grok-1.5v,"Introducing Grok-1.5V, our first-generation multimodal model. In addition to its strong text capabilities, Grok can now process a wide variety of visual information, including documents, diagrams, charts, screenshots, and photographs. Grok-1.5V will be available soon to our early testers and existing Grok users."
Grok-1.5V,Language,USA,xAI,2024-03-28,Industry,Code autocompletion,Speculative,100000.0,Hosted access (no API),Unspecified unreleased,Unreleased,314000000000.0,6200000000000.0,2400.0,9.26e+24,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,SOTA improvement,4,6472188,154003694.78442368,4448988000.0,1.9999568034557236e-09,70000000.0,369608867482.6168,168000000000.0,,https://x.ai/blog/grok-1.5v,"Introducing Grok-1.5V, our first-generation multimodal model. In addition to its strong text capabilities, Grok can now process a wide variety of visual information, including documents, diagrams, charts, screenshots, and photographs. Grok-1.5V will be available soon to our early testers and existing Grok users."
Grok-1.5V,Language,USA,xAI,2024-03-28,Industry,Code generation,Speculative,100000.0,Hosted access (no API),Unspecified unreleased,Unreleased,314000000000.0,6200000000000.0,2400.0,9.26e+24,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,SOTA improvement,4,6472188,154003694.78442368,4448988000.0,1.9999568034557236e-09,70000000.0,369608867482.6168,168000000000.0,,https://x.ai/blog/grok-1.5v,"Introducing Grok-1.5V, our first-generation multimodal model. In addition to its strong text capabilities, Grok can now process a wide variety of visual information, including documents, diagrams, charts, screenshots, and photographs. Grok-1.5V will be available soon to our early testers and existing Grok users."
Grok-1.5V,Language,USA,xAI,2024-03-28,Industry,Image captioning,Speculative,100000.0,Hosted access (no API),Unspecified unreleased,Unreleased,314000000000.0,6200000000000.0,2400.0,9.26e+24,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,SOTA improvement,4,6472188,154003694.78442368,4448988000.0,1.9999568034557236e-09,70000000.0,369608867482.6168,168000000000.0,,https://x.ai/blog/grok-1.5v,"Introducing Grok-1.5V, our first-generation multimodal model. In addition to its strong text capabilities, Grok can now process a wide variety of visual information, including documents, diagrams, charts, screenshots, and photographs. Grok-1.5V will be available soon to our early testers and existing Grok users."
Grok-1.5V,Language,USA,xAI,2024-03-28,Industry,Language modeling,Speculative,100000.0,Hosted access (no API),Unspecified unreleased,Unreleased,314000000000.0,6200000000000.0,2400.0,9.26e+24,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,SOTA improvement,4,6472188,154003694.78442368,4448988000.0,1.9999568034557236e-09,70000000.0,369608867482.6168,168000000000.0,,https://x.ai/blog/grok-1.5v,"Introducing Grok-1.5V, our first-generation multimodal model. In addition to its strong text capabilities, Grok can now process a wide variety of visual information, including documents, diagrams, charts, screenshots, and photographs. Grok-1.5V will be available soon to our early testers and existing Grok users."
Grok-1.5V,Language,USA,xAI,2024-03-28,Industry,Visual question answering,Speculative,100000.0,Hosted access (no API),Unspecified unreleased,Unreleased,314000000000.0,6200000000000.0,2400.0,9.26e+24,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,SOTA improvement,4,6472188,154003694.78442368,4448988000.0,1.9999568034557236e-09,70000000.0,369608867482.6168,168000000000.0,,https://x.ai/blog/grok-1.5v,"Introducing Grok-1.5V, our first-generation multimodal model. In addition to its strong text capabilities, Grok can now process a wide variety of visual information, including documents, diagrams, charts, screenshots, and photographs. Grok-1.5V will be available soon to our early testers and existing Grok users."
Grok-1.5V,Multimodal,USA,xAI,2024-03-28,Industry,Chat,Speculative,100000.0,Hosted access (no API),Unspecified unreleased,Unreleased,314000000000.0,6200000000000.0,2400.0,9.26e+24,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,SOTA improvement,4,6472188,154003694.78442368,4448988000.0,1.9999568034557236e-09,70000000.0,369608867482.6168,168000000000.0,,https://x.ai/blog/grok-1.5v,"Introducing Grok-1.5V, our first-generation multimodal model. In addition to its strong text capabilities, Grok can now process a wide variety of visual information, including documents, diagrams, charts, screenshots, and photographs. Grok-1.5V will be available soon to our early testers and existing Grok users."
Grok-1.5V,Multimodal,USA,xAI,2024-03-28,Industry,Code autocompletion,Speculative,100000.0,Hosted access (no API),Unspecified unreleased,Unreleased,314000000000.0,6200000000000.0,2400.0,9.26e+24,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,SOTA improvement,4,6472188,154003694.78442368,4448988000.0,1.9999568034557236e-09,70000000.0,369608867482.6168,168000000000.0,,https://x.ai/blog/grok-1.5v,"Introducing Grok-1.5V, our first-generation multimodal model. In addition to its strong text capabilities, Grok can now process a wide variety of visual information, including documents, diagrams, charts, screenshots, and photographs. Grok-1.5V will be available soon to our early testers and existing Grok users."
Grok-1.5V,Multimodal,USA,xAI,2024-03-28,Industry,Code generation,Speculative,100000.0,Hosted access (no API),Unspecified unreleased,Unreleased,314000000000.0,6200000000000.0,2400.0,9.26e+24,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,SOTA improvement,4,6472188,154003694.78442368,4448988000.0,1.9999568034557236e-09,70000000.0,369608867482.6168,168000000000.0,,https://x.ai/blog/grok-1.5v,"Introducing Grok-1.5V, our first-generation multimodal model. In addition to its strong text capabilities, Grok can now process a wide variety of visual information, including documents, diagrams, charts, screenshots, and photographs. Grok-1.5V will be available soon to our early testers and existing Grok users."
Grok-1.5V,Multimodal,USA,xAI,2024-03-28,Industry,Image captioning,Speculative,100000.0,Hosted access (no API),Unspecified unreleased,Unreleased,314000000000.0,6200000000000.0,2400.0,9.26e+24,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,SOTA improvement,4,6472188,154003694.78442368,4448988000.0,1.9999568034557236e-09,70000000.0,369608867482.6168,168000000000.0,,https://x.ai/blog/grok-1.5v,"Introducing Grok-1.5V, our first-generation multimodal model. In addition to its strong text capabilities, Grok can now process a wide variety of visual information, including documents, diagrams, charts, screenshots, and photographs. Grok-1.5V will be available soon to our early testers and existing Grok users."
Grok-1.5V,Multimodal,USA,xAI,2024-03-28,Industry,Language modeling,Speculative,100000.0,Hosted access (no API),Unspecified unreleased,Unreleased,314000000000.0,6200000000000.0,2400.0,9.26e+24,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,SOTA improvement,4,6472188,154003694.78442368,4448988000.0,1.9999568034557236e-09,70000000.0,369608867482.6168,168000000000.0,,https://x.ai/blog/grok-1.5v,"Introducing Grok-1.5V, our first-generation multimodal model. In addition to its strong text capabilities, Grok can now process a wide variety of visual information, including documents, diagrams, charts, screenshots, and photographs. Grok-1.5V will be available soon to our early testers and existing Grok users."
Grok-1.5V,Multimodal,USA,xAI,2024-03-28,Industry,Visual question answering,Speculative,100000.0,Hosted access (no API),Unspecified unreleased,Unreleased,314000000000.0,6200000000000.0,2400.0,9.26e+24,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,SOTA improvement,4,6472188,154003694.78442368,4448988000.0,1.9999568034557236e-09,70000000.0,369608867482.6168,168000000000.0,,https://x.ai/blog/grok-1.5v,"Introducing Grok-1.5V, our first-generation multimodal model. In addition to its strong text capabilities, Grok can now process a wide variety of visual information, including documents, diagrams, charts, screenshots, and photographs. Grok-1.5V will be available soon to our early testers and existing Grok users."
Grok-1.5V,Vision,USA,xAI,2024-03-28,Industry,Chat,Speculative,100000.0,Hosted access (no API),Unspecified unreleased,Unreleased,314000000000.0,6200000000000.0,2400.0,9.26e+24,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,SOTA improvement,4,6472188,154003694.78442368,4448988000.0,1.9999568034557236e-09,70000000.0,369608867482.6168,168000000000.0,,https://x.ai/blog/grok-1.5v,"Introducing Grok-1.5V, our first-generation multimodal model. In addition to its strong text capabilities, Grok can now process a wide variety of visual information, including documents, diagrams, charts, screenshots, and photographs. Grok-1.5V will be available soon to our early testers and existing Grok users."
Grok-1.5V,Vision,USA,xAI,2024-03-28,Industry,Code autocompletion,Speculative,100000.0,Hosted access (no API),Unspecified unreleased,Unreleased,314000000000.0,6200000000000.0,2400.0,9.26e+24,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,SOTA improvement,4,6472188,154003694.78442368,4448988000.0,1.9999568034557236e-09,70000000.0,369608867482.6168,168000000000.0,,https://x.ai/blog/grok-1.5v,"Introducing Grok-1.5V, our first-generation multimodal model. In addition to its strong text capabilities, Grok can now process a wide variety of visual information, including documents, diagrams, charts, screenshots, and photographs. Grok-1.5V will be available soon to our early testers and existing Grok users."
Grok-1.5V,Vision,USA,xAI,2024-03-28,Industry,Code generation,Speculative,100000.0,Hosted access (no API),Unspecified unreleased,Unreleased,314000000000.0,6200000000000.0,2400.0,9.26e+24,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,SOTA improvement,4,6472188,154003694.78442368,4448988000.0,1.9999568034557236e-09,70000000.0,369608867482.6168,168000000000.0,,https://x.ai/blog/grok-1.5v,"Introducing Grok-1.5V, our first-generation multimodal model. In addition to its strong text capabilities, Grok can now process a wide variety of visual information, including documents, diagrams, charts, screenshots, and photographs. Grok-1.5V will be available soon to our early testers and existing Grok users."
Grok-1.5V,Vision,USA,xAI,2024-03-28,Industry,Image captioning,Speculative,100000.0,Hosted access (no API),Unspecified unreleased,Unreleased,314000000000.0,6200000000000.0,2400.0,9.26e+24,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,SOTA improvement,4,6472188,154003694.78442368,4448988000.0,1.9999568034557236e-09,70000000.0,369608867482.6168,168000000000.0,,https://x.ai/blog/grok-1.5v,"Introducing Grok-1.5V, our first-generation multimodal model. In addition to its strong text capabilities, Grok can now process a wide variety of visual information, including documents, diagrams, charts, screenshots, and photographs. Grok-1.5V will be available soon to our early testers and existing Grok users."
Grok-1.5V,Vision,USA,xAI,2024-03-28,Industry,Language modeling,Speculative,100000.0,Hosted access (no API),Unspecified unreleased,Unreleased,314000000000.0,6200000000000.0,2400.0,9.26e+24,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,SOTA improvement,4,6472188,154003694.78442368,4448988000.0,1.9999568034557236e-09,70000000.0,369608867482.6168,168000000000.0,,https://x.ai/blog/grok-1.5v,"Introducing Grok-1.5V, our first-generation multimodal model. In addition to its strong text capabilities, Grok can now process a wide variety of visual information, including documents, diagrams, charts, screenshots, and photographs. Grok-1.5V will be available soon to our early testers and existing Grok users."
Grok-1.5V,Vision,USA,xAI,2024-03-28,Industry,Visual question answering,Speculative,100000.0,Hosted access (no API),Unspecified unreleased,Unreleased,314000000000.0,6200000000000.0,2400.0,9.26e+24,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,SOTA improvement,4,6472188,154003694.78442368,4448988000.0,1.9999568034557236e-09,70000000.0,369608867482.6168,168000000000.0,,https://x.ai/blog/grok-1.5v,"Introducing Grok-1.5V, our first-generation multimodal model. In addition to its strong text capabilities, Grok can now process a wide variety of visual information, including documents, diagrams, charts, screenshots, and photographs. Grok-1.5V will be available soon to our early testers and existing Grok users."
Grok-2,Language,USA,xAI,2024-08-13,Industry,Chat,Confident,100000.0,Hosted access (no API),Unspecified unreleased,Unreleased,314000000000.0,6200000000000.0,2400.0,2.96e+25,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,4,6472188,154003694.78442368,4448988000.0,6.256621621621622e-10,70000000.0,369608867482.6168,168000000000.0,Grok-2 is our frontier language model with state-of-the-art reasoning capabilities. This release includes two members of the Grok family: Grok-2 and Grok-2 mini. Both models are now being released to Grok users on the 𝕏 platform.,https://x.ai/blog/grok-2,Grok-2 Beta Release
Grok-2,Language,USA,xAI,2024-08-13,Industry,Code generation,Confident,100000.0,Hosted access (no API),Unspecified unreleased,Unreleased,314000000000.0,6200000000000.0,2400.0,2.96e+25,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,4,6472188,154003694.78442368,4448988000.0,6.256621621621622e-10,70000000.0,369608867482.6168,168000000000.0,Grok-2 is our frontier language model with state-of-the-art reasoning capabilities. This release includes two members of the Grok family: Grok-2 and Grok-2 mini. Both models are now being released to Grok users on the 𝕏 platform.,https://x.ai/blog/grok-2,Grok-2 Beta Release
Grok-2,Language,USA,xAI,2024-08-13,Industry,Language modeling/generation,Confident,100000.0,Hosted access (no API),Unspecified unreleased,Unreleased,314000000000.0,6200000000000.0,2400.0,2.96e+25,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,4,6472188,154003694.78442368,4448988000.0,6.256621621621622e-10,70000000.0,369608867482.6168,168000000000.0,Grok-2 is our frontier language model with state-of-the-art reasoning capabilities. This release includes two members of the Grok family: Grok-2 and Grok-2 mini. Both models are now being released to Grok users on the 𝕏 platform.,https://x.ai/blog/grok-2,Grok-2 Beta Release
Grok-2,Language,USA,xAI,2024-08-13,Industry,Question answering,Confident,100000.0,Hosted access (no API),Unspecified unreleased,Unreleased,314000000000.0,6200000000000.0,2400.0,2.96e+25,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,4,6472188,154003694.78442368,4448988000.0,6.256621621621622e-10,70000000.0,369608867482.6168,168000000000.0,Grok-2 is our frontier language model with state-of-the-art reasoning capabilities. This release includes two members of the Grok family: Grok-2 and Grok-2 mini. Both models are now being released to Grok users on the 𝕏 platform.,https://x.ai/blog/grok-2,Grok-2 Beta Release
Grok-2,Language,USA,xAI,2024-08-13,Industry,Visual question answering,Confident,100000.0,Hosted access (no API),Unspecified unreleased,Unreleased,314000000000.0,6200000000000.0,2400.0,2.96e+25,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,4,6472188,154003694.78442368,4448988000.0,6.256621621621622e-10,70000000.0,369608867482.6168,168000000000.0,Grok-2 is our frontier language model with state-of-the-art reasoning capabilities. This release includes two members of the Grok family: Grok-2 and Grok-2 mini. Both models are now being released to Grok users on the 𝕏 platform.,https://x.ai/blog/grok-2,Grok-2 Beta Release
Grok-2,Multimodal,USA,xAI,2024-08-13,Industry,Chat,Confident,100000.0,Hosted access (no API),Unspecified unreleased,Unreleased,314000000000.0,6200000000000.0,2400.0,2.96e+25,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,4,6472188,154003694.78442368,4448988000.0,6.256621621621622e-10,70000000.0,369608867482.6168,168000000000.0,Grok-2 is our frontier language model with state-of-the-art reasoning capabilities. This release includes two members of the Grok family: Grok-2 and Grok-2 mini. Both models are now being released to Grok users on the 𝕏 platform.,https://x.ai/blog/grok-2,Grok-2 Beta Release
Grok-2,Multimodal,USA,xAI,2024-08-13,Industry,Code generation,Confident,100000.0,Hosted access (no API),Unspecified unreleased,Unreleased,314000000000.0,6200000000000.0,2400.0,2.96e+25,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,4,6472188,154003694.78442368,4448988000.0,6.256621621621622e-10,70000000.0,369608867482.6168,168000000000.0,Grok-2 is our frontier language model with state-of-the-art reasoning capabilities. This release includes two members of the Grok family: Grok-2 and Grok-2 mini. Both models are now being released to Grok users on the 𝕏 platform.,https://x.ai/blog/grok-2,Grok-2 Beta Release
Grok-2,Multimodal,USA,xAI,2024-08-13,Industry,Language modeling/generation,Confident,100000.0,Hosted access (no API),Unspecified unreleased,Unreleased,314000000000.0,6200000000000.0,2400.0,2.96e+25,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,4,6472188,154003694.78442368,4448988000.0,6.256621621621622e-10,70000000.0,369608867482.6168,168000000000.0,Grok-2 is our frontier language model with state-of-the-art reasoning capabilities. This release includes two members of the Grok family: Grok-2 and Grok-2 mini. Both models are now being released to Grok users on the 𝕏 platform.,https://x.ai/blog/grok-2,Grok-2 Beta Release
Grok-2,Multimodal,USA,xAI,2024-08-13,Industry,Question answering,Confident,100000.0,Hosted access (no API),Unspecified unreleased,Unreleased,314000000000.0,6200000000000.0,2400.0,2.96e+25,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,4,6472188,154003694.78442368,4448988000.0,6.256621621621622e-10,70000000.0,369608867482.6168,168000000000.0,Grok-2 is our frontier language model with state-of-the-art reasoning capabilities. This release includes two members of the Grok family: Grok-2 and Grok-2 mini. Both models are now being released to Grok users on the 𝕏 platform.,https://x.ai/blog/grok-2,Grok-2 Beta Release
Grok-2,Multimodal,USA,xAI,2024-08-13,Industry,Visual question answering,Confident,100000.0,Hosted access (no API),Unspecified unreleased,Unreleased,314000000000.0,6200000000000.0,2400.0,2.96e+25,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,4,6472188,154003694.78442368,4448988000.0,6.256621621621622e-10,70000000.0,369608867482.6168,168000000000.0,Grok-2 is our frontier language model with state-of-the-art reasoning capabilities. This release includes two members of the Grok family: Grok-2 and Grok-2 mini. Both models are now being released to Grok users on the 𝕏 platform.,https://x.ai/blog/grok-2,Grok-2 Beta Release
Grok-2,Vision,USA,xAI,2024-08-13,Industry,Chat,Confident,100000.0,Hosted access (no API),Unspecified unreleased,Unreleased,314000000000.0,6200000000000.0,2400.0,2.96e+25,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,4,6472188,154003694.78442368,4448988000.0,6.256621621621622e-10,70000000.0,369608867482.6168,168000000000.0,Grok-2 is our frontier language model with state-of-the-art reasoning capabilities. This release includes two members of the Grok family: Grok-2 and Grok-2 mini. Both models are now being released to Grok users on the 𝕏 platform.,https://x.ai/blog/grok-2,Grok-2 Beta Release
Grok-2,Vision,USA,xAI,2024-08-13,Industry,Code generation,Confident,100000.0,Hosted access (no API),Unspecified unreleased,Unreleased,314000000000.0,6200000000000.0,2400.0,2.96e+25,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,4,6472188,154003694.78442368,4448988000.0,6.256621621621622e-10,70000000.0,369608867482.6168,168000000000.0,Grok-2 is our frontier language model with state-of-the-art reasoning capabilities. This release includes two members of the Grok family: Grok-2 and Grok-2 mini. Both models are now being released to Grok users on the 𝕏 platform.,https://x.ai/blog/grok-2,Grok-2 Beta Release
Grok-2,Vision,USA,xAI,2024-08-13,Industry,Language modeling/generation,Confident,100000.0,Hosted access (no API),Unspecified unreleased,Unreleased,314000000000.0,6200000000000.0,2400.0,2.96e+25,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,4,6472188,154003694.78442368,4448988000.0,6.256621621621622e-10,70000000.0,369608867482.6168,168000000000.0,Grok-2 is our frontier language model with state-of-the-art reasoning capabilities. This release includes two members of the Grok family: Grok-2 and Grok-2 mini. Both models are now being released to Grok users on the 𝕏 platform.,https://x.ai/blog/grok-2,Grok-2 Beta Release
Grok-2,Vision,USA,xAI,2024-08-13,Industry,Question answering,Confident,100000.0,Hosted access (no API),Unspecified unreleased,Unreleased,314000000000.0,6200000000000.0,2400.0,2.96e+25,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,4,6472188,154003694.78442368,4448988000.0,6.256621621621622e-10,70000000.0,369608867482.6168,168000000000.0,Grok-2 is our frontier language model with state-of-the-art reasoning capabilities. This release includes two members of the Grok family: Grok-2 and Grok-2 mini. Both models are now being released to Grok users on the 𝕏 platform.,https://x.ai/blog/grok-2,Grok-2 Beta Release
Grok-2,Vision,USA,xAI,2024-08-13,Industry,Visual question answering,Confident,100000.0,Hosted access (no API),Unspecified unreleased,Unreleased,314000000000.0,6200000000000.0,2400.0,2.96e+25,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,4,6472188,154003694.78442368,4448988000.0,6.256621621621622e-10,70000000.0,369608867482.6168,168000000000.0,Grok-2 is our frontier language model with state-of-the-art reasoning capabilities. This release includes two members of the Grok family: Grok-2 and Grok-2 mini. Both models are now being released to Grok users on the 𝕏 platform.,https://x.ai/blog/grok-2,Grok-2 Beta Release
Grok-3,Language,USA,xAI,2025-02-17,Industry,Chat,Confident,100000.0,Hosted access (no API),Unspecified unreleased,Unreleased,314000000000.0,6200000000000.0,2400.0,4.64e+26,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,4,6472188,154003694.78442368,4448988000.0,3.9912931034482756e-11,70000000.0,369608867482.6168,168000000000.0,,https://x.ai/blog/grok-3,Grok 3 Beta — The Age of Reasoning Agents
Grok-3,Language,USA,xAI,2025-02-17,Industry,Code generation,Confident,100000.0,Hosted access (no API),Unspecified unreleased,Unreleased,314000000000.0,6200000000000.0,2400.0,4.64e+26,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,4,6472188,154003694.78442368,4448988000.0,3.9912931034482756e-11,70000000.0,369608867482.6168,168000000000.0,,https://x.ai/blog/grok-3,Grok 3 Beta — The Age of Reasoning Agents
Grok-3,Language,USA,xAI,2025-02-17,Industry,Language modeling/generation,Confident,100000.0,Hosted access (no API),Unspecified unreleased,Unreleased,314000000000.0,6200000000000.0,2400.0,4.64e+26,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,4,6472188,154003694.78442368,4448988000.0,3.9912931034482756e-11,70000000.0,369608867482.6168,168000000000.0,,https://x.ai/blog/grok-3,Grok 3 Beta — The Age of Reasoning Agents
Grok-3,Language,USA,xAI,2025-02-17,Industry,Question answering,Confident,100000.0,Hosted access (no API),Unspecified unreleased,Unreleased,314000000000.0,6200000000000.0,2400.0,4.64e+26,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,4,6472188,154003694.78442368,4448988000.0,3.9912931034482756e-11,70000000.0,369608867482.6168,168000000000.0,,https://x.ai/blog/grok-3,Grok 3 Beta — The Age of Reasoning Agents
Grok-3,Language,USA,xAI,2025-02-17,Industry,Visual question answering,Confident,100000.0,Hosted access (no API),Unspecified unreleased,Unreleased,314000000000.0,6200000000000.0,2400.0,4.64e+26,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,4,6472188,154003694.78442368,4448988000.0,3.9912931034482756e-11,70000000.0,369608867482.6168,168000000000.0,,https://x.ai/blog/grok-3,Grok 3 Beta — The Age of Reasoning Agents
Grok-3,Multimodal,USA,xAI,2025-02-17,Industry,Chat,Confident,100000.0,Hosted access (no API),Unspecified unreleased,Unreleased,314000000000.0,6200000000000.0,2400.0,4.64e+26,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,4,6472188,154003694.78442368,4448988000.0,3.9912931034482756e-11,70000000.0,369608867482.6168,168000000000.0,,https://x.ai/blog/grok-3,Grok 3 Beta — The Age of Reasoning Agents
Grok-3,Multimodal,USA,xAI,2025-02-17,Industry,Code generation,Confident,100000.0,Hosted access (no API),Unspecified unreleased,Unreleased,314000000000.0,6200000000000.0,2400.0,4.64e+26,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,4,6472188,154003694.78442368,4448988000.0,3.9912931034482756e-11,70000000.0,369608867482.6168,168000000000.0,,https://x.ai/blog/grok-3,Grok 3 Beta — The Age of Reasoning Agents
Grok-3,Multimodal,USA,xAI,2025-02-17,Industry,Language modeling/generation,Confident,100000.0,Hosted access (no API),Unspecified unreleased,Unreleased,314000000000.0,6200000000000.0,2400.0,4.64e+26,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,4,6472188,154003694.78442368,4448988000.0,3.9912931034482756e-11,70000000.0,369608867482.6168,168000000000.0,,https://x.ai/blog/grok-3,Grok 3 Beta — The Age of Reasoning Agents
Grok-3,Multimodal,USA,xAI,2025-02-17,Industry,Question answering,Confident,100000.0,Hosted access (no API),Unspecified unreleased,Unreleased,314000000000.0,6200000000000.0,2400.0,4.64e+26,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,4,6472188,154003694.78442368,4448988000.0,3.9912931034482756e-11,70000000.0,369608867482.6168,168000000000.0,,https://x.ai/blog/grok-3,Grok 3 Beta — The Age of Reasoning Agents
Grok-3,Multimodal,USA,xAI,2025-02-17,Industry,Visual question answering,Confident,100000.0,Hosted access (no API),Unspecified unreleased,Unreleased,314000000000.0,6200000000000.0,2400.0,4.64e+26,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,4,6472188,154003694.78442368,4448988000.0,3.9912931034482756e-11,70000000.0,369608867482.6168,168000000000.0,,https://x.ai/blog/grok-3,Grok 3 Beta — The Age of Reasoning Agents
Grok-3,Vision,USA,xAI,2025-02-17,Industry,Chat,Confident,100000.0,Hosted access (no API),Unspecified unreleased,Unreleased,314000000000.0,6200000000000.0,2400.0,4.64e+26,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,4,6472188,154003694.78442368,4448988000.0,3.9912931034482756e-11,70000000.0,369608867482.6168,168000000000.0,,https://x.ai/blog/grok-3,Grok 3 Beta — The Age of Reasoning Agents
Grok-3,Vision,USA,xAI,2025-02-17,Industry,Code generation,Confident,100000.0,Hosted access (no API),Unspecified unreleased,Unreleased,314000000000.0,6200000000000.0,2400.0,4.64e+26,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,4,6472188,154003694.78442368,4448988000.0,3.9912931034482756e-11,70000000.0,369608867482.6168,168000000000.0,,https://x.ai/blog/grok-3,Grok 3 Beta — The Age of Reasoning Agents
Grok-3,Vision,USA,xAI,2025-02-17,Industry,Language modeling/generation,Confident,100000.0,Hosted access (no API),Unspecified unreleased,Unreleased,314000000000.0,6200000000000.0,2400.0,4.64e+26,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,4,6472188,154003694.78442368,4448988000.0,3.9912931034482756e-11,70000000.0,369608867482.6168,168000000000.0,,https://x.ai/blog/grok-3,Grok 3 Beta — The Age of Reasoning Agents
Grok-3,Vision,USA,xAI,2025-02-17,Industry,Question answering,Confident,100000.0,Hosted access (no API),Unspecified unreleased,Unreleased,314000000000.0,6200000000000.0,2400.0,4.64e+26,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,4,6472188,154003694.78442368,4448988000.0,3.9912931034482756e-11,70000000.0,369608867482.6168,168000000000.0,,https://x.ai/blog/grok-3,Grok 3 Beta — The Age of Reasoning Agents
Grok-3,Vision,USA,xAI,2025-02-17,Industry,Visual question answering,Confident,100000.0,Hosted access (no API),Unspecified unreleased,Unreleased,314000000000.0,6200000000000.0,2400.0,4.64e+26,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,4,6472188,154003694.78442368,4448988000.0,3.9912931034482756e-11,70000000.0,369608867482.6168,168000000000.0,,https://x.ai/blog/grok-3,Grok 3 Beta — The Age of Reasoning Agents
HyperCLOVA 204B,Language,South Korea,NAVER,2021-09-10,Industry,Language modeling/generation,Speculative,1024.0,Not-defined,Unspecified unreleased,Unreleased,204000000000.0,560000000000.0,643.2,1.476e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,SOTA improvement,4,6472188,788306.6236983632,10348226.56,3.896815718157182e-08,409600.0,507038820.36278725,263454720.00000003,,,
HyperCLOVA 82B,Language,South Korea,NAVER,2021-09-10,Industry,Chat,Confident,1024.0,API access,Unspecified unreleased,Unreleased,82000000000.0,300000000000.0,643.2,1.476e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Training cost,4,6472188,788306.6236983632,10348226.56,3.896815718157182e-08,409600.0,507038820.36278725,263454720.00000003,"GPT-3 shows remarkable in-context learning ability of large-scale language models (LMs) trained on hundreds of billion scale data. Here we address some remaining issues less reported by the GPT-3 paper, such as a non-English LM, the performances of different sized models, and the effect of recently introduced prompt optimization on in-context learning. To achieve this, we introduce HyperCLOVA, a Korean variant of 82B GPT-3 trained on a Korean-centric corpus of 560B tokens. Enhanced by our Korean-specific tokenization, HyperCLOVA with our training configuration shows state-of-the-art in-context zero-shot and few-shot learning performances on various downstream tasks in Korean. Also, we show the performance benefits of prompt-based learning and demonstrate how it can be integrated into the prompt engineering pipeline. Then we discuss the possibility of materializing the No Code AI paradigm by providing AI prototyping capabilities to non-experts of ML by introducing HyperCLOVA studio, an interactive prompt engineering interface. Lastly, we demonstrate the potential of our methods with three successful in-house applications.",https://arxiv.org/abs/2109.04650,What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers
HyperCLOVA 82B,Language,South Korea,NAVER,2021-09-10,Industry,Language modeling/generation,Confident,1024.0,API access,Unspecified unreleased,Unreleased,82000000000.0,300000000000.0,643.2,1.476e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Training cost,4,6472188,788306.6236983632,10348226.56,3.896815718157182e-08,409600.0,507038820.36278725,263454720.00000003,"GPT-3 shows remarkable in-context learning ability of large-scale language models (LMs) trained on hundreds of billion scale data. Here we address some remaining issues less reported by the GPT-3 paper, such as a non-English LM, the performances of different sized models, and the effect of recently introduced prompt optimization on in-context learning. To achieve this, we introduce HyperCLOVA, a Korean variant of 82B GPT-3 trained on a Korean-centric corpus of 560B tokens. Enhanced by our Korean-specific tokenization, HyperCLOVA with our training configuration shows state-of-the-art in-context zero-shot and few-shot learning performances on various downstream tasks in Korean. Also, we show the performance benefits of prompt-based learning and demonstrate how it can be integrated into the prompt engineering pipeline. Then we discuss the possibility of materializing the No Code AI paradigm by providing AI prototyping capabilities to non-experts of ML by introducing HyperCLOVA studio, an interactive prompt engineering interface. Lastly, we demonstrate the potential of our methods with three successful in-house applications.",https://arxiv.org/abs/2109.04650,What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers
HyperCLOVA 82B,Language,South Korea,NAVER,2021-09-10,Industry,Text classification,Confident,1024.0,API access,Unspecified unreleased,Unreleased,82000000000.0,300000000000.0,643.2,1.476e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Training cost,4,6472188,788306.6236983632,10348226.56,3.896815718157182e-08,409600.0,507038820.36278725,263454720.00000003,"GPT-3 shows remarkable in-context learning ability of large-scale language models (LMs) trained on hundreds of billion scale data. Here we address some remaining issues less reported by the GPT-3 paper, such as a non-English LM, the performances of different sized models, and the effect of recently introduced prompt optimization on in-context learning. To achieve this, we introduce HyperCLOVA, a Korean variant of 82B GPT-3 trained on a Korean-centric corpus of 560B tokens. Enhanced by our Korean-specific tokenization, HyperCLOVA with our training configuration shows state-of-the-art in-context zero-shot and few-shot learning performances on various downstream tasks in Korean. Also, we show the performance benefits of prompt-based learning and demonstrate how it can be integrated into the prompt engineering pipeline. Then we discuss the possibility of materializing the No Code AI paradigm by providing AI prototyping capabilities to non-experts of ML by introducing HyperCLOVA studio, an interactive prompt engineering interface. Lastly, we demonstrate the potential of our methods with three successful in-house applications.",https://arxiv.org/abs/2109.04650,What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers
HyperCLOVA 82B,Language,South Korea,NAVER,2021-09-10,Industry,Translation,Confident,1024.0,API access,Unspecified unreleased,Unreleased,82000000000.0,300000000000.0,643.2,1.476e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Training cost,4,6472188,788306.6236983632,10348226.56,3.896815718157182e-08,409600.0,507038820.36278725,263454720.00000003,"GPT-3 shows remarkable in-context learning ability of large-scale language models (LMs) trained on hundreds of billion scale data. Here we address some remaining issues less reported by the GPT-3 paper, such as a non-English LM, the performances of different sized models, and the effect of recently introduced prompt optimization on in-context learning. To achieve this, we introduce HyperCLOVA, a Korean variant of 82B GPT-3 trained on a Korean-centric corpus of 560B tokens. Enhanced by our Korean-specific tokenization, HyperCLOVA with our training configuration shows state-of-the-art in-context zero-shot and few-shot learning performances on various downstream tasks in Korean. Also, we show the performance benefits of prompt-based learning and demonstrate how it can be integrated into the prompt engineering pipeline. Then we discuss the possibility of materializing the No Code AI paradigm by providing AI prototyping capabilities to non-experts of ML by introducing HyperCLOVA studio, an interactive prompt engineering interface. Lastly, we demonstrate the potential of our methods with three successful in-house applications.",https://arxiv.org/abs/2109.04650,What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers
HyperCLOVA 82B,Language,South Korea,Search Solutions,2021-09-10,Industry,Chat,Confident,1024.0,API access,Unspecified unreleased,Unreleased,82000000000.0,300000000000.0,643.2,1.476e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Training cost,4,6472188,788306.6236983632,10348226.56,3.896815718157182e-08,409600.0,507038820.36278725,263454720.00000003,"GPT-3 shows remarkable in-context learning ability of large-scale language models (LMs) trained on hundreds of billion scale data. Here we address some remaining issues less reported by the GPT-3 paper, such as a non-English LM, the performances of different sized models, and the effect of recently introduced prompt optimization on in-context learning. To achieve this, we introduce HyperCLOVA, a Korean variant of 82B GPT-3 trained on a Korean-centric corpus of 560B tokens. Enhanced by our Korean-specific tokenization, HyperCLOVA with our training configuration shows state-of-the-art in-context zero-shot and few-shot learning performances on various downstream tasks in Korean. Also, we show the performance benefits of prompt-based learning and demonstrate how it can be integrated into the prompt engineering pipeline. Then we discuss the possibility of materializing the No Code AI paradigm by providing AI prototyping capabilities to non-experts of ML by introducing HyperCLOVA studio, an interactive prompt engineering interface. Lastly, we demonstrate the potential of our methods with three successful in-house applications.",https://arxiv.org/abs/2109.04650,What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers
HyperCLOVA 82B,Language,South Korea,Search Solutions,2021-09-10,Industry,Language modeling/generation,Confident,1024.0,API access,Unspecified unreleased,Unreleased,82000000000.0,300000000000.0,643.2,1.476e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Training cost,4,6472188,788306.6236983632,10348226.56,3.896815718157182e-08,409600.0,507038820.36278725,263454720.00000003,"GPT-3 shows remarkable in-context learning ability of large-scale language models (LMs) trained on hundreds of billion scale data. Here we address some remaining issues less reported by the GPT-3 paper, such as a non-English LM, the performances of different sized models, and the effect of recently introduced prompt optimization on in-context learning. To achieve this, we introduce HyperCLOVA, a Korean variant of 82B GPT-3 trained on a Korean-centric corpus of 560B tokens. Enhanced by our Korean-specific tokenization, HyperCLOVA with our training configuration shows state-of-the-art in-context zero-shot and few-shot learning performances on various downstream tasks in Korean. Also, we show the performance benefits of prompt-based learning and demonstrate how it can be integrated into the prompt engineering pipeline. Then we discuss the possibility of materializing the No Code AI paradigm by providing AI prototyping capabilities to non-experts of ML by introducing HyperCLOVA studio, an interactive prompt engineering interface. Lastly, we demonstrate the potential of our methods with three successful in-house applications.",https://arxiv.org/abs/2109.04650,What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers
HyperCLOVA 82B,Language,South Korea,Search Solutions,2021-09-10,Industry,Text classification,Confident,1024.0,API access,Unspecified unreleased,Unreleased,82000000000.0,300000000000.0,643.2,1.476e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Training cost,4,6472188,788306.6236983632,10348226.56,3.896815718157182e-08,409600.0,507038820.36278725,263454720.00000003,"GPT-3 shows remarkable in-context learning ability of large-scale language models (LMs) trained on hundreds of billion scale data. Here we address some remaining issues less reported by the GPT-3 paper, such as a non-English LM, the performances of different sized models, and the effect of recently introduced prompt optimization on in-context learning. To achieve this, we introduce HyperCLOVA, a Korean variant of 82B GPT-3 trained on a Korean-centric corpus of 560B tokens. Enhanced by our Korean-specific tokenization, HyperCLOVA with our training configuration shows state-of-the-art in-context zero-shot and few-shot learning performances on various downstream tasks in Korean. Also, we show the performance benefits of prompt-based learning and demonstrate how it can be integrated into the prompt engineering pipeline. Then we discuss the possibility of materializing the No Code AI paradigm by providing AI prototyping capabilities to non-experts of ML by introducing HyperCLOVA studio, an interactive prompt engineering interface. Lastly, we demonstrate the potential of our methods with three successful in-house applications.",https://arxiv.org/abs/2109.04650,What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers
HyperCLOVA 82B,Language,South Korea,Search Solutions,2021-09-10,Industry,Translation,Confident,1024.0,API access,Unspecified unreleased,Unreleased,82000000000.0,300000000000.0,643.2,1.476e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Training cost,4,6472188,788306.6236983632,10348226.56,3.896815718157182e-08,409600.0,507038820.36278725,263454720.00000003,"GPT-3 shows remarkable in-context learning ability of large-scale language models (LMs) trained on hundreds of billion scale data. Here we address some remaining issues less reported by the GPT-3 paper, such as a non-English LM, the performances of different sized models, and the effect of recently introduced prompt optimization on in-context learning. To achieve this, we introduce HyperCLOVA, a Korean variant of 82B GPT-3 trained on a Korean-centric corpus of 560B tokens. Enhanced by our Korean-specific tokenization, HyperCLOVA with our training configuration shows state-of-the-art in-context zero-shot and few-shot learning performances on various downstream tasks in Korean. Also, we show the performance benefits of prompt-based learning and demonstrate how it can be integrated into the prompt engineering pipeline. Then we discuss the possibility of materializing the No Code AI paradigm by providing AI prototyping capabilities to non-experts of ML by introducing HyperCLOVA studio, an interactive prompt engineering interface. Lastly, we demonstrate the potential of our methods with three successful in-house applications.",https://arxiv.org/abs/2109.04650,What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers
HyperCLOVA X,Language,South Korea,NAVER,2023-08-24,Industry,Chat,Speculative,1024.0,API access,Not-defined,Unreleased,82000000000.0,300000000000.0,643.2,1.476e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Training cost,4,6472188,788306.6236983632,10348226.56,3.896815718157182e-08,409600.0,507038820.36278725,263454720.00000003,"South Korean internet search company Naver on Thursday rolled out its own generative artificial intelligence tool, HyperCLOVA X. The company’s large language model (LLM) offers services such as a ChatGPT-like AI chatbot, CLOVA X, and a generative AI-based search engine, Cue, equivalent to Microsoft Bing. ","https://arxiv.org/abs/2404.01954
https://techcrunch.com/2023/08/24/koreas-internet-giant-naver-unveils-generative-ai-services/, https://www.ncloud.com/solution/featured/hyperclovax",Korea’s internet giant Naver unveils generative AI services
HyperCLOVA X,Language,South Korea,NAVER,2023-08-24,Industry,Code generation,Speculative,1024.0,API access,Not-defined,Unreleased,82000000000.0,300000000000.0,643.2,1.476e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Training cost,4,6472188,788306.6236983632,10348226.56,3.896815718157182e-08,409600.0,507038820.36278725,263454720.00000003,"South Korean internet search company Naver on Thursday rolled out its own generative artificial intelligence tool, HyperCLOVA X. The company’s large language model (LLM) offers services such as a ChatGPT-like AI chatbot, CLOVA X, and a generative AI-based search engine, Cue, equivalent to Microsoft Bing. ","https://arxiv.org/abs/2404.01954
https://techcrunch.com/2023/08/24/koreas-internet-giant-naver-unveils-generative-ai-services/, https://www.ncloud.com/solution/featured/hyperclovax",Korea’s internet giant Naver unveils generative AI services
HyperCLOVA X,Language,South Korea,NAVER,2023-08-24,Industry,Language modeling/generation,Speculative,1024.0,API access,Not-defined,Unreleased,82000000000.0,300000000000.0,643.2,1.476e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Training cost,4,6472188,788306.6236983632,10348226.56,3.896815718157182e-08,409600.0,507038820.36278725,263454720.00000003,"South Korean internet search company Naver on Thursday rolled out its own generative artificial intelligence tool, HyperCLOVA X. The company’s large language model (LLM) offers services such as a ChatGPT-like AI chatbot, CLOVA X, and a generative AI-based search engine, Cue, equivalent to Microsoft Bing. ","https://arxiv.org/abs/2404.01954
https://techcrunch.com/2023/08/24/koreas-internet-giant-naver-unveils-generative-ai-services/, https://www.ncloud.com/solution/featured/hyperclovax",Korea’s internet giant Naver unveils generative AI services
HyperCLOVA X,Language,South Korea,NAVER,2023-08-24,Industry,Quantitative reasoning,Speculative,1024.0,API access,Not-defined,Unreleased,82000000000.0,300000000000.0,643.2,1.476e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Training cost,4,6472188,788306.6236983632,10348226.56,3.896815718157182e-08,409600.0,507038820.36278725,263454720.00000003,"South Korean internet search company Naver on Thursday rolled out its own generative artificial intelligence tool, HyperCLOVA X. The company’s large language model (LLM) offers services such as a ChatGPT-like AI chatbot, CLOVA X, and a generative AI-based search engine, Cue, equivalent to Microsoft Bing. ","https://arxiv.org/abs/2404.01954
https://techcrunch.com/2023/08/24/koreas-internet-giant-naver-unveils-generative-ai-services/, https://www.ncloud.com/solution/featured/hyperclovax",Korea’s internet giant Naver unveils generative AI services
HyperCLOVA X,Language,South Korea,NAVER,2023-08-24,Industry,Question answering,Speculative,1024.0,API access,Not-defined,Unreleased,82000000000.0,300000000000.0,643.2,1.476e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Training cost,4,6472188,788306.6236983632,10348226.56,3.896815718157182e-08,409600.0,507038820.36278725,263454720.00000003,"South Korean internet search company Naver on Thursday rolled out its own generative artificial intelligence tool, HyperCLOVA X. The company’s large language model (LLM) offers services such as a ChatGPT-like AI chatbot, CLOVA X, and a generative AI-based search engine, Cue, equivalent to Microsoft Bing. ","https://arxiv.org/abs/2404.01954
https://techcrunch.com/2023/08/24/koreas-internet-giant-naver-unveils-generative-ai-services/, https://www.ncloud.com/solution/featured/hyperclovax",Korea’s internet giant Naver unveils generative AI services
HyperCLOVA X,Language,South Korea,NAVER,2023-08-24,Industry,Search,Speculative,1024.0,API access,Not-defined,Unreleased,82000000000.0,300000000000.0,643.2,1.476e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Training cost,4,6472188,788306.6236983632,10348226.56,3.896815718157182e-08,409600.0,507038820.36278725,263454720.00000003,"South Korean internet search company Naver on Thursday rolled out its own generative artificial intelligence tool, HyperCLOVA X. The company’s large language model (LLM) offers services such as a ChatGPT-like AI chatbot, CLOVA X, and a generative AI-based search engine, Cue, equivalent to Microsoft Bing. ","https://arxiv.org/abs/2404.01954
https://techcrunch.com/2023/08/24/koreas-internet-giant-naver-unveils-generative-ai-services/, https://www.ncloud.com/solution/featured/hyperclovax",Korea’s internet giant Naver unveils generative AI services
HyperCLOVA X,Language,South Korea,NAVER,2023-08-24,Industry,Translation,Speculative,1024.0,API access,Not-defined,Unreleased,82000000000.0,300000000000.0,643.2,1.476e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Training cost,4,6472188,788306.6236983632,10348226.56,3.896815718157182e-08,409600.0,507038820.36278725,263454720.00000003,"South Korean internet search company Naver on Thursday rolled out its own generative artificial intelligence tool, HyperCLOVA X. The company’s large language model (LLM) offers services such as a ChatGPT-like AI chatbot, CLOVA X, and a generative AI-based search engine, Cue, equivalent to Microsoft Bing. ","https://arxiv.org/abs/2404.01954
https://techcrunch.com/2023/08/24/koreas-internet-giant-naver-unveils-generative-ai-services/, https://www.ncloud.com/solution/featured/hyperclovax",Korea’s internet giant Naver unveils generative AI services
Inflection-1,Language,USA,Inflection AI,2023-06-23,Industry,Language modeling,Speculative,5000.0,Hosted access (no API),Not-defined,Unreleased,278477081453.6924,589652798174979.1,1282.4326530612243,1.0001e+24,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,SOTA improvement,4,6472188,7732579.928411527,222449400.0,1.8517748225177482e-08,3500000.0,9916512992.600765,4488514285.714285,"Large language models (LLMs) based on the Transformer architecture have been shown to possess a range of advanced capabilities in language generation and understanding. These capabilities have paved the way for deployment of LLMs in products like OpenAI’s Chat-GPT and Google’s Bard. At Inflection AI, our mission is to create personal AIs for everyone, and in May 2023 we released Pi (pi.ai) – an LLM-based personal AI which is designed to be empathetic, useful, and safe. In this work we introduce the foundation model powering Pi, dubbed Inflection-1, and evaluate its performance characteristics across a variety of benchmarks.",https://inflection.ai/assets/Inflection-1.pdf,Inflection-1 technical memo
Inflection-2,Language,USA,Inflection AI,2023-11-22,Industry,Chat,Confident,5000.0,Hosted access (no API),Unspecified unreleased,Unreleased,278477081453.6924,589652798174979.1,1282.4326530612243,1.001e+25,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Significant use,4,6472188,7732579.928411527,222449400.0,1.85010989010989e-09,3500000.0,9916512992.600765,4488514285.714285,"Today we are proud to announce that we have completed training of Inflection-2, the best model in the world for its compute class and the second most capable LLM in the world today. Our mission at Inflection is to create a personal AI for everyone. Just a few months ago, we announced Inflection-1 — a best-in-class language model that currently powers Pi. Our new model, Inflection-2, is substantially more capable than Inflection-1, demonstrating much improved factual knowledge, better stylistic control, and dramatically improved reasoning.",https://inflection.ai/inflection-2,Inflection-2: The Next Step Up
Inflection-2,Language,USA,Inflection AI,2023-11-22,Industry,Chat,Confident,5000.0,Hosted access (no API),Unspecified unreleased,Unreleased,278477081453.6924,589652798174979.1,1282.4326530612243,1.001e+25,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,4,6472188,7732579.928411527,222449400.0,1.85010989010989e-09,3500000.0,9916512992.600765,4488514285.714285,"Today we are proud to announce that we have completed training of Inflection-2, the best model in the world for its compute class and the second most capable LLM in the world today. Our mission at Inflection is to create a personal AI for everyone. Just a few months ago, we announced Inflection-1 — a best-in-class language model that currently powers Pi. Our new model, Inflection-2, is substantially more capable than Inflection-1, demonstrating much improved factual knowledge, better stylistic control, and dramatically improved reasoning.",https://inflection.ai/inflection-2,Inflection-2: The Next Step Up
Inflection-2,Language,USA,Inflection AI,2023-11-22,Industry,Language modeling,Confident,5000.0,Hosted access (no API),Unspecified unreleased,Unreleased,278477081453.6924,589652798174979.1,1282.4326530612243,1.001e+25,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Significant use,4,6472188,7732579.928411527,222449400.0,1.85010989010989e-09,3500000.0,9916512992.600765,4488514285.714285,"Today we are proud to announce that we have completed training of Inflection-2, the best model in the world for its compute class and the second most capable LLM in the world today. Our mission at Inflection is to create a personal AI for everyone. Just a few months ago, we announced Inflection-1 — a best-in-class language model that currently powers Pi. Our new model, Inflection-2, is substantially more capable than Inflection-1, demonstrating much improved factual knowledge, better stylistic control, and dramatically improved reasoning.",https://inflection.ai/inflection-2,Inflection-2: The Next Step Up
Inflection-2,Language,USA,Inflection AI,2023-11-22,Industry,Language modeling,Confident,5000.0,Hosted access (no API),Unspecified unreleased,Unreleased,278477081453.6924,589652798174979.1,1282.4326530612243,1.001e+25,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,4,6472188,7732579.928411527,222449400.0,1.85010989010989e-09,3500000.0,9916512992.600765,4488514285.714285,"Today we are proud to announce that we have completed training of Inflection-2, the best model in the world for its compute class and the second most capable LLM in the world today. Our mission at Inflection is to create a personal AI for everyone. Just a few months ago, we announced Inflection-1 — a best-in-class language model that currently powers Pi. Our new model, Inflection-2, is substantially more capable than Inflection-1, demonstrating much improved factual knowledge, better stylistic control, and dramatically improved reasoning.",https://inflection.ai/inflection-2,Inflection-2: The Next Step Up
Inflection-2,Language,USA,Inflection AI,2023-11-22,Industry,Language modeling/generation,Confident,5000.0,Hosted access (no API),Unspecified unreleased,Unreleased,278477081453.6924,589652798174979.1,1282.4326530612243,1.001e+25,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Significant use,4,6472188,7732579.928411527,222449400.0,1.85010989010989e-09,3500000.0,9916512992.600765,4488514285.714285,"Today we are proud to announce that we have completed training of Inflection-2, the best model in the world for its compute class and the second most capable LLM in the world today. Our mission at Inflection is to create a personal AI for everyone. Just a few months ago, we announced Inflection-1 — a best-in-class language model that currently powers Pi. Our new model, Inflection-2, is substantially more capable than Inflection-1, demonstrating much improved factual knowledge, better stylistic control, and dramatically improved reasoning.",https://inflection.ai/inflection-2,Inflection-2: The Next Step Up
Inflection-2,Language,USA,Inflection AI,2023-11-22,Industry,Language modeling/generation,Confident,5000.0,Hosted access (no API),Unspecified unreleased,Unreleased,278477081453.6924,589652798174979.1,1282.4326530612243,1.001e+25,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,4,6472188,7732579.928411527,222449400.0,1.85010989010989e-09,3500000.0,9916512992.600765,4488514285.714285,"Today we are proud to announce that we have completed training of Inflection-2, the best model in the world for its compute class and the second most capable LLM in the world today. Our mission at Inflection is to create a personal AI for everyone. Just a few months ago, we announced Inflection-1 — a best-in-class language model that currently powers Pi. Our new model, Inflection-2, is substantially more capable than Inflection-1, demonstrating much improved factual knowledge, better stylistic control, and dramatically improved reasoning.",https://inflection.ai/inflection-2,Inflection-2: The Next Step Up
Inflection-2,Language,USA,Inflection AI,2023-11-22,Industry,Question answering,Confident,5000.0,Hosted access (no API),Unspecified unreleased,Unreleased,278477081453.6924,589652798174979.1,1282.4326530612243,1.001e+25,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Significant use,4,6472188,7732579.928411527,222449400.0,1.85010989010989e-09,3500000.0,9916512992.600765,4488514285.714285,"Today we are proud to announce that we have completed training of Inflection-2, the best model in the world for its compute class and the second most capable LLM in the world today. Our mission at Inflection is to create a personal AI for everyone. Just a few months ago, we announced Inflection-1 — a best-in-class language model that currently powers Pi. Our new model, Inflection-2, is substantially more capable than Inflection-1, demonstrating much improved factual knowledge, better stylistic control, and dramatically improved reasoning.",https://inflection.ai/inflection-2,Inflection-2: The Next Step Up
Inflection-2,Language,USA,Inflection AI,2023-11-22,Industry,Question answering,Confident,5000.0,Hosted access (no API),Unspecified unreleased,Unreleased,278477081453.6924,589652798174979.1,1282.4326530612243,1.001e+25,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,4,6472188,7732579.928411527,222449400.0,1.85010989010989e-09,3500000.0,9916512992.600765,4488514285.714285,"Today we are proud to announce that we have completed training of Inflection-2, the best model in the world for its compute class and the second most capable LLM in the world today. Our mission at Inflection is to create a personal AI for everyone. Just a few months ago, we announced Inflection-1 — a best-in-class language model that currently powers Pi. Our new model, Inflection-2, is substantially more capable than Inflection-1, demonstrating much improved factual knowledge, better stylistic control, and dramatically improved reasoning.",https://inflection.ai/inflection-2,Inflection-2: The Next Step Up
Inflection-2.5,Language,USA,Inflection AI,2024-03-07,Industry,Chat,Speculative,5000.0,Hosted access (no API),Not-defined,Unreleased,278477081453.6924,589652798174979.1,1282.4326530612243,1.0001e+25,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Significant use,4,6472188,788306.6236983632,222449400.0,1.8517748225177484e-09,3500000.0,1010950154.8552281,4488514285.714285,"At Inflection, our mission is to create a personal AI for everyone. Last May, we released Pi—a personal AI, designed to be empathetic, helpful, and safe. In November we announced a new major foundation model, Inflection-2, the second best LLM in the world at the time.

Now we are adding IQ to Pi’s exceptional EQ.

We are launching Inflection-2.5, our upgraded in-house model that is competitive with all the world's leading LLMs like GPT-4 and Gemini. It couples raw capability with our signature personality and unique empathetic fine-tuning. Inflection-2.5 is available to all Pi's users today, at pi.ai, on iOS, on Android, or our new desktop app.",https://inflection.ai/inflection-2-5,Inflection-2.5: meet the world's best personal AI
InternLM,Language,China,SenseTime,2023-07-06,Academia,Language modeling,Confident,800.0,Not-defined,Not-defined,Not-defined,100000000000.0,1000000000.0,1560.0,6.000001e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,SOTA improvement,1,2000000,788306.6236983632,9084456.0,9.586765068872488e-09,320000.0,1229758332.9694467,499200000.0,"Pre-training a bilingual 100B Foundation model on data with over a trillion tokens, the model exhibits excellent performance in scenarios such as Chinese, English, and coding due to the appropriate data ratio. Based on the foundation model, the application of high-quality human annotated dialogue data combined with RLHF technology enables the InternLM large language model to respond to complex commands during human interaction, while also demonstrating responses in line with human morality and values.",https://internlm.org/,
InternLM,Language,China,SenseTime,2023-07-06,Industry,Language modeling,Confident,800.0,Not-defined,Not-defined,Not-defined,100000000000.0,1000000000.0,1560.0,6.000001e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,SOTA improvement,1,2000000,788306.6236983632,9084456.0,9.586765068872488e-09,320000.0,1229758332.9694467,499200000.0,"Pre-training a bilingual 100B Foundation model on data with over a trillion tokens, the model exhibits excellent performance in scenarios such as Chinese, English, and coding due to the appropriate data ratio. Based on the foundation model, the application of high-quality human annotated dialogue data combined with RLHF technology enables the InternLM large language model to respond to complex commands during human interaction, while also demonstrating responses in line with human morality and values.",https://internlm.org/,
InternLM,Language,China,Shanghai AI Lab,2023-07-06,Academia,Language modeling,Confident,800.0,Not-defined,Not-defined,Not-defined,100000000000.0,1000000000.0,1560.0,6.000001e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,SOTA improvement,1,2000000,788306.6236983632,9084456.0,9.586765068872488e-09,320000.0,1229758332.9694467,499200000.0,"Pre-training a bilingual 100B Foundation model on data with over a trillion tokens, the model exhibits excellent performance in scenarios such as Chinese, English, and coding due to the appropriate data ratio. Based on the foundation model, the application of high-quality human annotated dialogue data combined with RLHF technology enables the InternLM large language model to respond to complex commands during human interaction, while also demonstrating responses in line with human morality and values.",https://internlm.org/,
InternLM,Language,China,Shanghai AI Lab,2023-07-06,Industry,Language modeling,Confident,800.0,Not-defined,Not-defined,Not-defined,100000000000.0,1000000000.0,1560.0,6.000001e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,SOTA improvement,1,2000000,788306.6236983632,9084456.0,9.586765068872488e-09,320000.0,1229758332.9694467,499200000.0,"Pre-training a bilingual 100B Foundation model on data with over a trillion tokens, the model exhibits excellent performance in scenarios such as Chinese, English, and coding due to the appropriate data ratio. Based on the foundation model, the application of high-quality human annotated dialogue data combined with RLHF technology enables the InternLM large language model to respond to complex commands during human interaction, while also demonstrating responses in line with human morality and values.",https://internlm.org/,
InternLM,Language,Hong Kong,SenseTime,2023-07-06,Academia,Language modeling,Confident,800.0,Not-defined,Not-defined,Not-defined,100000000000.0,1000000000.0,1560.0,6.000001e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,SOTA improvement,1,2000000,788306.6236983632,9084456.0,9.586765068872488e-09,320000.0,1229758332.9694467,499200000.0,"Pre-training a bilingual 100B Foundation model on data with over a trillion tokens, the model exhibits excellent performance in scenarios such as Chinese, English, and coding due to the appropriate data ratio. Based on the foundation model, the application of high-quality human annotated dialogue data combined with RLHF technology enables the InternLM large language model to respond to complex commands during human interaction, while also demonstrating responses in line with human morality and values.",https://internlm.org/,
InternLM,Language,Hong Kong,SenseTime,2023-07-06,Industry,Language modeling,Confident,800.0,Not-defined,Not-defined,Not-defined,100000000000.0,1000000000.0,1560.0,6.000001e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,SOTA improvement,1,2000000,788306.6236983632,9084456.0,9.586765068872488e-09,320000.0,1229758332.9694467,499200000.0,"Pre-training a bilingual 100B Foundation model on data with over a trillion tokens, the model exhibits excellent performance in scenarios such as Chinese, English, and coding due to the appropriate data ratio. Based on the foundation model, the application of high-quality human annotated dialogue data combined with RLHF technology enables the InternLM large language model to respond to complex commands during human interaction, while also demonstrating responses in line with human morality and values.",https://internlm.org/,
InternLM,Language,Hong Kong,Shanghai AI Lab,2023-07-06,Academia,Language modeling,Confident,800.0,Not-defined,Not-defined,Not-defined,100000000000.0,1000000000.0,1560.0,6.000001e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,SOTA improvement,1,2000000,788306.6236983632,9084456.0,9.586765068872488e-09,320000.0,1229758332.9694467,499200000.0,"Pre-training a bilingual 100B Foundation model on data with over a trillion tokens, the model exhibits excellent performance in scenarios such as Chinese, English, and coding due to the appropriate data ratio. Based on the foundation model, the application of high-quality human annotated dialogue data combined with RLHF technology enables the InternLM large language model to respond to complex commands during human interaction, while also demonstrating responses in line with human morality and values.",https://internlm.org/,
InternLM,Language,Hong Kong,Shanghai AI Lab,2023-07-06,Industry,Language modeling,Confident,800.0,Not-defined,Not-defined,Not-defined,100000000000.0,1000000000.0,1560.0,6.000001e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,SOTA improvement,1,2000000,788306.6236983632,9084456.0,9.586765068872488e-09,320000.0,1229758332.9694467,499200000.0,"Pre-training a bilingual 100B Foundation model on data with over a trillion tokens, the model exhibits excellent performance in scenarios such as Chinese, English, and coding due to the appropriate data ratio. Based on the foundation model, the application of high-quality human annotated dialogue data combined with RLHF technology enables the InternLM large language model to respond to complex commands during human interaction, while also demonstrating responses in line with human morality and values.",https://internlm.org/,
InternLM2-20B,Language,China,Chinese University of Hong Kong (CUHK),2024-01-12,Academia,Chat,Confident,1024.0,Open weights (restricted use),Unspecified unreleased,Unreleased,20000000000.0,2600000000000.0,793.5,3.12e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Training cost,1,2000000,197336.5951439333,11628103.68,1.8436089743589742e-08,409600.0,156586588.24671108,325017600.0,"The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has sparked discussions on the advent of Artificial General Intelligence (AGI). However, replicating such advancements in open-source models has been challenging. This paper introduces InternLM2, an open-source LLM that outperforms its predecessors in comprehensive evaluations across 6 dimensions and 30 benchmarks, long-context modeling, and open-ended subjective evaluations through innovative pre-training and optimization techniques. The pre-training process of InternLM2 is meticulously detailed, highlighting the preparation of diverse data types including text, code, and long-context data. InternLM2 efficiently captures long-term dependencies, initially trained on 4k tokens before advancing to 32k tokens in pre-training and fine-tuning stages, exhibiting remarkable performance on the 200k ``Needle-in-a-Haystack"" test. InternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel Conditional Online Reinforcement Learning from Human Feedback (COOL RLHF) strategy that addresses conflicting human preferences and reward hacking. By releasing InternLM2 models in different training stages and model sizes, we provide the community with insights into the model's evolution.",https://arxiv.org/abs/2403.17297,InternLM2 Technical Report
InternLM2-20B,Language,China,Chinese University of Hong Kong (CUHK),2024-01-12,Academia,Language modeling/generation,Confident,1024.0,Open weights (restricted use),Unspecified unreleased,Unreleased,20000000000.0,2600000000000.0,793.5,3.12e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Training cost,1,2000000,197336.5951439333,11628103.68,1.8436089743589742e-08,409600.0,156586588.24671108,325017600.0,"The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has sparked discussions on the advent of Artificial General Intelligence (AGI). However, replicating such advancements in open-source models has been challenging. This paper introduces InternLM2, an open-source LLM that outperforms its predecessors in comprehensive evaluations across 6 dimensions and 30 benchmarks, long-context modeling, and open-ended subjective evaluations through innovative pre-training and optimization techniques. The pre-training process of InternLM2 is meticulously detailed, highlighting the preparation of diverse data types including text, code, and long-context data. InternLM2 efficiently captures long-term dependencies, initially trained on 4k tokens before advancing to 32k tokens in pre-training and fine-tuning stages, exhibiting remarkable performance on the 200k ``Needle-in-a-Haystack"" test. InternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel Conditional Online Reinforcement Learning from Human Feedback (COOL RLHF) strategy that addresses conflicting human preferences and reward hacking. By releasing InternLM2 models in different training stages and model sizes, we provide the community with insights into the model's evolution.",https://arxiv.org/abs/2403.17297,InternLM2 Technical Report
InternLM2-20B,Language,China,Chinese University of Hong Kong (CUHK),2024-01-12,Academia,Question answering,Confident,1024.0,Open weights (restricted use),Unspecified unreleased,Unreleased,20000000000.0,2600000000000.0,793.5,3.12e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Training cost,1,2000000,197336.5951439333,11628103.68,1.8436089743589742e-08,409600.0,156586588.24671108,325017600.0,"The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has sparked discussions on the advent of Artificial General Intelligence (AGI). However, replicating such advancements in open-source models has been challenging. This paper introduces InternLM2, an open-source LLM that outperforms its predecessors in comprehensive evaluations across 6 dimensions and 30 benchmarks, long-context modeling, and open-ended subjective evaluations through innovative pre-training and optimization techniques. The pre-training process of InternLM2 is meticulously detailed, highlighting the preparation of diverse data types including text, code, and long-context data. InternLM2 efficiently captures long-term dependencies, initially trained on 4k tokens before advancing to 32k tokens in pre-training and fine-tuning stages, exhibiting remarkable performance on the 200k ``Needle-in-a-Haystack"" test. InternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel Conditional Online Reinforcement Learning from Human Feedback (COOL RLHF) strategy that addresses conflicting human preferences and reward hacking. By releasing InternLM2 models in different training stages and model sizes, we provide the community with insights into the model's evolution.",https://arxiv.org/abs/2403.17297,InternLM2 Technical Report
InternLM2-20B,Language,China,Chinese University of Hong Kong (CUHK),2024-01-12,Industry,Chat,Confident,1024.0,Open weights (restricted use),Unspecified unreleased,Unreleased,20000000000.0,2600000000000.0,793.5,3.12e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Training cost,1,2000000,197336.5951439333,11628103.68,1.8436089743589742e-08,409600.0,156586588.24671108,325017600.0,"The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has sparked discussions on the advent of Artificial General Intelligence (AGI). However, replicating such advancements in open-source models has been challenging. This paper introduces InternLM2, an open-source LLM that outperforms its predecessors in comprehensive evaluations across 6 dimensions and 30 benchmarks, long-context modeling, and open-ended subjective evaluations through innovative pre-training and optimization techniques. The pre-training process of InternLM2 is meticulously detailed, highlighting the preparation of diverse data types including text, code, and long-context data. InternLM2 efficiently captures long-term dependencies, initially trained on 4k tokens before advancing to 32k tokens in pre-training and fine-tuning stages, exhibiting remarkable performance on the 200k ``Needle-in-a-Haystack"" test. InternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel Conditional Online Reinforcement Learning from Human Feedback (COOL RLHF) strategy that addresses conflicting human preferences and reward hacking. By releasing InternLM2 models in different training stages and model sizes, we provide the community with insights into the model's evolution.",https://arxiv.org/abs/2403.17297,InternLM2 Technical Report
InternLM2-20B,Language,China,Chinese University of Hong Kong (CUHK),2024-01-12,Industry,Language modeling/generation,Confident,1024.0,Open weights (restricted use),Unspecified unreleased,Unreleased,20000000000.0,2600000000000.0,793.5,3.12e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Training cost,1,2000000,197336.5951439333,11628103.68,1.8436089743589742e-08,409600.0,156586588.24671108,325017600.0,"The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has sparked discussions on the advent of Artificial General Intelligence (AGI). However, replicating such advancements in open-source models has been challenging. This paper introduces InternLM2, an open-source LLM that outperforms its predecessors in comprehensive evaluations across 6 dimensions and 30 benchmarks, long-context modeling, and open-ended subjective evaluations through innovative pre-training and optimization techniques. The pre-training process of InternLM2 is meticulously detailed, highlighting the preparation of diverse data types including text, code, and long-context data. InternLM2 efficiently captures long-term dependencies, initially trained on 4k tokens before advancing to 32k tokens in pre-training and fine-tuning stages, exhibiting remarkable performance on the 200k ``Needle-in-a-Haystack"" test. InternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel Conditional Online Reinforcement Learning from Human Feedback (COOL RLHF) strategy that addresses conflicting human preferences and reward hacking. By releasing InternLM2 models in different training stages and model sizes, we provide the community with insights into the model's evolution.",https://arxiv.org/abs/2403.17297,InternLM2 Technical Report
InternLM2-20B,Language,China,Chinese University of Hong Kong (CUHK),2024-01-12,Industry,Question answering,Confident,1024.0,Open weights (restricted use),Unspecified unreleased,Unreleased,20000000000.0,2600000000000.0,793.5,3.12e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Training cost,1,2000000,197336.5951439333,11628103.68,1.8436089743589742e-08,409600.0,156586588.24671108,325017600.0,"The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has sparked discussions on the advent of Artificial General Intelligence (AGI). However, replicating such advancements in open-source models has been challenging. This paper introduces InternLM2, an open-source LLM that outperforms its predecessors in comprehensive evaluations across 6 dimensions and 30 benchmarks, long-context modeling, and open-ended subjective evaluations through innovative pre-training and optimization techniques. The pre-training process of InternLM2 is meticulously detailed, highlighting the preparation of diverse data types including text, code, and long-context data. InternLM2 efficiently captures long-term dependencies, initially trained on 4k tokens before advancing to 32k tokens in pre-training and fine-tuning stages, exhibiting remarkable performance on the 200k ``Needle-in-a-Haystack"" test. InternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel Conditional Online Reinforcement Learning from Human Feedback (COOL RLHF) strategy that addresses conflicting human preferences and reward hacking. By releasing InternLM2 models in different training stages and model sizes, we provide the community with insights into the model's evolution.",https://arxiv.org/abs/2403.17297,InternLM2 Technical Report
InternLM2-20B,Language,China,Fudan University,2024-01-12,Academia,Chat,Confident,1024.0,Open weights (restricted use),Unspecified unreleased,Unreleased,20000000000.0,2600000000000.0,793.5,3.12e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Training cost,1,2000000,197336.5951439333,11628103.68,1.8436089743589742e-08,409600.0,156586588.24671108,325017600.0,"The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has sparked discussions on the advent of Artificial General Intelligence (AGI). However, replicating such advancements in open-source models has been challenging. This paper introduces InternLM2, an open-source LLM that outperforms its predecessors in comprehensive evaluations across 6 dimensions and 30 benchmarks, long-context modeling, and open-ended subjective evaluations through innovative pre-training and optimization techniques. The pre-training process of InternLM2 is meticulously detailed, highlighting the preparation of diverse data types including text, code, and long-context data. InternLM2 efficiently captures long-term dependencies, initially trained on 4k tokens before advancing to 32k tokens in pre-training and fine-tuning stages, exhibiting remarkable performance on the 200k ``Needle-in-a-Haystack"" test. InternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel Conditional Online Reinforcement Learning from Human Feedback (COOL RLHF) strategy that addresses conflicting human preferences and reward hacking. By releasing InternLM2 models in different training stages and model sizes, we provide the community with insights into the model's evolution.",https://arxiv.org/abs/2403.17297,InternLM2 Technical Report
InternLM2-20B,Language,China,Fudan University,2024-01-12,Academia,Language modeling/generation,Confident,1024.0,Open weights (restricted use),Unspecified unreleased,Unreleased,20000000000.0,2600000000000.0,793.5,3.12e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Training cost,1,2000000,197336.5951439333,11628103.68,1.8436089743589742e-08,409600.0,156586588.24671108,325017600.0,"The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has sparked discussions on the advent of Artificial General Intelligence (AGI). However, replicating such advancements in open-source models has been challenging. This paper introduces InternLM2, an open-source LLM that outperforms its predecessors in comprehensive evaluations across 6 dimensions and 30 benchmarks, long-context modeling, and open-ended subjective evaluations through innovative pre-training and optimization techniques. The pre-training process of InternLM2 is meticulously detailed, highlighting the preparation of diverse data types including text, code, and long-context data. InternLM2 efficiently captures long-term dependencies, initially trained on 4k tokens before advancing to 32k tokens in pre-training and fine-tuning stages, exhibiting remarkable performance on the 200k ``Needle-in-a-Haystack"" test. InternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel Conditional Online Reinforcement Learning from Human Feedback (COOL RLHF) strategy that addresses conflicting human preferences and reward hacking. By releasing InternLM2 models in different training stages and model sizes, we provide the community with insights into the model's evolution.",https://arxiv.org/abs/2403.17297,InternLM2 Technical Report
InternLM2-20B,Language,China,Fudan University,2024-01-12,Academia,Question answering,Confident,1024.0,Open weights (restricted use),Unspecified unreleased,Unreleased,20000000000.0,2600000000000.0,793.5,3.12e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Training cost,1,2000000,197336.5951439333,11628103.68,1.8436089743589742e-08,409600.0,156586588.24671108,325017600.0,"The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has sparked discussions on the advent of Artificial General Intelligence (AGI). However, replicating such advancements in open-source models has been challenging. This paper introduces InternLM2, an open-source LLM that outperforms its predecessors in comprehensive evaluations across 6 dimensions and 30 benchmarks, long-context modeling, and open-ended subjective evaluations through innovative pre-training and optimization techniques. The pre-training process of InternLM2 is meticulously detailed, highlighting the preparation of diverse data types including text, code, and long-context data. InternLM2 efficiently captures long-term dependencies, initially trained on 4k tokens before advancing to 32k tokens in pre-training and fine-tuning stages, exhibiting remarkable performance on the 200k ``Needle-in-a-Haystack"" test. InternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel Conditional Online Reinforcement Learning from Human Feedback (COOL RLHF) strategy that addresses conflicting human preferences and reward hacking. By releasing InternLM2 models in different training stages and model sizes, we provide the community with insights into the model's evolution.",https://arxiv.org/abs/2403.17297,InternLM2 Technical Report
InternLM2-20B,Language,China,Fudan University,2024-01-12,Industry,Chat,Confident,1024.0,Open weights (restricted use),Unspecified unreleased,Unreleased,20000000000.0,2600000000000.0,793.5,3.12e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Training cost,1,2000000,197336.5951439333,11628103.68,1.8436089743589742e-08,409600.0,156586588.24671108,325017600.0,"The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has sparked discussions on the advent of Artificial General Intelligence (AGI). However, replicating such advancements in open-source models has been challenging. This paper introduces InternLM2, an open-source LLM that outperforms its predecessors in comprehensive evaluations across 6 dimensions and 30 benchmarks, long-context modeling, and open-ended subjective evaluations through innovative pre-training and optimization techniques. The pre-training process of InternLM2 is meticulously detailed, highlighting the preparation of diverse data types including text, code, and long-context data. InternLM2 efficiently captures long-term dependencies, initially trained on 4k tokens before advancing to 32k tokens in pre-training and fine-tuning stages, exhibiting remarkable performance on the 200k ``Needle-in-a-Haystack"" test. InternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel Conditional Online Reinforcement Learning from Human Feedback (COOL RLHF) strategy that addresses conflicting human preferences and reward hacking. By releasing InternLM2 models in different training stages and model sizes, we provide the community with insights into the model's evolution.",https://arxiv.org/abs/2403.17297,InternLM2 Technical Report
InternLM2-20B,Language,China,Fudan University,2024-01-12,Industry,Language modeling/generation,Confident,1024.0,Open weights (restricted use),Unspecified unreleased,Unreleased,20000000000.0,2600000000000.0,793.5,3.12e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Training cost,1,2000000,197336.5951439333,11628103.68,1.8436089743589742e-08,409600.0,156586588.24671108,325017600.0,"The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has sparked discussions on the advent of Artificial General Intelligence (AGI). However, replicating such advancements in open-source models has been challenging. This paper introduces InternLM2, an open-source LLM that outperforms its predecessors in comprehensive evaluations across 6 dimensions and 30 benchmarks, long-context modeling, and open-ended subjective evaluations through innovative pre-training and optimization techniques. The pre-training process of InternLM2 is meticulously detailed, highlighting the preparation of diverse data types including text, code, and long-context data. InternLM2 efficiently captures long-term dependencies, initially trained on 4k tokens before advancing to 32k tokens in pre-training and fine-tuning stages, exhibiting remarkable performance on the 200k ``Needle-in-a-Haystack"" test. InternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel Conditional Online Reinforcement Learning from Human Feedback (COOL RLHF) strategy that addresses conflicting human preferences and reward hacking. By releasing InternLM2 models in different training stages and model sizes, we provide the community with insights into the model's evolution.",https://arxiv.org/abs/2403.17297,InternLM2 Technical Report
InternLM2-20B,Language,China,Fudan University,2024-01-12,Industry,Question answering,Confident,1024.0,Open weights (restricted use),Unspecified unreleased,Unreleased,20000000000.0,2600000000000.0,793.5,3.12e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Training cost,1,2000000,197336.5951439333,11628103.68,1.8436089743589742e-08,409600.0,156586588.24671108,325017600.0,"The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has sparked discussions on the advent of Artificial General Intelligence (AGI). However, replicating such advancements in open-source models has been challenging. This paper introduces InternLM2, an open-source LLM that outperforms its predecessors in comprehensive evaluations across 6 dimensions and 30 benchmarks, long-context modeling, and open-ended subjective evaluations through innovative pre-training and optimization techniques. The pre-training process of InternLM2 is meticulously detailed, highlighting the preparation of diverse data types including text, code, and long-context data. InternLM2 efficiently captures long-term dependencies, initially trained on 4k tokens before advancing to 32k tokens in pre-training and fine-tuning stages, exhibiting remarkable performance on the 200k ``Needle-in-a-Haystack"" test. InternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel Conditional Online Reinforcement Learning from Human Feedback (COOL RLHF) strategy that addresses conflicting human preferences and reward hacking. By releasing InternLM2 models in different training stages and model sizes, we provide the community with insights into the model's evolution.",https://arxiv.org/abs/2403.17297,InternLM2 Technical Report
InternLM2-20B,Language,China,SenseTime,2024-01-12,Academia,Chat,Confident,1024.0,Open weights (restricted use),Unspecified unreleased,Unreleased,20000000000.0,2600000000000.0,793.5,3.12e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Training cost,1,2000000,197336.5951439333,11628103.68,1.8436089743589742e-08,409600.0,156586588.24671108,325017600.0,"The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has sparked discussions on the advent of Artificial General Intelligence (AGI). However, replicating such advancements in open-source models has been challenging. This paper introduces InternLM2, an open-source LLM that outperforms its predecessors in comprehensive evaluations across 6 dimensions and 30 benchmarks, long-context modeling, and open-ended subjective evaluations through innovative pre-training and optimization techniques. The pre-training process of InternLM2 is meticulously detailed, highlighting the preparation of diverse data types including text, code, and long-context data. InternLM2 efficiently captures long-term dependencies, initially trained on 4k tokens before advancing to 32k tokens in pre-training and fine-tuning stages, exhibiting remarkable performance on the 200k ``Needle-in-a-Haystack"" test. InternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel Conditional Online Reinforcement Learning from Human Feedback (COOL RLHF) strategy that addresses conflicting human preferences and reward hacking. By releasing InternLM2 models in different training stages and model sizes, we provide the community with insights into the model's evolution.",https://arxiv.org/abs/2403.17297,InternLM2 Technical Report
InternLM2-20B,Language,China,SenseTime,2024-01-12,Academia,Language modeling/generation,Confident,1024.0,Open weights (restricted use),Unspecified unreleased,Unreleased,20000000000.0,2600000000000.0,793.5,3.12e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Training cost,1,2000000,197336.5951439333,11628103.68,1.8436089743589742e-08,409600.0,156586588.24671108,325017600.0,"The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has sparked discussions on the advent of Artificial General Intelligence (AGI). However, replicating such advancements in open-source models has been challenging. This paper introduces InternLM2, an open-source LLM that outperforms its predecessors in comprehensive evaluations across 6 dimensions and 30 benchmarks, long-context modeling, and open-ended subjective evaluations through innovative pre-training and optimization techniques. The pre-training process of InternLM2 is meticulously detailed, highlighting the preparation of diverse data types including text, code, and long-context data. InternLM2 efficiently captures long-term dependencies, initially trained on 4k tokens before advancing to 32k tokens in pre-training and fine-tuning stages, exhibiting remarkable performance on the 200k ``Needle-in-a-Haystack"" test. InternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel Conditional Online Reinforcement Learning from Human Feedback (COOL RLHF) strategy that addresses conflicting human preferences and reward hacking. By releasing InternLM2 models in different training stages and model sizes, we provide the community with insights into the model's evolution.",https://arxiv.org/abs/2403.17297,InternLM2 Technical Report
InternLM2-20B,Language,China,SenseTime,2024-01-12,Academia,Question answering,Confident,1024.0,Open weights (restricted use),Unspecified unreleased,Unreleased,20000000000.0,2600000000000.0,793.5,3.12e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Training cost,1,2000000,197336.5951439333,11628103.68,1.8436089743589742e-08,409600.0,156586588.24671108,325017600.0,"The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has sparked discussions on the advent of Artificial General Intelligence (AGI). However, replicating such advancements in open-source models has been challenging. This paper introduces InternLM2, an open-source LLM that outperforms its predecessors in comprehensive evaluations across 6 dimensions and 30 benchmarks, long-context modeling, and open-ended subjective evaluations through innovative pre-training and optimization techniques. The pre-training process of InternLM2 is meticulously detailed, highlighting the preparation of diverse data types including text, code, and long-context data. InternLM2 efficiently captures long-term dependencies, initially trained on 4k tokens before advancing to 32k tokens in pre-training and fine-tuning stages, exhibiting remarkable performance on the 200k ``Needle-in-a-Haystack"" test. InternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel Conditional Online Reinforcement Learning from Human Feedback (COOL RLHF) strategy that addresses conflicting human preferences and reward hacking. By releasing InternLM2 models in different training stages and model sizes, we provide the community with insights into the model's evolution.",https://arxiv.org/abs/2403.17297,InternLM2 Technical Report
InternLM2-20B,Language,China,SenseTime,2024-01-12,Industry,Chat,Confident,1024.0,Open weights (restricted use),Unspecified unreleased,Unreleased,20000000000.0,2600000000000.0,793.5,3.12e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Training cost,1,2000000,197336.5951439333,11628103.68,1.8436089743589742e-08,409600.0,156586588.24671108,325017600.0,"The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has sparked discussions on the advent of Artificial General Intelligence (AGI). However, replicating such advancements in open-source models has been challenging. This paper introduces InternLM2, an open-source LLM that outperforms its predecessors in comprehensive evaluations across 6 dimensions and 30 benchmarks, long-context modeling, and open-ended subjective evaluations through innovative pre-training and optimization techniques. The pre-training process of InternLM2 is meticulously detailed, highlighting the preparation of diverse data types including text, code, and long-context data. InternLM2 efficiently captures long-term dependencies, initially trained on 4k tokens before advancing to 32k tokens in pre-training and fine-tuning stages, exhibiting remarkable performance on the 200k ``Needle-in-a-Haystack"" test. InternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel Conditional Online Reinforcement Learning from Human Feedback (COOL RLHF) strategy that addresses conflicting human preferences and reward hacking. By releasing InternLM2 models in different training stages and model sizes, we provide the community with insights into the model's evolution.",https://arxiv.org/abs/2403.17297,InternLM2 Technical Report
InternLM2-20B,Language,China,SenseTime,2024-01-12,Industry,Language modeling/generation,Confident,1024.0,Open weights (restricted use),Unspecified unreleased,Unreleased,20000000000.0,2600000000000.0,793.5,3.12e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Training cost,1,2000000,197336.5951439333,11628103.68,1.8436089743589742e-08,409600.0,156586588.24671108,325017600.0,"The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has sparked discussions on the advent of Artificial General Intelligence (AGI). However, replicating such advancements in open-source models has been challenging. This paper introduces InternLM2, an open-source LLM that outperforms its predecessors in comprehensive evaluations across 6 dimensions and 30 benchmarks, long-context modeling, and open-ended subjective evaluations through innovative pre-training and optimization techniques. The pre-training process of InternLM2 is meticulously detailed, highlighting the preparation of diverse data types including text, code, and long-context data. InternLM2 efficiently captures long-term dependencies, initially trained on 4k tokens before advancing to 32k tokens in pre-training and fine-tuning stages, exhibiting remarkable performance on the 200k ``Needle-in-a-Haystack"" test. InternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel Conditional Online Reinforcement Learning from Human Feedback (COOL RLHF) strategy that addresses conflicting human preferences and reward hacking. By releasing InternLM2 models in different training stages and model sizes, we provide the community with insights into the model's evolution.",https://arxiv.org/abs/2403.17297,InternLM2 Technical Report
InternLM2-20B,Language,China,SenseTime,2024-01-12,Industry,Question answering,Confident,1024.0,Open weights (restricted use),Unspecified unreleased,Unreleased,20000000000.0,2600000000000.0,793.5,3.12e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Training cost,1,2000000,197336.5951439333,11628103.68,1.8436089743589742e-08,409600.0,156586588.24671108,325017600.0,"The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has sparked discussions on the advent of Artificial General Intelligence (AGI). However, replicating such advancements in open-source models has been challenging. This paper introduces InternLM2, an open-source LLM that outperforms its predecessors in comprehensive evaluations across 6 dimensions and 30 benchmarks, long-context modeling, and open-ended subjective evaluations through innovative pre-training and optimization techniques. The pre-training process of InternLM2 is meticulously detailed, highlighting the preparation of diverse data types including text, code, and long-context data. InternLM2 efficiently captures long-term dependencies, initially trained on 4k tokens before advancing to 32k tokens in pre-training and fine-tuning stages, exhibiting remarkable performance on the 200k ``Needle-in-a-Haystack"" test. InternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel Conditional Online Reinforcement Learning from Human Feedback (COOL RLHF) strategy that addresses conflicting human preferences and reward hacking. By releasing InternLM2 models in different training stages and model sizes, we provide the community with insights into the model's evolution.",https://arxiv.org/abs/2403.17297,InternLM2 Technical Report
InternLM2-20B,Language,China,Shanghai AI Lab,2024-01-12,Academia,Chat,Confident,1024.0,Open weights (restricted use),Unspecified unreleased,Unreleased,20000000000.0,2600000000000.0,793.5,3.12e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Training cost,1,2000000,197336.5951439333,11628103.68,1.8436089743589742e-08,409600.0,156586588.24671108,325017600.0,"The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has sparked discussions on the advent of Artificial General Intelligence (AGI). However, replicating such advancements in open-source models has been challenging. This paper introduces InternLM2, an open-source LLM that outperforms its predecessors in comprehensive evaluations across 6 dimensions and 30 benchmarks, long-context modeling, and open-ended subjective evaluations through innovative pre-training and optimization techniques. The pre-training process of InternLM2 is meticulously detailed, highlighting the preparation of diverse data types including text, code, and long-context data. InternLM2 efficiently captures long-term dependencies, initially trained on 4k tokens before advancing to 32k tokens in pre-training and fine-tuning stages, exhibiting remarkable performance on the 200k ``Needle-in-a-Haystack"" test. InternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel Conditional Online Reinforcement Learning from Human Feedback (COOL RLHF) strategy that addresses conflicting human preferences and reward hacking. By releasing InternLM2 models in different training stages and model sizes, we provide the community with insights into the model's evolution.",https://arxiv.org/abs/2403.17297,InternLM2 Technical Report
InternLM2-20B,Language,China,Shanghai AI Lab,2024-01-12,Academia,Language modeling/generation,Confident,1024.0,Open weights (restricted use),Unspecified unreleased,Unreleased,20000000000.0,2600000000000.0,793.5,3.12e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Training cost,1,2000000,197336.5951439333,11628103.68,1.8436089743589742e-08,409600.0,156586588.24671108,325017600.0,"The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has sparked discussions on the advent of Artificial General Intelligence (AGI). However, replicating such advancements in open-source models has been challenging. This paper introduces InternLM2, an open-source LLM that outperforms its predecessors in comprehensive evaluations across 6 dimensions and 30 benchmarks, long-context modeling, and open-ended subjective evaluations through innovative pre-training and optimization techniques. The pre-training process of InternLM2 is meticulously detailed, highlighting the preparation of diverse data types including text, code, and long-context data. InternLM2 efficiently captures long-term dependencies, initially trained on 4k tokens before advancing to 32k tokens in pre-training and fine-tuning stages, exhibiting remarkable performance on the 200k ``Needle-in-a-Haystack"" test. InternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel Conditional Online Reinforcement Learning from Human Feedback (COOL RLHF) strategy that addresses conflicting human preferences and reward hacking. By releasing InternLM2 models in different training stages and model sizes, we provide the community with insights into the model's evolution.",https://arxiv.org/abs/2403.17297,InternLM2 Technical Report
InternLM2-20B,Language,China,Shanghai AI Lab,2024-01-12,Academia,Question answering,Confident,1024.0,Open weights (restricted use),Unspecified unreleased,Unreleased,20000000000.0,2600000000000.0,793.5,3.12e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Training cost,1,2000000,197336.5951439333,11628103.68,1.8436089743589742e-08,409600.0,156586588.24671108,325017600.0,"The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has sparked discussions on the advent of Artificial General Intelligence (AGI). However, replicating such advancements in open-source models has been challenging. This paper introduces InternLM2, an open-source LLM that outperforms its predecessors in comprehensive evaluations across 6 dimensions and 30 benchmarks, long-context modeling, and open-ended subjective evaluations through innovative pre-training and optimization techniques. The pre-training process of InternLM2 is meticulously detailed, highlighting the preparation of diverse data types including text, code, and long-context data. InternLM2 efficiently captures long-term dependencies, initially trained on 4k tokens before advancing to 32k tokens in pre-training and fine-tuning stages, exhibiting remarkable performance on the 200k ``Needle-in-a-Haystack"" test. InternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel Conditional Online Reinforcement Learning from Human Feedback (COOL RLHF) strategy that addresses conflicting human preferences and reward hacking. By releasing InternLM2 models in different training stages and model sizes, we provide the community with insights into the model's evolution.",https://arxiv.org/abs/2403.17297,InternLM2 Technical Report
InternLM2-20B,Language,China,Shanghai AI Lab,2024-01-12,Industry,Chat,Confident,1024.0,Open weights (restricted use),Unspecified unreleased,Unreleased,20000000000.0,2600000000000.0,793.5,3.12e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Training cost,1,2000000,197336.5951439333,11628103.68,1.8436089743589742e-08,409600.0,156586588.24671108,325017600.0,"The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has sparked discussions on the advent of Artificial General Intelligence (AGI). However, replicating such advancements in open-source models has been challenging. This paper introduces InternLM2, an open-source LLM that outperforms its predecessors in comprehensive evaluations across 6 dimensions and 30 benchmarks, long-context modeling, and open-ended subjective evaluations through innovative pre-training and optimization techniques. The pre-training process of InternLM2 is meticulously detailed, highlighting the preparation of diverse data types including text, code, and long-context data. InternLM2 efficiently captures long-term dependencies, initially trained on 4k tokens before advancing to 32k tokens in pre-training and fine-tuning stages, exhibiting remarkable performance on the 200k ``Needle-in-a-Haystack"" test. InternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel Conditional Online Reinforcement Learning from Human Feedback (COOL RLHF) strategy that addresses conflicting human preferences and reward hacking. By releasing InternLM2 models in different training stages and model sizes, we provide the community with insights into the model's evolution.",https://arxiv.org/abs/2403.17297,InternLM2 Technical Report
InternLM2-20B,Language,China,Shanghai AI Lab,2024-01-12,Industry,Language modeling/generation,Confident,1024.0,Open weights (restricted use),Unspecified unreleased,Unreleased,20000000000.0,2600000000000.0,793.5,3.12e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Training cost,1,2000000,197336.5951439333,11628103.68,1.8436089743589742e-08,409600.0,156586588.24671108,325017600.0,"The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has sparked discussions on the advent of Artificial General Intelligence (AGI). However, replicating such advancements in open-source models has been challenging. This paper introduces InternLM2, an open-source LLM that outperforms its predecessors in comprehensive evaluations across 6 dimensions and 30 benchmarks, long-context modeling, and open-ended subjective evaluations through innovative pre-training and optimization techniques. The pre-training process of InternLM2 is meticulously detailed, highlighting the preparation of diverse data types including text, code, and long-context data. InternLM2 efficiently captures long-term dependencies, initially trained on 4k tokens before advancing to 32k tokens in pre-training and fine-tuning stages, exhibiting remarkable performance on the 200k ``Needle-in-a-Haystack"" test. InternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel Conditional Online Reinforcement Learning from Human Feedback (COOL RLHF) strategy that addresses conflicting human preferences and reward hacking. By releasing InternLM2 models in different training stages and model sizes, we provide the community with insights into the model's evolution.",https://arxiv.org/abs/2403.17297,InternLM2 Technical Report
InternLM2-20B,Language,China,Shanghai AI Lab,2024-01-12,Industry,Question answering,Confident,1024.0,Open weights (restricted use),Unspecified unreleased,Unreleased,20000000000.0,2600000000000.0,793.5,3.12e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Training cost,1,2000000,197336.5951439333,11628103.68,1.8436089743589742e-08,409600.0,156586588.24671108,325017600.0,"The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has sparked discussions on the advent of Artificial General Intelligence (AGI). However, replicating such advancements in open-source models has been challenging. This paper introduces InternLM2, an open-source LLM that outperforms its predecessors in comprehensive evaluations across 6 dimensions and 30 benchmarks, long-context modeling, and open-ended subjective evaluations through innovative pre-training and optimization techniques. The pre-training process of InternLM2 is meticulously detailed, highlighting the preparation of diverse data types including text, code, and long-context data. InternLM2 efficiently captures long-term dependencies, initially trained on 4k tokens before advancing to 32k tokens in pre-training and fine-tuning stages, exhibiting remarkable performance on the 200k ``Needle-in-a-Haystack"" test. InternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel Conditional Online Reinforcement Learning from Human Feedback (COOL RLHF) strategy that addresses conflicting human preferences and reward hacking. By releasing InternLM2 models in different training stages and model sizes, we provide the community with insights into the model's evolution.",https://arxiv.org/abs/2403.17297,InternLM2 Technical Report
InternLM2-20B,Language,Hong Kong,Chinese University of Hong Kong (CUHK),2024-01-12,Academia,Chat,Confident,1024.0,Open weights (restricted use),Unspecified unreleased,Unreleased,20000000000.0,2600000000000.0,793.5,3.12e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Training cost,1,2000000,197336.5951439333,11628103.68,1.8436089743589742e-08,409600.0,156586588.24671108,325017600.0,"The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has sparked discussions on the advent of Artificial General Intelligence (AGI). However, replicating such advancements in open-source models has been challenging. This paper introduces InternLM2, an open-source LLM that outperforms its predecessors in comprehensive evaluations across 6 dimensions and 30 benchmarks, long-context modeling, and open-ended subjective evaluations through innovative pre-training and optimization techniques. The pre-training process of InternLM2 is meticulously detailed, highlighting the preparation of diverse data types including text, code, and long-context data. InternLM2 efficiently captures long-term dependencies, initially trained on 4k tokens before advancing to 32k tokens in pre-training and fine-tuning stages, exhibiting remarkable performance on the 200k ``Needle-in-a-Haystack"" test. InternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel Conditional Online Reinforcement Learning from Human Feedback (COOL RLHF) strategy that addresses conflicting human preferences and reward hacking. By releasing InternLM2 models in different training stages and model sizes, we provide the community with insights into the model's evolution.",https://arxiv.org/abs/2403.17297,InternLM2 Technical Report
InternLM2-20B,Language,Hong Kong,Chinese University of Hong Kong (CUHK),2024-01-12,Academia,Language modeling/generation,Confident,1024.0,Open weights (restricted use),Unspecified unreleased,Unreleased,20000000000.0,2600000000000.0,793.5,3.12e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Training cost,1,2000000,197336.5951439333,11628103.68,1.8436089743589742e-08,409600.0,156586588.24671108,325017600.0,"The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has sparked discussions on the advent of Artificial General Intelligence (AGI). However, replicating such advancements in open-source models has been challenging. This paper introduces InternLM2, an open-source LLM that outperforms its predecessors in comprehensive evaluations across 6 dimensions and 30 benchmarks, long-context modeling, and open-ended subjective evaluations through innovative pre-training and optimization techniques. The pre-training process of InternLM2 is meticulously detailed, highlighting the preparation of diverse data types including text, code, and long-context data. InternLM2 efficiently captures long-term dependencies, initially trained on 4k tokens before advancing to 32k tokens in pre-training and fine-tuning stages, exhibiting remarkable performance on the 200k ``Needle-in-a-Haystack"" test. InternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel Conditional Online Reinforcement Learning from Human Feedback (COOL RLHF) strategy that addresses conflicting human preferences and reward hacking. By releasing InternLM2 models in different training stages and model sizes, we provide the community with insights into the model's evolution.",https://arxiv.org/abs/2403.17297,InternLM2 Technical Report
InternLM2-20B,Language,Hong Kong,Chinese University of Hong Kong (CUHK),2024-01-12,Academia,Question answering,Confident,1024.0,Open weights (restricted use),Unspecified unreleased,Unreleased,20000000000.0,2600000000000.0,793.5,3.12e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Training cost,1,2000000,197336.5951439333,11628103.68,1.8436089743589742e-08,409600.0,156586588.24671108,325017600.0,"The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has sparked discussions on the advent of Artificial General Intelligence (AGI). However, replicating such advancements in open-source models has been challenging. This paper introduces InternLM2, an open-source LLM that outperforms its predecessors in comprehensive evaluations across 6 dimensions and 30 benchmarks, long-context modeling, and open-ended subjective evaluations through innovative pre-training and optimization techniques. The pre-training process of InternLM2 is meticulously detailed, highlighting the preparation of diverse data types including text, code, and long-context data. InternLM2 efficiently captures long-term dependencies, initially trained on 4k tokens before advancing to 32k tokens in pre-training and fine-tuning stages, exhibiting remarkable performance on the 200k ``Needle-in-a-Haystack"" test. InternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel Conditional Online Reinforcement Learning from Human Feedback (COOL RLHF) strategy that addresses conflicting human preferences and reward hacking. By releasing InternLM2 models in different training stages and model sizes, we provide the community with insights into the model's evolution.",https://arxiv.org/abs/2403.17297,InternLM2 Technical Report
InternLM2-20B,Language,Hong Kong,Chinese University of Hong Kong (CUHK),2024-01-12,Industry,Chat,Confident,1024.0,Open weights (restricted use),Unspecified unreleased,Unreleased,20000000000.0,2600000000000.0,793.5,3.12e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Training cost,1,2000000,197336.5951439333,11628103.68,1.8436089743589742e-08,409600.0,156586588.24671108,325017600.0,"The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has sparked discussions on the advent of Artificial General Intelligence (AGI). However, replicating such advancements in open-source models has been challenging. This paper introduces InternLM2, an open-source LLM that outperforms its predecessors in comprehensive evaluations across 6 dimensions and 30 benchmarks, long-context modeling, and open-ended subjective evaluations through innovative pre-training and optimization techniques. The pre-training process of InternLM2 is meticulously detailed, highlighting the preparation of diverse data types including text, code, and long-context data. InternLM2 efficiently captures long-term dependencies, initially trained on 4k tokens before advancing to 32k tokens in pre-training and fine-tuning stages, exhibiting remarkable performance on the 200k ``Needle-in-a-Haystack"" test. InternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel Conditional Online Reinforcement Learning from Human Feedback (COOL RLHF) strategy that addresses conflicting human preferences and reward hacking. By releasing InternLM2 models in different training stages and model sizes, we provide the community with insights into the model's evolution.",https://arxiv.org/abs/2403.17297,InternLM2 Technical Report
InternLM2-20B,Language,Hong Kong,Chinese University of Hong Kong (CUHK),2024-01-12,Industry,Language modeling/generation,Confident,1024.0,Open weights (restricted use),Unspecified unreleased,Unreleased,20000000000.0,2600000000000.0,793.5,3.12e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Training cost,1,2000000,197336.5951439333,11628103.68,1.8436089743589742e-08,409600.0,156586588.24671108,325017600.0,"The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has sparked discussions on the advent of Artificial General Intelligence (AGI). However, replicating such advancements in open-source models has been challenging. This paper introduces InternLM2, an open-source LLM that outperforms its predecessors in comprehensive evaluations across 6 dimensions and 30 benchmarks, long-context modeling, and open-ended subjective evaluations through innovative pre-training and optimization techniques. The pre-training process of InternLM2 is meticulously detailed, highlighting the preparation of diverse data types including text, code, and long-context data. InternLM2 efficiently captures long-term dependencies, initially trained on 4k tokens before advancing to 32k tokens in pre-training and fine-tuning stages, exhibiting remarkable performance on the 200k ``Needle-in-a-Haystack"" test. InternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel Conditional Online Reinforcement Learning from Human Feedback (COOL RLHF) strategy that addresses conflicting human preferences and reward hacking. By releasing InternLM2 models in different training stages and model sizes, we provide the community with insights into the model's evolution.",https://arxiv.org/abs/2403.17297,InternLM2 Technical Report
InternLM2-20B,Language,Hong Kong,Chinese University of Hong Kong (CUHK),2024-01-12,Industry,Question answering,Confident,1024.0,Open weights (restricted use),Unspecified unreleased,Unreleased,20000000000.0,2600000000000.0,793.5,3.12e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Training cost,1,2000000,197336.5951439333,11628103.68,1.8436089743589742e-08,409600.0,156586588.24671108,325017600.0,"The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has sparked discussions on the advent of Artificial General Intelligence (AGI). However, replicating such advancements in open-source models has been challenging. This paper introduces InternLM2, an open-source LLM that outperforms its predecessors in comprehensive evaluations across 6 dimensions and 30 benchmarks, long-context modeling, and open-ended subjective evaluations through innovative pre-training and optimization techniques. The pre-training process of InternLM2 is meticulously detailed, highlighting the preparation of diverse data types including text, code, and long-context data. InternLM2 efficiently captures long-term dependencies, initially trained on 4k tokens before advancing to 32k tokens in pre-training and fine-tuning stages, exhibiting remarkable performance on the 200k ``Needle-in-a-Haystack"" test. InternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel Conditional Online Reinforcement Learning from Human Feedback (COOL RLHF) strategy that addresses conflicting human preferences and reward hacking. By releasing InternLM2 models in different training stages and model sizes, we provide the community with insights into the model's evolution.",https://arxiv.org/abs/2403.17297,InternLM2 Technical Report
InternLM2-20B,Language,Hong Kong,Fudan University,2024-01-12,Academia,Chat,Confident,1024.0,Open weights (restricted use),Unspecified unreleased,Unreleased,20000000000.0,2600000000000.0,793.5,3.12e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Training cost,1,2000000,197336.5951439333,11628103.68,1.8436089743589742e-08,409600.0,156586588.24671108,325017600.0,"The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has sparked discussions on the advent of Artificial General Intelligence (AGI). However, replicating such advancements in open-source models has been challenging. This paper introduces InternLM2, an open-source LLM that outperforms its predecessors in comprehensive evaluations across 6 dimensions and 30 benchmarks, long-context modeling, and open-ended subjective evaluations through innovative pre-training and optimization techniques. The pre-training process of InternLM2 is meticulously detailed, highlighting the preparation of diverse data types including text, code, and long-context data. InternLM2 efficiently captures long-term dependencies, initially trained on 4k tokens before advancing to 32k tokens in pre-training and fine-tuning stages, exhibiting remarkable performance on the 200k ``Needle-in-a-Haystack"" test. InternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel Conditional Online Reinforcement Learning from Human Feedback (COOL RLHF) strategy that addresses conflicting human preferences and reward hacking. By releasing InternLM2 models in different training stages and model sizes, we provide the community with insights into the model's evolution.",https://arxiv.org/abs/2403.17297,InternLM2 Technical Report
InternLM2-20B,Language,Hong Kong,Fudan University,2024-01-12,Academia,Language modeling/generation,Confident,1024.0,Open weights (restricted use),Unspecified unreleased,Unreleased,20000000000.0,2600000000000.0,793.5,3.12e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Training cost,1,2000000,197336.5951439333,11628103.68,1.8436089743589742e-08,409600.0,156586588.24671108,325017600.0,"The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has sparked discussions on the advent of Artificial General Intelligence (AGI). However, replicating such advancements in open-source models has been challenging. This paper introduces InternLM2, an open-source LLM that outperforms its predecessors in comprehensive evaluations across 6 dimensions and 30 benchmarks, long-context modeling, and open-ended subjective evaluations through innovative pre-training and optimization techniques. The pre-training process of InternLM2 is meticulously detailed, highlighting the preparation of diverse data types including text, code, and long-context data. InternLM2 efficiently captures long-term dependencies, initially trained on 4k tokens before advancing to 32k tokens in pre-training and fine-tuning stages, exhibiting remarkable performance on the 200k ``Needle-in-a-Haystack"" test. InternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel Conditional Online Reinforcement Learning from Human Feedback (COOL RLHF) strategy that addresses conflicting human preferences and reward hacking. By releasing InternLM2 models in different training stages and model sizes, we provide the community with insights into the model's evolution.",https://arxiv.org/abs/2403.17297,InternLM2 Technical Report
InternLM2-20B,Language,Hong Kong,Fudan University,2024-01-12,Academia,Question answering,Confident,1024.0,Open weights (restricted use),Unspecified unreleased,Unreleased,20000000000.0,2600000000000.0,793.5,3.12e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Training cost,1,2000000,197336.5951439333,11628103.68,1.8436089743589742e-08,409600.0,156586588.24671108,325017600.0,"The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has sparked discussions on the advent of Artificial General Intelligence (AGI). However, replicating such advancements in open-source models has been challenging. This paper introduces InternLM2, an open-source LLM that outperforms its predecessors in comprehensive evaluations across 6 dimensions and 30 benchmarks, long-context modeling, and open-ended subjective evaluations through innovative pre-training and optimization techniques. The pre-training process of InternLM2 is meticulously detailed, highlighting the preparation of diverse data types including text, code, and long-context data. InternLM2 efficiently captures long-term dependencies, initially trained on 4k tokens before advancing to 32k tokens in pre-training and fine-tuning stages, exhibiting remarkable performance on the 200k ``Needle-in-a-Haystack"" test. InternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel Conditional Online Reinforcement Learning from Human Feedback (COOL RLHF) strategy that addresses conflicting human preferences and reward hacking. By releasing InternLM2 models in different training stages and model sizes, we provide the community with insights into the model's evolution.",https://arxiv.org/abs/2403.17297,InternLM2 Technical Report
InternLM2-20B,Language,Hong Kong,Fudan University,2024-01-12,Industry,Chat,Confident,1024.0,Open weights (restricted use),Unspecified unreleased,Unreleased,20000000000.0,2600000000000.0,793.5,3.12e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Training cost,1,2000000,197336.5951439333,11628103.68,1.8436089743589742e-08,409600.0,156586588.24671108,325017600.0,"The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has sparked discussions on the advent of Artificial General Intelligence (AGI). However, replicating such advancements in open-source models has been challenging. This paper introduces InternLM2, an open-source LLM that outperforms its predecessors in comprehensive evaluations across 6 dimensions and 30 benchmarks, long-context modeling, and open-ended subjective evaluations through innovative pre-training and optimization techniques. The pre-training process of InternLM2 is meticulously detailed, highlighting the preparation of diverse data types including text, code, and long-context data. InternLM2 efficiently captures long-term dependencies, initially trained on 4k tokens before advancing to 32k tokens in pre-training and fine-tuning stages, exhibiting remarkable performance on the 200k ``Needle-in-a-Haystack"" test. InternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel Conditional Online Reinforcement Learning from Human Feedback (COOL RLHF) strategy that addresses conflicting human preferences and reward hacking. By releasing InternLM2 models in different training stages and model sizes, we provide the community with insights into the model's evolution.",https://arxiv.org/abs/2403.17297,InternLM2 Technical Report
InternLM2-20B,Language,Hong Kong,Fudan University,2024-01-12,Industry,Language modeling/generation,Confident,1024.0,Open weights (restricted use),Unspecified unreleased,Unreleased,20000000000.0,2600000000000.0,793.5,3.12e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Training cost,1,2000000,197336.5951439333,11628103.68,1.8436089743589742e-08,409600.0,156586588.24671108,325017600.0,"The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has sparked discussions on the advent of Artificial General Intelligence (AGI). However, replicating such advancements in open-source models has been challenging. This paper introduces InternLM2, an open-source LLM that outperforms its predecessors in comprehensive evaluations across 6 dimensions and 30 benchmarks, long-context modeling, and open-ended subjective evaluations through innovative pre-training and optimization techniques. The pre-training process of InternLM2 is meticulously detailed, highlighting the preparation of diverse data types including text, code, and long-context data. InternLM2 efficiently captures long-term dependencies, initially trained on 4k tokens before advancing to 32k tokens in pre-training and fine-tuning stages, exhibiting remarkable performance on the 200k ``Needle-in-a-Haystack"" test. InternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel Conditional Online Reinforcement Learning from Human Feedback (COOL RLHF) strategy that addresses conflicting human preferences and reward hacking. By releasing InternLM2 models in different training stages and model sizes, we provide the community with insights into the model's evolution.",https://arxiv.org/abs/2403.17297,InternLM2 Technical Report
InternLM2-20B,Language,Hong Kong,Fudan University,2024-01-12,Industry,Question answering,Confident,1024.0,Open weights (restricted use),Unspecified unreleased,Unreleased,20000000000.0,2600000000000.0,793.5,3.12e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Training cost,1,2000000,197336.5951439333,11628103.68,1.8436089743589742e-08,409600.0,156586588.24671108,325017600.0,"The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has sparked discussions on the advent of Artificial General Intelligence (AGI). However, replicating such advancements in open-source models has been challenging. This paper introduces InternLM2, an open-source LLM that outperforms its predecessors in comprehensive evaluations across 6 dimensions and 30 benchmarks, long-context modeling, and open-ended subjective evaluations through innovative pre-training and optimization techniques. The pre-training process of InternLM2 is meticulously detailed, highlighting the preparation of diverse data types including text, code, and long-context data. InternLM2 efficiently captures long-term dependencies, initially trained on 4k tokens before advancing to 32k tokens in pre-training and fine-tuning stages, exhibiting remarkable performance on the 200k ``Needle-in-a-Haystack"" test. InternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel Conditional Online Reinforcement Learning from Human Feedback (COOL RLHF) strategy that addresses conflicting human preferences and reward hacking. By releasing InternLM2 models in different training stages and model sizes, we provide the community with insights into the model's evolution.",https://arxiv.org/abs/2403.17297,InternLM2 Technical Report
InternLM2-20B,Language,Hong Kong,SenseTime,2024-01-12,Academia,Chat,Confident,1024.0,Open weights (restricted use),Unspecified unreleased,Unreleased,20000000000.0,2600000000000.0,793.5,3.12e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Training cost,1,2000000,197336.5951439333,11628103.68,1.8436089743589742e-08,409600.0,156586588.24671108,325017600.0,"The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has sparked discussions on the advent of Artificial General Intelligence (AGI). However, replicating such advancements in open-source models has been challenging. This paper introduces InternLM2, an open-source LLM that outperforms its predecessors in comprehensive evaluations across 6 dimensions and 30 benchmarks, long-context modeling, and open-ended subjective evaluations through innovative pre-training and optimization techniques. The pre-training process of InternLM2 is meticulously detailed, highlighting the preparation of diverse data types including text, code, and long-context data. InternLM2 efficiently captures long-term dependencies, initially trained on 4k tokens before advancing to 32k tokens in pre-training and fine-tuning stages, exhibiting remarkable performance on the 200k ``Needle-in-a-Haystack"" test. InternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel Conditional Online Reinforcement Learning from Human Feedback (COOL RLHF) strategy that addresses conflicting human preferences and reward hacking. By releasing InternLM2 models in different training stages and model sizes, we provide the community with insights into the model's evolution.",https://arxiv.org/abs/2403.17297,InternLM2 Technical Report
InternLM2-20B,Language,Hong Kong,SenseTime,2024-01-12,Academia,Language modeling/generation,Confident,1024.0,Open weights (restricted use),Unspecified unreleased,Unreleased,20000000000.0,2600000000000.0,793.5,3.12e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Training cost,1,2000000,197336.5951439333,11628103.68,1.8436089743589742e-08,409600.0,156586588.24671108,325017600.0,"The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has sparked discussions on the advent of Artificial General Intelligence (AGI). However, replicating such advancements in open-source models has been challenging. This paper introduces InternLM2, an open-source LLM that outperforms its predecessors in comprehensive evaluations across 6 dimensions and 30 benchmarks, long-context modeling, and open-ended subjective evaluations through innovative pre-training and optimization techniques. The pre-training process of InternLM2 is meticulously detailed, highlighting the preparation of diverse data types including text, code, and long-context data. InternLM2 efficiently captures long-term dependencies, initially trained on 4k tokens before advancing to 32k tokens in pre-training and fine-tuning stages, exhibiting remarkable performance on the 200k ``Needle-in-a-Haystack"" test. InternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel Conditional Online Reinforcement Learning from Human Feedback (COOL RLHF) strategy that addresses conflicting human preferences and reward hacking. By releasing InternLM2 models in different training stages and model sizes, we provide the community with insights into the model's evolution.",https://arxiv.org/abs/2403.17297,InternLM2 Technical Report
InternLM2-20B,Language,Hong Kong,SenseTime,2024-01-12,Academia,Question answering,Confident,1024.0,Open weights (restricted use),Unspecified unreleased,Unreleased,20000000000.0,2600000000000.0,793.5,3.12e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Training cost,1,2000000,197336.5951439333,11628103.68,1.8436089743589742e-08,409600.0,156586588.24671108,325017600.0,"The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has sparked discussions on the advent of Artificial General Intelligence (AGI). However, replicating such advancements in open-source models has been challenging. This paper introduces InternLM2, an open-source LLM that outperforms its predecessors in comprehensive evaluations across 6 dimensions and 30 benchmarks, long-context modeling, and open-ended subjective evaluations through innovative pre-training and optimization techniques. The pre-training process of InternLM2 is meticulously detailed, highlighting the preparation of diverse data types including text, code, and long-context data. InternLM2 efficiently captures long-term dependencies, initially trained on 4k tokens before advancing to 32k tokens in pre-training and fine-tuning stages, exhibiting remarkable performance on the 200k ``Needle-in-a-Haystack"" test. InternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel Conditional Online Reinforcement Learning from Human Feedback (COOL RLHF) strategy that addresses conflicting human preferences and reward hacking. By releasing InternLM2 models in different training stages and model sizes, we provide the community with insights into the model's evolution.",https://arxiv.org/abs/2403.17297,InternLM2 Technical Report
InternLM2-20B,Language,Hong Kong,SenseTime,2024-01-12,Industry,Chat,Confident,1024.0,Open weights (restricted use),Unspecified unreleased,Unreleased,20000000000.0,2600000000000.0,793.5,3.12e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Training cost,1,2000000,197336.5951439333,11628103.68,1.8436089743589742e-08,409600.0,156586588.24671108,325017600.0,"The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has sparked discussions on the advent of Artificial General Intelligence (AGI). However, replicating such advancements in open-source models has been challenging. This paper introduces InternLM2, an open-source LLM that outperforms its predecessors in comprehensive evaluations across 6 dimensions and 30 benchmarks, long-context modeling, and open-ended subjective evaluations through innovative pre-training and optimization techniques. The pre-training process of InternLM2 is meticulously detailed, highlighting the preparation of diverse data types including text, code, and long-context data. InternLM2 efficiently captures long-term dependencies, initially trained on 4k tokens before advancing to 32k tokens in pre-training and fine-tuning stages, exhibiting remarkable performance on the 200k ``Needle-in-a-Haystack"" test. InternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel Conditional Online Reinforcement Learning from Human Feedback (COOL RLHF) strategy that addresses conflicting human preferences and reward hacking. By releasing InternLM2 models in different training stages and model sizes, we provide the community with insights into the model's evolution.",https://arxiv.org/abs/2403.17297,InternLM2 Technical Report
InternLM2-20B,Language,Hong Kong,SenseTime,2024-01-12,Industry,Language modeling/generation,Confident,1024.0,Open weights (restricted use),Unspecified unreleased,Unreleased,20000000000.0,2600000000000.0,793.5,3.12e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Training cost,1,2000000,197336.5951439333,11628103.68,1.8436089743589742e-08,409600.0,156586588.24671108,325017600.0,"The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has sparked discussions on the advent of Artificial General Intelligence (AGI). However, replicating such advancements in open-source models has been challenging. This paper introduces InternLM2, an open-source LLM that outperforms its predecessors in comprehensive evaluations across 6 dimensions and 30 benchmarks, long-context modeling, and open-ended subjective evaluations through innovative pre-training and optimization techniques. The pre-training process of InternLM2 is meticulously detailed, highlighting the preparation of diverse data types including text, code, and long-context data. InternLM2 efficiently captures long-term dependencies, initially trained on 4k tokens before advancing to 32k tokens in pre-training and fine-tuning stages, exhibiting remarkable performance on the 200k ``Needle-in-a-Haystack"" test. InternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel Conditional Online Reinforcement Learning from Human Feedback (COOL RLHF) strategy that addresses conflicting human preferences and reward hacking. By releasing InternLM2 models in different training stages and model sizes, we provide the community with insights into the model's evolution.",https://arxiv.org/abs/2403.17297,InternLM2 Technical Report
InternLM2-20B,Language,Hong Kong,SenseTime,2024-01-12,Industry,Question answering,Confident,1024.0,Open weights (restricted use),Unspecified unreleased,Unreleased,20000000000.0,2600000000000.0,793.5,3.12e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Training cost,1,2000000,197336.5951439333,11628103.68,1.8436089743589742e-08,409600.0,156586588.24671108,325017600.0,"The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has sparked discussions on the advent of Artificial General Intelligence (AGI). However, replicating such advancements in open-source models has been challenging. This paper introduces InternLM2, an open-source LLM that outperforms its predecessors in comprehensive evaluations across 6 dimensions and 30 benchmarks, long-context modeling, and open-ended subjective evaluations through innovative pre-training and optimization techniques. The pre-training process of InternLM2 is meticulously detailed, highlighting the preparation of diverse data types including text, code, and long-context data. InternLM2 efficiently captures long-term dependencies, initially trained on 4k tokens before advancing to 32k tokens in pre-training and fine-tuning stages, exhibiting remarkable performance on the 200k ``Needle-in-a-Haystack"" test. InternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel Conditional Online Reinforcement Learning from Human Feedback (COOL RLHF) strategy that addresses conflicting human preferences and reward hacking. By releasing InternLM2 models in different training stages and model sizes, we provide the community with insights into the model's evolution.",https://arxiv.org/abs/2403.17297,InternLM2 Technical Report
InternLM2-20B,Language,Hong Kong,Shanghai AI Lab,2024-01-12,Academia,Chat,Confident,1024.0,Open weights (restricted use),Unspecified unreleased,Unreleased,20000000000.0,2600000000000.0,793.5,3.12e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Training cost,1,2000000,197336.5951439333,11628103.68,1.8436089743589742e-08,409600.0,156586588.24671108,325017600.0,"The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has sparked discussions on the advent of Artificial General Intelligence (AGI). However, replicating such advancements in open-source models has been challenging. This paper introduces InternLM2, an open-source LLM that outperforms its predecessors in comprehensive evaluations across 6 dimensions and 30 benchmarks, long-context modeling, and open-ended subjective evaluations through innovative pre-training and optimization techniques. The pre-training process of InternLM2 is meticulously detailed, highlighting the preparation of diverse data types including text, code, and long-context data. InternLM2 efficiently captures long-term dependencies, initially trained on 4k tokens before advancing to 32k tokens in pre-training and fine-tuning stages, exhibiting remarkable performance on the 200k ``Needle-in-a-Haystack"" test. InternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel Conditional Online Reinforcement Learning from Human Feedback (COOL RLHF) strategy that addresses conflicting human preferences and reward hacking. By releasing InternLM2 models in different training stages and model sizes, we provide the community with insights into the model's evolution.",https://arxiv.org/abs/2403.17297,InternLM2 Technical Report
InternLM2-20B,Language,Hong Kong,Shanghai AI Lab,2024-01-12,Academia,Language modeling/generation,Confident,1024.0,Open weights (restricted use),Unspecified unreleased,Unreleased,20000000000.0,2600000000000.0,793.5,3.12e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Training cost,1,2000000,197336.5951439333,11628103.68,1.8436089743589742e-08,409600.0,156586588.24671108,325017600.0,"The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has sparked discussions on the advent of Artificial General Intelligence (AGI). However, replicating such advancements in open-source models has been challenging. This paper introduces InternLM2, an open-source LLM that outperforms its predecessors in comprehensive evaluations across 6 dimensions and 30 benchmarks, long-context modeling, and open-ended subjective evaluations through innovative pre-training and optimization techniques. The pre-training process of InternLM2 is meticulously detailed, highlighting the preparation of diverse data types including text, code, and long-context data. InternLM2 efficiently captures long-term dependencies, initially trained on 4k tokens before advancing to 32k tokens in pre-training and fine-tuning stages, exhibiting remarkable performance on the 200k ``Needle-in-a-Haystack"" test. InternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel Conditional Online Reinforcement Learning from Human Feedback (COOL RLHF) strategy that addresses conflicting human preferences and reward hacking. By releasing InternLM2 models in different training stages and model sizes, we provide the community with insights into the model's evolution.",https://arxiv.org/abs/2403.17297,InternLM2 Technical Report
InternLM2-20B,Language,Hong Kong,Shanghai AI Lab,2024-01-12,Academia,Question answering,Confident,1024.0,Open weights (restricted use),Unspecified unreleased,Unreleased,20000000000.0,2600000000000.0,793.5,3.12e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Training cost,1,2000000,197336.5951439333,11628103.68,1.8436089743589742e-08,409600.0,156586588.24671108,325017600.0,"The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has sparked discussions on the advent of Artificial General Intelligence (AGI). However, replicating such advancements in open-source models has been challenging. This paper introduces InternLM2, an open-source LLM that outperforms its predecessors in comprehensive evaluations across 6 dimensions and 30 benchmarks, long-context modeling, and open-ended subjective evaluations through innovative pre-training and optimization techniques. The pre-training process of InternLM2 is meticulously detailed, highlighting the preparation of diverse data types including text, code, and long-context data. InternLM2 efficiently captures long-term dependencies, initially trained on 4k tokens before advancing to 32k tokens in pre-training and fine-tuning stages, exhibiting remarkable performance on the 200k ``Needle-in-a-Haystack"" test. InternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel Conditional Online Reinforcement Learning from Human Feedback (COOL RLHF) strategy that addresses conflicting human preferences and reward hacking. By releasing InternLM2 models in different training stages and model sizes, we provide the community with insights into the model's evolution.",https://arxiv.org/abs/2403.17297,InternLM2 Technical Report
InternLM2-20B,Language,Hong Kong,Shanghai AI Lab,2024-01-12,Industry,Chat,Confident,1024.0,Open weights (restricted use),Unspecified unreleased,Unreleased,20000000000.0,2600000000000.0,793.5,3.12e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Training cost,1,2000000,197336.5951439333,11628103.68,1.8436089743589742e-08,409600.0,156586588.24671108,325017600.0,"The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has sparked discussions on the advent of Artificial General Intelligence (AGI). However, replicating such advancements in open-source models has been challenging. This paper introduces InternLM2, an open-source LLM that outperforms its predecessors in comprehensive evaluations across 6 dimensions and 30 benchmarks, long-context modeling, and open-ended subjective evaluations through innovative pre-training and optimization techniques. The pre-training process of InternLM2 is meticulously detailed, highlighting the preparation of diverse data types including text, code, and long-context data. InternLM2 efficiently captures long-term dependencies, initially trained on 4k tokens before advancing to 32k tokens in pre-training and fine-tuning stages, exhibiting remarkable performance on the 200k ``Needle-in-a-Haystack"" test. InternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel Conditional Online Reinforcement Learning from Human Feedback (COOL RLHF) strategy that addresses conflicting human preferences and reward hacking. By releasing InternLM2 models in different training stages and model sizes, we provide the community with insights into the model's evolution.",https://arxiv.org/abs/2403.17297,InternLM2 Technical Report
InternLM2-20B,Language,Hong Kong,Shanghai AI Lab,2024-01-12,Industry,Language modeling/generation,Confident,1024.0,Open weights (restricted use),Unspecified unreleased,Unreleased,20000000000.0,2600000000000.0,793.5,3.12e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Training cost,1,2000000,197336.5951439333,11628103.68,1.8436089743589742e-08,409600.0,156586588.24671108,325017600.0,"The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has sparked discussions on the advent of Artificial General Intelligence (AGI). However, replicating such advancements in open-source models has been challenging. This paper introduces InternLM2, an open-source LLM that outperforms its predecessors in comprehensive evaluations across 6 dimensions and 30 benchmarks, long-context modeling, and open-ended subjective evaluations through innovative pre-training and optimization techniques. The pre-training process of InternLM2 is meticulously detailed, highlighting the preparation of diverse data types including text, code, and long-context data. InternLM2 efficiently captures long-term dependencies, initially trained on 4k tokens before advancing to 32k tokens in pre-training and fine-tuning stages, exhibiting remarkable performance on the 200k ``Needle-in-a-Haystack"" test. InternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel Conditional Online Reinforcement Learning from Human Feedback (COOL RLHF) strategy that addresses conflicting human preferences and reward hacking. By releasing InternLM2 models in different training stages and model sizes, we provide the community with insights into the model's evolution.",https://arxiv.org/abs/2403.17297,InternLM2 Technical Report
InternLM2-20B,Language,Hong Kong,Shanghai AI Lab,2024-01-12,Industry,Question answering,Confident,1024.0,Open weights (restricted use),Unspecified unreleased,Unreleased,20000000000.0,2600000000000.0,793.5,3.12e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Training cost,1,2000000,197336.5951439333,11628103.68,1.8436089743589742e-08,409600.0,156586588.24671108,325017600.0,"The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has sparked discussions on the advent of Artificial General Intelligence (AGI). However, replicating such advancements in open-source models has been challenging. This paper introduces InternLM2, an open-source LLM that outperforms its predecessors in comprehensive evaluations across 6 dimensions and 30 benchmarks, long-context modeling, and open-ended subjective evaluations through innovative pre-training and optimization techniques. The pre-training process of InternLM2 is meticulously detailed, highlighting the preparation of diverse data types including text, code, and long-context data. InternLM2 efficiently captures long-term dependencies, initially trained on 4k tokens before advancing to 32k tokens in pre-training and fine-tuning stages, exhibiting remarkable performance on the 200k ``Needle-in-a-Haystack"" test. InternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel Conditional Online Reinforcement Learning from Human Feedback (COOL RLHF) strategy that addresses conflicting human preferences and reward hacking. By releasing InternLM2 models in different training stages and model sizes, we provide the community with insights into the model's evolution.",https://arxiv.org/abs/2403.17297,InternLM2 Technical Report
Konan LLM 41B,Language,South Korea,Konan Technology,2023-12-15,Industry,Language modeling/generation,Likely,1000.0,Hosted access (no API),Unspecified unreleased,Unreleased,41000000000.0,700000000000.0,1728.0,1.722e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Training cost,1,2048,3155569.9410964646,10105690.0,3.3401277584204415e-08,400000.0,5452824858.214691,691200000.0,"Konan LLM is a Large Language Model developed in-house by Konan Technology. Konan Technology optimized for super-large AI training, it leverages high-quality, large-scale data and over 20 years of expertise in natural language processing.
Konan LLM supports all corporate documentation and creative tasks,
leading the way in workplace innovation.","https://en.konantech.com/en/llm/konanllm
https://techfinch.kr/ai/konan-technology-unveils-konan-llm--its-own-ai-language-model
https://www.dbpia.co.kr/journal/articleDetail?nodeId=NODE11610127 ",Konan LLM: A Korean Large Language Model
Konan LLM 41B,Vision,South Korea,Konan Technology,2023-12-15,Industry,Language modeling/generation,Likely,1000.0,Hosted access (no API),Unspecified unreleased,Unreleased,41000000000.0,700000000000.0,1728.0,1.722e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Training cost,1,2048,3155569.9410964646,10105690.0,3.3401277584204415e-08,400000.0,5452824858.214691,691200000.0,"Konan LLM is a Large Language Model developed in-house by Konan Technology. Konan Technology optimized for super-large AI training, it leverages high-quality, large-scale data and over 20 years of expertise in natural language processing.
Konan LLM supports all corporate documentation and creative tasks,
leading the way in workplace innovation.","https://en.konantech.com/en/llm/konanllm
https://techfinch.kr/ai/konan-technology-unveils-konan-llm--its-own-ai-language-model
https://www.dbpia.co.kr/journal/articleDetail?nodeId=NODE11610127 ",Konan LLM: A Korean Large Language Model
KwaiYii 175B,Language,China,Kuaishou Technology,2024-02-14,Industry,Language modeling/generation,Confident,128.0,Not-defined,Not-defined,Unreleased,175000000000.0,3000000000000.0,1282.4326530612243,1.3e+24,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Training cost,1,4000000,788306.6236983632,1293528.32,4.424384615384615e-09,51200.0,1010950154.8552281,65660551.83673468,"In June 2024, quick-handed NLP experts reported on the “KwaiYii” model at the Global Artificial Intelligence Technology Conference. The model was developed in early 2023, and the 175B model was released at the end of February 2024. Many capabilities are close to the latest version of GPT-4. Introduced eight key technological innovations and landing practices in scenes such as AI Xiaofu, including solutions to various challenges, which will continue to be iterated in the future.",https://blog.csdn.net/kuaishoutech/article/details/140542568,
LLaMA-33B,Language,USA,Meta AI,2023-02-27,Industry,Code generation,Confident,2048.0,Open weights (non-commercial),"CCNet,GitHub,Wikipedia,books,arXiv,Stack Exchange",Unreleased,32500000000.0,1340000000000.0,500.0,2.7300000000001e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Training cost,1,4000000,3155569.9410964646,20696453.12,0.4746,819200.0,1577784970.5482323,409600000.0,"We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.",https://arxiv.org/abs/2302.13971,LLaMA: Open and Efficient Foundation Language Models
LLaMA-33B,Language,USA,Meta AI,2023-02-27,Industry,Language modeling,Confident,2048.0,Open weights (non-commercial),"CCNet,GitHub,Wikipedia,books,arXiv,Stack Exchange",Unreleased,32500000000.0,1340000000000.0,500.0,2.7300000000001e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Training cost,1,4000000,3155569.9410964646,20696453.12,0.4746,819200.0,1577784970.5482323,409600000.0,"We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.",https://arxiv.org/abs/2302.13971,LLaMA: Open and Efficient Foundation Language Models
LLaMA-65B,Language,USA,Meta AI,2023-02-24,Industry,Code generation,Confident,2048.0,Open weights (non-commercial),"CCNet,GitHub,Wikipedia,books,arXiv,Stack Exchange",Unreleased,65200000000.0,1340000000000.0,500.0,5.5e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Historical significance,1,4000000,1814594.7887512865,20696453.12,0.4746,819200.0,907297394.3756433,409600000.0,"We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.",https://arxiv.org/abs/2302.13971,LLaMA: Open and Efficient Foundation Language Models
LLaMA-65B,Language,USA,Meta AI,2023-02-24,Industry,Code generation,Confident,2048.0,Open weights (non-commercial),"CCNet,GitHub,Wikipedia,books,arXiv,Stack Exchange",Unreleased,65200000000.0,1340000000000.0,500.0,5.5e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Highly cited,1,4000000,1814594.7887512865,20696453.12,0.4746,819200.0,907297394.3756433,409600000.0,"We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.",https://arxiv.org/abs/2302.13971,LLaMA: Open and Efficient Foundation Language Models
LLaMA-65B,Language,USA,Meta AI,2023-02-24,Industry,Language modeling,Confident,2048.0,Open weights (non-commercial),"CCNet,GitHub,Wikipedia,books,arXiv,Stack Exchange",Unreleased,65200000000.0,1340000000000.0,500.0,5.5e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Historical significance,1,4000000,1814594.7887512865,20696453.12,0.4746,819200.0,907297394.3756433,409600000.0,"We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.",https://arxiv.org/abs/2302.13971,LLaMA: Open and Efficient Foundation Language Models
LLaMA-65B,Language,USA,Meta AI,2023-02-24,Industry,Language modeling,Confident,2048.0,Open weights (non-commercial),"CCNet,GitHub,Wikipedia,books,arXiv,Stack Exchange",Unreleased,65200000000.0,1340000000000.0,500.0,5.5e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Highly cited,1,4000000,1814594.7887512865,20696453.12,0.4746,819200.0,907297394.3756433,409600000.0,"We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.",https://arxiv.org/abs/2302.13971,LLaMA: Open and Efficient Foundation Language Models
LaMDA,Language,USA,Google,2022-02-10,Industry,Language modeling,Confident,1024.0,Unreleased,Infiniset,Unreleased,137000000000.0,1560000000000.0,1385.0,3.55e+23,1877.1621580193864,6589.05,450.0,123000000000000.0,223459000000000.0,32000000000.0,Google TPU v3,Historical significance,1,256000,1024572.2817390452,6747187.2,0.565,460800.0,1419032610.2085776,638208000.0,"We present LaMDA: Language Models for Dialog Applications. LaMDA is a family of Transformer-based neural language models specialized for dialog, which have up to 137B parameters and are pre-trained on 1.56T words of public dialog data and web text. While model scaling alone can improve quality, it shows less improvements on safety and factual grounding. We demonstrate that fine-tuning with annotated data and enabling the model to consult external knowledge sources can lead to significant improvements towards the two key challenges of safety and factual grounding. The first challenge, safety, involves ensuring that the model's responses are consistent with a set of human values, such as preventing harmful suggestions and unfair bias. We quantify safety using a metric based on an illustrative set of human values, and we find that filtering candidate responses using a LaMDA classifier fine-tuned with a small amount of crowdworker-annotated data offers a promising approach to improving model safety. The second challenge, factual grounding, involves enabling the model to consult external knowledge sources, such as an information retrieval system, a language translator, and a calculator. We quantify factuality using a groundedness metric, and we find that our approach enables the model to generate responses grounded in known sources, rather than responses that merely sound plausible. Finally, we explore the use of LaMDA in the domains of education and content recommendations, and analyze their helpfulness and role consistency.",https://arxiv.org/abs/2201.08239,LaMDA: Language Models for Dialog Applications
LingoWhale-8B,Language,China,DeepLang AI,2023-11-01,Industry,Code generation,Confident,128.0,Open weights (non-commercial),Not-defined,Open (non-commercial),8000000000.0,2500000000000.0,4096.0,2.4e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Significant use,1,4000000,113138.0969599182,1293528.32,2.3965416666666668e-08,51200.0,463413645.14782494,209715200.0,"LingoWhale-8B is the first open-source model in the LingoWhale series introduced by DeepLangAI. It's a bilingual (Chinese-English) large language model.

LingoWhale-8B has been pre-trained on a large volume of high-quality bilingual data and exhibits powerful capabilities as a foundation model. It has achieved leading results on multiple public benchmarks. During its pre-training phase, the model was trained with a context window of 8K, allowing it to comprehend and generate longer sequences.

LingoWhale-8B is fully open for academic research. Users can apply for commercial use by email, and once granted official commercial permission, they can use it for free.",https://github.com/DeepLangAI/LingoWhale-8B/blob/main/README_EN.md,
LingoWhale-8B,Language,China,DeepLang AI,2023-11-01,Industry,Language modeling/generation,Confident,128.0,Open weights (non-commercial),Not-defined,Open (non-commercial),8000000000.0,2500000000000.0,4096.0,2.4e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Significant use,1,4000000,113138.0969599182,1293528.32,2.3965416666666668e-08,51200.0,463413645.14782494,209715200.0,"LingoWhale-8B is the first open-source model in the LingoWhale series introduced by DeepLangAI. It's a bilingual (Chinese-English) large language model.

LingoWhale-8B has been pre-trained on a large volume of high-quality bilingual data and exhibits powerful capabilities as a foundation model. It has achieved leading results on multiple public benchmarks. During its pre-training phase, the model was trained with a context window of 8K, allowing it to comprehend and generate longer sequences.

LingoWhale-8B is fully open for academic research. Users can apply for commercial use by email, and once granted official commercial permission, they can use it for free.",https://github.com/DeepLangAI/LingoWhale-8B/blob/main/README_EN.md,
LingoWhale-8B,Language,China,DeepLang AI,2023-11-01,Industry,Translation,Confident,128.0,Open weights (non-commercial),Not-defined,Open (non-commercial),8000000000.0,2500000000000.0,4096.0,2.4e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Significant use,1,4000000,113138.0969599182,1293528.32,2.3965416666666668e-08,51200.0,463413645.14782494,209715200.0,"LingoWhale-8B is the first open-source model in the LingoWhale series introduced by DeepLangAI. It's a bilingual (Chinese-English) large language model.

LingoWhale-8B has been pre-trained on a large volume of high-quality bilingual data and exhibits powerful capabilities as a foundation model. It has achieved leading results on multiple public benchmarks. During its pre-training phase, the model was trained with a context window of 8K, allowing it to comprehend and generate longer sequences.

LingoWhale-8B is fully open for academic research. Users can apply for commercial use by email, and once granted official commercial permission, they can use it for free.",https://github.com/DeepLangAI/LingoWhale-8B/blob/main/README_EN.md,
Llama 2-13B,Language,USA,Meta AI,2023-07-18,Industry,Language modeling,Confident,1000.0,Open weights (restricted use),Llama 2 dataset,Unreleased,13000000000.0,2000000000000.0,1728.0,1.6e+23,1877.1621580193864,27922.725,550.0,1213583000000000.0,1301500000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Training cost,1,2048,28281.566571321346,27922725.0,7.584893749999999e-08,550000.0,48870547.03524329,950400000.0,"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closedsource models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.","https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/
https://arxiv.org/abs/2307.09288",Llama 2: Open Foundation and Fine-Tuned Chat Models
Llama 2-34B,Language,USA,Meta AI,2023-07-18,Industry,Language modeling,Confident,1000.0,Unreleased,Llama 2 dataset,Unreleased,34000000000.0,2000000000000.0,1728.0,4.08e+23,1877.1621580193864,27922.725,550.0,1213583000000000.0,1301500000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Training cost,1,2048,56450.0592214581,27922725.0,2.974468137254902e-08,550000.0,97545702.33467959,950400000.0,"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned
large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.
Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our
models outperform open-source chat models on most benchmarks we tested, and based on
our human evaluations for helpfulness and safety, may be a suitable substitute for closedsource models. We provide a detailed description of our approach to fine-tuning and safety
improvements of Llama 2-Chat in order to enable the community to build on our work and
contribute to the responsible development of LLMs.","https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/
https://arxiv.org/abs/2307.09288",Llama 2: Open Foundation and Fine-Tuned Chat Models
Llama 2-70B,Language,USA,Meta AI,2023-07-18,Industry,Language modeling,Confident,1000.0,Open weights (restricted use),Llama 2 dataset,Unreleased,70000000000.0,1500000000000.0,1728.0,8.1e+23,1877.1621580193864,31236.156,580.0,1341258400000000.0,1437000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Historical significance,1,4000000,884796.5963891526,31236156.0,0.419197502,580000.0,1528928518.5604556,1002240000.0,"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closedsource models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.","https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/
https://arxiv.org/abs/2307.09288",Llama 2: Open Foundation and Fine-Tuned Chat Models
Llama 2-70B,Language,USA,Meta AI,2023-07-18,Industry,Language modeling,Confident,1000.0,Open weights (restricted use),Llama 2 dataset,Unreleased,70000000000.0,1500000000000.0,1728.0,8.1e+23,1877.1621580193864,31236.156,580.0,1341258400000000.0,1437000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Significant use,1,4000000,884796.5963891526,31236156.0,0.419197502,580000.0,1528928518.5604556,1002240000.0,"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closedsource models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.","https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/
https://arxiv.org/abs/2307.09288",Llama 2: Open Foundation and Fine-Tuned Chat Models
Llama 2-70B,Language,USA,Meta AI,2023-07-18,Industry,Language modeling,Confident,1000.0,Open weights (restricted use),Llama 2 dataset,Unreleased,70000000000.0,1500000000000.0,1728.0,8.1e+23,1877.1621580193864,31236.156,580.0,1341258400000000.0,1437000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Highly cited,1,4000000,884796.5963891526,31236156.0,0.419197502,580000.0,1528928518.5604556,1002240000.0,"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closedsource models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.","https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/
https://arxiv.org/abs/2307.09288",Llama 2: Open Foundation and Fine-Tuned Chat Models
Llama 2-70B,Language,USA,Meta AI,2023-07-18,Industry,Language modeling,Confident,1000.0,Open weights (restricted use),Llama 2 dataset,Unreleased,70000000000.0,1500000000000.0,1728.0,8.1e+23,1877.1621580193864,31236.156,580.0,1341258400000000.0,1437000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Training cost,1,4000000,884796.5963891526,31236156.0,0.419197502,580000.0,1528928518.5604556,1002240000.0,"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closedsource models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.","https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/
https://arxiv.org/abs/2307.09288",Llama 2: Open Foundation and Fine-Tuned Chat Models
Llama 3-70B,Language,USA,Meta AI,2024-04-18,Industry,Chat,Confident,1000.0,Open weights (restricted use),Llama 3 dataset,Unreleased,70000000000.0,15000000000000.0,1728.0,7.861e+24,1877.1621580193864,31236.156,580.0,1341258400000000.0,1437000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Significant use,1,4000000,884796.5963891526,31236156.0,1.7062185472586185e-09,580000.0,1528928518.5604556,1002240000.0,,https://ai.meta.com/blog/meta-llama-3/,Introducing Meta Llama 3: The most capable openly available LLM to date
Llama 3-70B,Language,USA,Meta AI,2024-04-18,Industry,Code generation,Confident,1000.0,Open weights (restricted use),Llama 3 dataset,Unreleased,70000000000.0,15000000000000.0,1728.0,7.861e+24,1877.1621580193864,31236.156,580.0,1341258400000000.0,1437000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Significant use,1,4000000,884796.5963891526,31236156.0,1.7062185472586185e-09,580000.0,1528928518.5604556,1002240000.0,,https://ai.meta.com/blog/meta-llama-3/,Introducing Meta Llama 3: The most capable openly available LLM to date
Llama 3-70B,Language,USA,Meta AI,2024-04-18,Industry,Language modeling/generation,Confident,1000.0,Open weights (restricted use),Llama 3 dataset,Unreleased,70000000000.0,15000000000000.0,1728.0,7.861e+24,1877.1621580193864,31236.156,580.0,1341258400000000.0,1437000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Significant use,1,4000000,884796.5963891526,31236156.0,1.7062185472586185e-09,580000.0,1528928518.5604556,1002240000.0,,https://ai.meta.com/blog/meta-llama-3/,Introducing Meta Llama 3: The most capable openly available LLM to date
Llama 3-8B,Language,USA,Meta AI,2024-04-18,Industry,Chat,Confident,1000.0,Open weights (restricted use),Llama 3 dataset,Unreleased,8000000000.0,15000000000000.0,1728.0,7.2e+23,1877.1621580193864,31236.156,580.0,1341258400000000.0,1437000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,1,2048,56450.0592214581,31236156.0,1.862858888888889e-08,580000.0,97545702.33467959,1002240000.0,,https://ai.meta.com/blog/meta-llama-3/,Introducing Meta Llama 3: The most capable openly available LLM to date
Llama 3-8B,Language,USA,Meta AI,2024-04-18,Industry,Code generation,Confident,1000.0,Open weights (restricted use),Llama 3 dataset,Unreleased,8000000000.0,15000000000000.0,1728.0,7.2e+23,1877.1621580193864,31236.156,580.0,1341258400000000.0,1437000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,1,2048,56450.0592214581,31236156.0,1.862858888888889e-08,580000.0,97545702.33467959,1002240000.0,,https://ai.meta.com/blog/meta-llama-3/,Introducing Meta Llama 3: The most capable openly available LLM to date
Llama 3-8B,Language,USA,Meta AI,2024-04-18,Industry,Language modeling/generation,Confident,1000.0,Open weights (restricted use),Llama 3 dataset,Unreleased,8000000000.0,15000000000000.0,1728.0,7.2e+23,1877.1621580193864,31236.156,580.0,1341258400000000.0,1437000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,1,2048,56450.0592214581,31236156.0,1.862858888888889e-08,580000.0,97545702.33467959,1002240000.0,,https://ai.meta.com/blog/meta-llama-3/,Introducing Meta Llama 3: The most capable openly available LLM to date
Llama 3.1-405B,Language,USA,Meta AI,2024-07-23,Industry,Language modeling/generation,Confident,16384.0,Open weights (restricted use),Llama 3 dataset,Open (restricted use),405000000000.0,15600000000000.0,2142.0,3.8e+25,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,SOTA improvement,1,16000000,25280251.591531232,728922193.92,0.4042,11468800.0,54150298909.0599,24566169600.0,"Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",https://ai.meta.com/research/publications/the-llama-3-herd-of-models/,The Llama 3 Herd of Models
Llama 3.1-405B,Language,USA,Meta AI,2024-07-23,Industry,Language modeling/generation,Confident,16384.0,Open weights (restricted use),Llama 3 dataset,Open (restricted use),405000000000.0,15600000000000.0,2142.0,3.8e+25,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,1,16000000,25280251.591531232,728922193.92,0.4042,11468800.0,54150298909.0599,24566169600.0,"Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",https://ai.meta.com/research/publications/the-llama-3-herd-of-models/,The Llama 3 Herd of Models
Llama 3.1-70B,Language,USA,Meta AI,2024-07-23,Industry,Language modeling/generation,Confident,16384.0,Open weights (restricted use),Llama 3 dataset,Open (restricted use),70000000000.0,15600000000000.0,2142.0,7.929e+24,1877.1621580193864,38465.46,645.4545454545455,1619822909090909.0,1732636363636363.8,80000000000.0,NVIDIA H100 SXM5 80GB,SOTA improvement,1,16000000,25280251.591531232,630218096.64,0.4042,10575127.272727273,54150298909.0599,22651922618.18182,"Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",https://ai.meta.com/research/publications/the-llama-3-herd-of-models/,The Llama 3 Herd of Models
Llama 3.1-8B,Language,USA,Meta AI,2024-07-23,Industry,Language modeling/generation,Unverified,16384.0,Open weights (restricted use),Llama 3 dataset,Open (restricted use),8000000000.0,15600000000000.0,2142.0,1.224e+24,1877.1621580193864,38465.46,645.4545454545455,1619822909090909.0,1732636363636363.8,80000000000.0,NVIDIA H100 SXM5 80GB,SOTA improvement,1,16000000,25280251.591531232,630218096.64,0.4042,10575127.272727273,54150298909.0599,22651922618.18182,"Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",https://ai.meta.com/research/publications/the-llama-3-herd-of-models/,The Llama 3 Herd of Models
Llama 3.2 3B,Language,USA,Meta AI,2024-09-24,Industry,Language modeling/generation,Confident,1000.0,Open weights (restricted use),Unspecified unreleased,Unreleased,3210000000.0,9000000000000.0,1728.0,1.7334e+23,1877.1621580193864,33445.11,600.0,1426375333333333.2,1527333333333333.2,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,1,2048,788306.6236983632,33445110.0,8.228771970308834e-08,600000.0,1362193845.7507715,1036800000.0,"Today, we’re releasing Llama 3.2, which includes small and medium-sized vision LLMs (11B and 90B), and lightweight, text-only models (1B and 3B) that fit onto edge and mobile devices, including pre-trained and instruction-tuned versions.
The Llama 3.2 1B and 3B models support context length of 128K tokens and are state-of-the-art in their class for on-device use cases like summarization, instruction following, and rewriting tasks running locally at the edge. These models are enabled on day one for Qualcomm and MediaTek hardware and optimized for Arm processors.
We’re sharing the first official Llama Stack distributions, which will greatly simplify the way developers work with Llama models in different environments, including single-node, on-prem, cloud, and on-device, enabling turnkey deployment of retrieval-augmented generation (RAG) and tooling-enabled applications with integrated safety.
We’ve been working closely with partners like AWS, Databricks, Dell Technologies, Fireworks, Infosys, and Together AI to build Llama Stack distributions for their downstream enterprise clients. On-device distribution is via PyTorch ExecuTorch, and single-node distribution is via Ollama.
We continue to share our work because we believe openness drives innovation and is good for developers, Meta, and the world. Llama is already leading the way on openness, modifiability, and cost efficiency—enabling more people to have creative, useful, and life-changing breakthroughs using generative AI.
We’re making Llama 3.2 models available for download on llama.com and Hugging Face, as well as available for immediate development on our broad ecosystem of partner platforms, including AMD, AWS, Databricks, Dell, Google Cloud, Groq, IBM, Intel, Microsoft Azure, NVIDIA, Oracle Cloud, Snowflake, and more.",https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/,"Llama 3.2: Revolutionizing edge AI and vision with open, customizable models"
Llama 3.2 3B,Language,USA,Meta AI,2024-09-24,Industry,Quantitative reasoning,Confident,1000.0,Open weights (restricted use),Unspecified unreleased,Unreleased,3210000000.0,9000000000000.0,1728.0,1.7334e+23,1877.1621580193864,33445.11,600.0,1426375333333333.2,1527333333333333.2,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,1,2048,788306.6236983632,33445110.0,8.228771970308834e-08,600000.0,1362193845.7507715,1036800000.0,"Today, we’re releasing Llama 3.2, which includes small and medium-sized vision LLMs (11B and 90B), and lightweight, text-only models (1B and 3B) that fit onto edge and mobile devices, including pre-trained and instruction-tuned versions.
The Llama 3.2 1B and 3B models support context length of 128K tokens and are state-of-the-art in their class for on-device use cases like summarization, instruction following, and rewriting tasks running locally at the edge. These models are enabled on day one for Qualcomm and MediaTek hardware and optimized for Arm processors.
We’re sharing the first official Llama Stack distributions, which will greatly simplify the way developers work with Llama models in different environments, including single-node, on-prem, cloud, and on-device, enabling turnkey deployment of retrieval-augmented generation (RAG) and tooling-enabled applications with integrated safety.
We’ve been working closely with partners like AWS, Databricks, Dell Technologies, Fireworks, Infosys, and Together AI to build Llama Stack distributions for their downstream enterprise clients. On-device distribution is via PyTorch ExecuTorch, and single-node distribution is via Ollama.
We continue to share our work because we believe openness drives innovation and is good for developers, Meta, and the world. Llama is already leading the way on openness, modifiability, and cost efficiency—enabling more people to have creative, useful, and life-changing breakthroughs using generative AI.
We’re making Llama 3.2 models available for download on llama.com and Hugging Face, as well as available for immediate development on our broad ecosystem of partner platforms, including AMD, AWS, Databricks, Dell, Google Cloud, Groq, IBM, Intel, Microsoft Azure, NVIDIA, Oracle Cloud, Snowflake, and more.",https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/,"Llama 3.2: Revolutionizing edge AI and vision with open, customizable models"
Llama 3.2 3B,Language,USA,Meta AI,2024-09-24,Industry,Question answering,Confident,1000.0,Open weights (restricted use),Unspecified unreleased,Unreleased,3210000000.0,9000000000000.0,1728.0,1.7334e+23,1877.1621580193864,33445.11,600.0,1426375333333333.2,1527333333333333.2,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,1,2048,788306.6236983632,33445110.0,8.228771970308834e-08,600000.0,1362193845.7507715,1036800000.0,"Today, we’re releasing Llama 3.2, which includes small and medium-sized vision LLMs (11B and 90B), and lightweight, text-only models (1B and 3B) that fit onto edge and mobile devices, including pre-trained and instruction-tuned versions.
The Llama 3.2 1B and 3B models support context length of 128K tokens and are state-of-the-art in their class for on-device use cases like summarization, instruction following, and rewriting tasks running locally at the edge. These models are enabled on day one for Qualcomm and MediaTek hardware and optimized for Arm processors.
We’re sharing the first official Llama Stack distributions, which will greatly simplify the way developers work with Llama models in different environments, including single-node, on-prem, cloud, and on-device, enabling turnkey deployment of retrieval-augmented generation (RAG) and tooling-enabled applications with integrated safety.
We’ve been working closely with partners like AWS, Databricks, Dell Technologies, Fireworks, Infosys, and Together AI to build Llama Stack distributions for their downstream enterprise clients. On-device distribution is via PyTorch ExecuTorch, and single-node distribution is via Ollama.
We continue to share our work because we believe openness drives innovation and is good for developers, Meta, and the world. Llama is already leading the way on openness, modifiability, and cost efficiency—enabling more people to have creative, useful, and life-changing breakthroughs using generative AI.
We’re making Llama 3.2 models available for download on llama.com and Hugging Face, as well as available for immediate development on our broad ecosystem of partner platforms, including AMD, AWS, Databricks, Dell, Google Cloud, Groq, IBM, Intel, Microsoft Azure, NVIDIA, Oracle Cloud, Snowflake, and more.",https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/,"Llama 3.2: Revolutionizing edge AI and vision with open, customizable models"
Llama 3.2 3B,Language,USA,Meta AI,2024-09-24,Industry,Text summarization,Confident,1000.0,Open weights (restricted use),Unspecified unreleased,Unreleased,3210000000.0,9000000000000.0,1728.0,1.7334e+23,1877.1621580193864,33445.11,600.0,1426375333333333.2,1527333333333333.2,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,1,2048,788306.6236983632,33445110.0,8.228771970308834e-08,600000.0,1362193845.7507715,1036800000.0,"Today, we’re releasing Llama 3.2, which includes small and medium-sized vision LLMs (11B and 90B), and lightweight, text-only models (1B and 3B) that fit onto edge and mobile devices, including pre-trained and instruction-tuned versions.
The Llama 3.2 1B and 3B models support context length of 128K tokens and are state-of-the-art in their class for on-device use cases like summarization, instruction following, and rewriting tasks running locally at the edge. These models are enabled on day one for Qualcomm and MediaTek hardware and optimized for Arm processors.
We’re sharing the first official Llama Stack distributions, which will greatly simplify the way developers work with Llama models in different environments, including single-node, on-prem, cloud, and on-device, enabling turnkey deployment of retrieval-augmented generation (RAG) and tooling-enabled applications with integrated safety.
We’ve been working closely with partners like AWS, Databricks, Dell Technologies, Fireworks, Infosys, and Together AI to build Llama Stack distributions for their downstream enterprise clients. On-device distribution is via PyTorch ExecuTorch, and single-node distribution is via Ollama.
We continue to share our work because we believe openness drives innovation and is good for developers, Meta, and the world. Llama is already leading the way on openness, modifiability, and cost efficiency—enabling more people to have creative, useful, and life-changing breakthroughs using generative AI.
We’re making Llama 3.2 models available for download on llama.com and Hugging Face, as well as available for immediate development on our broad ecosystem of partner platforms, including AMD, AWS, Databricks, Dell, Google Cloud, Groq, IBM, Intel, Microsoft Azure, NVIDIA, Oracle Cloud, Snowflake, and more.",https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/,"Llama 3.2: Revolutionizing edge AI and vision with open, customizable models"
Llama 3.2 3B,Language,USA,Meta AI,2024-09-24,Industry,Translation,Confident,1000.0,Open weights (restricted use),Unspecified unreleased,Unreleased,3210000000.0,9000000000000.0,1728.0,1.7334e+23,1877.1621580193864,33445.11,600.0,1426375333333333.2,1527333333333333.2,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,1,2048,788306.6236983632,33445110.0,8.228771970308834e-08,600000.0,1362193845.7507715,1036800000.0,"Today, we’re releasing Llama 3.2, which includes small and medium-sized vision LLMs (11B and 90B), and lightweight, text-only models (1B and 3B) that fit onto edge and mobile devices, including pre-trained and instruction-tuned versions.
The Llama 3.2 1B and 3B models support context length of 128K tokens and are state-of-the-art in their class for on-device use cases like summarization, instruction following, and rewriting tasks running locally at the edge. These models are enabled on day one for Qualcomm and MediaTek hardware and optimized for Arm processors.
We’re sharing the first official Llama Stack distributions, which will greatly simplify the way developers work with Llama models in different environments, including single-node, on-prem, cloud, and on-device, enabling turnkey deployment of retrieval-augmented generation (RAG) and tooling-enabled applications with integrated safety.
We’ve been working closely with partners like AWS, Databricks, Dell Technologies, Fireworks, Infosys, and Together AI to build Llama Stack distributions for their downstream enterprise clients. On-device distribution is via PyTorch ExecuTorch, and single-node distribution is via Ollama.
We continue to share our work because we believe openness drives innovation and is good for developers, Meta, and the world. Llama is already leading the way on openness, modifiability, and cost efficiency—enabling more people to have creative, useful, and life-changing breakthroughs using generative AI.
We’re making Llama 3.2 models available for download on llama.com and Hugging Face, as well as available for immediate development on our broad ecosystem of partner platforms, including AMD, AWS, Databricks, Dell, Google Cloud, Groq, IBM, Intel, Microsoft Azure, NVIDIA, Oracle Cloud, Snowflake, and more.",https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/,"Llama 3.2: Revolutionizing edge AI and vision with open, customizable models"
Llama 3.3,Language,USA,Meta AI,2024-12-06,Industry,Code generation,Confident,16384.0,Open weights (restricted use),Unspecified unreleased,Unreleased,70000000000.0,15000000000000.0,2142.0,6.8649768e+24,1877.1621580193864,37863.018,640.0,1596609200000000.0,1708000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,1,16000000,788306.6236983632,620347686.912,2.325731384846049e-09,10485760.0,1688552787.961894,22460497920.0,"The Meta Llama 3.3 multilingual large language model (LLM) is a pretrained and instruction tuned generative model in 70B (text in/text out). The Llama 3.3 instruction tuned text only model is optimized for multilingual dialogue use cases and outperforms many of the available open source and closed chat models on common industry benchmarks.

Model developer: Meta

Model Architecture: Llama 3.3 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.",https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_3/,Meta Llama 3.3 multilingual large language model (LLM) 
Llama 3.3,Language,USA,Meta AI,2024-12-06,Industry,Language modeling/generation,Confident,16384.0,Open weights (restricted use),Unspecified unreleased,Unreleased,70000000000.0,15000000000000.0,2142.0,6.8649768e+24,1877.1621580193864,37863.018,640.0,1596609200000000.0,1708000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,1,16000000,788306.6236983632,620347686.912,2.325731384846049e-09,10485760.0,1688552787.961894,22460497920.0,"The Meta Llama 3.3 multilingual large language model (LLM) is a pretrained and instruction tuned generative model in 70B (text in/text out). The Llama 3.3 instruction tuned text only model is optimized for multilingual dialogue use cases and outperforms many of the available open source and closed chat models on common industry benchmarks.

Model developer: Meta

Model Architecture: Llama 3.3 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.",https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_3/,Meta Llama 3.3 multilingual large language model (LLM) 
Llama 3.3,Language,USA,Meta AI,2024-12-06,Industry,Question answering,Confident,16384.0,Open weights (restricted use),Unspecified unreleased,Unreleased,70000000000.0,15000000000000.0,2142.0,6.8649768e+24,1877.1621580193864,37863.018,640.0,1596609200000000.0,1708000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,1,16000000,788306.6236983632,620347686.912,2.325731384846049e-09,10485760.0,1688552787.961894,22460497920.0,"The Meta Llama 3.3 multilingual large language model (LLM) is a pretrained and instruction tuned generative model in 70B (text in/text out). The Llama 3.3 instruction tuned text only model is optimized for multilingual dialogue use cases and outperforms many of the available open source and closed chat models on common industry benchmarks.

Model developer: Meta

Model Architecture: Llama 3.3 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.",https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_3/,Meta Llama 3.3 multilingual large language model (LLM) 
Llama 3.3,Language,USA,Meta AI,2024-12-06,Industry,Translation,Confident,16384.0,Open weights (restricted use),Unspecified unreleased,Unreleased,70000000000.0,15000000000000.0,2142.0,6.8649768e+24,1877.1621580193864,37863.018,640.0,1596609200000000.0,1708000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,1,16000000,788306.6236983632,620347686.912,2.325731384846049e-09,10485760.0,1688552787.961894,22460497920.0,"The Meta Llama 3.3 multilingual large language model (LLM) is a pretrained and instruction tuned generative model in 70B (text in/text out). The Llama 3.3 instruction tuned text only model is optimized for multilingual dialogue use cases and outperforms many of the available open source and closed chat models on common industry benchmarks.

Model developer: Meta

Model Architecture: Llama 3.3 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.",https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_3/,Meta Llama 3.3 multilingual large language model (LLM) 
Llama 4 Behemoth,Language,USA,Meta AI,Not-defined,Industry,Chat,Likely,1000.0,Not-defined,Not-defined,Not-defined,2000000000000.0,30000000000000.0,1728.0,5.18400000000001e+25,1877.1621580193864,31236.156,580.0,1341258400000000.0,1437000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Significant use,27,2048,56450.0592214581,31236156.0,2.5873040123456744e-10,580000.0,97545702.33467959,1002240000.0,"We’re sharing the first models in the Llama 4 herd, which will enable people to build more personalized multimodal experiences.
...
Llama 4 Behemoth, a 288 billion active parameter model with 16 experts that is our most powerful yet and among the world’s smartest LLMs. Llama 4 Behemoth outperforms GPT-4.5, Claude Sonnet 3.7, and Gemini 2.0 Pro on several STEM benchmarks. Llama 4 Behemoth is still training, and we’re excited to share more details about it even while it’s still in flight.
Download the Llama 4 Scout and Llama 4 Maverick models today on llama.com and Hugging Face. Try Meta AI built with Llama 4 in WhatsApp, Messenger, Instagram Direct, and on the web.",https://ai.meta.com/blog/llama-4-multimodal-intelligence/,The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation
Llama 4 Behemoth,Language,USA,Meta AI,Not-defined,Industry,Code generation,Likely,1000.0,Not-defined,Not-defined,Not-defined,2000000000000.0,30000000000000.0,1728.0,5.18400000000001e+25,1877.1621580193864,31236.156,580.0,1341258400000000.0,1437000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Significant use,27,2048,56450.0592214581,31236156.0,2.5873040123456744e-10,580000.0,97545702.33467959,1002240000.0,"We’re sharing the first models in the Llama 4 herd, which will enable people to build more personalized multimodal experiences.
...
Llama 4 Behemoth, a 288 billion active parameter model with 16 experts that is our most powerful yet and among the world’s smartest LLMs. Llama 4 Behemoth outperforms GPT-4.5, Claude Sonnet 3.7, and Gemini 2.0 Pro on several STEM benchmarks. Llama 4 Behemoth is still training, and we’re excited to share more details about it even while it’s still in flight.
Download the Llama 4 Scout and Llama 4 Maverick models today on llama.com and Hugging Face. Try Meta AI built with Llama 4 in WhatsApp, Messenger, Instagram Direct, and on the web.",https://ai.meta.com/blog/llama-4-multimodal-intelligence/,The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation
Llama 4 Behemoth,Multimodal,USA,Meta AI,Not-defined,Industry,Chat,Likely,1000.0,Not-defined,Not-defined,Not-defined,2000000000000.0,30000000000000.0,1728.0,5.18400000000001e+25,1877.1621580193864,31236.156,580.0,1341258400000000.0,1437000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Significant use,27,2048,56450.0592214581,31236156.0,2.5873040123456744e-10,580000.0,97545702.33467959,1002240000.0,"We’re sharing the first models in the Llama 4 herd, which will enable people to build more personalized multimodal experiences.
...
Llama 4 Behemoth, a 288 billion active parameter model with 16 experts that is our most powerful yet and among the world’s smartest LLMs. Llama 4 Behemoth outperforms GPT-4.5, Claude Sonnet 3.7, and Gemini 2.0 Pro on several STEM benchmarks. Llama 4 Behemoth is still training, and we’re excited to share more details about it even while it’s still in flight.
Download the Llama 4 Scout and Llama 4 Maverick models today on llama.com and Hugging Face. Try Meta AI built with Llama 4 in WhatsApp, Messenger, Instagram Direct, and on the web.",https://ai.meta.com/blog/llama-4-multimodal-intelligence/,The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation
Llama 4 Behemoth,Multimodal,USA,Meta AI,Not-defined,Industry,Code generation,Likely,1000.0,Not-defined,Not-defined,Not-defined,2000000000000.0,30000000000000.0,1728.0,5.18400000000001e+25,1877.1621580193864,31236.156,580.0,1341258400000000.0,1437000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Significant use,27,2048,56450.0592214581,31236156.0,2.5873040123456744e-10,580000.0,97545702.33467959,1002240000.0,"We’re sharing the first models in the Llama 4 herd, which will enable people to build more personalized multimodal experiences.
...
Llama 4 Behemoth, a 288 billion active parameter model with 16 experts that is our most powerful yet and among the world’s smartest LLMs. Llama 4 Behemoth outperforms GPT-4.5, Claude Sonnet 3.7, and Gemini 2.0 Pro on several STEM benchmarks. Llama 4 Behemoth is still training, and we’re excited to share more details about it even while it’s still in flight.
Download the Llama 4 Scout and Llama 4 Maverick models today on llama.com and Hugging Face. Try Meta AI built with Llama 4 in WhatsApp, Messenger, Instagram Direct, and on the web.",https://ai.meta.com/blog/llama-4-multimodal-intelligence/,The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation
Llama 4 Maverick,Language,USA,Meta AI,2025-04-05,Industry,Chat,Unverified,1000.0,Open weights (restricted use),Not-defined,Not-defined,400000000000.0,30000000000000.0,1728.0,1.4916e+25,1877.1621580193864,31236.156,580.0,1341258400000000.0,1437000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,SOTA improvement,27,2048,56450.0592214581,31236156.0,8.99207830517565e-10,580000.0,97545702.33467959,1002240000.0,"We’re sharing the first models in the Llama 4 herd, which will enable people to build more personalized multimodal experiences.
Llama 4 Scout, a 17 billion active parameter model with 16 experts, is the best multimodal model in the world in its class and is more powerful than all previous generation Llama models, while fitting in a single NVIDIA H100 GPU. Additionally, Llama 4 Scout offers an industry-leading context window of 10M and delivers better results than Gemma 3, Gemini 2.0 Flash-Lite, and Mistral 3.1 across a broad range of widely reported benchmarks.
Llama 4 Maverick, a 17 billion active parameter model with 128 experts, is the best multimodal model in its class, beating GPT-4o and Gemini 2.0 Flash across a broad range of widely reported benchmarks, while achieving comparable results to the new DeepSeek v3 on reasoning and coding—at less than half the active parameters. Llama 4 Maverick offers a best-in-class performance to cost ratio with an experimental chat version scoring ELO of 1417 on LMArena.
These models are our best yet thanks to distillation from Llama 4 Behemoth, a 288 billion active parameter model with 16 experts that is our most powerful yet and among the world’s smartest LLMs. Llama 4 Behemoth outperforms GPT-4.5, Claude Sonnet 3.7, and Gemini 2.0 Pro on several STEM benchmarks. Llama 4 Behemoth is still training, and we’re excited to share more details about it even while it’s still in flight.
Download the Llama 4 Scout and Llama 4 Maverick models today on llama.com and Hugging Face. Try Meta AI built with Llama 4 in WhatsApp, Messenger, Instagram Direct, and on the web.",https://ai.meta.com/blog/llama-4-multimodal-intelligence/,The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation
Llama 4 Maverick,Language,USA,Meta AI,2025-04-05,Industry,Code generation,Unverified,1000.0,Open weights (restricted use),Not-defined,Not-defined,400000000000.0,30000000000000.0,1728.0,1.4916e+25,1877.1621580193864,31236.156,580.0,1341258400000000.0,1437000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,SOTA improvement,27,2048,56450.0592214581,31236156.0,8.99207830517565e-10,580000.0,97545702.33467959,1002240000.0,"We’re sharing the first models in the Llama 4 herd, which will enable people to build more personalized multimodal experiences.
Llama 4 Scout, a 17 billion active parameter model with 16 experts, is the best multimodal model in the world in its class and is more powerful than all previous generation Llama models, while fitting in a single NVIDIA H100 GPU. Additionally, Llama 4 Scout offers an industry-leading context window of 10M and delivers better results than Gemma 3, Gemini 2.0 Flash-Lite, and Mistral 3.1 across a broad range of widely reported benchmarks.
Llama 4 Maverick, a 17 billion active parameter model with 128 experts, is the best multimodal model in its class, beating GPT-4o and Gemini 2.0 Flash across a broad range of widely reported benchmarks, while achieving comparable results to the new DeepSeek v3 on reasoning and coding—at less than half the active parameters. Llama 4 Maverick offers a best-in-class performance to cost ratio with an experimental chat version scoring ELO of 1417 on LMArena.
These models are our best yet thanks to distillation from Llama 4 Behemoth, a 288 billion active parameter model with 16 experts that is our most powerful yet and among the world’s smartest LLMs. Llama 4 Behemoth outperforms GPT-4.5, Claude Sonnet 3.7, and Gemini 2.0 Pro on several STEM benchmarks. Llama 4 Behemoth is still training, and we’re excited to share more details about it even while it’s still in flight.
Download the Llama 4 Scout and Llama 4 Maverick models today on llama.com and Hugging Face. Try Meta AI built with Llama 4 in WhatsApp, Messenger, Instagram Direct, and on the web.",https://ai.meta.com/blog/llama-4-multimodal-intelligence/,The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation
Llama 4 Maverick,Multimodal,USA,Meta AI,2025-04-05,Industry,Chat,Unverified,1000.0,Open weights (restricted use),Not-defined,Not-defined,400000000000.0,30000000000000.0,1728.0,1.4916e+25,1877.1621580193864,31236.156,580.0,1341258400000000.0,1437000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,SOTA improvement,27,2048,56450.0592214581,31236156.0,8.99207830517565e-10,580000.0,97545702.33467959,1002240000.0,"We’re sharing the first models in the Llama 4 herd, which will enable people to build more personalized multimodal experiences.
Llama 4 Scout, a 17 billion active parameter model with 16 experts, is the best multimodal model in the world in its class and is more powerful than all previous generation Llama models, while fitting in a single NVIDIA H100 GPU. Additionally, Llama 4 Scout offers an industry-leading context window of 10M and delivers better results than Gemma 3, Gemini 2.0 Flash-Lite, and Mistral 3.1 across a broad range of widely reported benchmarks.
Llama 4 Maverick, a 17 billion active parameter model with 128 experts, is the best multimodal model in its class, beating GPT-4o and Gemini 2.0 Flash across a broad range of widely reported benchmarks, while achieving comparable results to the new DeepSeek v3 on reasoning and coding—at less than half the active parameters. Llama 4 Maverick offers a best-in-class performance to cost ratio with an experimental chat version scoring ELO of 1417 on LMArena.
These models are our best yet thanks to distillation from Llama 4 Behemoth, a 288 billion active parameter model with 16 experts that is our most powerful yet and among the world’s smartest LLMs. Llama 4 Behemoth outperforms GPT-4.5, Claude Sonnet 3.7, and Gemini 2.0 Pro on several STEM benchmarks. Llama 4 Behemoth is still training, and we’re excited to share more details about it even while it’s still in flight.
Download the Llama 4 Scout and Llama 4 Maverick models today on llama.com and Hugging Face. Try Meta AI built with Llama 4 in WhatsApp, Messenger, Instagram Direct, and on the web.",https://ai.meta.com/blog/llama-4-multimodal-intelligence/,The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation
Llama 4 Maverick,Multimodal,USA,Meta AI,2025-04-05,Industry,Code generation,Unverified,1000.0,Open weights (restricted use),Not-defined,Not-defined,400000000000.0,30000000000000.0,1728.0,1.4916e+25,1877.1621580193864,31236.156,580.0,1341258400000000.0,1437000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,SOTA improvement,27,2048,56450.0592214581,31236156.0,8.99207830517565e-10,580000.0,97545702.33467959,1002240000.0,"We’re sharing the first models in the Llama 4 herd, which will enable people to build more personalized multimodal experiences.
Llama 4 Scout, a 17 billion active parameter model with 16 experts, is the best multimodal model in the world in its class and is more powerful than all previous generation Llama models, while fitting in a single NVIDIA H100 GPU. Additionally, Llama 4 Scout offers an industry-leading context window of 10M and delivers better results than Gemma 3, Gemini 2.0 Flash-Lite, and Mistral 3.1 across a broad range of widely reported benchmarks.
Llama 4 Maverick, a 17 billion active parameter model with 128 experts, is the best multimodal model in its class, beating GPT-4o and Gemini 2.0 Flash across a broad range of widely reported benchmarks, while achieving comparable results to the new DeepSeek v3 on reasoning and coding—at less than half the active parameters. Llama 4 Maverick offers a best-in-class performance to cost ratio with an experimental chat version scoring ELO of 1417 on LMArena.
These models are our best yet thanks to distillation from Llama 4 Behemoth, a 288 billion active parameter model with 16 experts that is our most powerful yet and among the world’s smartest LLMs. Llama 4 Behemoth outperforms GPT-4.5, Claude Sonnet 3.7, and Gemini 2.0 Pro on several STEM benchmarks. Llama 4 Behemoth is still training, and we’re excited to share more details about it even while it’s still in flight.
Download the Llama 4 Scout and Llama 4 Maverick models today on llama.com and Hugging Face. Try Meta AI built with Llama 4 in WhatsApp, Messenger, Instagram Direct, and on the web.",https://ai.meta.com/blog/llama-4-multimodal-intelligence/,The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation
Llama 4 Scout,Language,USA,Meta AI,2025-04-05,Industry,Chat,Unverified,1000.0,Open weights (restricted use),Not-defined,Not-defined,109000000000.0,30000000000000.0,1728.0,4.08e+24,1877.1621580193864,27922.725,550.0,1213583000000000.0,1301500000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Training cost,27,2048,788306.6236983632,27922725.0,2.9744681372549014e-09,550000.0,1362193845.7507715,950400000.0,"We’re sharing the first models in the Llama 4 herd, which will enable people to build more personalized multimodal experiences.
Llama 4 Scout, a 17 billion active parameter model with 16 experts, is the best multimodal model in the world in its class and is more powerful than all previous generation Llama models, while fitting in a single NVIDIA H100 GPU. Additionally, Llama 4 Scout offers an industry-leading context window of 10M and delivers better results than Gemma 3, Gemini 2.0 Flash-Lite, and Mistral 3.1 across a broad range of widely reported benchmarks.
Llama 4 Maverick, a 17 billion active parameter model with 128 experts, is the best multimodal model in its class, beating GPT-4o and Gemini 2.0 Flash across a broad range of widely reported benchmarks, while achieving comparable results to the new DeepSeek v3 on reasoning and coding—at less than half the active parameters. Llama 4 Maverick offers a best-in-class performance to cost ratio with an experimental chat version scoring ELO of 1417 on LMArena.
These models are our best yet thanks to distillation from Llama 4 Behemoth, a 288 billion active parameter model with 16 experts that is our most powerful yet and among the world’s smartest LLMs. Llama 4 Behemoth outperforms GPT-4.5, Claude Sonnet 3.7, and Gemini 2.0 Pro on several STEM benchmarks. Llama 4 Behemoth is still training, and we’re excited to share more details about it even while it’s still in flight.
Download the Llama 4 Scout and Llama 4 Maverick models today on llama.com and Hugging Face. Try Meta AI built with Llama 4 in WhatsApp, Messenger, Instagram Direct, and on the web.",https://ai.meta.com/blog/llama-4-multimodal-intelligence/,The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation
Llama 4 Scout,Language,USA,Meta AI,2025-04-05,Industry,Code generation,Unverified,1000.0,Open weights (restricted use),Not-defined,Not-defined,109000000000.0,30000000000000.0,1728.0,4.08e+24,1877.1621580193864,27922.725,550.0,1213583000000000.0,1301500000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Training cost,27,2048,788306.6236983632,27922725.0,2.9744681372549014e-09,550000.0,1362193845.7507715,950400000.0,"We’re sharing the first models in the Llama 4 herd, which will enable people to build more personalized multimodal experiences.
Llama 4 Scout, a 17 billion active parameter model with 16 experts, is the best multimodal model in the world in its class and is more powerful than all previous generation Llama models, while fitting in a single NVIDIA H100 GPU. Additionally, Llama 4 Scout offers an industry-leading context window of 10M and delivers better results than Gemma 3, Gemini 2.0 Flash-Lite, and Mistral 3.1 across a broad range of widely reported benchmarks.
Llama 4 Maverick, a 17 billion active parameter model with 128 experts, is the best multimodal model in its class, beating GPT-4o and Gemini 2.0 Flash across a broad range of widely reported benchmarks, while achieving comparable results to the new DeepSeek v3 on reasoning and coding—at less than half the active parameters. Llama 4 Maverick offers a best-in-class performance to cost ratio with an experimental chat version scoring ELO of 1417 on LMArena.
These models are our best yet thanks to distillation from Llama 4 Behemoth, a 288 billion active parameter model with 16 experts that is our most powerful yet and among the world’s smartest LLMs. Llama 4 Behemoth outperforms GPT-4.5, Claude Sonnet 3.7, and Gemini 2.0 Pro on several STEM benchmarks. Llama 4 Behemoth is still training, and we’re excited to share more details about it even while it’s still in flight.
Download the Llama 4 Scout and Llama 4 Maverick models today on llama.com and Hugging Face. Try Meta AI built with Llama 4 in WhatsApp, Messenger, Instagram Direct, and on the web.",https://ai.meta.com/blog/llama-4-multimodal-intelligence/,The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation
Llama 4 Scout,Multimodal,USA,Meta AI,2025-04-05,Industry,Chat,Unverified,1000.0,Open weights (restricted use),Not-defined,Not-defined,109000000000.0,30000000000000.0,1728.0,4.08e+24,1877.1621580193864,27922.725,550.0,1213583000000000.0,1301500000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Training cost,27,2048,788306.6236983632,27922725.0,2.9744681372549014e-09,550000.0,1362193845.7507715,950400000.0,"We’re sharing the first models in the Llama 4 herd, which will enable people to build more personalized multimodal experiences.
Llama 4 Scout, a 17 billion active parameter model with 16 experts, is the best multimodal model in the world in its class and is more powerful than all previous generation Llama models, while fitting in a single NVIDIA H100 GPU. Additionally, Llama 4 Scout offers an industry-leading context window of 10M and delivers better results than Gemma 3, Gemini 2.0 Flash-Lite, and Mistral 3.1 across a broad range of widely reported benchmarks.
Llama 4 Maverick, a 17 billion active parameter model with 128 experts, is the best multimodal model in its class, beating GPT-4o and Gemini 2.0 Flash across a broad range of widely reported benchmarks, while achieving comparable results to the new DeepSeek v3 on reasoning and coding—at less than half the active parameters. Llama 4 Maverick offers a best-in-class performance to cost ratio with an experimental chat version scoring ELO of 1417 on LMArena.
These models are our best yet thanks to distillation from Llama 4 Behemoth, a 288 billion active parameter model with 16 experts that is our most powerful yet and among the world’s smartest LLMs. Llama 4 Behemoth outperforms GPT-4.5, Claude Sonnet 3.7, and Gemini 2.0 Pro on several STEM benchmarks. Llama 4 Behemoth is still training, and we’re excited to share more details about it even while it’s still in flight.
Download the Llama 4 Scout and Llama 4 Maverick models today on llama.com and Hugging Face. Try Meta AI built with Llama 4 in WhatsApp, Messenger, Instagram Direct, and on the web.",https://ai.meta.com/blog/llama-4-multimodal-intelligence/,The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation
Llama 4 Scout,Multimodal,USA,Meta AI,2025-04-05,Industry,Code generation,Unverified,1000.0,Open weights (restricted use),Not-defined,Not-defined,109000000000.0,30000000000000.0,1728.0,4.08e+24,1877.1621580193864,27922.725,550.0,1213583000000000.0,1301500000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Training cost,27,2048,788306.6236983632,27922725.0,2.9744681372549014e-09,550000.0,1362193845.7507715,950400000.0,"We’re sharing the first models in the Llama 4 herd, which will enable people to build more personalized multimodal experiences.
Llama 4 Scout, a 17 billion active parameter model with 16 experts, is the best multimodal model in the world in its class and is more powerful than all previous generation Llama models, while fitting in a single NVIDIA H100 GPU. Additionally, Llama 4 Scout offers an industry-leading context window of 10M and delivers better results than Gemma 3, Gemini 2.0 Flash-Lite, and Mistral 3.1 across a broad range of widely reported benchmarks.
Llama 4 Maverick, a 17 billion active parameter model with 128 experts, is the best multimodal model in its class, beating GPT-4o and Gemini 2.0 Flash across a broad range of widely reported benchmarks, while achieving comparable results to the new DeepSeek v3 on reasoning and coding—at less than half the active parameters. Llama 4 Maverick offers a best-in-class performance to cost ratio with an experimental chat version scoring ELO of 1417 on LMArena.
These models are our best yet thanks to distillation from Llama 4 Behemoth, a 288 billion active parameter model with 16 experts that is our most powerful yet and among the world’s smartest LLMs. Llama 4 Behemoth outperforms GPT-4.5, Claude Sonnet 3.7, and Gemini 2.0 Pro on several STEM benchmarks. Llama 4 Behemoth is still training, and we’re excited to share more details about it even while it’s still in flight.
Download the Llama 4 Scout and Llama 4 Maverick models today on llama.com and Hugging Face. Try Meta AI built with Llama 4 in WhatsApp, Messenger, Instagram Direct, and on the web.",https://ai.meta.com/blog/llama-4-multimodal-intelligence/,The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation
Luminous-extended,Language,Germany,Aleph Alpha,2022-08-15,Industry,Language modeling/generation,Confident,512.0,API access,Not-defined,Unreleased,30000000000.0,460000000000.0,1344.0,1.0019457e+23,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,None,4,6472188,91700208.69015627,7680000.0,5.7408899504234616e-08,204800.0,123245080479.57002,275251200.0,"Aleph Alpha luminous-extended is the second largest model which is faster and cheaper than Luminous-supreme. the model can perform information extraction, language simplification and has multi-capable image description capability. You can try Aleph Alpha models with predefined examples for free. Go to at the Jumpstart page on their site and click through the examples on Classification and Labelling, Generation, Information Extraction, Translation and Conversion and Multimodal. Aleph Alpha are based in Europe, which allows customers with sensitive data to process their information in compliance with European regulations for data protection and security on a sovereign, European computing infrastructure.",https://docs.aleph-alpha.com/docs/Deprecated%20Luminous/Deprecated-Luminous/model-card/,
Luminous-supreme,Language,Germany,Aleph Alpha,2022-08-15,Industry,Language generation,Confident,512.0,API access,Not-defined,Unreleased,70000000000.0,1069300000000.0,2016.0,3.5461e+23,1877.1621580193864,13177.785,400.0,575206000000000.0,624000000000000.0,60000000000.0,"NVIDIA A100 SXM4 40 GB,NVIDIA A100 SXM4 80 GB",None,4,6472188,91700208.69015627,6747025.92,1.622080595583881e-08,204800.0,184867620719.35504,412876800.0,"The Luminous series is a family of large language models. Large language models are powerful technological tools that can process and produce text. These capabilities emerge during model “training” where the model is exposed to significant amounts of human text data. Similar to a person who deliberately absorbs information while reading a whole library and half of the internet, large language models acquire structural understanding (and not necessarily also knowledge) of language and accumulated information about the world.

The Luminous family currently consists of three vanilla models, which differ in complexity and ability. They are, from the smallest to the largest, luminous-base, luminous-extended and luminous-supreme. All Luminous models are trained in the five most commonly spoken European languages: English, German, French, Italian and Spanish.",https://docs.aleph-alpha.com/docs/introduction/model-card/,Model Card Luminous
Lyra-Fr 10B,Language,France,LightOn,2023-12-15,Industry,Language modeling/generation,Likely,4096.0,API access,Not-defined,Not-defined,10000000000.0,1340000000000.0,4320.0,2.7300000000001e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,4194304,3155569.9410964646,41392906.24,2.10684981684974e-08,1638400.0,13632062145.536728,7077888000.0,"We are thrilled to announce the availability of the LightOn Lyra-fr foundation model for customers using Amazon SageMaker. LightOn is a leader in building foundation models specializing in European languages. Lyra-fr is a state-of-the-art French language model that can be used to build conversational AI, copywriting tools, text classifiers, semantic search, and more. You can easily try out this model and use it with Amazon SageMaker JumpStart. JumpStart is the machine learning (ML) hub of SageMaker that provides access to foundation models in addition to built-in algorithms and end-to-end solution templates to help you quickly get started with ML.",https://aws.amazon.com/fr/blogs/machine-learning/lighton-lyra-fr-model-is-now-available-on-amazon-sagemaker/,LightOn Lyra-fr model is now available on Amazon SageMaker
Lyra-Fr 10B,Language,France,LightOn,2023-12-15,Industry,Question answering,Likely,4096.0,API access,Not-defined,Not-defined,10000000000.0,1340000000000.0,4320.0,2.7300000000001e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,4194304,3155569.9410964646,41392906.24,2.10684981684974e-08,1638400.0,13632062145.536728,7077888000.0,"We are thrilled to announce the availability of the LightOn Lyra-fr foundation model for customers using Amazon SageMaker. LightOn is a leader in building foundation models specializing in European languages. Lyra-fr is a state-of-the-art French language model that can be used to build conversational AI, copywriting tools, text classifiers, semantic search, and more. You can easily try out this model and use it with Amazon SageMaker JumpStart. JumpStart is the machine learning (ML) hub of SageMaker that provides access to foundation models in addition to built-in algorithms and end-to-end solution templates to help you quickly get started with ML.",https://aws.amazon.com/fr/blogs/machine-learning/lighton-lyra-fr-model-is-now-available-on-amazon-sagemaker/,LightOn Lyra-fr model is now available on Amazon SageMaker
Lyra-Fr 10B,Language,France,LightOn,2023-12-15,Industry,Semantic search,Likely,4096.0,API access,Not-defined,Not-defined,10000000000.0,1340000000000.0,4320.0,2.7300000000001e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,4194304,3155569.9410964646,41392906.24,2.10684981684974e-08,1638400.0,13632062145.536728,7077888000.0,"We are thrilled to announce the availability of the LightOn Lyra-fr foundation model for customers using Amazon SageMaker. LightOn is a leader in building foundation models specializing in European languages. Lyra-fr is a state-of-the-art French language model that can be used to build conversational AI, copywriting tools, text classifiers, semantic search, and more. You can easily try out this model and use it with Amazon SageMaker JumpStart. JumpStart is the machine learning (ML) hub of SageMaker that provides access to foundation models in addition to built-in algorithms and end-to-end solution templates to help you quickly get started with ML.",https://aws.amazon.com/fr/blogs/machine-learning/lighton-lyra-fr-model-is-now-available-on-amazon-sagemaker/,LightOn Lyra-fr model is now available on Amazon SageMaker
Lyra-Fr 10B,Language,France,LightOn,2023-12-15,Industry,Sentiment classification,Likely,4096.0,API access,Not-defined,Not-defined,10000000000.0,1340000000000.0,4320.0,2.7300000000001e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,4194304,3155569.9410964646,41392906.24,2.10684981684974e-08,1638400.0,13632062145.536728,7077888000.0,"We are thrilled to announce the availability of the LightOn Lyra-fr foundation model for customers using Amazon SageMaker. LightOn is a leader in building foundation models specializing in European languages. Lyra-fr is a state-of-the-art French language model that can be used to build conversational AI, copywriting tools, text classifiers, semantic search, and more. You can easily try out this model and use it with Amazon SageMaker JumpStart. JumpStart is the machine learning (ML) hub of SageMaker that provides access to foundation models in addition to built-in algorithms and end-to-end solution templates to help you quickly get started with ML.",https://aws.amazon.com/fr/blogs/machine-learning/lighton-lyra-fr-model-is-now-available-on-amazon-sagemaker/,LightOn Lyra-fr model is now available on Amazon SageMaker
Lyra-Fr 10B,Language,France,LightOn,2023-12-15,Industry,Text classification,Likely,4096.0,API access,Not-defined,Not-defined,10000000000.0,1340000000000.0,4320.0,2.7300000000001e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,4194304,3155569.9410964646,41392906.24,2.10684981684974e-08,1638400.0,13632062145.536728,7077888000.0,"We are thrilled to announce the availability of the LightOn Lyra-fr foundation model for customers using Amazon SageMaker. LightOn is a leader in building foundation models specializing in European languages. Lyra-fr is a state-of-the-art French language model that can be used to build conversational AI, copywriting tools, text classifiers, semantic search, and more. You can easily try out this model and use it with Amazon SageMaker JumpStart. JumpStart is the machine learning (ML) hub of SageMaker that provides access to foundation models in addition to built-in algorithms and end-to-end solution templates to help you quickly get started with ML.",https://aws.amazon.com/fr/blogs/machine-learning/lighton-lyra-fr-model-is-now-available-on-amazon-sagemaker/,LightOn Lyra-fr model is now available on Amazon SageMaker
Lyra-Fr 10B,Language,France,LightOn,2023-12-15,Industry,Text summarization,Likely,4096.0,API access,Not-defined,Not-defined,10000000000.0,1340000000000.0,4320.0,2.7300000000001e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,4194304,3155569.9410964646,41392906.24,2.10684981684974e-08,1638400.0,13632062145.536728,7077888000.0,"We are thrilled to announce the availability of the LightOn Lyra-fr foundation model for customers using Amazon SageMaker. LightOn is a leader in building foundation models specializing in European languages. Lyra-fr is a state-of-the-art French language model that can be used to build conversational AI, copywriting tools, text classifiers, semantic search, and more. You can easily try out this model and use it with Amazon SageMaker JumpStart. JumpStart is the machine learning (ML) hub of SageMaker that provides access to foundation models in addition to built-in algorithms and end-to-end solution templates to help you quickly get started with ML.",https://aws.amazon.com/fr/blogs/machine-learning/lighton-lyra-fr-model-is-now-available-on-amazon-sagemaker/,LightOn Lyra-fr model is now available on Amazon SageMaker
MM1-30B,Language,USA,Apple,2024-03-14,Industry,Chat,Likely,512.0,Unreleased,"Conceptual Captions (CC3M),Conceptual Captions 12M (CC12M),COYO-700M,Unspecified unreleased,OBELICS",Unreleased,30000000000.0,1500000000000.0,278.4,4.86e+23,1877.1621580193864,29744.94,550.0,1213583000000000.0,1301500000000000.0,60000000000.0,NVIDIA H100 SXM5 80GB,SOTA improvement,1,2359296,788306.6236983632,15229409.28,2.4970843621399178e-08,281600.0,219464564.0376243,78397440.0,"In this work, we discuss building performant Multimodal Large Language Models (MLLMs). In particular, we study the importance of various architecture components and data choices. Through careful and comprehensive ablations of the image encoder, the vision language connector, and various pre-training data choices, we identified several crucial design lessons. For example, we demonstrate that for large-scale multimodal pre-training using a careful mix of image-caption, interleaved image-text, and text-only data is crucial for achieving state-of-the-art (SOTA) few-shot results across multiple benchmarks, compared to other published pre-training results. Further, we show that the image encoder together with image resolution and the image token count has substantial impact, while the vision-language connector design is of comparatively negligible importance. By scaling up the presented recipe, we build MM1, a family of multimodal models up to 30B parameters, including both dense models and mixture-of-experts (MoE) variants, that are SOTA in pre-training metrics and achieve competitive performance after supervised fine-tuning on a range of established multimodal benchmarks. Thanks to large-scale pre-training, MM1 enjoys appealing properties such as enhanced in-context learning, and multi-image reasoning, enabling few-shot chain-of-thought prompting.",https://arxiv.org/abs/2403.09611,"MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training"
MM1-30B,Language,USA,Apple,2024-03-14,Industry,Image captioning,Likely,512.0,Unreleased,"Conceptual Captions (CC3M),Conceptual Captions 12M (CC12M),COYO-700M,Unspecified unreleased,OBELICS",Unreleased,30000000000.0,1500000000000.0,278.4,4.86e+23,1877.1621580193864,29744.94,550.0,1213583000000000.0,1301500000000000.0,60000000000.0,NVIDIA H100 SXM5 80GB,SOTA improvement,1,2359296,788306.6236983632,15229409.28,2.4970843621399178e-08,281600.0,219464564.0376243,78397440.0,"In this work, we discuss building performant Multimodal Large Language Models (MLLMs). In particular, we study the importance of various architecture components and data choices. Through careful and comprehensive ablations of the image encoder, the vision language connector, and various pre-training data choices, we identified several crucial design lessons. For example, we demonstrate that for large-scale multimodal pre-training using a careful mix of image-caption, interleaved image-text, and text-only data is crucial for achieving state-of-the-art (SOTA) few-shot results across multiple benchmarks, compared to other published pre-training results. Further, we show that the image encoder together with image resolution and the image token count has substantial impact, while the vision-language connector design is of comparatively negligible importance. By scaling up the presented recipe, we build MM1, a family of multimodal models up to 30B parameters, including both dense models and mixture-of-experts (MoE) variants, that are SOTA in pre-training metrics and achieve competitive performance after supervised fine-tuning on a range of established multimodal benchmarks. Thanks to large-scale pre-training, MM1 enjoys appealing properties such as enhanced in-context learning, and multi-image reasoning, enabling few-shot chain-of-thought prompting.",https://arxiv.org/abs/2403.09611,"MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training"
MM1-30B,Multimodal,USA,Apple,2024-03-14,Industry,Chat,Likely,512.0,Unreleased,"Conceptual Captions (CC3M),Conceptual Captions 12M (CC12M),COYO-700M,Unspecified unreleased,OBELICS",Unreleased,30000000000.0,1500000000000.0,278.4,4.86e+23,1877.1621580193864,29744.94,550.0,1213583000000000.0,1301500000000000.0,60000000000.0,NVIDIA H100 SXM5 80GB,SOTA improvement,1,2359296,788306.6236983632,15229409.28,2.4970843621399178e-08,281600.0,219464564.0376243,78397440.0,"In this work, we discuss building performant Multimodal Large Language Models (MLLMs). In particular, we study the importance of various architecture components and data choices. Through careful and comprehensive ablations of the image encoder, the vision language connector, and various pre-training data choices, we identified several crucial design lessons. For example, we demonstrate that for large-scale multimodal pre-training using a careful mix of image-caption, interleaved image-text, and text-only data is crucial for achieving state-of-the-art (SOTA) few-shot results across multiple benchmarks, compared to other published pre-training results. Further, we show that the image encoder together with image resolution and the image token count has substantial impact, while the vision-language connector design is of comparatively negligible importance. By scaling up the presented recipe, we build MM1, a family of multimodal models up to 30B parameters, including both dense models and mixture-of-experts (MoE) variants, that are SOTA in pre-training metrics and achieve competitive performance after supervised fine-tuning on a range of established multimodal benchmarks. Thanks to large-scale pre-training, MM1 enjoys appealing properties such as enhanced in-context learning, and multi-image reasoning, enabling few-shot chain-of-thought prompting.",https://arxiv.org/abs/2403.09611,"MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training"
MM1-30B,Multimodal,USA,Apple,2024-03-14,Industry,Image captioning,Likely,512.0,Unreleased,"Conceptual Captions (CC3M),Conceptual Captions 12M (CC12M),COYO-700M,Unspecified unreleased,OBELICS",Unreleased,30000000000.0,1500000000000.0,278.4,4.86e+23,1877.1621580193864,29744.94,550.0,1213583000000000.0,1301500000000000.0,60000000000.0,NVIDIA H100 SXM5 80GB,SOTA improvement,1,2359296,788306.6236983632,15229409.28,2.4970843621399178e-08,281600.0,219464564.0376243,78397440.0,"In this work, we discuss building performant Multimodal Large Language Models (MLLMs). In particular, we study the importance of various architecture components and data choices. Through careful and comprehensive ablations of the image encoder, the vision language connector, and various pre-training data choices, we identified several crucial design lessons. For example, we demonstrate that for large-scale multimodal pre-training using a careful mix of image-caption, interleaved image-text, and text-only data is crucial for achieving state-of-the-art (SOTA) few-shot results across multiple benchmarks, compared to other published pre-training results. Further, we show that the image encoder together with image resolution and the image token count has substantial impact, while the vision-language connector design is of comparatively negligible importance. By scaling up the presented recipe, we build MM1, a family of multimodal models up to 30B parameters, including both dense models and mixture-of-experts (MoE) variants, that are SOTA in pre-training metrics and achieve competitive performance after supervised fine-tuning on a range of established multimodal benchmarks. Thanks to large-scale pre-training, MM1 enjoys appealing properties such as enhanced in-context learning, and multi-image reasoning, enabling few-shot chain-of-thought prompting.",https://arxiv.org/abs/2403.09611,"MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training"
MM1-30B,Vision,USA,Apple,2024-03-14,Industry,Chat,Likely,512.0,Unreleased,"Conceptual Captions (CC3M),Conceptual Captions 12M (CC12M),COYO-700M,Unspecified unreleased,OBELICS",Unreleased,30000000000.0,1500000000000.0,278.4,4.86e+23,1877.1621580193864,29744.94,550.0,1213583000000000.0,1301500000000000.0,60000000000.0,NVIDIA H100 SXM5 80GB,SOTA improvement,1,2359296,788306.6236983632,15229409.28,2.4970843621399178e-08,281600.0,219464564.0376243,78397440.0,"In this work, we discuss building performant Multimodal Large Language Models (MLLMs). In particular, we study the importance of various architecture components and data choices. Through careful and comprehensive ablations of the image encoder, the vision language connector, and various pre-training data choices, we identified several crucial design lessons. For example, we demonstrate that for large-scale multimodal pre-training using a careful mix of image-caption, interleaved image-text, and text-only data is crucial for achieving state-of-the-art (SOTA) few-shot results across multiple benchmarks, compared to other published pre-training results. Further, we show that the image encoder together with image resolution and the image token count has substantial impact, while the vision-language connector design is of comparatively negligible importance. By scaling up the presented recipe, we build MM1, a family of multimodal models up to 30B parameters, including both dense models and mixture-of-experts (MoE) variants, that are SOTA in pre-training metrics and achieve competitive performance after supervised fine-tuning on a range of established multimodal benchmarks. Thanks to large-scale pre-training, MM1 enjoys appealing properties such as enhanced in-context learning, and multi-image reasoning, enabling few-shot chain-of-thought prompting.",https://arxiv.org/abs/2403.09611,"MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training"
MM1-30B,Vision,USA,Apple,2024-03-14,Industry,Image captioning,Likely,512.0,Unreleased,"Conceptual Captions (CC3M),Conceptual Captions 12M (CC12M),COYO-700M,Unspecified unreleased,OBELICS",Unreleased,30000000000.0,1500000000000.0,278.4,4.86e+23,1877.1621580193864,29744.94,550.0,1213583000000000.0,1301500000000000.0,60000000000.0,NVIDIA H100 SXM5 80GB,SOTA improvement,1,2359296,788306.6236983632,15229409.28,2.4970843621399178e-08,281600.0,219464564.0376243,78397440.0,"In this work, we discuss building performant Multimodal Large Language Models (MLLMs). In particular, we study the importance of various architecture components and data choices. Through careful and comprehensive ablations of the image encoder, the vision language connector, and various pre-training data choices, we identified several crucial design lessons. For example, we demonstrate that for large-scale multimodal pre-training using a careful mix of image-caption, interleaved image-text, and text-only data is crucial for achieving state-of-the-art (SOTA) few-shot results across multiple benchmarks, compared to other published pre-training results. Further, we show that the image encoder together with image resolution and the image token count has substantial impact, while the vision-language connector design is of comparatively negligible importance. By scaling up the presented recipe, we build MM1, a family of multimodal models up to 30B parameters, including both dense models and mixture-of-experts (MoE) variants, that are SOTA in pre-training metrics and achieve competitive performance after supervised fine-tuning on a range of established multimodal benchmarks. Thanks to large-scale pre-training, MM1 enjoys appealing properties such as enhanced in-context learning, and multi-image reasoning, enabling few-shot chain-of-thought prompting.",https://arxiv.org/abs/2403.09611,"MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training"
MPT-30B,Language,USA,MosaicML,2023-06-22,Industry,Code generation,Confident,512.0,Open weights (unrestricted),"mC4,C4,RedPajama,The Stack",Open source,30000000000.0,1050000000000.0,278.4,1.8900000000001e+23,1877.1621580193864,29744.94,550.0,1213583000000000.0,1301500000000000.0,60000000000.0,NVIDIA H100 SXM5 80GB,SOTA improvement,2,2000000,788306.6236983632,15229409.28,6.421074074073735e-08,281600.0,219464564.0376243,78397440.0,"MPT-30B is a decoder-style transformer pretrained from scratch on 1T tokens of English text and code. This model was trained by MosaicML.

MPT-30B is part of the family of Mosaic Pretrained Transformer (MPT) models, which use a modified transformer architecture optimized for efficient training and inference.

MPT-30B comes with special features that differentiate it from other LLMs, including an 8k token context window (which can be further extended via finetuning; see MPT-7B-StoryWriter), support for context-length extrapolation via ALiBi, and efficient inference + training via FlashAttention. It also has strong coding abilities thanks to its pretraining mix. MPT models can also be served efficiently with both standard HuggingFace pipelines and NVIDIA's FasterTransformer. The size of MPT-30B was also specifically chosen to make it easy to deploy on a single GPU—either 1xA100-80GB in 16-bit precision or 1xA100-40GB in 8-bit precision.",https://huggingface.co/mosaicml/mpt-30b,
MPT-30B,Language,USA,MosaicML,2023-06-22,Industry,Language generation,Confident,512.0,Open weights (unrestricted),"mC4,C4,RedPajama,The Stack",Open source,30000000000.0,1050000000000.0,278.4,1.8900000000001e+23,1877.1621580193864,29744.94,550.0,1213583000000000.0,1301500000000000.0,60000000000.0,NVIDIA H100 SXM5 80GB,SOTA improvement,2,2000000,788306.6236983632,15229409.28,6.421074074073735e-08,281600.0,219464564.0376243,78397440.0,"MPT-30B is a decoder-style transformer pretrained from scratch on 1T tokens of English text and code. This model was trained by MosaicML.

MPT-30B is part of the family of Mosaic Pretrained Transformer (MPT) models, which use a modified transformer architecture optimized for efficient training and inference.

MPT-30B comes with special features that differentiate it from other LLMs, including an 8k token context window (which can be further extended via finetuning; see MPT-7B-StoryWriter), support for context-length extrapolation via ALiBi, and efficient inference + training via FlashAttention. It also has strong coding abilities thanks to its pretraining mix. MPT models can also be served efficiently with both standard HuggingFace pipelines and NVIDIA's FasterTransformer. The size of MPT-30B was also specifically chosen to make it easy to deploy on a single GPU—either 1xA100-80GB in 16-bit precision or 1xA100-40GB in 8-bit precision.",https://huggingface.co/mosaicml/mpt-30b,
Make-A-Video,Video,USA,Meta AI,2022-09-29,Industry,Video generation,Unknown,6144.0,Not-defined,"LAION,WebVid-10M,HD-VILA-100M",Not-defined,30000000000.0,26600000000.0,331.0,1.65e+24,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,SOTA improvement,4,6472188,788306.6236983632,273345822.71999997,1.1224e-08,4300800.0,260929492.44415823,1423564800.0,"We propose Make-A-Video -- an approach for directly translating the tremendous recent progress in Text-to-Image (T2I) generation to Text-to-Video (T2V). Our intuition is simple: learn what the world looks like and how it is described from paired text-image data, and learn how the world moves from unsupervised video footage. Make-A-Video has three advantages: (1) it accelerates training of the T2V model (it does not need to learn visual and multimodal representations from scratch), (2) it does not require paired text-video data, and (3) the generated videos inherit the vastness (diversity in aesthetic, fantastical depictions, etc.) of today's image generation models. We design a simple yet effective way to build on T2I models with novel and effective spatial-temporal modules. First, we decompose the full temporal U-Net and attention tensors and approximate them in space and time. Second, we design a spatial temporal pipeline to generate high resolution and frame rate videos with a video decoder, interpolation model and two super resolution models that can enable various applications besides T2V. In all aspects, spatial and temporal resolution, faithfulness to text, and quality, Make-A-Video sets the new state-of-the-art in text-to-video generation, as determined by both qualitative and quantitative measures.",https://arxiv.org/abs/2209.14792,Make-A-Video: Text-to-Video Generation without Text-Video Data
Mamba2-Hybrid,Language,USA,NVIDIA,2024-06-12,Industry,Language modeling/generation,Likely,1024.0,Open weights (unrestricted),Unspecified unreleased,Open source,8660000000.0,3500000000000.0,1282.4326530612243,1.8186e+23,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,4,6472188,788306.6236983632,45557637.12,1.018343780930386e-07,716800.0,1010950154.8552281,919247725.7142856,"Selective state-space models (SSMs) like Mamba overcome some of the shortcomings of Transformers, such as quadratic computational complexity with sequence length and large inference-time memory requirements from the key-value cache. Moreover, recent studies have shown that SSMs can match or exceed the language modeling capabilities of Transformers, making them an attractive alternative. In a controlled setting (e.g., same data), however, studies so far have only presented small scale experiments comparing SSMs to Transformers. To understand the strengths and weaknesses of these architectures at larger scales, we present a direct comparison between 8B-parameter Mamba, Mamba-2, and Transformer models trained on the same datasets of up to 3.5T tokens. We also compare these models to a hybrid architecture consisting of 43% Mamba-2, 7% attention, and 50% MLP layers (Mamba-2-Hybrid). Using a diverse set of tasks, we answer the question of whether Mamba models can match Transformers at larger training budgets. Our results show that while pure SSMs match or exceed Transformers on many tasks, they lag behind Transformers on tasks which require strong copying or in-context learning abilities (e.g., 5-shot MMLU, Phonebook) or long-context reasoning. In contrast, we find that the 8B Mamba-2-Hybrid exceeds the 8B Transformer on all 12 standard tasks we evaluated (+2.65 points on average) and is predicted to be up to 8x faster when generating tokens at inference time. To validate long-context capabilities, we provide additional experiments evaluating variants of the Mamba-2-Hybrid and Transformer extended to support 16K, 32K, and 128K sequences. On an additional 23 long-context tasks, the hybrid model continues to closely match or exceed the Transformer on average. To enable further study, we release the checkpoints as well as the code used to train our models as part of NVIDIA's Megatron-LM project.",https://arxiv.org/abs/2406.07887,An Empirical Study of Mamba-based Language Models
Mamba2-Hybrid,Language,USA,NVIDIA,2024-06-12,Industry,Question answering,Likely,1024.0,Open weights (unrestricted),Unspecified unreleased,Open source,8660000000.0,3500000000000.0,1282.4326530612243,1.8186e+23,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,4,6472188,788306.6236983632,45557637.12,1.018343780930386e-07,716800.0,1010950154.8552281,919247725.7142856,"Selective state-space models (SSMs) like Mamba overcome some of the shortcomings of Transformers, such as quadratic computational complexity with sequence length and large inference-time memory requirements from the key-value cache. Moreover, recent studies have shown that SSMs can match or exceed the language modeling capabilities of Transformers, making them an attractive alternative. In a controlled setting (e.g., same data), however, studies so far have only presented small scale experiments comparing SSMs to Transformers. To understand the strengths and weaknesses of these architectures at larger scales, we present a direct comparison between 8B-parameter Mamba, Mamba-2, and Transformer models trained on the same datasets of up to 3.5T tokens. We also compare these models to a hybrid architecture consisting of 43% Mamba-2, 7% attention, and 50% MLP layers (Mamba-2-Hybrid). Using a diverse set of tasks, we answer the question of whether Mamba models can match Transformers at larger training budgets. Our results show that while pure SSMs match or exceed Transformers on many tasks, they lag behind Transformers on tasks which require strong copying or in-context learning abilities (e.g., 5-shot MMLU, Phonebook) or long-context reasoning. In contrast, we find that the 8B Mamba-2-Hybrid exceeds the 8B Transformer on all 12 standard tasks we evaluated (+2.65 points on average) and is predicted to be up to 8x faster when generating tokens at inference time. To validate long-context capabilities, we provide additional experiments evaluating variants of the Mamba-2-Hybrid and Transformer extended to support 16K, 32K, and 128K sequences. On an additional 23 long-context tasks, the hybrid model continues to closely match or exceed the Transformer on average. To enable further study, we release the checkpoints as well as the code used to train our models as part of NVIDIA's Megatron-LM project.",https://arxiv.org/abs/2406.07887,An Empirical Study of Mamba-based Language Models
MathGPT,Language,China,TAL Education Group (Xueersi),2023-08-24,Industry,Quantitative reasoning,Unknown,25000.0,Hosted access (no API),Not-defined,Unreleased,50558868480.0,4900000000000.0,2280.0,2.578e+24,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Historical significance,2,4200000,788306.6236983632,375000000.0,2.231210240496509e-09,10000000.0,1797339102.032268,22800000000.0,"During its 20th-anniversary event, TAL Education Group launched the public beta testing of its innovative mathematical large model, MathGPT. This LLM model is designed primarily for global mathematics enthusiasts and research institutions, marking a significant milestone as China’s first large model tailored for mathematics.",https://www.gizmochina.com/2023/08/24/mathgpt-launch-public-beta-next-gen-math-assistant/,TAL’s MathGPT launches public beta: your next-gen math assistant
Meena,Language,USA,Google Brain,2020-01-28,Industry,Text autocompletion,Confident,1024.0,Unreleased,Not-defined,Unreleased,2600000000.0,40000000000.0,720.0,1.12e+23,1877.1621580193864,6589.05,450.0,123000000000000.0,223459000000000.0,32000000000.0,Google TPU v3,SOTA improvement,164,82655,1032664.6317417204,6747187.2,0.34306622,460800.0,743518534.8540387,331776000.0,"We present Meena, a multi-turn open-domain chatbot trained end-to-end on data mined and filtered from public domain social media conversations. This 2.6B parameter neural network is simply trained to minimize perplexity of the next token. We also propose a human evaluation metric called Sensibleness and Specificity Average (SSA), which captures key elements of a human-like multi-turn conversation. Our experiments show strong correlation between perplexity and SSA. The fact that the best perplexity end-to-end trained Meena scores high on SSA (72% on multi-turn evaluation) suggests that a human-level SSA of 86% is potentially within reach if we can better optimize perplexity. Additionally, the full version of Meena (with a filtering mechanism and tuned decoding) scores 79% SSA, 23% higher in absolute SSA than the existing chatbots we evaluated.",https://arxiv.org/abs/2001.09977,Towards a Human-like Open-Domain Chatbot
MegaScale (175B),Language,China,ByteDance,2024-02-23,Academia,Language modeling/generation,Confident,12288.0,Unreleased,Not-defined,Unreleased,175000000000.0,225000000000.0,42.0,2.7385671436e+23,2166.3,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Training cost,0,3932160,788306.6236983632,124178718.72,0.48,4915200.0,33108878.195331253,206438400.0,"We present the design, implementation and engineering experience in building and deploying MegaScale, a production system for training large language models (LLMs) at the scale of more than 10,000 GPUs. Training LLMs at this scale brings unprecedented challenges to training efficiency and stability. We take a full-stack approach that co-designs the algorithmic and system components across model block and optimizer design, computation and communication overlapping, operator optimization, data pipeline, and network performance tuning. Maintaining high efficiency throughout the training process (i.e., stability) is an important consideration in production given the long extent of LLM training jobs. Many hard stability issues only emerge at large scale, and in-depth observability is the key to address them. We develop a set of diagnosis tools to monitor system components and events deep in the stack, identify root causes, and derive effective techniques to achieve fault tolerance and mitigate stragglers. MegaScale achieves 55.2% Model FLOPs Utilization (MFU) when training a 175B LLM model on 12,288 GPUs, improving the MFU by 1.34x compared to Megatron-LM. We share our operational experience in identifying and fixing failures and stragglers. We hope by articulating the problems and sharing our experience from a systems perspective, this work can inspire future LLM systems research.",https://arxiv.org/abs/2402.15627,"MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs"
MegaScale (175B),Language,China,ByteDance,2024-02-23,Industry,Language modeling/generation,Confident,12288.0,Unreleased,Not-defined,Unreleased,175000000000.0,225000000000.0,42.0,2.7385671436e+23,2166.3,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Training cost,0,3932160,788306.6236983632,124178718.72,0.48,4915200.0,33108878.195331253,206438400.0,"We present the design, implementation and engineering experience in building and deploying MegaScale, a production system for training large language models (LLMs) at the scale of more than 10,000 GPUs. Training LLMs at this scale brings unprecedented challenges to training efficiency and stability. We take a full-stack approach that co-designs the algorithmic and system components across model block and optimizer design, computation and communication overlapping, operator optimization, data pipeline, and network performance tuning. Maintaining high efficiency throughout the training process (i.e., stability) is an important consideration in production given the long extent of LLM training jobs. Many hard stability issues only emerge at large scale, and in-depth observability is the key to address them. We develop a set of diagnosis tools to monitor system components and events deep in the stack, identify root causes, and derive effective techniques to achieve fault tolerance and mitigate stragglers. MegaScale achieves 55.2% Model FLOPs Utilization (MFU) when training a 175B LLM model on 12,288 GPUs, improving the MFU by 1.34x compared to Megatron-LM. We share our operational experience in identifying and fixing failures and stragglers. We hope by articulating the problems and sharing our experience from a systems perspective, this work can inspire future LLM systems research.",https://arxiv.org/abs/2402.15627,"MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs"
MegaScale (175B),Language,China,Peking University,2024-02-23,Academia,Language modeling/generation,Confident,12288.0,Unreleased,Not-defined,Unreleased,175000000000.0,225000000000.0,42.0,2.7385671436e+23,2166.3,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Training cost,0,3932160,788306.6236983632,124178718.72,0.48,4915200.0,33108878.195331253,206438400.0,"We present the design, implementation and engineering experience in building and deploying MegaScale, a production system for training large language models (LLMs) at the scale of more than 10,000 GPUs. Training LLMs at this scale brings unprecedented challenges to training efficiency and stability. We take a full-stack approach that co-designs the algorithmic and system components across model block and optimizer design, computation and communication overlapping, operator optimization, data pipeline, and network performance tuning. Maintaining high efficiency throughout the training process (i.e., stability) is an important consideration in production given the long extent of LLM training jobs. Many hard stability issues only emerge at large scale, and in-depth observability is the key to address them. We develop a set of diagnosis tools to monitor system components and events deep in the stack, identify root causes, and derive effective techniques to achieve fault tolerance and mitigate stragglers. MegaScale achieves 55.2% Model FLOPs Utilization (MFU) when training a 175B LLM model on 12,288 GPUs, improving the MFU by 1.34x compared to Megatron-LM. We share our operational experience in identifying and fixing failures and stragglers. We hope by articulating the problems and sharing our experience from a systems perspective, this work can inspire future LLM systems research.",https://arxiv.org/abs/2402.15627,"MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs"
MegaScale (175B),Language,China,Peking University,2024-02-23,Industry,Language modeling/generation,Confident,12288.0,Unreleased,Not-defined,Unreleased,175000000000.0,225000000000.0,42.0,2.7385671436e+23,2166.3,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Training cost,0,3932160,788306.6236983632,124178718.72,0.48,4915200.0,33108878.195331253,206438400.0,"We present the design, implementation and engineering experience in building and deploying MegaScale, a production system for training large language models (LLMs) at the scale of more than 10,000 GPUs. Training LLMs at this scale brings unprecedented challenges to training efficiency and stability. We take a full-stack approach that co-designs the algorithmic and system components across model block and optimizer design, computation and communication overlapping, operator optimization, data pipeline, and network performance tuning. Maintaining high efficiency throughout the training process (i.e., stability) is an important consideration in production given the long extent of LLM training jobs. Many hard stability issues only emerge at large scale, and in-depth observability is the key to address them. We develop a set of diagnosis tools to monitor system components and events deep in the stack, identify root causes, and derive effective techniques to achieve fault tolerance and mitigate stragglers. MegaScale achieves 55.2% Model FLOPs Utilization (MFU) when training a 175B LLM model on 12,288 GPUs, improving the MFU by 1.34x compared to Megatron-LM. We share our operational experience in identifying and fixing failures and stragglers. We hope by articulating the problems and sharing our experience from a systems perspective, this work can inspire future LLM systems research.",https://arxiv.org/abs/2402.15627,"MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs"
MegaScale (530B),Language,China,ByteDance,2024-02-23,Academia,Language modeling/generation,Confident,11200.0,Unreleased,Not-defined,Unreleased,530000000000.0,300000000000.0,117.9,9.6910000000001e+23,2166.3,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Training cost,0,3932160,788306.6236983632,113183728.0,0.48,4480000.0,92941350.93403703,528192000.0,"We present the design, implementation and engineering experience in building and deploying MegaScale, a production system for training large language models (LLMs) at the scale of more than 10,000 GPUs. Training LLMs at this scale brings unprecedented challenges to training efficiency and stability. We take a full-stack approach that co-designs the algorithmic and system components across model block and optimizer design, computation and communication overlapping, operator optimization, data pipeline, and network performance tuning. Maintaining high efficiency throughout the training process (i.e., stability) is an important consideration in production given the long extent of LLM training jobs. Many hard stability issues only emerge at large scale, and in-depth observability is the key to address them. We develop a set of diagnosis tools to monitor system components and events deep in the stack, identify root causes, and derive effective techniques to achieve fault tolerance and mitigate stragglers. MegaScale achieves 55.2% Model FLOPs Utilization (MFU) when training a 175B LLM model on 12,288 GPUs, improving the MFU by 1.34x compared to Megatron-LM. We share our operational experience in identifying and fixing failures and stragglers. We hope by articulating the problems and sharing our experience from a systems perspective, this work can inspire future LLM systems research.",https://arxiv.org/abs/2402.15627,"MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs"
MegaScale (530B),Language,China,ByteDance,2024-02-23,Industry,Language modeling/generation,Confident,11200.0,Unreleased,Not-defined,Unreleased,530000000000.0,300000000000.0,117.9,9.6910000000001e+23,2166.3,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Training cost,0,3932160,788306.6236983632,113183728.0,0.48,4480000.0,92941350.93403703,528192000.0,"We present the design, implementation and engineering experience in building and deploying MegaScale, a production system for training large language models (LLMs) at the scale of more than 10,000 GPUs. Training LLMs at this scale brings unprecedented challenges to training efficiency and stability. We take a full-stack approach that co-designs the algorithmic and system components across model block and optimizer design, computation and communication overlapping, operator optimization, data pipeline, and network performance tuning. Maintaining high efficiency throughout the training process (i.e., stability) is an important consideration in production given the long extent of LLM training jobs. Many hard stability issues only emerge at large scale, and in-depth observability is the key to address them. We develop a set of diagnosis tools to monitor system components and events deep in the stack, identify root causes, and derive effective techniques to achieve fault tolerance and mitigate stragglers. MegaScale achieves 55.2% Model FLOPs Utilization (MFU) when training a 175B LLM model on 12,288 GPUs, improving the MFU by 1.34x compared to Megatron-LM. We share our operational experience in identifying and fixing failures and stragglers. We hope by articulating the problems and sharing our experience from a systems perspective, this work can inspire future LLM systems research.",https://arxiv.org/abs/2402.15627,"MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs"
MegaScale (530B),Language,China,Peking University,2024-02-23,Academia,Language modeling/generation,Confident,11200.0,Unreleased,Not-defined,Unreleased,530000000000.0,300000000000.0,117.9,9.6910000000001e+23,2166.3,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Training cost,0,3932160,788306.6236983632,113183728.0,0.48,4480000.0,92941350.93403703,528192000.0,"We present the design, implementation and engineering experience in building and deploying MegaScale, a production system for training large language models (LLMs) at the scale of more than 10,000 GPUs. Training LLMs at this scale brings unprecedented challenges to training efficiency and stability. We take a full-stack approach that co-designs the algorithmic and system components across model block and optimizer design, computation and communication overlapping, operator optimization, data pipeline, and network performance tuning. Maintaining high efficiency throughout the training process (i.e., stability) is an important consideration in production given the long extent of LLM training jobs. Many hard stability issues only emerge at large scale, and in-depth observability is the key to address them. We develop a set of diagnosis tools to monitor system components and events deep in the stack, identify root causes, and derive effective techniques to achieve fault tolerance and mitigate stragglers. MegaScale achieves 55.2% Model FLOPs Utilization (MFU) when training a 175B LLM model on 12,288 GPUs, improving the MFU by 1.34x compared to Megatron-LM. We share our operational experience in identifying and fixing failures and stragglers. We hope by articulating the problems and sharing our experience from a systems perspective, this work can inspire future LLM systems research.",https://arxiv.org/abs/2402.15627,"MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs"
MegaScale (530B),Language,China,Peking University,2024-02-23,Industry,Language modeling/generation,Confident,11200.0,Unreleased,Not-defined,Unreleased,530000000000.0,300000000000.0,117.9,9.6910000000001e+23,2166.3,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Training cost,0,3932160,788306.6236983632,113183728.0,0.48,4480000.0,92941350.93403703,528192000.0,"We present the design, implementation and engineering experience in building and deploying MegaScale, a production system for training large language models (LLMs) at the scale of more than 10,000 GPUs. Training LLMs at this scale brings unprecedented challenges to training efficiency and stability. We take a full-stack approach that co-designs the algorithmic and system components across model block and optimizer design, computation and communication overlapping, operator optimization, data pipeline, and network performance tuning. Maintaining high efficiency throughout the training process (i.e., stability) is an important consideration in production given the long extent of LLM training jobs. Many hard stability issues only emerge at large scale, and in-depth observability is the key to address them. We develop a set of diagnosis tools to monitor system components and events deep in the stack, identify root causes, and derive effective techniques to achieve fault tolerance and mitigate stragglers. MegaScale achieves 55.2% Model FLOPs Utilization (MFU) when training a 175B LLM model on 12,288 GPUs, improving the MFU by 1.34x compared to Megatron-LM. We share our operational experience in identifying and fixing failures and stragglers. We hope by articulating the problems and sharing our experience from a systems perspective, this work can inspire future LLM systems research.",https://arxiv.org/abs/2402.15627,"MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs"
MegaScale (Production),Language,China,ByteDance,2024-02-23,Academia,Language modeling/generation,Speculative,12288.0,Unreleased,Not-defined,Unreleased,530000000000.0,300000000000.0,504.0,3.9e+24,2166.3,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,SOTA improvement,0,3932160,10849658.51586543,124178718.72,0.48,4915200.0,5468227891.996177,2477260800.0,"We present the design, implementation and engineering experience in building and deploying MegaScale, a production system for training large language models (LLMs) at the scale of more than 10,000 GPUs. Training LLMs at this scale brings unprecedented challenges to training efficiency and stability. We take a full-stack approach that co-designs the algorithmic and system components across model block and optimizer design, computation and communication overlapping, operator optimization, data pipeline, and network performance tuning. Maintaining high efficiency throughout the training process (i.e., stability) is an important consideration in production given the long extent of LLM training jobs. Many hard stability issues only emerge at large scale, and in-depth observability is the key to address them. We develop a set of diagnosis tools to monitor system components and events deep in the stack, identify root causes, and derive effective techniques to achieve fault tolerance and mitigate stragglers. MegaScale achieves 55.2% Model FLOPs Utilization (MFU) when training a 175B LLM model on 12,288 GPUs, improving the MFU by 1.34x compared to Megatron-LM. We share our operational experience in identifying and fixing failures and stragglers. We hope by articulating the problems and sharing our experience from a systems perspective, this work can inspire future LLM systems research.",https://arxiv.org/abs/2402.15627,"MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs"
MegaScale (Production),Language,China,ByteDance,2024-02-23,Industry,Language modeling/generation,Speculative,12288.0,Unreleased,Not-defined,Unreleased,530000000000.0,300000000000.0,504.0,3.9e+24,2166.3,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,SOTA improvement,0,3932160,10849658.51586543,124178718.72,0.48,4915200.0,5468227891.996177,2477260800.0,"We present the design, implementation and engineering experience in building and deploying MegaScale, a production system for training large language models (LLMs) at the scale of more than 10,000 GPUs. Training LLMs at this scale brings unprecedented challenges to training efficiency and stability. We take a full-stack approach that co-designs the algorithmic and system components across model block and optimizer design, computation and communication overlapping, operator optimization, data pipeline, and network performance tuning. Maintaining high efficiency throughout the training process (i.e., stability) is an important consideration in production given the long extent of LLM training jobs. Many hard stability issues only emerge at large scale, and in-depth observability is the key to address them. We develop a set of diagnosis tools to monitor system components and events deep in the stack, identify root causes, and derive effective techniques to achieve fault tolerance and mitigate stragglers. MegaScale achieves 55.2% Model FLOPs Utilization (MFU) when training a 175B LLM model on 12,288 GPUs, improving the MFU by 1.34x compared to Megatron-LM. We share our operational experience in identifying and fixing failures and stragglers. We hope by articulating the problems and sharing our experience from a systems perspective, this work can inspire future LLM systems research.",https://arxiv.org/abs/2402.15627,"MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs"
MegaScale (Production),Language,China,Peking University,2024-02-23,Academia,Language modeling/generation,Speculative,12288.0,Unreleased,Not-defined,Unreleased,530000000000.0,300000000000.0,504.0,3.9e+24,2166.3,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,SOTA improvement,0,3932160,10849658.51586543,124178718.72,0.48,4915200.0,5468227891.996177,2477260800.0,"We present the design, implementation and engineering experience in building and deploying MegaScale, a production system for training large language models (LLMs) at the scale of more than 10,000 GPUs. Training LLMs at this scale brings unprecedented challenges to training efficiency and stability. We take a full-stack approach that co-designs the algorithmic and system components across model block and optimizer design, computation and communication overlapping, operator optimization, data pipeline, and network performance tuning. Maintaining high efficiency throughout the training process (i.e., stability) is an important consideration in production given the long extent of LLM training jobs. Many hard stability issues only emerge at large scale, and in-depth observability is the key to address them. We develop a set of diagnosis tools to monitor system components and events deep in the stack, identify root causes, and derive effective techniques to achieve fault tolerance and mitigate stragglers. MegaScale achieves 55.2% Model FLOPs Utilization (MFU) when training a 175B LLM model on 12,288 GPUs, improving the MFU by 1.34x compared to Megatron-LM. We share our operational experience in identifying and fixing failures and stragglers. We hope by articulating the problems and sharing our experience from a systems perspective, this work can inspire future LLM systems research.",https://arxiv.org/abs/2402.15627,"MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs"
MegaScale (Production),Language,China,Peking University,2024-02-23,Industry,Language modeling/generation,Speculative,12288.0,Unreleased,Not-defined,Unreleased,530000000000.0,300000000000.0,504.0,3.9e+24,2166.3,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,SOTA improvement,0,3932160,10849658.51586543,124178718.72,0.48,4915200.0,5468227891.996177,2477260800.0,"We present the design, implementation and engineering experience in building and deploying MegaScale, a production system for training large language models (LLMs) at the scale of more than 10,000 GPUs. Training LLMs at this scale brings unprecedented challenges to training efficiency and stability. We take a full-stack approach that co-designs the algorithmic and system components across model block and optimizer design, computation and communication overlapping, operator optimization, data pipeline, and network performance tuning. Maintaining high efficiency throughout the training process (i.e., stability) is an important consideration in production given the long extent of LLM training jobs. Many hard stability issues only emerge at large scale, and in-depth observability is the key to address them. We develop a set of diagnosis tools to monitor system components and events deep in the stack, identify root causes, and derive effective techniques to achieve fault tolerance and mitigate stragglers. MegaScale achieves 55.2% Model FLOPs Utilization (MFU) when training a 175B LLM model on 12,288 GPUs, improving the MFU by 1.34x compared to Megatron-LM. We share our operational experience in identifying and fixing failures and stragglers. We hope by articulating the problems and sharing our experience from a systems perspective, this work can inspire future LLM systems research.",https://arxiv.org/abs/2402.15627,"MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs"
Megatron-Turing NLG 530B,Language,USA,Microsoft,2021-10-11,Industry,Language modeling,Not-defined,4480.0,Unreleased,"Common Crawl,The Pile,CC-Stories,Realnews",Unreleased,530000000000.0,270000000000.0,770.0,1.17e+24,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,SOTA improvement,4,3932160,3989424.741941752,50872953.6,0.302,1792000.0,3071857051.295149,1379840000.0,"Pretrained general-purpose language models can achieve state-of-the-art accuracies in various natural language processing domains by adapting to downstream tasks via zero-shot, few-shot and fine-tuning techniques. Because of their success, the size of these models has increased rapidly, requiring high-performance hardware, software, and algorithmic techniques to enable training such large models. As the result of a joint effort between Microsoft and NVIDIA, we present details on the training of the largest monolithic transformer based language model, Megatron-Turing NLG 530B (MT-NLG), with 530 billion parameters. In this paper, we first focus on the infrastructure as well as the 3D parallelism methodology used to train this model using DeepSpeed and Megatron. Next, we detail the training process, the design of our training corpus, and our data curation techniques, which we believe is a key ingredient to the success of the model. Finally, we discuss various evaluation results, as well as other interesting observations and new properties exhibited by MT-NLG. We demonstrate that MT-NLG achieves superior zero-, one-, and few-shot learning accuracies on several NLP benchmarks and establishes new state-of-the-art results. We believe that our contributions will help further the development of large-scale training infrastructures, large-scale language models, and natural language generations.",https://arxiv.org/abs/2201.11990,"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model"
Megatron-Turing NLG 530B,Language,USA,Microsoft,2021-10-11,Industry,Language modeling,Not-defined,4480.0,Unreleased,"Common Crawl,The Pile,CC-Stories,Realnews",Unreleased,530000000000.0,270000000000.0,770.0,1.17e+24,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Training cost,4,3932160,3989424.741941752,50872953.6,0.302,1792000.0,3071857051.295149,1379840000.0,"Pretrained general-purpose language models can achieve state-of-the-art accuracies in various natural language processing domains by adapting to downstream tasks via zero-shot, few-shot and fine-tuning techniques. Because of their success, the size of these models has increased rapidly, requiring high-performance hardware, software, and algorithmic techniques to enable training such large models. As the result of a joint effort between Microsoft and NVIDIA, we present details on the training of the largest monolithic transformer based language model, Megatron-Turing NLG 530B (MT-NLG), with 530 billion parameters. In this paper, we first focus on the infrastructure as well as the 3D parallelism methodology used to train this model using DeepSpeed and Megatron. Next, we detail the training process, the design of our training corpus, and our data curation techniques, which we believe is a key ingredient to the success of the model. Finally, we discuss various evaluation results, as well as other interesting observations and new properties exhibited by MT-NLG. We demonstrate that MT-NLG achieves superior zero-, one-, and few-shot learning accuracies on several NLP benchmarks and establishes new state-of-the-art results. We believe that our contributions will help further the development of large-scale training infrastructures, large-scale language models, and natural language generations.",https://arxiv.org/abs/2201.11990,"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model"
Megatron-Turing NLG 530B,Language,USA,NVIDIA,2021-10-11,Industry,Language modeling,Not-defined,4480.0,Unreleased,"Common Crawl,The Pile,CC-Stories,Realnews",Unreleased,530000000000.0,270000000000.0,770.0,1.17e+24,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,SOTA improvement,4,3932160,3989424.741941752,50872953.6,0.302,1792000.0,3071857051.295149,1379840000.0,"Pretrained general-purpose language models can achieve state-of-the-art accuracies in various natural language processing domains by adapting to downstream tasks via zero-shot, few-shot and fine-tuning techniques. Because of their success, the size of these models has increased rapidly, requiring high-performance hardware, software, and algorithmic techniques to enable training such large models. As the result of a joint effort between Microsoft and NVIDIA, we present details on the training of the largest monolithic transformer based language model, Megatron-Turing NLG 530B (MT-NLG), with 530 billion parameters. In this paper, we first focus on the infrastructure as well as the 3D parallelism methodology used to train this model using DeepSpeed and Megatron. Next, we detail the training process, the design of our training corpus, and our data curation techniques, which we believe is a key ingredient to the success of the model. Finally, we discuss various evaluation results, as well as other interesting observations and new properties exhibited by MT-NLG. We demonstrate that MT-NLG achieves superior zero-, one-, and few-shot learning accuracies on several NLP benchmarks and establishes new state-of-the-art results. We believe that our contributions will help further the development of large-scale training infrastructures, large-scale language models, and natural language generations.",https://arxiv.org/abs/2201.11990,"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model"
Megatron-Turing NLG 530B,Language,USA,NVIDIA,2021-10-11,Industry,Language modeling,Not-defined,4480.0,Unreleased,"Common Crawl,The Pile,CC-Stories,Realnews",Unreleased,530000000000.0,270000000000.0,770.0,1.17e+24,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Training cost,4,3932160,3989424.741941752,50872953.6,0.302,1792000.0,3071857051.295149,1379840000.0,"Pretrained general-purpose language models can achieve state-of-the-art accuracies in various natural language processing domains by adapting to downstream tasks via zero-shot, few-shot and fine-tuning techniques. Because of their success, the size of these models has increased rapidly, requiring high-performance hardware, software, and algorithmic techniques to enable training such large models. As the result of a joint effort between Microsoft and NVIDIA, we present details on the training of the largest monolithic transformer based language model, Megatron-Turing NLG 530B (MT-NLG), with 530 billion parameters. In this paper, we first focus on the infrastructure as well as the 3D parallelism methodology used to train this model using DeepSpeed and Megatron. Next, we detail the training process, the design of our training corpus, and our data curation techniques, which we believe is a key ingredient to the success of the model. Finally, we discuss various evaluation results, as well as other interesting observations and new properties exhibited by MT-NLG. We demonstrate that MT-NLG achieves superior zero-, one-, and few-shot learning accuracies on several NLP benchmarks and establishes new state-of-the-art results. We believe that our contributions will help further the development of large-scale training infrastructures, large-scale language models, and natural language generations.",https://arxiv.org/abs/2201.11990,"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model"
Movie Gen Audio,Audio,USA,Meta AI,2024-10-04,Industry,Audio generation,Confident,384.0,Unreleased,Not-defined,Not-defined,13000000000.0,26600000000.0,360.0,1.4e+23,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,4,6472188,788306.6236983632,17084113.919999998,1.3228285714285714e-07,268800.0,283790384.53141075,96768000.0,"We present Movie Gen, a cast of foundation models that generates high-quality, 1080p HD videos
with different aspect ratios and synchronized audio. We also show additional capabilities such as
precise instruction-based video editing and generation of personalized videos based on a user’s image.
Our models set a new state-of-the-art on multiple tasks: text-to-video synthesis, video personalization,
video editing, video-to-audio generation, and text-to-audio generation. Our largest video generation
model is a 30B parameter transformer trained with a maximum context length of 73K video tokens,
corresponding to a generated video of 16 seconds at 16 frames-per-second. We show multiple technical
innovations and simplifications on the architecture, latent spaces, training objectives and recipes, data
curation, evaluation protocols, parallelization techniques, and inference optimizations that allow us to
reap the benefits of scaling pre-training data, model size, and training compute for training large scale
media generation models. We hope this paper helps the research community to accelerate progress
and innovation in media generation models.
All videos from this paper are available at https://go.fb.me/MovieGenResearchVideos.",https://ai.meta.com/static-resource/movie-gen-research-paper,Movie Gen: A Cast of Media Foundation Models
Movie Gen Video,Video,USA,Meta AI,2024-10-04,Industry,Video generation,Confident,6144.0,Unreleased,Not-defined,Not-defined,30000000000.0,26600000000.0,331.0,1.65e+24,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,4,6472188,9473720.068741636,273345822.71999997,1.1224e-08,4300800.0,3135801342.753482,1423564800.0,"We present Movie Gen, a cast of foundation models that generates high-quality, 1080p HD videos
with different aspect ratios and synchronized audio. We also show additional capabilities such as
precise instruction-based video editing and generation of personalized videos based on a user’s image.
Our models set a new state-of-the-art on multiple tasks: text-to-video synthesis, video personalization,
video editing, video-to-audio generation, and text-to-audio generation. Our largest video generation
model is a 30B parameter transformer trained with a maximum context length of 73K video tokens,
corresponding to a generated video of 16 seconds at 16 frames-per-second. We show multiple technical
innovations and simplifications on the architecture, latent spaces, training objectives and recipes, data
curation, evaluation protocols, parallelization techniques, and inference optimizations that allow us to
reap the benefits of scaling pre-training data, model size, and training compute for training large scale
media generation models. We hope this paper helps the research community to accelerate progress
and innovation in media generation models.
All videos from this paper are available at https://go.fb.me/MovieGenResearchVideos.",https://ai.meta.com/static-resource/movie-gen-research-paper,Movie Gen: A Cast of Media Foundation Models
NEC LLM 13B,Language,USA,NEC Laboratories,2023-07-06,Industry,Chat,Confident,512.0,Not-defined,Not-defined,Not-defined,13000000000.0,2000000000000.0,720.0,8.04e+23,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,None,1,36864000,788306.6236983632,7680000.0,7.154303482587064e-09,204800.0,567580769.0628215,147456000.0,,https://jpn.nec.com/press/202307/20230706_02.html,NEC、130億パラメータで世界トップクラスの日本語性能を有する軽量なLLMを開発
NEC LLM 13B,Language,USA,NEC Laboratories,2023-07-06,Industry,Language modeling/generation,Confident,512.0,Not-defined,Not-defined,Not-defined,13000000000.0,2000000000000.0,720.0,8.04e+23,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,None,1,36864000,788306.6236983632,7680000.0,7.154303482587064e-09,204800.0,567580769.0628215,147456000.0,,https://jpn.nec.com/press/202307/20230706_02.html,NEC、130億パラメータで世界トップクラスの日本語性能を有する軽量なLLMを開発
Nanbeige-16B,Language,China,Nanbeige LLM Lab,2023-11-01,Industry,Chat,Likely,128.0,Open weights (unrestricted),Unspecified unreleased,Open source,16000000000.0,2500000000000.0,940.0,2.4e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Significant use,1,4000000,113138.0969599182,1293528.32,2.3965416666666668e-08,51200.0,106349811.1423231,48128000.0,"Nanbeige-16B is a 16 billion parameter language model developed by Nanbeige LLM Lab. It uses 2.5T Tokens for pre-training. The training data includes a large amount of high-quality internet corpus, various books, code, etc. It has achieved good results on various authoritative evaluation data sets. This release includes the Base, Chat, Base-32k and Chat-32k.",https://github.com/Nanbeige/Nanbeige/blob/main/README_EN.md,
Nanbeige-16B,Language,China,Nanbeige LLM Lab,2023-11-01,Industry,Code generation,Likely,128.0,Open weights (unrestricted),Unspecified unreleased,Open source,16000000000.0,2500000000000.0,940.0,2.4e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Significant use,1,4000000,113138.0969599182,1293528.32,2.3965416666666668e-08,51200.0,106349811.1423231,48128000.0,"Nanbeige-16B is a 16 billion parameter language model developed by Nanbeige LLM Lab. It uses 2.5T Tokens for pre-training. The training data includes a large amount of high-quality internet corpus, various books, code, etc. It has achieved good results on various authoritative evaluation data sets. This release includes the Base, Chat, Base-32k and Chat-32k.",https://github.com/Nanbeige/Nanbeige/blob/main/README_EN.md,
Nanbeige-16B,Language,China,Nanbeige LLM Lab,2023-11-01,Industry,Language modeling/generation,Likely,128.0,Open weights (unrestricted),Unspecified unreleased,Open source,16000000000.0,2500000000000.0,940.0,2.4e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Significant use,1,4000000,113138.0969599182,1293528.32,2.3965416666666668e-08,51200.0,106349811.1423231,48128000.0,"Nanbeige-16B is a 16 billion parameter language model developed by Nanbeige LLM Lab. It uses 2.5T Tokens for pre-training. The training data includes a large amount of high-quality internet corpus, various books, code, etc. It has achieved good results on various authoritative evaluation data sets. This release includes the Base, Chat, Base-32k and Chat-32k.",https://github.com/Nanbeige/Nanbeige/blob/main/README_EN.md,
Nanbeige-16B,Language,China,Nanbeige LLM Lab,2023-11-01,Industry,Question answering,Likely,128.0,Open weights (unrestricted),Unspecified unreleased,Open source,16000000000.0,2500000000000.0,940.0,2.4e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Significant use,1,4000000,113138.0969599182,1293528.32,2.3965416666666668e-08,51200.0,106349811.1423231,48128000.0,"Nanbeige-16B is a 16 billion parameter language model developed by Nanbeige LLM Lab. It uses 2.5T Tokens for pre-training. The training data includes a large amount of high-quality internet corpus, various books, code, etc. It has achieved good results on various authoritative evaluation data sets. This release includes the Base, Chat, Base-32k and Chat-32k.",https://github.com/Nanbeige/Nanbeige/blob/main/README_EN.md,
Nemotron-3-8B,Language,USA,NVIDIA,2023-11-15,Industry,Chat,Confident,1024.0,Open weights (restricted use),"Unspecified unreleased,Flan,P3 (Public Pool of Prompts)",Not-defined,8000000000.0,3800000000000.0,456.0,1.8e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,SOTA improvement,1,4194304,904992.9349364166,10348226.56,0.3473,409600.0,412676778.33100593,186777600.0,"Large language models (LLMs) are revolutionizing data science, enabling advanced capabilities in natural language understanding, AI, and machine learning. Custom LLMs, tailored for domain-specific insights, are finding increased traction in enterprise applications.

The NVIDIA Nemotron-3 8B family of foundation models is a powerful new tool for building production-ready generative AI applications for the enterprise–fostering innovations ranging from customer service AI chatbots to cutting-edge AI products.","https://developer.nvidia.com/blog/nvidia-ai-foundation-models-build-custom-enterprise-chatbots-and-co-pilots-with-production-ready-llms/

https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/nemotron-3-8b-base-4k",NVIDIA AI Foundation Models: Build Custom Enterprise Chatbots and Co-Pilots with Production-Ready LLMs
Nemotron-3-8B,Language,USA,NVIDIA,2023-11-15,Industry,Code generation,Confident,1024.0,Open weights (restricted use),"Unspecified unreleased,Flan,P3 (Public Pool of Prompts)",Not-defined,8000000000.0,3800000000000.0,456.0,1.8e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,SOTA improvement,1,4194304,904992.9349364166,10348226.56,0.3473,409600.0,412676778.33100593,186777600.0,"Large language models (LLMs) are revolutionizing data science, enabling advanced capabilities in natural language understanding, AI, and machine learning. Custom LLMs, tailored for domain-specific insights, are finding increased traction in enterprise applications.

The NVIDIA Nemotron-3 8B family of foundation models is a powerful new tool for building production-ready generative AI applications for the enterprise–fostering innovations ranging from customer service AI chatbots to cutting-edge AI products.","https://developer.nvidia.com/blog/nvidia-ai-foundation-models-build-custom-enterprise-chatbots-and-co-pilots-with-production-ready-llms/

https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/nemotron-3-8b-base-4k",NVIDIA AI Foundation Models: Build Custom Enterprise Chatbots and Co-Pilots with Production-Ready LLMs
Nemotron-3-8B,Language,USA,NVIDIA,2023-11-15,Industry,Language generation,Confident,1024.0,Open weights (restricted use),"Unspecified unreleased,Flan,P3 (Public Pool of Prompts)",Not-defined,8000000000.0,3800000000000.0,456.0,1.8e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,SOTA improvement,1,4194304,904992.9349364166,10348226.56,0.3473,409600.0,412676778.33100593,186777600.0,"Large language models (LLMs) are revolutionizing data science, enabling advanced capabilities in natural language understanding, AI, and machine learning. Custom LLMs, tailored for domain-specific insights, are finding increased traction in enterprise applications.

The NVIDIA Nemotron-3 8B family of foundation models is a powerful new tool for building production-ready generative AI applications for the enterprise–fostering innovations ranging from customer service AI chatbots to cutting-edge AI products.","https://developer.nvidia.com/blog/nvidia-ai-foundation-models-build-custom-enterprise-chatbots-and-co-pilots-with-production-ready-llms/

https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/nemotron-3-8b-base-4k",NVIDIA AI Foundation Models: Build Custom Enterprise Chatbots and Co-Pilots with Production-Ready LLMs
Nemotron-3-8B,Language,USA,NVIDIA,2023-11-15,Industry,Language modeling/generation,Confident,1024.0,Open weights (restricted use),"Unspecified unreleased,Flan,P3 (Public Pool of Prompts)",Not-defined,8000000000.0,3800000000000.0,456.0,1.8e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,SOTA improvement,1,4194304,904992.9349364166,10348226.56,0.3473,409600.0,412676778.33100593,186777600.0,"Large language models (LLMs) are revolutionizing data science, enabling advanced capabilities in natural language understanding, AI, and machine learning. Custom LLMs, tailored for domain-specific insights, are finding increased traction in enterprise applications.

The NVIDIA Nemotron-3 8B family of foundation models is a powerful new tool for building production-ready generative AI applications for the enterprise–fostering innovations ranging from customer service AI chatbots to cutting-edge AI products.","https://developer.nvidia.com/blog/nvidia-ai-foundation-models-build-custom-enterprise-chatbots-and-co-pilots-with-production-ready-llms/

https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/nemotron-3-8b-base-4k",NVIDIA AI Foundation Models: Build Custom Enterprise Chatbots and Co-Pilots with Production-Ready LLMs
Nemotron-3-8B,Language,USA,NVIDIA,2023-11-15,Industry,Question answering,Confident,1024.0,Open weights (restricted use),"Unspecified unreleased,Flan,P3 (Public Pool of Prompts)",Not-defined,8000000000.0,3800000000000.0,456.0,1.8e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,SOTA improvement,1,4194304,904992.9349364166,10348226.56,0.3473,409600.0,412676778.33100593,186777600.0,"Large language models (LLMs) are revolutionizing data science, enabling advanced capabilities in natural language understanding, AI, and machine learning. Custom LLMs, tailored for domain-specific insights, are finding increased traction in enterprise applications.

The NVIDIA Nemotron-3 8B family of foundation models is a powerful new tool for building production-ready generative AI applications for the enterprise–fostering innovations ranging from customer service AI chatbots to cutting-edge AI products.","https://developer.nvidia.com/blog/nvidia-ai-foundation-models-build-custom-enterprise-chatbots-and-co-pilots-with-production-ready-llms/

https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/nemotron-3-8b-base-4k",NVIDIA AI Foundation Models: Build Custom Enterprise Chatbots and Co-Pilots with Production-Ready LLMs
Nemotron-3-8B,Language,USA,NVIDIA,2023-11-15,Industry,Translation,Confident,1024.0,Open weights (restricted use),"Unspecified unreleased,Flan,P3 (Public Pool of Prompts)",Not-defined,8000000000.0,3800000000000.0,456.0,1.8e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,SOTA improvement,1,4194304,904992.9349364166,10348226.56,0.3473,409600.0,412676778.33100593,186777600.0,"Large language models (LLMs) are revolutionizing data science, enabling advanced capabilities in natural language understanding, AI, and machine learning. Custom LLMs, tailored for domain-specific insights, are finding increased traction in enterprise applications.

The NVIDIA Nemotron-3 8B family of foundation models is a powerful new tool for building production-ready generative AI applications for the enterprise–fostering innovations ranging from customer service AI chatbots to cutting-edge AI products.","https://developer.nvidia.com/blog/nvidia-ai-foundation-models-build-custom-enterprise-chatbots-and-co-pilots-with-production-ready-llms/

https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/nemotron-3-8b-base-4k",NVIDIA AI Foundation Models: Build Custom Enterprise Chatbots and Co-Pilots with Production-Ready LLMs
Nemotron-4 15B,Language,USA,NVIDIA,2024-02-27,Industry,Code generation,Confident,3072.0,Unreleased,Unspecified unreleased,Unreleased,15000000000.0,8000000000000.0,312.0,7.5005116e+23,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,1,4194304,788306.6236983632,136672911.35999998,0.410675,2150400.0,245951666.59388933,670924800.0,"We introduce Nemotron-4 15B, a 15-billion-parameter large multilingual language model trained on 8 trillion text tokens. Nemotron-4 15B demonstrates strong performance when assessed on English, multilingual, and coding tasks: it outperforms all existing similarly-sized open models on 4 out of 7 downstream evaluation areas and achieves competitive performance to the leading open models in the remaining ones. Specifically, Nemotron-4 15B exhibits the best multilingual capabilities of all similarly-sized models, even outperforming models over four times larger and those explicitly specialized for multilingual tasks.",https://arxiv.org/abs/2402.16819,Nemotron-4 15B Technical Report
Nemotron-4 15B,Language,USA,NVIDIA,2024-02-27,Industry,Language modeling/generation,Confident,3072.0,Unreleased,Unspecified unreleased,Unreleased,15000000000.0,8000000000000.0,312.0,7.5005116e+23,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,1,4194304,788306.6236983632,136672911.35999998,0.410675,2150400.0,245951666.59388933,670924800.0,"We introduce Nemotron-4 15B, a 15-billion-parameter large multilingual language model trained on 8 trillion text tokens. Nemotron-4 15B demonstrates strong performance when assessed on English, multilingual, and coding tasks: it outperforms all existing similarly-sized open models on 4 out of 7 downstream evaluation areas and achieves competitive performance to the leading open models in the remaining ones. Specifically, Nemotron-4 15B exhibits the best multilingual capabilities of all similarly-sized models, even outperforming models over four times larger and those explicitly specialized for multilingual tasks.",https://arxiv.org/abs/2402.16819,Nemotron-4 15B Technical Report
Nemotron-4 15B,Language,USA,NVIDIA,2024-02-27,Industry,Quantitative reasoning,Confident,3072.0,Unreleased,Unspecified unreleased,Unreleased,15000000000.0,8000000000000.0,312.0,7.5005116e+23,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,1,4194304,788306.6236983632,136672911.35999998,0.410675,2150400.0,245951666.59388933,670924800.0,"We introduce Nemotron-4 15B, a 15-billion-parameter large multilingual language model trained on 8 trillion text tokens. Nemotron-4 15B demonstrates strong performance when assessed on English, multilingual, and coding tasks: it outperforms all existing similarly-sized open models on 4 out of 7 downstream evaluation areas and achieves competitive performance to the leading open models in the remaining ones. Specifically, Nemotron-4 15B exhibits the best multilingual capabilities of all similarly-sized models, even outperforming models over four times larger and those explicitly specialized for multilingual tasks.",https://arxiv.org/abs/2402.16819,Nemotron-4 15B Technical Report
Nemotron-4 15B,Language,USA,NVIDIA,2024-02-27,Industry,Question answering,Confident,3072.0,Unreleased,Unspecified unreleased,Unreleased,15000000000.0,8000000000000.0,312.0,7.5005116e+23,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,1,4194304,788306.6236983632,136672911.35999998,0.410675,2150400.0,245951666.59388933,670924800.0,"We introduce Nemotron-4 15B, a 15-billion-parameter large multilingual language model trained on 8 trillion text tokens. Nemotron-4 15B demonstrates strong performance when assessed on English, multilingual, and coding tasks: it outperforms all existing similarly-sized open models on 4 out of 7 downstream evaluation areas and achieves competitive performance to the leading open models in the remaining ones. Specifically, Nemotron-4 15B exhibits the best multilingual capabilities of all similarly-sized models, even outperforming models over four times larger and those explicitly specialized for multilingual tasks.",https://arxiv.org/abs/2402.16819,Nemotron-4 15B Technical Report
Nemotron-4 15B,Language,USA,NVIDIA,2024-02-27,Industry,Translation,Confident,3072.0,Unreleased,Unspecified unreleased,Unreleased,15000000000.0,8000000000000.0,312.0,7.5005116e+23,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,1,4194304,788306.6236983632,136672911.35999998,0.410675,2150400.0,245951666.59388933,670924800.0,"We introduce Nemotron-4 15B, a 15-billion-parameter large multilingual language model trained on 8 trillion text tokens. Nemotron-4 15B demonstrates strong performance when assessed on English, multilingual, and coding tasks: it outperforms all existing similarly-sized open models on 4 out of 7 downstream evaluation areas and achieves competitive performance to the leading open models in the remaining ones. Specifically, Nemotron-4 15B exhibits the best multilingual capabilities of all similarly-sized models, even outperforming models over four times larger and those explicitly specialized for multilingual tasks.",https://arxiv.org/abs/2402.16819,Nemotron-4 15B Technical Report
Nemotron-4 340B,Language,USA,NVIDIA,2024-06-14,Industry,Chat,Confident,6144.0,Open weights (unrestricted),Unspecified unreleased,Unreleased,340000000000.0,6750000000000.0,2200.0,1.8e+25,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,1,4194304,9483521.861161152,273345822.71999997,0.410675,4300800.0,20863748094.554535,9461760000.0,"We release the Nemotron-4 340B model family, including Nemotron-4-340B-Base, Nemotron-4-
340B-Instruct, and Nemotron-4-340B-Reward. Our models are open access under the NVIDIA Open
Model License Agreement, a permissive model license that allows distribution, modification, and use of
the models and its outputs. These models perform competitively to open access models on a wide range
of evaluation benchmarks, and were sized to fit on a single DGX H100 with 8 GPUs when deployed in
FP8 precision. We believe that the community can benefit from these models in various research studies
and commercial applications, especially for generating synthetic data to train smaller language models.
Notably, over 98% of data used in our model alignment process is synthetically generated, showcasing
the effectiveness of these models in generating synthetic data. To further support open research and
facilitate model development, we are also open-sourcing the synthetic data generation pipeline used in
our model alignment process.

(from technical report: https://d1qx31qr3h6wln.cloudfront.net/publications/Nemotron_4_340B_8T_0.pdf )",https://blogs.nvidia.com/blog/nemotron-4-synthetic-data-generation-llm-training/ ,NVIDIA Releases Open Synthetic Data Generation Pipeline for Training Large Language Models
Nemotron-4 340B,Language,USA,NVIDIA,2024-06-14,Industry,Language modeling/generation,Confident,6144.0,Open weights (unrestricted),Unspecified unreleased,Unreleased,340000000000.0,6750000000000.0,2200.0,1.8e+25,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,1,4194304,9483521.861161152,273345822.71999997,0.410675,4300800.0,20863748094.554535,9461760000.0,"We release the Nemotron-4 340B model family, including Nemotron-4-340B-Base, Nemotron-4-
340B-Instruct, and Nemotron-4-340B-Reward. Our models are open access under the NVIDIA Open
Model License Agreement, a permissive model license that allows distribution, modification, and use of
the models and its outputs. These models perform competitively to open access models on a wide range
of evaluation benchmarks, and were sized to fit on a single DGX H100 with 8 GPUs when deployed in
FP8 precision. We believe that the community can benefit from these models in various research studies
and commercial applications, especially for generating synthetic data to train smaller language models.
Notably, over 98% of data used in our model alignment process is synthetically generated, showcasing
the effectiveness of these models in generating synthetic data. To further support open research and
facilitate model development, we are also open-sourcing the synthetic data generation pipeline used in
our model alignment process.

(from technical report: https://d1qx31qr3h6wln.cloudfront.net/publications/Nemotron_4_340B_8T_0.pdf )",https://blogs.nvidia.com/blog/nemotron-4-synthetic-data-generation-llm-training/ ,NVIDIA Releases Open Synthetic Data Generation Pipeline for Training Large Language Models
Nemotron-4 340B,Language,USA,NVIDIA,2024-06-14,Industry,Question answering,Confident,6144.0,Open weights (unrestricted),Unspecified unreleased,Unreleased,340000000000.0,6750000000000.0,2200.0,1.8e+25,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,1,4194304,9483521.861161152,273345822.71999997,0.410675,4300800.0,20863748094.554535,9461760000.0,"We release the Nemotron-4 340B model family, including Nemotron-4-340B-Base, Nemotron-4-
340B-Instruct, and Nemotron-4-340B-Reward. Our models are open access under the NVIDIA Open
Model License Agreement, a permissive model license that allows distribution, modification, and use of
the models and its outputs. These models perform competitively to open access models on a wide range
of evaluation benchmarks, and were sized to fit on a single DGX H100 with 8 GPUs when deployed in
FP8 precision. We believe that the community can benefit from these models in various research studies
and commercial applications, especially for generating synthetic data to train smaller language models.
Notably, over 98% of data used in our model alignment process is synthetically generated, showcasing
the effectiveness of these models in generating synthetic data. To further support open research and
facilitate model development, we are also open-sourcing the synthetic data generation pipeline used in
our model alignment process.

(from technical report: https://d1qx31qr3h6wln.cloudfront.net/publications/Nemotron_4_340B_8T_0.pdf )",https://blogs.nvidia.com/blog/nemotron-4-synthetic-data-generation-llm-training/ ,NVIDIA Releases Open Synthetic Data Generation Pipeline for Training Large Language Models
OLMo-7B,Language,USA,Allen Institute for AI,2024-02-01,Academia,Chat,Confident,480.0,Open weights (unrestricted),Dolma,Open source,7000000000.0,2000000000000.0,720.0,1.0332e+23,1877.1621580193864,12302.845,450.0,766305000000000.0,503500000000000.0,84000000000.0,"AMD Radeon Instinct MI250X,NVIDIA A100",Significant use,1,262144,788306.6236983632,5905365.6,7.416811846689896e-08,216000.0,567580769.0628215,155520000.0,"Language models (LMs) have become ubiquitous in both NLP research and in commercial product offerings. As their commercial importance has surged, the most powerful models have become closed off, gated behind proprietary interfaces, with important details of their training data, architectures, and development undisclosed. Given the importance of these details in scientifically studying these models, including their biases and potential risks, we believe it is essential for the research community to have access to powerful, truly open LMs. To this end, this technical report details the first release of OLMo, a state-of-the-art, truly Open Language Model and its framework to build and study the science of language modeling. Unlike most prior efforts that have only released model weights and inference code, we release OLMo and the whole framework, including training data and training and evaluation code. We hope this release will empower and strengthen the open research community and inspire a new wave of innovation.",https://arxiv.org/abs/2402.00838v1,OLMo: Accelerating the Science of Language Models
OLMo-7B,Language,USA,Allen Institute for AI,2024-02-01,Academia,Language modeling/generation,Confident,480.0,Open weights (unrestricted),Dolma,Open source,7000000000.0,2000000000000.0,720.0,1.0332e+23,1877.1621580193864,12302.845,450.0,766305000000000.0,503500000000000.0,84000000000.0,"AMD Radeon Instinct MI250X,NVIDIA A100",Significant use,1,262144,788306.6236983632,5905365.6,7.416811846689896e-08,216000.0,567580769.0628215,155520000.0,"Language models (LMs) have become ubiquitous in both NLP research and in commercial product offerings. As their commercial importance has surged, the most powerful models have become closed off, gated behind proprietary interfaces, with important details of their training data, architectures, and development undisclosed. Given the importance of these details in scientifically studying these models, including their biases and potential risks, we believe it is essential for the research community to have access to powerful, truly open LMs. To this end, this technical report details the first release of OLMo, a state-of-the-art, truly Open Language Model and its framework to build and study the science of language modeling. Unlike most prior efforts that have only released model weights and inference code, we release OLMo and the whole framework, including training data and training and evaluation code. We hope this release will empower and strengthen the open research community and inspire a new wave of innovation.",https://arxiv.org/abs/2402.00838v1,OLMo: Accelerating the Science of Language Models
OLMo-7B,Language,USA,Allen Institute for AI,2024-02-01,Research collective,Chat,Confident,480.0,Open weights (unrestricted),Dolma,Open source,7000000000.0,2000000000000.0,720.0,1.0332e+23,1877.1621580193864,12302.845,450.0,766305000000000.0,503500000000000.0,84000000000.0,"AMD Radeon Instinct MI250X,NVIDIA A100",Significant use,1,262144,788306.6236983632,5905365.6,7.416811846689896e-08,216000.0,567580769.0628215,155520000.0,"Language models (LMs) have become ubiquitous in both NLP research and in commercial product offerings. As their commercial importance has surged, the most powerful models have become closed off, gated behind proprietary interfaces, with important details of their training data, architectures, and development undisclosed. Given the importance of these details in scientifically studying these models, including their biases and potential risks, we believe it is essential for the research community to have access to powerful, truly open LMs. To this end, this technical report details the first release of OLMo, a state-of-the-art, truly Open Language Model and its framework to build and study the science of language modeling. Unlike most prior efforts that have only released model weights and inference code, we release OLMo and the whole framework, including training data and training and evaluation code. We hope this release will empower and strengthen the open research community and inspire a new wave of innovation.",https://arxiv.org/abs/2402.00838v1,OLMo: Accelerating the Science of Language Models
OLMo-7B,Language,USA,Allen Institute for AI,2024-02-01,Research collective,Language modeling/generation,Confident,480.0,Open weights (unrestricted),Dolma,Open source,7000000000.0,2000000000000.0,720.0,1.0332e+23,1877.1621580193864,12302.845,450.0,766305000000000.0,503500000000000.0,84000000000.0,"AMD Radeon Instinct MI250X,NVIDIA A100",Significant use,1,262144,788306.6236983632,5905365.6,7.416811846689896e-08,216000.0,567580769.0628215,155520000.0,"Language models (LMs) have become ubiquitous in both NLP research and in commercial product offerings. As their commercial importance has surged, the most powerful models have become closed off, gated behind proprietary interfaces, with important details of their training data, architectures, and development undisclosed. Given the importance of these details in scientifically studying these models, including their biases and potential risks, we believe it is essential for the research community to have access to powerful, truly open LMs. To this end, this technical report details the first release of OLMo, a state-of-the-art, truly Open Language Model and its framework to build and study the science of language modeling. Unlike most prior efforts that have only released model weights and inference code, we release OLMo and the whole framework, including training data and training and evaluation code. We hope this release will empower and strengthen the open research community and inspire a new wave of innovation.",https://arxiv.org/abs/2402.00838v1,OLMo: Accelerating the Science of Language Models
OLMo-7B,Language,USA,University of Washington,2024-02-01,Academia,Chat,Confident,480.0,Open weights (unrestricted),Dolma,Open source,7000000000.0,2000000000000.0,720.0,1.0332e+23,1877.1621580193864,12302.845,450.0,766305000000000.0,503500000000000.0,84000000000.0,"AMD Radeon Instinct MI250X,NVIDIA A100",Significant use,1,262144,788306.6236983632,5905365.6,7.416811846689896e-08,216000.0,567580769.0628215,155520000.0,"Language models (LMs) have become ubiquitous in both NLP research and in commercial product offerings. As their commercial importance has surged, the most powerful models have become closed off, gated behind proprietary interfaces, with important details of their training data, architectures, and development undisclosed. Given the importance of these details in scientifically studying these models, including their biases and potential risks, we believe it is essential for the research community to have access to powerful, truly open LMs. To this end, this technical report details the first release of OLMo, a state-of-the-art, truly Open Language Model and its framework to build and study the science of language modeling. Unlike most prior efforts that have only released model weights and inference code, we release OLMo and the whole framework, including training data and training and evaluation code. We hope this release will empower and strengthen the open research community and inspire a new wave of innovation.",https://arxiv.org/abs/2402.00838v1,OLMo: Accelerating the Science of Language Models
OLMo-7B,Language,USA,University of Washington,2024-02-01,Academia,Language modeling/generation,Confident,480.0,Open weights (unrestricted),Dolma,Open source,7000000000.0,2000000000000.0,720.0,1.0332e+23,1877.1621580193864,12302.845,450.0,766305000000000.0,503500000000000.0,84000000000.0,"AMD Radeon Instinct MI250X,NVIDIA A100",Significant use,1,262144,788306.6236983632,5905365.6,7.416811846689896e-08,216000.0,567580769.0628215,155520000.0,"Language models (LMs) have become ubiquitous in both NLP research and in commercial product offerings. As their commercial importance has surged, the most powerful models have become closed off, gated behind proprietary interfaces, with important details of their training data, architectures, and development undisclosed. Given the importance of these details in scientifically studying these models, including their biases and potential risks, we believe it is essential for the research community to have access to powerful, truly open LMs. To this end, this technical report details the first release of OLMo, a state-of-the-art, truly Open Language Model and its framework to build and study the science of language modeling. Unlike most prior efforts that have only released model weights and inference code, we release OLMo and the whole framework, including training data and training and evaluation code. We hope this release will empower and strengthen the open research community and inspire a new wave of innovation.",https://arxiv.org/abs/2402.00838v1,OLMo: Accelerating the Science of Language Models
OLMo-7B,Language,USA,University of Washington,2024-02-01,Research collective,Chat,Confident,480.0,Open weights (unrestricted),Dolma,Open source,7000000000.0,2000000000000.0,720.0,1.0332e+23,1877.1621580193864,12302.845,450.0,766305000000000.0,503500000000000.0,84000000000.0,"AMD Radeon Instinct MI250X,NVIDIA A100",Significant use,1,262144,788306.6236983632,5905365.6,7.416811846689896e-08,216000.0,567580769.0628215,155520000.0,"Language models (LMs) have become ubiquitous in both NLP research and in commercial product offerings. As their commercial importance has surged, the most powerful models have become closed off, gated behind proprietary interfaces, with important details of their training data, architectures, and development undisclosed. Given the importance of these details in scientifically studying these models, including their biases and potential risks, we believe it is essential for the research community to have access to powerful, truly open LMs. To this end, this technical report details the first release of OLMo, a state-of-the-art, truly Open Language Model and its framework to build and study the science of language modeling. Unlike most prior efforts that have only released model weights and inference code, we release OLMo and the whole framework, including training data and training and evaluation code. We hope this release will empower and strengthen the open research community and inspire a new wave of innovation.",https://arxiv.org/abs/2402.00838v1,OLMo: Accelerating the Science of Language Models
OLMo-7B,Language,USA,University of Washington,2024-02-01,Research collective,Language modeling/generation,Confident,480.0,Open weights (unrestricted),Dolma,Open source,7000000000.0,2000000000000.0,720.0,1.0332e+23,1877.1621580193864,12302.845,450.0,766305000000000.0,503500000000000.0,84000000000.0,"AMD Radeon Instinct MI250X,NVIDIA A100",Significant use,1,262144,788306.6236983632,5905365.6,7.416811846689896e-08,216000.0,567580769.0628215,155520000.0,"Language models (LMs) have become ubiquitous in both NLP research and in commercial product offerings. As their commercial importance has surged, the most powerful models have become closed off, gated behind proprietary interfaces, with important details of their training data, architectures, and development undisclosed. Given the importance of these details in scientifically studying these models, including their biases and potential risks, we believe it is essential for the research community to have access to powerful, truly open LMs. To this end, this technical report details the first release of OLMo, a state-of-the-art, truly Open Language Model and its framework to build and study the science of language modeling. Unlike most prior efforts that have only released model weights and inference code, we release OLMo and the whole framework, including training data and training and evaluation code. We hope this release will empower and strengthen the open research community and inspire a new wave of innovation.",https://arxiv.org/abs/2402.00838v1,OLMo: Accelerating the Science of Language Models
OPT-175B,Language,USA,Meta AI,2022-05-02,Industry,Chat,Confident,1024.0,Open weights (non-commercial),"The Pile,BookCorpus (BooksCorpus, Toronto Book Corpus),CC-Stories,Pushshift Reddit",Open source,175000000000.0,180000000000.0,793.5,4.3e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Significant use,1,2000000,909984.4296839886,11628103.68,0.47115,409600.0,722072644.954245,325017600.0,"Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3,1 while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models",https://arxiv.org/abs/2205.01068,OPT: Open Pre-trained Transformer Language Models
OPT-175B,Language,USA,Meta AI,2022-05-02,Industry,Chat,Confident,1024.0,Open weights (non-commercial),"The Pile,BookCorpus (BooksCorpus, Toronto Book Corpus),CC-Stories,Pushshift Reddit",Open source,175000000000.0,180000000000.0,793.5,4.3e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Highly cited,1,2000000,909984.4296839886,11628103.68,0.47115,409600.0,722072644.954245,325017600.0,"Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3,1 while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models",https://arxiv.org/abs/2205.01068,OPT: Open Pre-trained Transformer Language Models
OPT-175B,Language,USA,Meta AI,2022-05-02,Industry,Language modeling,Confident,1024.0,Open weights (non-commercial),"The Pile,BookCorpus (BooksCorpus, Toronto Book Corpus),CC-Stories,Pushshift Reddit",Open source,175000000000.0,180000000000.0,793.5,4.3e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Significant use,1,2000000,909984.4296839886,11628103.68,0.47115,409600.0,722072644.954245,325017600.0,"Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3,1 while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models",https://arxiv.org/abs/2205.01068,OPT: Open Pre-trained Transformer Language Models
OPT-175B,Language,USA,Meta AI,2022-05-02,Industry,Language modeling,Confident,1024.0,Open weights (non-commercial),"The Pile,BookCorpus (BooksCorpus, Toronto Book Corpus),CC-Stories,Pushshift Reddit",Open source,175000000000.0,180000000000.0,793.5,4.3e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Highly cited,1,2000000,909984.4296839886,11628103.68,0.47115,409600.0,722072644.954245,325017600.0,"Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3,1 while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models",https://arxiv.org/abs/2205.01068,OPT: Open Pre-trained Transformer Language Models
OPT-175B,Language,USA,Meta AI,2022-05-02,Industry,Language modeling/generation,Confident,1024.0,Open weights (non-commercial),"The Pile,BookCorpus (BooksCorpus, Toronto Book Corpus),CC-Stories,Pushshift Reddit",Open source,175000000000.0,180000000000.0,793.5,4.3e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Significant use,1,2000000,909984.4296839886,11628103.68,0.47115,409600.0,722072644.954245,325017600.0,"Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3,1 while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models",https://arxiv.org/abs/2205.01068,OPT: Open Pre-trained Transformer Language Models
OPT-175B,Language,USA,Meta AI,2022-05-02,Industry,Language modeling/generation,Confident,1024.0,Open weights (non-commercial),"The Pile,BookCorpus (BooksCorpus, Toronto Book Corpus),CC-Stories,Pushshift Reddit",Open source,175000000000.0,180000000000.0,793.5,4.3e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Highly cited,1,2000000,909984.4296839886,11628103.68,0.47115,409600.0,722072644.954245,325017600.0,"Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3,1 while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models",https://arxiv.org/abs/2205.01068,OPT: Open Pre-trained Transformer Language Models
OPT-175B,Language,USA,Meta AI,2022-05-02,Industry,Question answering,Confident,1024.0,Open weights (non-commercial),"The Pile,BookCorpus (BooksCorpus, Toronto Book Corpus),CC-Stories,Pushshift Reddit",Open source,175000000000.0,180000000000.0,793.5,4.3e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Significant use,1,2000000,909984.4296839886,11628103.68,0.47115,409600.0,722072644.954245,325017600.0,"Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3,1 while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models",https://arxiv.org/abs/2205.01068,OPT: Open Pre-trained Transformer Language Models
OPT-175B,Language,USA,Meta AI,2022-05-02,Industry,Question answering,Confident,1024.0,Open weights (non-commercial),"The Pile,BookCorpus (BooksCorpus, Toronto Book Corpus),CC-Stories,Pushshift Reddit",Open source,175000000000.0,180000000000.0,793.5,4.3e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Highly cited,1,2000000,909984.4296839886,11628103.68,0.47115,409600.0,722072644.954245,325017600.0,"Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3,1 while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models",https://arxiv.org/abs/2205.01068,OPT: Open Pre-trained Transformer Language Models
OPT-66B,Language,USA,Meta AI,2022-06-21,Industry,Chat,Confident,1024.0,Open weights (non-commercial),"The Pile,BookCorpus (BooksCorpus, Toronto Book Corpus),CC-Stories,Pushshift Reddit",Open source,66000000000.0,180000000000.0,793.5,1.100000000001e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Training cost,1,262144,788306.6236983632,11628103.68,0.47115,409600.0,625521305.9046512,325017600.0,"Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.",https://arxiv.org/abs/2205.01068,OPT: Open Pre-trained Transformer Language Models
OPT-66B,Language,USA,Meta AI,2022-06-21,Industry,Language modeling,Confident,1024.0,Open weights (non-commercial),"The Pile,BookCorpus (BooksCorpus, Toronto Book Corpus),CC-Stories,Pushshift Reddit",Open source,66000000000.0,180000000000.0,793.5,1.100000000001e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Training cost,1,262144,788306.6236983632,11628103.68,0.47115,409600.0,625521305.9046512,325017600.0,"Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.",https://arxiv.org/abs/2205.01068,OPT: Open Pre-trained Transformer Language Models
OPT-66B,Language,USA,Meta AI,2022-06-21,Industry,Language modeling/generation,Confident,1024.0,Open weights (non-commercial),"The Pile,BookCorpus (BooksCorpus, Toronto Book Corpus),CC-Stories,Pushshift Reddit",Open source,66000000000.0,180000000000.0,793.5,1.100000000001e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Training cost,1,262144,788306.6236983632,11628103.68,0.47115,409600.0,625521305.9046512,325017600.0,"Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.",https://arxiv.org/abs/2205.01068,OPT: Open Pre-trained Transformer Language Models
OPT-66B,Language,USA,Meta AI,2022-06-21,Industry,Question answering,Confident,1024.0,Open weights (non-commercial),"The Pile,BookCorpus (BooksCorpus, Toronto Book Corpus),CC-Stories,Pushshift Reddit",Open source,66000000000.0,180000000000.0,793.5,1.100000000001e+23,1877.1621580193864,11355.57,400.0,575206000000000.0,624000000000000.0,80000000000.0,NVIDIA A100 SXM4 80 GB,Training cost,1,262144,788306.6236983632,11628103.68,0.47115,409600.0,625521305.9046512,325017600.0,"Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.",https://arxiv.org/abs/2205.01068,OPT: Open Pre-trained Transformer Language Models
PLaMo-100B,Language,Japan,Preferred Networks Inc,2024-06-14,Industry,Language modeling/generation,Unverified,480.0,API access,Not-defined,Not-defined,100000000000.0,1500000000000.0,720.0,1.2e+24,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Historical significance,1,4000000,1814594.7887512865,7200000.0,4.793383333333332e-09,192000.0,1306508247.9009264,138240000.0,"Preferred Elements (PFE), a subsidiary of Preferred Networks (PFN), has been developing a 100 billion (100B) parameter LLM called ""PLaMo-100B"" since February. The pre-training part of the development of PLaMo-100B was completed in May, so in this article we will introduce the pre-training part of this model.",https://tech.preferred.jp/ja/blog/plamo-100b/,"Pre-training of the proprietary LLM ""PLaMo-100B"" with 100 billion parameters"
PLaMo-13B,Language,Japan,Preferred Networks Inc,2023-09-28,Industry,Chat,Confident,480.0,Open weights (unrestricted),"C4,Project Gutenberg,RedPajama,mC4,Wikipedia (ja)",Unreleased,13000000000.0,1500000000000.0,720.0,1.17e+23,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,4000000,3155569.9410964646,7200000.0,4.916290598290598e-08,192000.0,2272010357.5894547,138240000.0,,https://huggingface.co/pfnet/plamo-13b, PLaMo-13B
PLaMo-13B,Language,Japan,Preferred Networks Inc,2023-09-28,Industry,Language modeling/generation,Confident,480.0,Open weights (unrestricted),"C4,Project Gutenberg,RedPajama,mC4,Wikipedia (ja)",Unreleased,13000000000.0,1500000000000.0,720.0,1.17e+23,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,4000000,3155569.9410964646,7200000.0,4.916290598290598e-08,192000.0,2272010357.5894547,138240000.0,,https://huggingface.co/pfnet/plamo-13b, PLaMo-13B
PLaMo-13B,Language,Japan,Preferred Networks Inc,2023-09-28,Industry,Question answering,Confident,480.0,Open weights (unrestricted),"C4,Project Gutenberg,RedPajama,mC4,Wikipedia (ja)",Unreleased,13000000000.0,1500000000000.0,720.0,1.17e+23,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,4000000,3155569.9410964646,7200000.0,4.916290598290598e-08,192000.0,2272010357.5894547,138240000.0,,https://huggingface.co/pfnet/plamo-13b, PLaMo-13B
PaLI,Language,USA,Google,2022-09-14,Industry,Language modeling/generation,Likely,1024.0,Unreleased,WebLI,Unreleased,16900000000.0,1600000000.0,240.0,1.69e+23,1877.1621580193864,6796.58,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,SOTA improvement,1,6472188,436202.7314577782,6959697.92,1.6272189349112424e-08,196608.0,104688655.54986677,47185920.0,"Effective scaling and a flexible task interface enable large language models to excel at many tasks. We present PaLI (Pathways Language and Image model), a model that extends this approach to the joint modeling of language and vision. PaLI generates text based on visual and textual inputs, and with this interface performs many vision, language, and multimodal tasks, in many languages. To train PaLI, we make use of large pre-trained encoder-decoder language models and Vision Transformers (ViTs). This allows us to capitalize on their existing capabilities and leverage the substantial cost of training them. We find that joint scaling of the vision and language components is important. Since existing Transformers for language are much larger than their vision counterparts, we train a large, 4-billion parameter ViT (ViT-e) to quantify the benefits from even larger-capacity vision models. To train PaLI, we create a large multilingual mix of pretraining tasks, based on a new image-text training set containing 10B images and texts in over 100 languages. PaLI achieves state-of-the-art in multiple vision and language tasks (such as captioning, visual question-answering, scene-text understanding), while retaining a simple, modular, and scalable design.",https://arxiv.org/abs/2209.06794v4,PaLI: A Jointly-Scaled Multilingual Language-Image Model
PaLI,Language,USA,Google,2022-09-14,Industry,Visual question answering,Likely,1024.0,Unreleased,WebLI,Unreleased,16900000000.0,1600000000.0,240.0,1.69e+23,1877.1621580193864,6796.58,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,SOTA improvement,1,6472188,436202.7314577782,6959697.92,1.6272189349112424e-08,196608.0,104688655.54986677,47185920.0,"Effective scaling and a flexible task interface enable large language models to excel at many tasks. We present PaLI (Pathways Language and Image model), a model that extends this approach to the joint modeling of language and vision. PaLI generates text based on visual and textual inputs, and with this interface performs many vision, language, and multimodal tasks, in many languages. To train PaLI, we make use of large pre-trained encoder-decoder language models and Vision Transformers (ViTs). This allows us to capitalize on their existing capabilities and leverage the substantial cost of training them. We find that joint scaling of the vision and language components is important. Since existing Transformers for language are much larger than their vision counterparts, we train a large, 4-billion parameter ViT (ViT-e) to quantify the benefits from even larger-capacity vision models. To train PaLI, we create a large multilingual mix of pretraining tasks, based on a new image-text training set containing 10B images and texts in over 100 languages. PaLI achieves state-of-the-art in multiple vision and language tasks (such as captioning, visual question-answering, scene-text understanding), while retaining a simple, modular, and scalable design.",https://arxiv.org/abs/2209.06794v4,PaLI: A Jointly-Scaled Multilingual Language-Image Model
PaLI,Multimodal,USA,Google,2022-09-14,Industry,Language modeling/generation,Likely,1024.0,Unreleased,WebLI,Unreleased,16900000000.0,1600000000.0,240.0,1.69e+23,1877.1621580193864,6796.58,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,SOTA improvement,1,6472188,436202.7314577782,6959697.92,1.6272189349112424e-08,196608.0,104688655.54986677,47185920.0,"Effective scaling and a flexible task interface enable large language models to excel at many tasks. We present PaLI (Pathways Language and Image model), a model that extends this approach to the joint modeling of language and vision. PaLI generates text based on visual and textual inputs, and with this interface performs many vision, language, and multimodal tasks, in many languages. To train PaLI, we make use of large pre-trained encoder-decoder language models and Vision Transformers (ViTs). This allows us to capitalize on their existing capabilities and leverage the substantial cost of training them. We find that joint scaling of the vision and language components is important. Since existing Transformers for language are much larger than their vision counterparts, we train a large, 4-billion parameter ViT (ViT-e) to quantify the benefits from even larger-capacity vision models. To train PaLI, we create a large multilingual mix of pretraining tasks, based on a new image-text training set containing 10B images and texts in over 100 languages. PaLI achieves state-of-the-art in multiple vision and language tasks (such as captioning, visual question-answering, scene-text understanding), while retaining a simple, modular, and scalable design.",https://arxiv.org/abs/2209.06794v4,PaLI: A Jointly-Scaled Multilingual Language-Image Model
PaLI,Multimodal,USA,Google,2022-09-14,Industry,Visual question answering,Likely,1024.0,Unreleased,WebLI,Unreleased,16900000000.0,1600000000.0,240.0,1.69e+23,1877.1621580193864,6796.58,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,SOTA improvement,1,6472188,436202.7314577782,6959697.92,1.6272189349112424e-08,196608.0,104688655.54986677,47185920.0,"Effective scaling and a flexible task interface enable large language models to excel at many tasks. We present PaLI (Pathways Language and Image model), a model that extends this approach to the joint modeling of language and vision. PaLI generates text based on visual and textual inputs, and with this interface performs many vision, language, and multimodal tasks, in many languages. To train PaLI, we make use of large pre-trained encoder-decoder language models and Vision Transformers (ViTs). This allows us to capitalize on their existing capabilities and leverage the substantial cost of training them. We find that joint scaling of the vision and language components is important. Since existing Transformers for language are much larger than their vision counterparts, we train a large, 4-billion parameter ViT (ViT-e) to quantify the benefits from even larger-capacity vision models. To train PaLI, we create a large multilingual mix of pretraining tasks, based on a new image-text training set containing 10B images and texts in over 100 languages. PaLI achieves state-of-the-art in multiple vision and language tasks (such as captioning, visual question-answering, scene-text understanding), while retaining a simple, modular, and scalable design.",https://arxiv.org/abs/2209.06794v4,PaLI: A Jointly-Scaled Multilingual Language-Image Model
PaLI,Vision,USA,Google,2022-09-14,Industry,Language modeling/generation,Likely,1024.0,Unreleased,WebLI,Unreleased,16900000000.0,1600000000.0,240.0,1.69e+23,1877.1621580193864,6796.58,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,SOTA improvement,1,6472188,436202.7314577782,6959697.92,1.6272189349112424e-08,196608.0,104688655.54986677,47185920.0,"Effective scaling and a flexible task interface enable large language models to excel at many tasks. We present PaLI (Pathways Language and Image model), a model that extends this approach to the joint modeling of language and vision. PaLI generates text based on visual and textual inputs, and with this interface performs many vision, language, and multimodal tasks, in many languages. To train PaLI, we make use of large pre-trained encoder-decoder language models and Vision Transformers (ViTs). This allows us to capitalize on their existing capabilities and leverage the substantial cost of training them. We find that joint scaling of the vision and language components is important. Since existing Transformers for language are much larger than their vision counterparts, we train a large, 4-billion parameter ViT (ViT-e) to quantify the benefits from even larger-capacity vision models. To train PaLI, we create a large multilingual mix of pretraining tasks, based on a new image-text training set containing 10B images and texts in over 100 languages. PaLI achieves state-of-the-art in multiple vision and language tasks (such as captioning, visual question-answering, scene-text understanding), while retaining a simple, modular, and scalable design.",https://arxiv.org/abs/2209.06794v4,PaLI: A Jointly-Scaled Multilingual Language-Image Model
PaLI,Vision,USA,Google,2022-09-14,Industry,Visual question answering,Likely,1024.0,Unreleased,WebLI,Unreleased,16900000000.0,1600000000.0,240.0,1.69e+23,1877.1621580193864,6796.58,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,SOTA improvement,1,6472188,436202.7314577782,6959697.92,1.6272189349112424e-08,196608.0,104688655.54986677,47185920.0,"Effective scaling and a flexible task interface enable large language models to excel at many tasks. We present PaLI (Pathways Language and Image model), a model that extends this approach to the joint modeling of language and vision. PaLI generates text based on visual and textual inputs, and with this interface performs many vision, language, and multimodal tasks, in many languages. To train PaLI, we make use of large pre-trained encoder-decoder language models and Vision Transformers (ViTs). This allows us to capitalize on their existing capabilities and leverage the substantial cost of training them. We find that joint scaling of the vision and language components is important. Since existing Transformers for language are much larger than their vision counterparts, we train a large, 4-billion parameter ViT (ViT-e) to quantify the benefits from even larger-capacity vision models. To train PaLI, we create a large multilingual mix of pretraining tasks, based on a new image-text training set containing 10B images and texts in over 100 languages. PaLI achieves state-of-the-art in multiple vision and language tasks (such as captioning, visual question-answering, scene-text understanding), while retaining a simple, modular, and scalable design.",https://arxiv.org/abs/2209.06794v4,PaLI: A Jointly-Scaled Multilingual Language-Image Model
PaLM (540B),Language,Multinational,Google Research,2022-04-04,Industry,Code generation,Confident,6144.0,Unreleased,"Wikipedia,GLaM dataset,LaMBDA dataset,GitHub",Unreleased,540350000000.0,585000000000.0,1536.0,2.5272e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Highly cited,1,4000000,2621496.0548983514,41758187.519999996,0.462,1179648.0,4026617940.323868,1811939328.0,"Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.",https://arxiv.org/abs/2204.02311,PaLM: Scaling Language Modeling with Pathways
PaLM (540B),Language,Multinational,Google Research,2022-04-04,Industry,Code generation,Confident,6144.0,Unreleased,"Wikipedia,GLaM dataset,LaMBDA dataset,GitHub",Unreleased,540350000000.0,585000000000.0,1536.0,2.5272e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,SOTA improvement,1,4000000,2621496.0548983514,41758187.519999996,0.462,1179648.0,4026617940.323868,1811939328.0,"Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.",https://arxiv.org/abs/2204.02311,PaLM: Scaling Language Modeling with Pathways
PaLM (540B),Language,Multinational,Google Research,2022-04-04,Industry,Code generation,Confident,6144.0,Unreleased,"Wikipedia,GLaM dataset,LaMBDA dataset,GitHub",Unreleased,540350000000.0,585000000000.0,1536.0,2.5272e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4000000,2621496.0548983514,41758187.519999996,0.462,1179648.0,4026617940.323868,1811939328.0,"Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.",https://arxiv.org/abs/2204.02311,PaLM: Scaling Language Modeling with Pathways
PaLM (540B),Language,Multinational,Google Research,2022-04-04,Industry,Language modeling,Confident,6144.0,Unreleased,"Wikipedia,GLaM dataset,LaMBDA dataset,GitHub",Unreleased,540350000000.0,585000000000.0,1536.0,2.5272e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Highly cited,1,4000000,2621496.0548983514,41758187.519999996,0.462,1179648.0,4026617940.323868,1811939328.0,"Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.",https://arxiv.org/abs/2204.02311,PaLM: Scaling Language Modeling with Pathways
PaLM (540B),Language,Multinational,Google Research,2022-04-04,Industry,Language modeling,Confident,6144.0,Unreleased,"Wikipedia,GLaM dataset,LaMBDA dataset,GitHub",Unreleased,540350000000.0,585000000000.0,1536.0,2.5272e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,SOTA improvement,1,4000000,2621496.0548983514,41758187.519999996,0.462,1179648.0,4026617940.323868,1811939328.0,"Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.",https://arxiv.org/abs/2204.02311,PaLM: Scaling Language Modeling with Pathways
PaLM (540B),Language,Multinational,Google Research,2022-04-04,Industry,Language modeling,Confident,6144.0,Unreleased,"Wikipedia,GLaM dataset,LaMBDA dataset,GitHub",Unreleased,540350000000.0,585000000000.0,1536.0,2.5272e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4000000,2621496.0548983514,41758187.519999996,0.462,1179648.0,4026617940.323868,1811939328.0,"Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.",https://arxiv.org/abs/2204.02311,PaLM: Scaling Language Modeling with Pathways
PaLM (540B),Language,Multinational,Google Research,2022-04-04,Industry,Translation,Confident,6144.0,Unreleased,"Wikipedia,GLaM dataset,LaMBDA dataset,GitHub",Unreleased,540350000000.0,585000000000.0,1536.0,2.5272e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Highly cited,1,4000000,2621496.0548983514,41758187.519999996,0.462,1179648.0,4026617940.323868,1811939328.0,"Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.",https://arxiv.org/abs/2204.02311,PaLM: Scaling Language Modeling with Pathways
PaLM (540B),Language,Multinational,Google Research,2022-04-04,Industry,Translation,Confident,6144.0,Unreleased,"Wikipedia,GLaM dataset,LaMBDA dataset,GitHub",Unreleased,540350000000.0,585000000000.0,1536.0,2.5272e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,SOTA improvement,1,4000000,2621496.0548983514,41758187.519999996,0.462,1179648.0,4026617940.323868,1811939328.0,"Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.",https://arxiv.org/abs/2204.02311,PaLM: Scaling Language Modeling with Pathways
PaLM (540B),Language,Multinational,Google Research,2022-04-04,Industry,Translation,Confident,6144.0,Unreleased,"Wikipedia,GLaM dataset,LaMBDA dataset,GitHub",Unreleased,540350000000.0,585000000000.0,1536.0,2.5272e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4000000,2621496.0548983514,41758187.519999996,0.462,1179648.0,4026617940.323868,1811939328.0,"Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.",https://arxiv.org/abs/2204.02311,PaLM: Scaling Language Modeling with Pathways
PaLM (540B),Language,USA,Google Research,2022-04-04,Industry,Code generation,Confident,6144.0,Unreleased,"Wikipedia,GLaM dataset,LaMBDA dataset,GitHub",Unreleased,540350000000.0,585000000000.0,1536.0,2.5272e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Highly cited,1,4000000,2621496.0548983514,41758187.519999996,0.462,1179648.0,4026617940.323868,1811939328.0,"Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.",https://arxiv.org/abs/2204.02311,PaLM: Scaling Language Modeling with Pathways
PaLM (540B),Language,USA,Google Research,2022-04-04,Industry,Code generation,Confident,6144.0,Unreleased,"Wikipedia,GLaM dataset,LaMBDA dataset,GitHub",Unreleased,540350000000.0,585000000000.0,1536.0,2.5272e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,SOTA improvement,1,4000000,2621496.0548983514,41758187.519999996,0.462,1179648.0,4026617940.323868,1811939328.0,"Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.",https://arxiv.org/abs/2204.02311,PaLM: Scaling Language Modeling with Pathways
PaLM (540B),Language,USA,Google Research,2022-04-04,Industry,Code generation,Confident,6144.0,Unreleased,"Wikipedia,GLaM dataset,LaMBDA dataset,GitHub",Unreleased,540350000000.0,585000000000.0,1536.0,2.5272e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4000000,2621496.0548983514,41758187.519999996,0.462,1179648.0,4026617940.323868,1811939328.0,"Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.",https://arxiv.org/abs/2204.02311,PaLM: Scaling Language Modeling with Pathways
PaLM (540B),Language,USA,Google Research,2022-04-04,Industry,Language modeling,Confident,6144.0,Unreleased,"Wikipedia,GLaM dataset,LaMBDA dataset,GitHub",Unreleased,540350000000.0,585000000000.0,1536.0,2.5272e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Highly cited,1,4000000,2621496.0548983514,41758187.519999996,0.462,1179648.0,4026617940.323868,1811939328.0,"Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.",https://arxiv.org/abs/2204.02311,PaLM: Scaling Language Modeling with Pathways
PaLM (540B),Language,USA,Google Research,2022-04-04,Industry,Language modeling,Confident,6144.0,Unreleased,"Wikipedia,GLaM dataset,LaMBDA dataset,GitHub",Unreleased,540350000000.0,585000000000.0,1536.0,2.5272e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,SOTA improvement,1,4000000,2621496.0548983514,41758187.519999996,0.462,1179648.0,4026617940.323868,1811939328.0,"Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.",https://arxiv.org/abs/2204.02311,PaLM: Scaling Language Modeling with Pathways
PaLM (540B),Language,USA,Google Research,2022-04-04,Industry,Language modeling,Confident,6144.0,Unreleased,"Wikipedia,GLaM dataset,LaMBDA dataset,GitHub",Unreleased,540350000000.0,585000000000.0,1536.0,2.5272e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4000000,2621496.0548983514,41758187.519999996,0.462,1179648.0,4026617940.323868,1811939328.0,"Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.",https://arxiv.org/abs/2204.02311,PaLM: Scaling Language Modeling with Pathways
PaLM (540B),Language,USA,Google Research,2022-04-04,Industry,Translation,Confident,6144.0,Unreleased,"Wikipedia,GLaM dataset,LaMBDA dataset,GitHub",Unreleased,540350000000.0,585000000000.0,1536.0,2.5272e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Highly cited,1,4000000,2621496.0548983514,41758187.519999996,0.462,1179648.0,4026617940.323868,1811939328.0,"Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.",https://arxiv.org/abs/2204.02311,PaLM: Scaling Language Modeling with Pathways
PaLM (540B),Language,USA,Google Research,2022-04-04,Industry,Translation,Confident,6144.0,Unreleased,"Wikipedia,GLaM dataset,LaMBDA dataset,GitHub",Unreleased,540350000000.0,585000000000.0,1536.0,2.5272e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,SOTA improvement,1,4000000,2621496.0548983514,41758187.519999996,0.462,1179648.0,4026617940.323868,1811939328.0,"Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.",https://arxiv.org/abs/2204.02311,PaLM: Scaling Language Modeling with Pathways
PaLM (540B),Language,USA,Google Research,2022-04-04,Industry,Translation,Confident,6144.0,Unreleased,"Wikipedia,GLaM dataset,LaMBDA dataset,GitHub",Unreleased,540350000000.0,585000000000.0,1536.0,2.5272e+24,1877.1621580193864,6796.579999999999,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4000000,2621496.0548983514,41758187.519999996,0.462,1179648.0,4026617940.323868,1811939328.0,"Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.",https://arxiv.org/abs/2204.02311,PaLM: Scaling Language Modeling with Pathways
PaLM 2,Language,USA,Google,2023-05-10,Industry,Language modeling,Likely,6144.0,API access,Not-defined,Unreleased,340000000000.0,2700000000000.0,1536.0,7.34e+24,1877.1621580193864,6796.58,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,SOTA improvement,1,4000000,2621496.0548983514,41758187.519999996,3.7465940054495914e-10,1179648.0,4026617940.323868,1811939328.0,"We introduce PaLM 2, a new state-of-the-art language model that has better multilingual and reasoning capabilities and is more compute-efficient than its predecessor PaLM (Chowdhery et al., 2022). PaLM 2 is a Transformer-based model trained using a mixture of objectives similar to UL2 (Tay et al., 2023). Through extensive evaluations on English and multilingual language, and reasoning tasks, we demonstrate that PaLM 2 has significantly improved quality on downstream tasks across different model sizes, while simultaneously exhibiting faster and more efficient inference compared to PaLM. This improved efficiency enables broader deployment while also allowing the model to respond faster, for a more natural pace of interaction. PaLM 2 demonstrates robust reasoning capabilities exemplified by large improvements over PaLM on BIG-Bench and other reasoning tasks. PaLM 2 exhibits stable performance on a suite of responsible AI evaluations, and enables inference-time control over toxicity without additional overhead or impact on other capabilities. Overall, PaLM 2 achieves state-of-the-art performance across a diverse set of tasks and capabilities.",https://arxiv.org/abs/2305.10403,PaLM 2 Technical Report
PaLM 2,Language,USA,Google,2023-05-10,Industry,Language modeling,Likely,6144.0,API access,Not-defined,Unreleased,340000000000.0,2700000000000.0,1536.0,7.34e+24,1877.1621580193864,6796.58,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4000000,2621496.0548983514,41758187.519999996,3.7465940054495914e-10,1179648.0,4026617940.323868,1811939328.0,"We introduce PaLM 2, a new state-of-the-art language model that has better multilingual and reasoning capabilities and is more compute-efficient than its predecessor PaLM (Chowdhery et al., 2022). PaLM 2 is a Transformer-based model trained using a mixture of objectives similar to UL2 (Tay et al., 2023). Through extensive evaluations on English and multilingual language, and reasoning tasks, we demonstrate that PaLM 2 has significantly improved quality on downstream tasks across different model sizes, while simultaneously exhibiting faster and more efficient inference compared to PaLM. This improved efficiency enables broader deployment while also allowing the model to respond faster, for a more natural pace of interaction. PaLM 2 demonstrates robust reasoning capabilities exemplified by large improvements over PaLM on BIG-Bench and other reasoning tasks. PaLM 2 exhibits stable performance on a suite of responsible AI evaluations, and enables inference-time control over toxicity without additional overhead or impact on other capabilities. Overall, PaLM 2 achieves state-of-the-art performance across a diverse set of tasks and capabilities.",https://arxiv.org/abs/2305.10403,PaLM 2 Technical Report
PaLM 2,Language,USA,Google,2023-05-10,Industry,Language modeling/generation,Likely,6144.0,API access,Not-defined,Unreleased,340000000000.0,2700000000000.0,1536.0,7.34e+24,1877.1621580193864,6796.58,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,SOTA improvement,1,4000000,2621496.0548983514,41758187.519999996,3.7465940054495914e-10,1179648.0,4026617940.323868,1811939328.0,"We introduce PaLM 2, a new state-of-the-art language model that has better multilingual and reasoning capabilities and is more compute-efficient than its predecessor PaLM (Chowdhery et al., 2022). PaLM 2 is a Transformer-based model trained using a mixture of objectives similar to UL2 (Tay et al., 2023). Through extensive evaluations on English and multilingual language, and reasoning tasks, we demonstrate that PaLM 2 has significantly improved quality on downstream tasks across different model sizes, while simultaneously exhibiting faster and more efficient inference compared to PaLM. This improved efficiency enables broader deployment while also allowing the model to respond faster, for a more natural pace of interaction. PaLM 2 demonstrates robust reasoning capabilities exemplified by large improvements over PaLM on BIG-Bench and other reasoning tasks. PaLM 2 exhibits stable performance on a suite of responsible AI evaluations, and enables inference-time control over toxicity without additional overhead or impact on other capabilities. Overall, PaLM 2 achieves state-of-the-art performance across a diverse set of tasks and capabilities.",https://arxiv.org/abs/2305.10403,PaLM 2 Technical Report
PaLM 2,Language,USA,Google,2023-05-10,Industry,Language modeling/generation,Likely,6144.0,API access,Not-defined,Unreleased,340000000000.0,2700000000000.0,1536.0,7.34e+24,1877.1621580193864,6796.58,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4000000,2621496.0548983514,41758187.519999996,3.7465940054495914e-10,1179648.0,4026617940.323868,1811939328.0,"We introduce PaLM 2, a new state-of-the-art language model that has better multilingual and reasoning capabilities and is more compute-efficient than its predecessor PaLM (Chowdhery et al., 2022). PaLM 2 is a Transformer-based model trained using a mixture of objectives similar to UL2 (Tay et al., 2023). Through extensive evaluations on English and multilingual language, and reasoning tasks, we demonstrate that PaLM 2 has significantly improved quality on downstream tasks across different model sizes, while simultaneously exhibiting faster and more efficient inference compared to PaLM. This improved efficiency enables broader deployment while also allowing the model to respond faster, for a more natural pace of interaction. PaLM 2 demonstrates robust reasoning capabilities exemplified by large improvements over PaLM on BIG-Bench and other reasoning tasks. PaLM 2 exhibits stable performance on a suite of responsible AI evaluations, and enables inference-time control over toxicity without additional overhead or impact on other capabilities. Overall, PaLM 2 achieves state-of-the-art performance across a diverse set of tasks and capabilities.",https://arxiv.org/abs/2305.10403,PaLM 2 Technical Report
PanGu-Σ,Language,China,Huawei Noah's Ark Lab,2023-03-20,Industry,Code generation,Confident,512.0,Unreleased,Not-defined,Unreleased,1085000000000.0,246750000000.0,2400.0,4.67e+23,1877.1621580193864,9086.19,310.0,256000000000000.0,512000000000000.0,54246185353.93311,Huawei Ascend 910,SOTA improvement,1,524288,351495.4313105972,4652129.28,5.481798715203427e-09,158720.0,843589035.1454333,380928000.0,"The scaling of large language models has greatly improved natural language understanding, generation, and reasoning. In this work, we develop a system that trained a trillion-parameter language model on a cluster of Ascend 910 AI processors and MindSpore framework, and present the language model with 1.085T parameters named PanGu-{\Sigma}. With parameter inherent from PanGu-{\alpha}, we extend the dense Transformer model to sparse one with Random Routed Experts (RRE), and efficiently train the model over 329B tokens by using Expert Computation and Storage Separation(ECSS). This resulted in a 6.3x increase in training throughput through heterogeneous computing. Our experimental findings show that PanGu-{\Sigma} provides state-of-the-art performance in zero-shot learning of various Chinese NLP downstream tasks. Moreover, it demonstrates strong abilities when fine-tuned in application data of open-domain dialogue, question answering, machine translation and code generation.",https://arxiv.org/abs/2303.10845,PanGu-Σ: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing
PanGu-Σ,Language,China,Huawei Noah's Ark Lab,2023-03-20,Industry,Language modeling,Confident,512.0,Unreleased,Not-defined,Unreleased,1085000000000.0,246750000000.0,2400.0,4.67e+23,1877.1621580193864,9086.19,310.0,256000000000000.0,512000000000000.0,54246185353.93311,Huawei Ascend 910,SOTA improvement,1,524288,351495.4313105972,4652129.28,5.481798715203427e-09,158720.0,843589035.1454333,380928000.0,"The scaling of large language models has greatly improved natural language understanding, generation, and reasoning. In this work, we develop a system that trained a trillion-parameter language model on a cluster of Ascend 910 AI processors and MindSpore framework, and present the language model with 1.085T parameters named PanGu-{\Sigma}. With parameter inherent from PanGu-{\alpha}, we extend the dense Transformer model to sparse one with Random Routed Experts (RRE), and efficiently train the model over 329B tokens by using Expert Computation and Storage Separation(ECSS). This resulted in a 6.3x increase in training throughput through heterogeneous computing. Our experimental findings show that PanGu-{\Sigma} provides state-of-the-art performance in zero-shot learning of various Chinese NLP downstream tasks. Moreover, it demonstrates strong abilities when fine-tuned in application data of open-domain dialogue, question answering, machine translation and code generation.",https://arxiv.org/abs/2303.10845,PanGu-Σ: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing
PanGu-Σ,Language,China,Huawei Noah's Ark Lab,2023-03-20,Industry,Question answering,Confident,512.0,Unreleased,Not-defined,Unreleased,1085000000000.0,246750000000.0,2400.0,4.67e+23,1877.1621580193864,9086.19,310.0,256000000000000.0,512000000000000.0,54246185353.93311,Huawei Ascend 910,SOTA improvement,1,524288,351495.4313105972,4652129.28,5.481798715203427e-09,158720.0,843589035.1454333,380928000.0,"The scaling of large language models has greatly improved natural language understanding, generation, and reasoning. In this work, we develop a system that trained a trillion-parameter language model on a cluster of Ascend 910 AI processors and MindSpore framework, and present the language model with 1.085T parameters named PanGu-{\Sigma}. With parameter inherent from PanGu-{\alpha}, we extend the dense Transformer model to sparse one with Random Routed Experts (RRE), and efficiently train the model over 329B tokens by using Expert Computation and Storage Separation(ECSS). This resulted in a 6.3x increase in training throughput through heterogeneous computing. Our experimental findings show that PanGu-{\Sigma} provides state-of-the-art performance in zero-shot learning of various Chinese NLP downstream tasks. Moreover, it demonstrates strong abilities when fine-tuned in application data of open-domain dialogue, question answering, machine translation and code generation.",https://arxiv.org/abs/2303.10845,PanGu-Σ: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing
PanGu-Σ,Language,China,Huawei Noah's Ark Lab,2023-03-20,Industry,Translation,Confident,512.0,Unreleased,Not-defined,Unreleased,1085000000000.0,246750000000.0,2400.0,4.67e+23,1877.1621580193864,9086.19,310.0,256000000000000.0,512000000000000.0,54246185353.93311,Huawei Ascend 910,SOTA improvement,1,524288,351495.4313105972,4652129.28,5.481798715203427e-09,158720.0,843589035.1454333,380928000.0,"The scaling of large language models has greatly improved natural language understanding, generation, and reasoning. In this work, we develop a system that trained a trillion-parameter language model on a cluster of Ascend 910 AI processors and MindSpore framework, and present the language model with 1.085T parameters named PanGu-{\Sigma}. With parameter inherent from PanGu-{\alpha}, we extend the dense Transformer model to sparse one with Random Routed Experts (RRE), and efficiently train the model over 329B tokens by using Expert Computation and Storage Separation(ECSS). This resulted in a 6.3x increase in training throughput through heterogeneous computing. Our experimental findings show that PanGu-{\Sigma} provides state-of-the-art performance in zero-shot learning of various Chinese NLP downstream tasks. Moreover, it demonstrates strong abilities when fine-tuned in application data of open-domain dialogue, question answering, machine translation and code generation.",https://arxiv.org/abs/2303.10845,PanGu-Σ: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing
Pangu 3.0,Image generation,China,Huawei,2023-07-07,Industry,Image generation,Likely,512.0,API access,Not-defined,Not-defined,100000000000.0,246750000000.0,2400.0,4.67e+23,1877.1621580193864,9086.19,310.0,256000000000000.0,512000000000000.0,54246185353.93311,Huawei Ascend 910,SOTA improvement,1,524288,351495.4313105972,4652129.28,5.481798715203427e-09,158720.0,843589035.1454333,380928000.0,"Huawei Cloud Pangu models were designed to focus on the practical needs of specific industry scenarios. The newly-launched Pangu Models 3.0 use a ""5+N+X"" three-layer architecture.

The L0 layer consists of five foundation models: NLP, CV, multimodal, prediction, and scientific computing, which provide general skills to power an endless possibility of industry-specific applications. Pangu Models 3.0 are available in different sizes: 10 billion parameters, 38 billion parameters, 71 billion parameters, and 100 billion parameters, meeting diverse customer needs and different standards on latency and response times. Brand-new capability sets are also provided, such as knowledge-based Q&A, copywriting, and code generation for the Pangu NLP model; and image generation and understanding for the Pangu multimodal model. All of these capability sets will be made available to customers and partners, and will be consistent regardless of the model size (number of parameters).",https://www.huaweicloud.com/intl/en-us/news/20230707180809498.html,"Reshaping Industries with AI: Huawei Cloud Launches Pangu Models 3.0 and Ascend AI Cloud Services
"
Pangu 3.0,Image generation,China,Huawei,2023-07-07,Industry,Language modeling/generation,Likely,512.0,API access,Not-defined,Not-defined,100000000000.0,246750000000.0,2400.0,4.67e+23,1877.1621580193864,9086.19,310.0,256000000000000.0,512000000000000.0,54246185353.93311,Huawei Ascend 910,SOTA improvement,1,524288,351495.4313105972,4652129.28,5.481798715203427e-09,158720.0,843589035.1454333,380928000.0,"Huawei Cloud Pangu models were designed to focus on the practical needs of specific industry scenarios. The newly-launched Pangu Models 3.0 use a ""5+N+X"" three-layer architecture.

The L0 layer consists of five foundation models: NLP, CV, multimodal, prediction, and scientific computing, which provide general skills to power an endless possibility of industry-specific applications. Pangu Models 3.0 are available in different sizes: 10 billion parameters, 38 billion parameters, 71 billion parameters, and 100 billion parameters, meeting diverse customer needs and different standards on latency and response times. Brand-new capability sets are also provided, such as knowledge-based Q&A, copywriting, and code generation for the Pangu NLP model; and image generation and understanding for the Pangu multimodal model. All of these capability sets will be made available to customers and partners, and will be consistent regardless of the model size (number of parameters).",https://www.huaweicloud.com/intl/en-us/news/20230707180809498.html,"Reshaping Industries with AI: Huawei Cloud Launches Pangu Models 3.0 and Ascend AI Cloud Services
"
Pangu 3.0,Language,China,Huawei,2023-07-07,Industry,Image generation,Likely,512.0,API access,Not-defined,Not-defined,100000000000.0,246750000000.0,2400.0,4.67e+23,1877.1621580193864,9086.19,310.0,256000000000000.0,512000000000000.0,54246185353.93311,Huawei Ascend 910,SOTA improvement,1,524288,351495.4313105972,4652129.28,5.481798715203427e-09,158720.0,843589035.1454333,380928000.0,"Huawei Cloud Pangu models were designed to focus on the practical needs of specific industry scenarios. The newly-launched Pangu Models 3.0 use a ""5+N+X"" three-layer architecture.

The L0 layer consists of five foundation models: NLP, CV, multimodal, prediction, and scientific computing, which provide general skills to power an endless possibility of industry-specific applications. Pangu Models 3.0 are available in different sizes: 10 billion parameters, 38 billion parameters, 71 billion parameters, and 100 billion parameters, meeting diverse customer needs and different standards on latency and response times. Brand-new capability sets are also provided, such as knowledge-based Q&A, copywriting, and code generation for the Pangu NLP model; and image generation and understanding for the Pangu multimodal model. All of these capability sets will be made available to customers and partners, and will be consistent regardless of the model size (number of parameters).",https://www.huaweicloud.com/intl/en-us/news/20230707180809498.html,"Reshaping Industries with AI: Huawei Cloud Launches Pangu Models 3.0 and Ascend AI Cloud Services
"
Pangu 3.0,Language,China,Huawei,2023-07-07,Industry,Language modeling/generation,Likely,512.0,API access,Not-defined,Not-defined,100000000000.0,246750000000.0,2400.0,4.67e+23,1877.1621580193864,9086.19,310.0,256000000000000.0,512000000000000.0,54246185353.93311,Huawei Ascend 910,SOTA improvement,1,524288,351495.4313105972,4652129.28,5.481798715203427e-09,158720.0,843589035.1454333,380928000.0,"Huawei Cloud Pangu models were designed to focus on the practical needs of specific industry scenarios. The newly-launched Pangu Models 3.0 use a ""5+N+X"" three-layer architecture.

The L0 layer consists of five foundation models: NLP, CV, multimodal, prediction, and scientific computing, which provide general skills to power an endless possibility of industry-specific applications. Pangu Models 3.0 are available in different sizes: 10 billion parameters, 38 billion parameters, 71 billion parameters, and 100 billion parameters, meeting diverse customer needs and different standards on latency and response times. Brand-new capability sets are also provided, such as knowledge-based Q&A, copywriting, and code generation for the Pangu NLP model; and image generation and understanding for the Pangu multimodal model. All of these capability sets will be made available to customers and partners, and will be consistent regardless of the model size (number of parameters).",https://www.huaweicloud.com/intl/en-us/news/20230707180809498.html,"Reshaping Industries with AI: Huawei Cloud Launches Pangu Models 3.0 and Ascend AI Cloud Services
"
Pangu 3.0,Multimodal,China,Huawei,2023-07-07,Industry,Image generation,Likely,512.0,API access,Not-defined,Not-defined,100000000000.0,246750000000.0,2400.0,4.67e+23,1877.1621580193864,9086.19,310.0,256000000000000.0,512000000000000.0,54246185353.93311,Huawei Ascend 910,SOTA improvement,1,524288,351495.4313105972,4652129.28,5.481798715203427e-09,158720.0,843589035.1454333,380928000.0,"Huawei Cloud Pangu models were designed to focus on the practical needs of specific industry scenarios. The newly-launched Pangu Models 3.0 use a ""5+N+X"" three-layer architecture.

The L0 layer consists of five foundation models: NLP, CV, multimodal, prediction, and scientific computing, which provide general skills to power an endless possibility of industry-specific applications. Pangu Models 3.0 are available in different sizes: 10 billion parameters, 38 billion parameters, 71 billion parameters, and 100 billion parameters, meeting diverse customer needs and different standards on latency and response times. Brand-new capability sets are also provided, such as knowledge-based Q&A, copywriting, and code generation for the Pangu NLP model; and image generation and understanding for the Pangu multimodal model. All of these capability sets will be made available to customers and partners, and will be consistent regardless of the model size (number of parameters).",https://www.huaweicloud.com/intl/en-us/news/20230707180809498.html,"Reshaping Industries with AI: Huawei Cloud Launches Pangu Models 3.0 and Ascend AI Cloud Services
"
Pangu 3.0,Multimodal,China,Huawei,2023-07-07,Industry,Language modeling/generation,Likely,512.0,API access,Not-defined,Not-defined,100000000000.0,246750000000.0,2400.0,4.67e+23,1877.1621580193864,9086.19,310.0,256000000000000.0,512000000000000.0,54246185353.93311,Huawei Ascend 910,SOTA improvement,1,524288,351495.4313105972,4652129.28,5.481798715203427e-09,158720.0,843589035.1454333,380928000.0,"Huawei Cloud Pangu models were designed to focus on the practical needs of specific industry scenarios. The newly-launched Pangu Models 3.0 use a ""5+N+X"" three-layer architecture.

The L0 layer consists of five foundation models: NLP, CV, multimodal, prediction, and scientific computing, which provide general skills to power an endless possibility of industry-specific applications. Pangu Models 3.0 are available in different sizes: 10 billion parameters, 38 billion parameters, 71 billion parameters, and 100 billion parameters, meeting diverse customer needs and different standards on latency and response times. Brand-new capability sets are also provided, such as knowledge-based Q&A, copywriting, and code generation for the Pangu NLP model; and image generation and understanding for the Pangu multimodal model. All of these capability sets will be made available to customers and partners, and will be consistent regardless of the model size (number of parameters).",https://www.huaweicloud.com/intl/en-us/news/20230707180809498.html,"Reshaping Industries with AI: Huawei Cloud Launches Pangu Models 3.0 and Ascend AI Cloud Services
"
Pangu 3.0,Vision,China,Huawei,2023-07-07,Industry,Image generation,Likely,512.0,API access,Not-defined,Not-defined,100000000000.0,246750000000.0,2400.0,4.67e+23,1877.1621580193864,9086.19,310.0,256000000000000.0,512000000000000.0,54246185353.93311,Huawei Ascend 910,SOTA improvement,1,524288,351495.4313105972,4652129.28,5.481798715203427e-09,158720.0,843589035.1454333,380928000.0,"Huawei Cloud Pangu models were designed to focus on the practical needs of specific industry scenarios. The newly-launched Pangu Models 3.0 use a ""5+N+X"" three-layer architecture.

The L0 layer consists of five foundation models: NLP, CV, multimodal, prediction, and scientific computing, which provide general skills to power an endless possibility of industry-specific applications. Pangu Models 3.0 are available in different sizes: 10 billion parameters, 38 billion parameters, 71 billion parameters, and 100 billion parameters, meeting diverse customer needs and different standards on latency and response times. Brand-new capability sets are also provided, such as knowledge-based Q&A, copywriting, and code generation for the Pangu NLP model; and image generation and understanding for the Pangu multimodal model. All of these capability sets will be made available to customers and partners, and will be consistent regardless of the model size (number of parameters).",https://www.huaweicloud.com/intl/en-us/news/20230707180809498.html,"Reshaping Industries with AI: Huawei Cloud Launches Pangu Models 3.0 and Ascend AI Cloud Services
"
Pangu 3.0,Vision,China,Huawei,2023-07-07,Industry,Language modeling/generation,Likely,512.0,API access,Not-defined,Not-defined,100000000000.0,246750000000.0,2400.0,4.67e+23,1877.1621580193864,9086.19,310.0,256000000000000.0,512000000000000.0,54246185353.93311,Huawei Ascend 910,SOTA improvement,1,524288,351495.4313105972,4652129.28,5.481798715203427e-09,158720.0,843589035.1454333,380928000.0,"Huawei Cloud Pangu models were designed to focus on the practical needs of specific industry scenarios. The newly-launched Pangu Models 3.0 use a ""5+N+X"" three-layer architecture.

The L0 layer consists of five foundation models: NLP, CV, multimodal, prediction, and scientific computing, which provide general skills to power an endless possibility of industry-specific applications. Pangu Models 3.0 are available in different sizes: 10 billion parameters, 38 billion parameters, 71 billion parameters, and 100 billion parameters, meeting diverse customer needs and different standards on latency and response times. Brand-new capability sets are also provided, such as knowledge-based Q&A, copywriting, and code generation for the Pangu NLP model; and image generation and understanding for the Pangu multimodal model. All of these capability sets will be made available to customers and partners, and will be consistent regardless of the model size (number of parameters).",https://www.huaweicloud.com/intl/en-us/news/20230707180809498.html,"Reshaping Industries with AI: Huawei Cloud Launches Pangu Models 3.0 and Ascend AI Cloud Services
"
Pharia-1-LLM-7B,Language,Germany,Aleph Alpha,2024-08-26,Industry,Language modeling/generation,Confident,256.0,Open weights (non-commercial),Common Crawl,Not-defined,7041544704.0,7700000000000.0,720.0,4.43e+23,1877.1621580193864,27922.725,550.0,1213583000000000.0,1301500000000000.0,80000000000.0,"NVIDIA A100 SXM4 80 GB,NVIDIA H100 SXM5 80GB",SOTA improvement,1,4000000,904992.9349364166,7148217.6,2.739465011286682e-08,140800.0,651594913.15422,101376000.0,"We are pleased to announce our new foundation model family that includes Pharia-1-LLM-7B-control and Pharia-1-LLM-7B-control-aligned, now publicly available under the Open Aleph License, which explicitly allows for non-commercial research and educational use. Pharia-1-LLM-7B-control is engineered to deliver concise, length-controlled responses that match the performance of leading open-source models in the 7B to 8B parameter range and is culturally and linguistically optimized for German, French, and Spanish by being trained on a multilingual base corpus. Pharia-1-LLM-7B-control is trained on carefully curated data in compliance with applicable EU and national regulations, including copyright and data privacy laws. With improved token efficiency, Pharia-1-LLM-7B-control excels in domain-specific applications, particularly in the automotive and engineering industries, and can be aligned to user preferences, making it suitable for critical applications without the risk of shutdown behavior. As such, it serves as a valuable addition to the community’s selection of weight-available foundation models. Pharia-1-LLM-7B-control-aligned has been added with additional safety guardrails via alignment methods.",https://huggingface.co/Aleph-Alpha/Pharia-1-LLM-7B-control,Introducing Pharia-1-LLM: transparent and compliant
Pharia-1-LLM-7B,Language,Germany,Aleph Alpha,2024-08-26,Industry,Question answering,Confident,256.0,Open weights (non-commercial),Common Crawl,Not-defined,7041544704.0,7700000000000.0,720.0,4.43e+23,1877.1621580193864,27922.725,550.0,1213583000000000.0,1301500000000000.0,80000000000.0,"NVIDIA A100 SXM4 80 GB,NVIDIA H100 SXM5 80GB",SOTA improvement,1,4000000,904992.9349364166,7148217.6,2.739465011286682e-08,140800.0,651594913.15422,101376000.0,"We are pleased to announce our new foundation model family that includes Pharia-1-LLM-7B-control and Pharia-1-LLM-7B-control-aligned, now publicly available under the Open Aleph License, which explicitly allows for non-commercial research and educational use. Pharia-1-LLM-7B-control is engineered to deliver concise, length-controlled responses that match the performance of leading open-source models in the 7B to 8B parameter range and is culturally and linguistically optimized for German, French, and Spanish by being trained on a multilingual base corpus. Pharia-1-LLM-7B-control is trained on carefully curated data in compliance with applicable EU and national regulations, including copyright and data privacy laws. With improved token efficiency, Pharia-1-LLM-7B-control excels in domain-specific applications, particularly in the automotive and engineering industries, and can be aligned to user preferences, making it suitable for critical applications without the risk of shutdown behavior. As such, it serves as a valuable addition to the community’s selection of weight-available foundation models. Pharia-1-LLM-7B-control-aligned has been added with additional safety guardrails via alignment methods.",https://huggingface.co/Aleph-Alpha/Pharia-1-LLM-7B-control,Introducing Pharia-1-LLM: transparent and compliant
Pharia-1-LLM-7B,Language,Germany,Aleph Alpha,2024-08-26,Industry,Translation,Confident,256.0,Open weights (non-commercial),Common Crawl,Not-defined,7041544704.0,7700000000000.0,720.0,4.43e+23,1877.1621580193864,27922.725,550.0,1213583000000000.0,1301500000000000.0,80000000000.0,"NVIDIA A100 SXM4 80 GB,NVIDIA H100 SXM5 80GB",SOTA improvement,1,4000000,904992.9349364166,7148217.6,2.739465011286682e-08,140800.0,651594913.15422,101376000.0,"We are pleased to announce our new foundation model family that includes Pharia-1-LLM-7B-control and Pharia-1-LLM-7B-control-aligned, now publicly available under the Open Aleph License, which explicitly allows for non-commercial research and educational use. Pharia-1-LLM-7B-control is engineered to deliver concise, length-controlled responses that match the performance of leading open-source models in the 7B to 8B parameter range and is culturally and linguistically optimized for German, French, and Spanish by being trained on a multilingual base corpus. Pharia-1-LLM-7B-control is trained on carefully curated data in compliance with applicable EU and national regulations, including copyright and data privacy laws. With improved token efficiency, Pharia-1-LLM-7B-control excels in domain-specific applications, particularly in the automotive and engineering industries, and can be aligned to user preferences, making it suitable for critical applications without the risk of shutdown behavior. As such, it serves as a valuable addition to the community’s selection of weight-available foundation models. Pharia-1-LLM-7B-control-aligned has been added with additional safety guardrails via alignment methods.",https://huggingface.co/Aleph-Alpha/Pharia-1-LLM-7B-control,Introducing Pharia-1-LLM: transparent and compliant
Phi-3.5-MoE,Language,USA,Microsoft,2024-04-23,Industry,Code generation,Confident,512.0,Open weights (unrestricted),Unspecified unreleased,Unreleased,60800000000.0,4900000000000.0,552.0,3.0202896e+23,1877.1621580193864,29744.94,550.0,1213583000000000.0,1301500000000000.0,60000000000.0,NVIDIA H100 SXM5 80GB,None,4,6472188,904992.9349364166,15229409.28,4.018101442987454e-08,281600.0,499556100.0849019,155443200.0,"We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone. Our training dataset is a scaled-up version of the one used for phi-2, composed of heavily filtered publicly available web data and synthetic data. The model is also further aligned for robustness, safety, and chat format. We also provide parameter-scaling results with a 7B, 14B models trained for 4.8T tokens, called phi-3-small, phi-3-medium, both significantly more capable than phi-3-mini (e.g., respectively 75%, 78% on MMLU, and 8.7, 8.9 on MT-bench). To enhance multilingual, multimodal, and long-context capabilities, we introduce three models in the phi-3.5 series: phi-3.5-mini, phi-3.5-MoE, and phi-3.5-Vision. The phi-3.5-MoE, a 16 x 3.8B MoE model with 6.6 billion active parameters, achieves superior performance in language reasoning, math, and code tasks compared to other open-source models of similar scale, such as Llama 3.1 and the Mixtral series, and on par with Gemini-1.5-Flash and GPT-4o-mini. Meanwhile, phi-3.5-Vision, a 4.2 billion parameter model derived from phi-3.5-mini, excels in reasoning tasks and is adept at handling both single-image and text prompts, as well as multi-image and text prompts.",https://arxiv.org/abs/2404.14219,Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone
Phi-3.5-MoE,Language,USA,Microsoft,2024-04-23,Industry,Language modeling/generation,Confident,512.0,Open weights (unrestricted),Unspecified unreleased,Unreleased,60800000000.0,4900000000000.0,552.0,3.0202896e+23,1877.1621580193864,29744.94,550.0,1213583000000000.0,1301500000000000.0,60000000000.0,NVIDIA H100 SXM5 80GB,None,4,6472188,904992.9349364166,15229409.28,4.018101442987454e-08,281600.0,499556100.0849019,155443200.0,"We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone. Our training dataset is a scaled-up version of the one used for phi-2, composed of heavily filtered publicly available web data and synthetic data. The model is also further aligned for robustness, safety, and chat format. We also provide parameter-scaling results with a 7B, 14B models trained for 4.8T tokens, called phi-3-small, phi-3-medium, both significantly more capable than phi-3-mini (e.g., respectively 75%, 78% on MMLU, and 8.7, 8.9 on MT-bench). To enhance multilingual, multimodal, and long-context capabilities, we introduce three models in the phi-3.5 series: phi-3.5-mini, phi-3.5-MoE, and phi-3.5-Vision. The phi-3.5-MoE, a 16 x 3.8B MoE model with 6.6 billion active parameters, achieves superior performance in language reasoning, math, and code tasks compared to other open-source models of similar scale, such as Llama 3.1 and the Mixtral series, and on par with Gemini-1.5-Flash and GPT-4o-mini. Meanwhile, phi-3.5-Vision, a 4.2 billion parameter model derived from phi-3.5-mini, excels in reasoning tasks and is adept at handling both single-image and text prompts, as well as multi-image and text prompts.",https://arxiv.org/abs/2404.14219,Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone
Phi-3.5-MoE,Language,USA,Microsoft,2024-04-23,Industry,Quantitative reasoning,Confident,512.0,Open weights (unrestricted),Unspecified unreleased,Unreleased,60800000000.0,4900000000000.0,552.0,3.0202896e+23,1877.1621580193864,29744.94,550.0,1213583000000000.0,1301500000000000.0,60000000000.0,NVIDIA H100 SXM5 80GB,None,4,6472188,904992.9349364166,15229409.28,4.018101442987454e-08,281600.0,499556100.0849019,155443200.0,"We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone. Our training dataset is a scaled-up version of the one used for phi-2, composed of heavily filtered publicly available web data and synthetic data. The model is also further aligned for robustness, safety, and chat format. We also provide parameter-scaling results with a 7B, 14B models trained for 4.8T tokens, called phi-3-small, phi-3-medium, both significantly more capable than phi-3-mini (e.g., respectively 75%, 78% on MMLU, and 8.7, 8.9 on MT-bench). To enhance multilingual, multimodal, and long-context capabilities, we introduce three models in the phi-3.5 series: phi-3.5-mini, phi-3.5-MoE, and phi-3.5-Vision. The phi-3.5-MoE, a 16 x 3.8B MoE model with 6.6 billion active parameters, achieves superior performance in language reasoning, math, and code tasks compared to other open-source models of similar scale, such as Llama 3.1 and the Mixtral series, and on par with Gemini-1.5-Flash and GPT-4o-mini. Meanwhile, phi-3.5-Vision, a 4.2 billion parameter model derived from phi-3.5-mini, excels in reasoning tasks and is adept at handling both single-image and text prompts, as well as multi-image and text prompts.",https://arxiv.org/abs/2404.14219,Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone
Phi-3.5-MoE,Language,USA,Microsoft,2024-04-23,Industry,Question answering,Confident,512.0,Open weights (unrestricted),Unspecified unreleased,Unreleased,60800000000.0,4900000000000.0,552.0,3.0202896e+23,1877.1621580193864,29744.94,550.0,1213583000000000.0,1301500000000000.0,60000000000.0,NVIDIA H100 SXM5 80GB,None,4,6472188,904992.9349364166,15229409.28,4.018101442987454e-08,281600.0,499556100.0849019,155443200.0,"We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone. Our training dataset is a scaled-up version of the one used for phi-2, composed of heavily filtered publicly available web data and synthetic data. The model is also further aligned for robustness, safety, and chat format. We also provide parameter-scaling results with a 7B, 14B models trained for 4.8T tokens, called phi-3-small, phi-3-medium, both significantly more capable than phi-3-mini (e.g., respectively 75%, 78% on MMLU, and 8.7, 8.9 on MT-bench). To enhance multilingual, multimodal, and long-context capabilities, we introduce three models in the phi-3.5 series: phi-3.5-mini, phi-3.5-MoE, and phi-3.5-Vision. The phi-3.5-MoE, a 16 x 3.8B MoE model with 6.6 billion active parameters, achieves superior performance in language reasoning, math, and code tasks compared to other open-source models of similar scale, such as Llama 3.1 and the Mixtral series, and on par with Gemini-1.5-Flash and GPT-4o-mini. Meanwhile, phi-3.5-Vision, a 4.2 billion parameter model derived from phi-3.5-mini, excels in reasoning tasks and is adept at handling both single-image and text prompts, as well as multi-image and text prompts.",https://arxiv.org/abs/2404.14219,Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone
Phi-3.5-MoE,Language,USA,Microsoft,2024-04-23,Industry,Translation,Confident,512.0,Open weights (unrestricted),Unspecified unreleased,Unreleased,60800000000.0,4900000000000.0,552.0,3.0202896e+23,1877.1621580193864,29744.94,550.0,1213583000000000.0,1301500000000000.0,60000000000.0,NVIDIA H100 SXM5 80GB,None,4,6472188,904992.9349364166,15229409.28,4.018101442987454e-08,281600.0,499556100.0849019,155443200.0,"We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone. Our training dataset is a scaled-up version of the one used for phi-2, composed of heavily filtered publicly available web data and synthetic data. The model is also further aligned for robustness, safety, and chat format. We also provide parameter-scaling results with a 7B, 14B models trained for 4.8T tokens, called phi-3-small, phi-3-medium, both significantly more capable than phi-3-mini (e.g., respectively 75%, 78% on MMLU, and 8.7, 8.9 on MT-bench). To enhance multilingual, multimodal, and long-context capabilities, we introduce three models in the phi-3.5 series: phi-3.5-mini, phi-3.5-MoE, and phi-3.5-Vision. The phi-3.5-MoE, a 16 x 3.8B MoE model with 6.6 billion active parameters, achieves superior performance in language reasoning, math, and code tasks compared to other open-source models of similar scale, such as Llama 3.1 and the Mixtral series, and on par with Gemini-1.5-Flash and GPT-4o-mini. Meanwhile, phi-3.5-Vision, a 4.2 billion parameter model derived from phi-3.5-mini, excels in reasoning tasks and is adept at handling both single-image and text prompts, as well as multi-image and text prompts.",https://arxiv.org/abs/2404.14219,Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone
Phi-4,Language,UK,Microsoft Research,2024-12-12,Industry,Code generation,Confident,1920.0,Open weights (unrestricted),Unspecified unreleased,Unreleased,14000000000.0,10000000000000.0,504.0,9.3202015e+23,1877.1621580193864,29744.94,550.0,1213583000000000.0,1301500000000000.0,60000000000.0,NVIDIA H100 SXM5 80GB,None,4,6472188,904992.9349364166,57110284.8,1.3020995307880414e-08,1056000.0,456116439.20795393,532224000.0,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",https://arxiv.org/abs/2412.08905,Phi-4 Technical Report
Phi-4,Language,UK,Microsoft Research,2024-12-12,Industry,Language modeling/generation,Confident,1920.0,Open weights (unrestricted),Unspecified unreleased,Unreleased,14000000000.0,10000000000000.0,504.0,9.3202015e+23,1877.1621580193864,29744.94,550.0,1213583000000000.0,1301500000000000.0,60000000000.0,NVIDIA H100 SXM5 80GB,None,4,6472188,904992.9349364166,57110284.8,1.3020995307880414e-08,1056000.0,456116439.20795393,532224000.0,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",https://arxiv.org/abs/2412.08905,Phi-4 Technical Report
Phi-4,Language,UK,Microsoft Research,2024-12-12,Industry,Quantitative reasoning,Confident,1920.0,Open weights (unrestricted),Unspecified unreleased,Unreleased,14000000000.0,10000000000000.0,504.0,9.3202015e+23,1877.1621580193864,29744.94,550.0,1213583000000000.0,1301500000000000.0,60000000000.0,NVIDIA H100 SXM5 80GB,None,4,6472188,904992.9349364166,57110284.8,1.3020995307880414e-08,1056000.0,456116439.20795393,532224000.0,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",https://arxiv.org/abs/2412.08905,Phi-4 Technical Report
Phi-4,Language,UK,Microsoft Research,2024-12-12,Industry,Question answering,Confident,1920.0,Open weights (unrestricted),Unspecified unreleased,Unreleased,14000000000.0,10000000000000.0,504.0,9.3202015e+23,1877.1621580193864,29744.94,550.0,1213583000000000.0,1301500000000000.0,60000000000.0,NVIDIA H100 SXM5 80GB,None,4,6472188,904992.9349364166,57110284.8,1.3020995307880414e-08,1056000.0,456116439.20795393,532224000.0,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",https://arxiv.org/abs/2412.08905,Phi-4 Technical Report
Phi-4,Language,USA,Microsoft Research,2024-12-12,Industry,Code generation,Confident,1920.0,Open weights (unrestricted),Unspecified unreleased,Unreleased,14000000000.0,10000000000000.0,504.0,9.3202015e+23,1877.1621580193864,29744.94,550.0,1213583000000000.0,1301500000000000.0,60000000000.0,NVIDIA H100 SXM5 80GB,None,4,6472188,904992.9349364166,57110284.8,1.3020995307880414e-08,1056000.0,456116439.20795393,532224000.0,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",https://arxiv.org/abs/2412.08905,Phi-4 Technical Report
Phi-4,Language,USA,Microsoft Research,2024-12-12,Industry,Language modeling/generation,Confident,1920.0,Open weights (unrestricted),Unspecified unreleased,Unreleased,14000000000.0,10000000000000.0,504.0,9.3202015e+23,1877.1621580193864,29744.94,550.0,1213583000000000.0,1301500000000000.0,60000000000.0,NVIDIA H100 SXM5 80GB,None,4,6472188,904992.9349364166,57110284.8,1.3020995307880414e-08,1056000.0,456116439.20795393,532224000.0,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",https://arxiv.org/abs/2412.08905,Phi-4 Technical Report
Phi-4,Language,USA,Microsoft Research,2024-12-12,Industry,Quantitative reasoning,Confident,1920.0,Open weights (unrestricted),Unspecified unreleased,Unreleased,14000000000.0,10000000000000.0,504.0,9.3202015e+23,1877.1621580193864,29744.94,550.0,1213583000000000.0,1301500000000000.0,60000000000.0,NVIDIA H100 SXM5 80GB,None,4,6472188,904992.9349364166,57110284.8,1.3020995307880414e-08,1056000.0,456116439.20795393,532224000.0,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",https://arxiv.org/abs/2412.08905,Phi-4 Technical Report
Phi-4,Language,USA,Microsoft Research,2024-12-12,Industry,Question answering,Confident,1920.0,Open weights (unrestricted),Unspecified unreleased,Unreleased,14000000000.0,10000000000000.0,504.0,9.3202015e+23,1877.1621580193864,29744.94,550.0,1213583000000000.0,1301500000000000.0,60000000000.0,NVIDIA H100 SXM5 80GB,None,4,6472188,904992.9349364166,57110284.8,1.3020995307880414e-08,1056000.0,456116439.20795393,532224000.0,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",https://arxiv.org/abs/2412.08905,Phi-4 Technical Report
Poro 34B,Language,Finland,High-Performance Language Technologies (HPLT),2023-12-14,Academia,Code generation,Confident,512.0,Open weights (unrestricted),"mC4,SlimPajama,StarCoder,Dolma",Not-defined,34200000000.0,1000000000000.0,1394.0,2.052e+23,1877.1621580193864,14500.0,500.0,957440000000000.0,383000000000000.0,128000000000.0,AMD Radeon Instinct MI250X,SOTA improvement,1,4000000,197336.5951439333,7424000.0,4.66588693957115e-08,256000.0,275087213.630643,356864000.0,"The pretraining of state-of-the-art large language models now requires trillions of words of text, which is orders of magnitude more than available for the vast majority of languages. While including text in more than one language is an obvious way to acquire more pretraining data, multilinguality is often seen as a curse, and most model training efforts continue to focus near-exclusively on individual large languages. We believe that multilinguality can be a blessing and that it should be possible to substantially improve over the capabilities of monolingual models for small languages through multilingual training. In this study, we introduce Poro 34B, a 34 billion parameter model trained for 1 trillion tokens of Finnish, English, and programming languages, and demonstrate that a multilingual training approach can produce a model that not only substantially advances over the capabilities of existing models for Finnish, but also excels in translation and is competitive in its class in generating English and programming languages. We release the model parameters, scripts, and data under open licenses at this https URL.",https://arxiv.org/abs/2404.01856,Poro 34B and the Blessing of Multilinguality
Poro 34B,Language,Finland,High-Performance Language Technologies (HPLT),2023-12-14,Academia,Language modeling/generation,Confident,512.0,Open weights (unrestricted),"mC4,SlimPajama,StarCoder,Dolma",Not-defined,34200000000.0,1000000000000.0,1394.0,2.052e+23,1877.1621580193864,14500.0,500.0,957440000000000.0,383000000000000.0,128000000000.0,AMD Radeon Instinct MI250X,SOTA improvement,1,4000000,197336.5951439333,7424000.0,4.66588693957115e-08,256000.0,275087213.630643,356864000.0,"The pretraining of state-of-the-art large language models now requires trillions of words of text, which is orders of magnitude more than available for the vast majority of languages. While including text in more than one language is an obvious way to acquire more pretraining data, multilinguality is often seen as a curse, and most model training efforts continue to focus near-exclusively on individual large languages. We believe that multilinguality can be a blessing and that it should be possible to substantially improve over the capabilities of monolingual models for small languages through multilingual training. In this study, we introduce Poro 34B, a 34 billion parameter model trained for 1 trillion tokens of Finnish, English, and programming languages, and demonstrate that a multilingual training approach can produce a model that not only substantially advances over the capabilities of existing models for Finnish, but also excels in translation and is competitive in its class in generating English and programming languages. We release the model parameters, scripts, and data under open licenses at this https URL.",https://arxiv.org/abs/2404.01856,Poro 34B and the Blessing of Multilinguality
Poro 34B,Language,Finland,High-Performance Language Technologies (HPLT),2023-12-14,Research collective,Code generation,Confident,512.0,Open weights (unrestricted),"mC4,SlimPajama,StarCoder,Dolma",Not-defined,34200000000.0,1000000000000.0,1394.0,2.052e+23,1877.1621580193864,14500.0,500.0,957440000000000.0,383000000000000.0,128000000000.0,AMD Radeon Instinct MI250X,SOTA improvement,1,4000000,197336.5951439333,7424000.0,4.66588693957115e-08,256000.0,275087213.630643,356864000.0,"The pretraining of state-of-the-art large language models now requires trillions of words of text, which is orders of magnitude more than available for the vast majority of languages. While including text in more than one language is an obvious way to acquire more pretraining data, multilinguality is often seen as a curse, and most model training efforts continue to focus near-exclusively on individual large languages. We believe that multilinguality can be a blessing and that it should be possible to substantially improve over the capabilities of monolingual models for small languages through multilingual training. In this study, we introduce Poro 34B, a 34 billion parameter model trained for 1 trillion tokens of Finnish, English, and programming languages, and demonstrate that a multilingual training approach can produce a model that not only substantially advances over the capabilities of existing models for Finnish, but also excels in translation and is competitive in its class in generating English and programming languages. We release the model parameters, scripts, and data under open licenses at this https URL.",https://arxiv.org/abs/2404.01856,Poro 34B and the Blessing of Multilinguality
Poro 34B,Language,Finland,High-Performance Language Technologies (HPLT),2023-12-14,Research collective,Language modeling/generation,Confident,512.0,Open weights (unrestricted),"mC4,SlimPajama,StarCoder,Dolma",Not-defined,34200000000.0,1000000000000.0,1394.0,2.052e+23,1877.1621580193864,14500.0,500.0,957440000000000.0,383000000000000.0,128000000000.0,AMD Radeon Instinct MI250X,SOTA improvement,1,4000000,197336.5951439333,7424000.0,4.66588693957115e-08,256000.0,275087213.630643,356864000.0,"The pretraining of state-of-the-art large language models now requires trillions of words of text, which is orders of magnitude more than available for the vast majority of languages. While including text in more than one language is an obvious way to acquire more pretraining data, multilinguality is often seen as a curse, and most model training efforts continue to focus near-exclusively on individual large languages. We believe that multilinguality can be a blessing and that it should be possible to substantially improve over the capabilities of monolingual models for small languages through multilingual training. In this study, we introduce Poro 34B, a 34 billion parameter model trained for 1 trillion tokens of Finnish, English, and programming languages, and demonstrate that a multilingual training approach can produce a model that not only substantially advances over the capabilities of existing models for Finnish, but also excels in translation and is competitive in its class in generating English and programming languages. We release the model parameters, scripts, and data under open licenses at this https URL.",https://arxiv.org/abs/2404.01856,Poro 34B and the Blessing of Multilinguality
Poro 34B,Language,Finland,University of Turku,2023-12-14,Academia,Code generation,Confident,512.0,Open weights (unrestricted),"mC4,SlimPajama,StarCoder,Dolma",Not-defined,34200000000.0,1000000000000.0,1394.0,2.052e+23,1877.1621580193864,14500.0,500.0,957440000000000.0,383000000000000.0,128000000000.0,AMD Radeon Instinct MI250X,SOTA improvement,1,4000000,197336.5951439333,7424000.0,4.66588693957115e-08,256000.0,275087213.630643,356864000.0,"The pretraining of state-of-the-art large language models now requires trillions of words of text, which is orders of magnitude more than available for the vast majority of languages. While including text in more than one language is an obvious way to acquire more pretraining data, multilinguality is often seen as a curse, and most model training efforts continue to focus near-exclusively on individual large languages. We believe that multilinguality can be a blessing and that it should be possible to substantially improve over the capabilities of monolingual models for small languages through multilingual training. In this study, we introduce Poro 34B, a 34 billion parameter model trained for 1 trillion tokens of Finnish, English, and programming languages, and demonstrate that a multilingual training approach can produce a model that not only substantially advances over the capabilities of existing models for Finnish, but also excels in translation and is competitive in its class in generating English and programming languages. We release the model parameters, scripts, and data under open licenses at this https URL.",https://arxiv.org/abs/2404.01856,Poro 34B and the Blessing of Multilinguality
Poro 34B,Language,Finland,University of Turku,2023-12-14,Academia,Language modeling/generation,Confident,512.0,Open weights (unrestricted),"mC4,SlimPajama,StarCoder,Dolma",Not-defined,34200000000.0,1000000000000.0,1394.0,2.052e+23,1877.1621580193864,14500.0,500.0,957440000000000.0,383000000000000.0,128000000000.0,AMD Radeon Instinct MI250X,SOTA improvement,1,4000000,197336.5951439333,7424000.0,4.66588693957115e-08,256000.0,275087213.630643,356864000.0,"The pretraining of state-of-the-art large language models now requires trillions of words of text, which is orders of magnitude more than available for the vast majority of languages. While including text in more than one language is an obvious way to acquire more pretraining data, multilinguality is often seen as a curse, and most model training efforts continue to focus near-exclusively on individual large languages. We believe that multilinguality can be a blessing and that it should be possible to substantially improve over the capabilities of monolingual models for small languages through multilingual training. In this study, we introduce Poro 34B, a 34 billion parameter model trained for 1 trillion tokens of Finnish, English, and programming languages, and demonstrate that a multilingual training approach can produce a model that not only substantially advances over the capabilities of existing models for Finnish, but also excels in translation and is competitive in its class in generating English and programming languages. We release the model parameters, scripts, and data under open licenses at this https URL.",https://arxiv.org/abs/2404.01856,Poro 34B and the Blessing of Multilinguality
Poro 34B,Language,Finland,University of Turku,2023-12-14,Research collective,Code generation,Confident,512.0,Open weights (unrestricted),"mC4,SlimPajama,StarCoder,Dolma",Not-defined,34200000000.0,1000000000000.0,1394.0,2.052e+23,1877.1621580193864,14500.0,500.0,957440000000000.0,383000000000000.0,128000000000.0,AMD Radeon Instinct MI250X,SOTA improvement,1,4000000,197336.5951439333,7424000.0,4.66588693957115e-08,256000.0,275087213.630643,356864000.0,"The pretraining of state-of-the-art large language models now requires trillions of words of text, which is orders of magnitude more than available for the vast majority of languages. While including text in more than one language is an obvious way to acquire more pretraining data, multilinguality is often seen as a curse, and most model training efforts continue to focus near-exclusively on individual large languages. We believe that multilinguality can be a blessing and that it should be possible to substantially improve over the capabilities of monolingual models for small languages through multilingual training. In this study, we introduce Poro 34B, a 34 billion parameter model trained for 1 trillion tokens of Finnish, English, and programming languages, and demonstrate that a multilingual training approach can produce a model that not only substantially advances over the capabilities of existing models for Finnish, but also excels in translation and is competitive in its class in generating English and programming languages. We release the model parameters, scripts, and data under open licenses at this https URL.",https://arxiv.org/abs/2404.01856,Poro 34B and the Blessing of Multilinguality
Poro 34B,Language,Finland,University of Turku,2023-12-14,Research collective,Language modeling/generation,Confident,512.0,Open weights (unrestricted),"mC4,SlimPajama,StarCoder,Dolma",Not-defined,34200000000.0,1000000000000.0,1394.0,2.052e+23,1877.1621580193864,14500.0,500.0,957440000000000.0,383000000000000.0,128000000000.0,AMD Radeon Instinct MI250X,SOTA improvement,1,4000000,197336.5951439333,7424000.0,4.66588693957115e-08,256000.0,275087213.630643,356864000.0,"The pretraining of state-of-the-art large language models now requires trillions of words of text, which is orders of magnitude more than available for the vast majority of languages. While including text in more than one language is an obvious way to acquire more pretraining data, multilinguality is often seen as a curse, and most model training efforts continue to focus near-exclusively on individual large languages. We believe that multilinguality can be a blessing and that it should be possible to substantially improve over the capabilities of monolingual models for small languages through multilingual training. In this study, we introduce Poro 34B, a 34 billion parameter model trained for 1 trillion tokens of Finnish, English, and programming languages, and demonstrate that a multilingual training approach can produce a model that not only substantially advances over the capabilities of existing models for Finnish, but also excels in translation and is competitive in its class in generating English and programming languages. We release the model parameters, scripts, and data under open licenses at this https URL.",https://arxiv.org/abs/2404.01856,Poro 34B and the Blessing of Multilinguality
Poro 34B,Language,Multinational,High-Performance Language Technologies (HPLT),2023-12-14,Academia,Code generation,Confident,512.0,Open weights (unrestricted),"mC4,SlimPajama,StarCoder,Dolma",Not-defined,34200000000.0,1000000000000.0,1394.0,2.052e+23,1877.1621580193864,14500.0,500.0,957440000000000.0,383000000000000.0,128000000000.0,AMD Radeon Instinct MI250X,SOTA improvement,1,4000000,197336.5951439333,7424000.0,4.66588693957115e-08,256000.0,275087213.630643,356864000.0,"The pretraining of state-of-the-art large language models now requires trillions of words of text, which is orders of magnitude more than available for the vast majority of languages. While including text in more than one language is an obvious way to acquire more pretraining data, multilinguality is often seen as a curse, and most model training efforts continue to focus near-exclusively on individual large languages. We believe that multilinguality can be a blessing and that it should be possible to substantially improve over the capabilities of monolingual models for small languages through multilingual training. In this study, we introduce Poro 34B, a 34 billion parameter model trained for 1 trillion tokens of Finnish, English, and programming languages, and demonstrate that a multilingual training approach can produce a model that not only substantially advances over the capabilities of existing models for Finnish, but also excels in translation and is competitive in its class in generating English and programming languages. We release the model parameters, scripts, and data under open licenses at this https URL.",https://arxiv.org/abs/2404.01856,Poro 34B and the Blessing of Multilinguality
Poro 34B,Language,Multinational,High-Performance Language Technologies (HPLT),2023-12-14,Academia,Language modeling/generation,Confident,512.0,Open weights (unrestricted),"mC4,SlimPajama,StarCoder,Dolma",Not-defined,34200000000.0,1000000000000.0,1394.0,2.052e+23,1877.1621580193864,14500.0,500.0,957440000000000.0,383000000000000.0,128000000000.0,AMD Radeon Instinct MI250X,SOTA improvement,1,4000000,197336.5951439333,7424000.0,4.66588693957115e-08,256000.0,275087213.630643,356864000.0,"The pretraining of state-of-the-art large language models now requires trillions of words of text, which is orders of magnitude more than available for the vast majority of languages. While including text in more than one language is an obvious way to acquire more pretraining data, multilinguality is often seen as a curse, and most model training efforts continue to focus near-exclusively on individual large languages. We believe that multilinguality can be a blessing and that it should be possible to substantially improve over the capabilities of monolingual models for small languages through multilingual training. In this study, we introduce Poro 34B, a 34 billion parameter model trained for 1 trillion tokens of Finnish, English, and programming languages, and demonstrate that a multilingual training approach can produce a model that not only substantially advances over the capabilities of existing models for Finnish, but also excels in translation and is competitive in its class in generating English and programming languages. We release the model parameters, scripts, and data under open licenses at this https URL.",https://arxiv.org/abs/2404.01856,Poro 34B and the Blessing of Multilinguality
Poro 34B,Language,Multinational,High-Performance Language Technologies (HPLT),2023-12-14,Research collective,Code generation,Confident,512.0,Open weights (unrestricted),"mC4,SlimPajama,StarCoder,Dolma",Not-defined,34200000000.0,1000000000000.0,1394.0,2.052e+23,1877.1621580193864,14500.0,500.0,957440000000000.0,383000000000000.0,128000000000.0,AMD Radeon Instinct MI250X,SOTA improvement,1,4000000,197336.5951439333,7424000.0,4.66588693957115e-08,256000.0,275087213.630643,356864000.0,"The pretraining of state-of-the-art large language models now requires trillions of words of text, which is orders of magnitude more than available for the vast majority of languages. While including text in more than one language is an obvious way to acquire more pretraining data, multilinguality is often seen as a curse, and most model training efforts continue to focus near-exclusively on individual large languages. We believe that multilinguality can be a blessing and that it should be possible to substantially improve over the capabilities of monolingual models for small languages through multilingual training. In this study, we introduce Poro 34B, a 34 billion parameter model trained for 1 trillion tokens of Finnish, English, and programming languages, and demonstrate that a multilingual training approach can produce a model that not only substantially advances over the capabilities of existing models for Finnish, but also excels in translation and is competitive in its class in generating English and programming languages. We release the model parameters, scripts, and data under open licenses at this https URL.",https://arxiv.org/abs/2404.01856,Poro 34B and the Blessing of Multilinguality
Poro 34B,Language,Multinational,High-Performance Language Technologies (HPLT),2023-12-14,Research collective,Language modeling/generation,Confident,512.0,Open weights (unrestricted),"mC4,SlimPajama,StarCoder,Dolma",Not-defined,34200000000.0,1000000000000.0,1394.0,2.052e+23,1877.1621580193864,14500.0,500.0,957440000000000.0,383000000000000.0,128000000000.0,AMD Radeon Instinct MI250X,SOTA improvement,1,4000000,197336.5951439333,7424000.0,4.66588693957115e-08,256000.0,275087213.630643,356864000.0,"The pretraining of state-of-the-art large language models now requires trillions of words of text, which is orders of magnitude more than available for the vast majority of languages. While including text in more than one language is an obvious way to acquire more pretraining data, multilinguality is often seen as a curse, and most model training efforts continue to focus near-exclusively on individual large languages. We believe that multilinguality can be a blessing and that it should be possible to substantially improve over the capabilities of monolingual models for small languages through multilingual training. In this study, we introduce Poro 34B, a 34 billion parameter model trained for 1 trillion tokens of Finnish, English, and programming languages, and demonstrate that a multilingual training approach can produce a model that not only substantially advances over the capabilities of existing models for Finnish, but also excels in translation and is competitive in its class in generating English and programming languages. We release the model parameters, scripts, and data under open licenses at this https URL.",https://arxiv.org/abs/2404.01856,Poro 34B and the Blessing of Multilinguality
Poro 34B,Language,Multinational,University of Turku,2023-12-14,Academia,Code generation,Confident,512.0,Open weights (unrestricted),"mC4,SlimPajama,StarCoder,Dolma",Not-defined,34200000000.0,1000000000000.0,1394.0,2.052e+23,1877.1621580193864,14500.0,500.0,957440000000000.0,383000000000000.0,128000000000.0,AMD Radeon Instinct MI250X,SOTA improvement,1,4000000,197336.5951439333,7424000.0,4.66588693957115e-08,256000.0,275087213.630643,356864000.0,"The pretraining of state-of-the-art large language models now requires trillions of words of text, which is orders of magnitude more than available for the vast majority of languages. While including text in more than one language is an obvious way to acquire more pretraining data, multilinguality is often seen as a curse, and most model training efforts continue to focus near-exclusively on individual large languages. We believe that multilinguality can be a blessing and that it should be possible to substantially improve over the capabilities of monolingual models for small languages through multilingual training. In this study, we introduce Poro 34B, a 34 billion parameter model trained for 1 trillion tokens of Finnish, English, and programming languages, and demonstrate that a multilingual training approach can produce a model that not only substantially advances over the capabilities of existing models for Finnish, but also excels in translation and is competitive in its class in generating English and programming languages. We release the model parameters, scripts, and data under open licenses at this https URL.",https://arxiv.org/abs/2404.01856,Poro 34B and the Blessing of Multilinguality
Poro 34B,Language,Multinational,University of Turku,2023-12-14,Academia,Language modeling/generation,Confident,512.0,Open weights (unrestricted),"mC4,SlimPajama,StarCoder,Dolma",Not-defined,34200000000.0,1000000000000.0,1394.0,2.052e+23,1877.1621580193864,14500.0,500.0,957440000000000.0,383000000000000.0,128000000000.0,AMD Radeon Instinct MI250X,SOTA improvement,1,4000000,197336.5951439333,7424000.0,4.66588693957115e-08,256000.0,275087213.630643,356864000.0,"The pretraining of state-of-the-art large language models now requires trillions of words of text, which is orders of magnitude more than available for the vast majority of languages. While including text in more than one language is an obvious way to acquire more pretraining data, multilinguality is often seen as a curse, and most model training efforts continue to focus near-exclusively on individual large languages. We believe that multilinguality can be a blessing and that it should be possible to substantially improve over the capabilities of monolingual models for small languages through multilingual training. In this study, we introduce Poro 34B, a 34 billion parameter model trained for 1 trillion tokens of Finnish, English, and programming languages, and demonstrate that a multilingual training approach can produce a model that not only substantially advances over the capabilities of existing models for Finnish, but also excels in translation and is competitive in its class in generating English and programming languages. We release the model parameters, scripts, and data under open licenses at this https URL.",https://arxiv.org/abs/2404.01856,Poro 34B and the Blessing of Multilinguality
Poro 34B,Language,Multinational,University of Turku,2023-12-14,Research collective,Code generation,Confident,512.0,Open weights (unrestricted),"mC4,SlimPajama,StarCoder,Dolma",Not-defined,34200000000.0,1000000000000.0,1394.0,2.052e+23,1877.1621580193864,14500.0,500.0,957440000000000.0,383000000000000.0,128000000000.0,AMD Radeon Instinct MI250X,SOTA improvement,1,4000000,197336.5951439333,7424000.0,4.66588693957115e-08,256000.0,275087213.630643,356864000.0,"The pretraining of state-of-the-art large language models now requires trillions of words of text, which is orders of magnitude more than available for the vast majority of languages. While including text in more than one language is an obvious way to acquire more pretraining data, multilinguality is often seen as a curse, and most model training efforts continue to focus near-exclusively on individual large languages. We believe that multilinguality can be a blessing and that it should be possible to substantially improve over the capabilities of monolingual models for small languages through multilingual training. In this study, we introduce Poro 34B, a 34 billion parameter model trained for 1 trillion tokens of Finnish, English, and programming languages, and demonstrate that a multilingual training approach can produce a model that not only substantially advances over the capabilities of existing models for Finnish, but also excels in translation and is competitive in its class in generating English and programming languages. We release the model parameters, scripts, and data under open licenses at this https URL.",https://arxiv.org/abs/2404.01856,Poro 34B and the Blessing of Multilinguality
Poro 34B,Language,Multinational,University of Turku,2023-12-14,Research collective,Language modeling/generation,Confident,512.0,Open weights (unrestricted),"mC4,SlimPajama,StarCoder,Dolma",Not-defined,34200000000.0,1000000000000.0,1394.0,2.052e+23,1877.1621580193864,14500.0,500.0,957440000000000.0,383000000000000.0,128000000000.0,AMD Radeon Instinct MI250X,SOTA improvement,1,4000000,197336.5951439333,7424000.0,4.66588693957115e-08,256000.0,275087213.630643,356864000.0,"The pretraining of state-of-the-art large language models now requires trillions of words of text, which is orders of magnitude more than available for the vast majority of languages. While including text in more than one language is an obvious way to acquire more pretraining data, multilinguality is often seen as a curse, and most model training efforts continue to focus near-exclusively on individual large languages. We believe that multilinguality can be a blessing and that it should be possible to substantially improve over the capabilities of monolingual models for small languages through multilingual training. In this study, we introduce Poro 34B, a 34 billion parameter model trained for 1 trillion tokens of Finnish, English, and programming languages, and demonstrate that a multilingual training approach can produce a model that not only substantially advances over the capabilities of existing models for Finnish, but also excels in translation and is competitive in its class in generating English and programming languages. We release the model parameters, scripts, and data under open licenses at this https URL.",https://arxiv.org/abs/2404.01856,Poro 34B and the Blessing of Multilinguality
Qwen-14B,Language,China,Alibaba,2023-09-28,Industry,Language modeling/generation,Confident,128.0,Open weights (restricted use),Not-defined,Unreleased,14000000000.0,3000000000000.0,940.0,2.5e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Training cost,1,4000000,154003694.78442368,1293528.32,2.3006799999999998e-08,51200.0,144763473097.35825,48128000.0,"Large language models (LLMs) have revolutionized the field of artificial intelligence, enabling natural language processing tasks that were previously thought to be exclusive to humans. In this work, we introduce Qwen, the first installment of our large language model series. Qwen is a comprehensive language model series that encompasses distinct models with varying parameter counts. It includes Qwen, the base pretrained language models, and Qwen-Chat, the chat models finetuned with human alignment techniques. The base language models consistently demonstrate superior performance across a multitude of downstream tasks, and the chat models, particularly those trained using Reinforcement Learning from Human Feedback (RLHF), are highly competitive. The chat models possess advanced tool-use and planning capabilities for creating agent applications, showcasing impressive performance even when compared to bigger models on complex tasks like utilizing a code interpreter. Furthermore, we have developed coding-specialized models, Code-Qwen and Code-Qwen-Chat, as well as mathematics-focused models, Math-Qwen-Chat, which are built upon base language models. These models demonstrate significantly improved performance in comparison with open-source models, and slightly fall behind the proprietary models.",https://arxiv.org/abs/2309.16609,Qwen Technical Report
Qwen-72B,Language,China,Alibaba,2023-11-30,Industry,Chat,Confident,128.0,Open weights (restricted use),Not-defined,Unreleased,72000000000.0,3000000000000.0,500.0,1.3e+24,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,SOTA improvement,1,4000000,197336.5951439333,1293528.32,4.424384615384615e-09,51200.0,98668297.57196665,25600000.0,"Qwen-72B is the 72B-parameter version of the large language model series, Qwen (abbr. Tongyi Qianwen), proposed by Alibaba Cloud. Qwen-72B is a Transformer-based large language model, which is pretrained on a large volume of data, including web texts, books, codes, etc. Additionally, based on the pretrained Qwen-72B, we release Qwen-72B-Chat, a large-model-based AI assistant, which is trained with alignment techniques.",https://huggingface.co/Qwen/Qwen-72B,
Qwen-72B,Language,China,Alibaba,2023-11-30,Industry,Code generation,Confident,128.0,Open weights (restricted use),Not-defined,Unreleased,72000000000.0,3000000000000.0,500.0,1.3e+24,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,SOTA improvement,1,4000000,197336.5951439333,1293528.32,4.424384615384615e-09,51200.0,98668297.57196665,25600000.0,"Qwen-72B is the 72B-parameter version of the large language model series, Qwen (abbr. Tongyi Qianwen), proposed by Alibaba Cloud. Qwen-72B is a Transformer-based large language model, which is pretrained on a large volume of data, including web texts, books, codes, etc. Additionally, based on the pretrained Qwen-72B, we release Qwen-72B-Chat, a large-model-based AI assistant, which is trained with alignment techniques.",https://huggingface.co/Qwen/Qwen-72B,
Qwen-7B,Language,China,Alibaba,2023-09-28,Industry,Language modeling/generation,Confident,128.0,Open weights (restricted use),Unspecified unreleased,Unreleased,7000000000.0,2400000000000.0,347.4,1.01e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Significant use,1,4000000,197336.5951439333,1293528.32,5.6947524752475247e-08,51200.0,68554733.15300243,17786880.0,"Large language models (LLMs) have revolutionized the field of artificial intelligence, enabling natural language processing tasks that were previously thought to be exclusive to humans. In this work, we introduce Qwen, the first installment of our large language model series. Qwen is a comprehensive language model series that encompasses distinct models with varying parameter counts. It includes Qwen, the base pretrained language models, and Qwen-Chat, the chat models finetuned with human alignment techniques. The base language models consistently demonstrate superior performance across a multitude of downstream tasks, and the chat models, particularly those trained using Reinforcement Learning from Human Feedback (RLHF), are highly competitive. The chat models possess advanced tool-use and planning capabilities for creating agent applications, showcasing impressive performance even when compared to bigger models on complex tasks like utilizing a code interpreter. Furthermore, we have developed coding-specialized models, Code-Qwen and Code-Qwen-Chat, as well as mathematics-focused models, Math-Qwen-Chat, which are built upon base language models. These models demonstrate significantly improved performance in comparison with open-source models, and slightly fall behind the proprietary models.","https://arxiv.org/abs/2309.16609, https://huggingface.co/Qwen/Qwen-7B",Qwen Technical Report
Qwen-7B,Language,China,Alibaba,2023-09-28,Industry,Translation,Confident,128.0,Open weights (restricted use),Unspecified unreleased,Unreleased,7000000000.0,2400000000000.0,347.4,1.01e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Significant use,1,4000000,197336.5951439333,1293528.32,5.6947524752475247e-08,51200.0,68554733.15300243,17786880.0,"Large language models (LLMs) have revolutionized the field of artificial intelligence, enabling natural language processing tasks that were previously thought to be exclusive to humans. In this work, we introduce Qwen, the first installment of our large language model series. Qwen is a comprehensive language model series that encompasses distinct models with varying parameter counts. It includes Qwen, the base pretrained language models, and Qwen-Chat, the chat models finetuned with human alignment techniques. The base language models consistently demonstrate superior performance across a multitude of downstream tasks, and the chat models, particularly those trained using Reinforcement Learning from Human Feedback (RLHF), are highly competitive. The chat models possess advanced tool-use and planning capabilities for creating agent applications, showcasing impressive performance even when compared to bigger models on complex tasks like utilizing a code interpreter. Furthermore, we have developed coding-specialized models, Code-Qwen and Code-Qwen-Chat, as well as mathematics-focused models, Math-Qwen-Chat, which are built upon base language models. These models demonstrate significantly improved performance in comparison with open-source models, and slightly fall behind the proprietary models.","https://arxiv.org/abs/2309.16609, https://huggingface.co/Qwen/Qwen-7B",Qwen Technical Report
Qwen1.5-110B,Language,China,Alibaba,2024-04-25,Industry,Chat,Confident,1024.0,Open weights (unrestricted),Unspecified unreleased,Unreleased,110000000000.0,3000000000000.0,1282.4326530612243,1.3e+24,1877.1621580193864,7406.0,303.0,107206000000000.0,303958000000000.0,54246185353.93311,NVIDIA A100,Training cost,1,4000000,788306.6236983632,7583744.0,8.246615384615384e-10,310272.0,1010950154.8552281,397902944.1306122,"The Qwen1.5-110B is the largest model in the Qwen1.5 series, and it is also the first one with over 100 billion parameters in the series. It demonstrates competitive performance against the very recently released SOTA model Llama-3-70B and it is significantly better than the 72B model. This tells us that there is still a lot of room in model size scaling for better performance. While the releease of Llama-3 indicates the significance of data scaling to an extremely large scale, we believe we can get the best of both worlds by scaling both data and model size in our future release. Stay tuned for Qwen2!",https://qwenlm.github.io/blog/qwen1.5-110b/?ref=upstract.com,Qwen1.5-110B: The First 100B+ Model of the Qwen1.5 Series
Qwen1.5-110B,Language,China,Alibaba,2024-04-25,Industry,Code generation,Confident,1024.0,Open weights (unrestricted),Unspecified unreleased,Unreleased,110000000000.0,3000000000000.0,1282.4326530612243,1.3e+24,1877.1621580193864,7406.0,303.0,107206000000000.0,303958000000000.0,54246185353.93311,NVIDIA A100,Training cost,1,4000000,788306.6236983632,7583744.0,8.246615384615384e-10,310272.0,1010950154.8552281,397902944.1306122,"The Qwen1.5-110B is the largest model in the Qwen1.5 series, and it is also the first one with over 100 billion parameters in the series. It demonstrates competitive performance against the very recently released SOTA model Llama-3-70B and it is significantly better than the 72B model. This tells us that there is still a lot of room in model size scaling for better performance. While the releease of Llama-3 indicates the significance of data scaling to an extremely large scale, we believe we can get the best of both worlds by scaling both data and model size in our future release. Stay tuned for Qwen2!",https://qwenlm.github.io/blog/qwen1.5-110b/?ref=upstract.com,Qwen1.5-110B: The First 100B+ Model of the Qwen1.5 Series
Qwen1.5-110B,Language,China,Alibaba,2024-04-25,Industry,Language modeling/generation,Confident,1024.0,Open weights (unrestricted),Unspecified unreleased,Unreleased,110000000000.0,3000000000000.0,1282.4326530612243,1.3e+24,1877.1621580193864,7406.0,303.0,107206000000000.0,303958000000000.0,54246185353.93311,NVIDIA A100,Training cost,1,4000000,788306.6236983632,7583744.0,8.246615384615384e-10,310272.0,1010950154.8552281,397902944.1306122,"The Qwen1.5-110B is the largest model in the Qwen1.5 series, and it is also the first one with over 100 billion parameters in the series. It demonstrates competitive performance against the very recently released SOTA model Llama-3-70B and it is significantly better than the 72B model. This tells us that there is still a lot of room in model size scaling for better performance. While the releease of Llama-3 indicates the significance of data scaling to an extremely large scale, we believe we can get the best of both worlds by scaling both data and model size in our future release. Stay tuned for Qwen2!",https://qwenlm.github.io/blog/qwen1.5-110b/?ref=upstract.com,Qwen1.5-110B: The First 100B+ Model of the Qwen1.5 Series
Qwen1.5-110B,Language,China,Alibaba,2024-04-25,Industry,Quantitative reasoning,Confident,1024.0,Open weights (unrestricted),Unspecified unreleased,Unreleased,110000000000.0,3000000000000.0,1282.4326530612243,1.3e+24,1877.1621580193864,7406.0,303.0,107206000000000.0,303958000000000.0,54246185353.93311,NVIDIA A100,Training cost,1,4000000,788306.6236983632,7583744.0,8.246615384615384e-10,310272.0,1010950154.8552281,397902944.1306122,"The Qwen1.5-110B is the largest model in the Qwen1.5 series, and it is also the first one with over 100 billion parameters in the series. It demonstrates competitive performance against the very recently released SOTA model Llama-3-70B and it is significantly better than the 72B model. This tells us that there is still a lot of room in model size scaling for better performance. While the releease of Llama-3 indicates the significance of data scaling to an extremely large scale, we believe we can get the best of both worlds by scaling both data and model size in our future release. Stay tuned for Qwen2!",https://qwenlm.github.io/blog/qwen1.5-110b/?ref=upstract.com,Qwen1.5-110B: The First 100B+ Model of the Qwen1.5 Series
Qwen1.5-110B,Language,China,Alibaba,2024-04-25,Industry,Translation,Confident,1024.0,Open weights (unrestricted),Unspecified unreleased,Unreleased,110000000000.0,3000000000000.0,1282.4326530612243,1.3e+24,1877.1621580193864,7406.0,303.0,107206000000000.0,303958000000000.0,54246185353.93311,NVIDIA A100,Training cost,1,4000000,788306.6236983632,7583744.0,8.246615384615384e-10,310272.0,1010950154.8552281,397902944.1306122,"The Qwen1.5-110B is the largest model in the Qwen1.5 series, and it is also the first one with over 100 billion parameters in the series. It demonstrates competitive performance against the very recently released SOTA model Llama-3-70B and it is significantly better than the 72B model. This tells us that there is still a lot of room in model size scaling for better performance. While the releease of Llama-3 indicates the significance of data scaling to an extremely large scale, we believe we can get the best of both worlds by scaling both data and model size in our future release. Stay tuned for Qwen2!",https://qwenlm.github.io/blog/qwen1.5-110b/?ref=upstract.com,Qwen1.5-110B: The First 100B+ Model of the Qwen1.5 Series
Qwen1.5-7B,Language,China,Alibaba,2024-02-04,Industry,Chat,Confident,1024.0,Open weights (unrestricted),Unspecified unreleased,Unreleased,7000000000.0,4000000000000.0,1282.4326530612243,1.68e+23,1877.1621580193864,7406.0,303.0,107206000000000.0,303958000000000.0,54246185353.93311,NVIDIA A800,Training cost,1,4000000,788306.6236983632,7583744.0,6.381309523809525e-09,310272.0,1010950154.8552281,397902944.1306122,"Qwen1.5 is the beta version of Qwen2, a transformer-based decoder-only language model pretrained on a large amount of data. In comparison with the previous released Qwen, the improvements include:

8 model sizes, including 0.5B, 1.8B, 4B, 7B, 14B, 32B and 72B dense models, and an MoE model of 14B with 2.7B activated;
Significant performance improvement in human preference for chat models;
Multilingual support of both base and chat models;
Stable support of 32K context length for models of all sizes
No need of trust_remote_code.",https://huggingface.co/Qwen/Qwen1.5-7B,Introducing Qwen1.5
Qwen1.5-7B,Language,China,Alibaba,2024-02-04,Industry,Code generation,Confident,1024.0,Open weights (unrestricted),Unspecified unreleased,Unreleased,7000000000.0,4000000000000.0,1282.4326530612243,1.68e+23,1877.1621580193864,7406.0,303.0,107206000000000.0,303958000000000.0,54246185353.93311,NVIDIA A800,Training cost,1,4000000,788306.6236983632,7583744.0,6.381309523809525e-09,310272.0,1010950154.8552281,397902944.1306122,"Qwen1.5 is the beta version of Qwen2, a transformer-based decoder-only language model pretrained on a large amount of data. In comparison with the previous released Qwen, the improvements include:

8 model sizes, including 0.5B, 1.8B, 4B, 7B, 14B, 32B and 72B dense models, and an MoE model of 14B with 2.7B activated;
Significant performance improvement in human preference for chat models;
Multilingual support of both base and chat models;
Stable support of 32K context length for models of all sizes
No need of trust_remote_code.",https://huggingface.co/Qwen/Qwen1.5-7B,Introducing Qwen1.5
Qwen1.5-7B,Language,China,Alibaba,2024-02-04,Industry,Language modeling/generation,Confident,1024.0,Open weights (unrestricted),Unspecified unreleased,Unreleased,7000000000.0,4000000000000.0,1282.4326530612243,1.68e+23,1877.1621580193864,7406.0,303.0,107206000000000.0,303958000000000.0,54246185353.93311,NVIDIA A800,Training cost,1,4000000,788306.6236983632,7583744.0,6.381309523809525e-09,310272.0,1010950154.8552281,397902944.1306122,"Qwen1.5 is the beta version of Qwen2, a transformer-based decoder-only language model pretrained on a large amount of data. In comparison with the previous released Qwen, the improvements include:

8 model sizes, including 0.5B, 1.8B, 4B, 7B, 14B, 32B and 72B dense models, and an MoE model of 14B with 2.7B activated;
Significant performance improvement in human preference for chat models;
Multilingual support of both base and chat models;
Stable support of 32K context length for models of all sizes
No need of trust_remote_code.",https://huggingface.co/Qwen/Qwen1.5-7B,Introducing Qwen1.5
Qwen1.5-7B,Language,China,Alibaba,2024-02-04,Industry,Quantitative reasoning,Confident,1024.0,Open weights (unrestricted),Unspecified unreleased,Unreleased,7000000000.0,4000000000000.0,1282.4326530612243,1.68e+23,1877.1621580193864,7406.0,303.0,107206000000000.0,303958000000000.0,54246185353.93311,NVIDIA A800,Training cost,1,4000000,788306.6236983632,7583744.0,6.381309523809525e-09,310272.0,1010950154.8552281,397902944.1306122,"Qwen1.5 is the beta version of Qwen2, a transformer-based decoder-only language model pretrained on a large amount of data. In comparison with the previous released Qwen, the improvements include:

8 model sizes, including 0.5B, 1.8B, 4B, 7B, 14B, 32B and 72B dense models, and an MoE model of 14B with 2.7B activated;
Significant performance improvement in human preference for chat models;
Multilingual support of both base and chat models;
Stable support of 32K context length for models of all sizes
No need of trust_remote_code.",https://huggingface.co/Qwen/Qwen1.5-7B,Introducing Qwen1.5
Qwen1.5-7B,Language,China,Alibaba,2024-02-04,Industry,Translation,Confident,1024.0,Open weights (unrestricted),Unspecified unreleased,Unreleased,7000000000.0,4000000000000.0,1282.4326530612243,1.68e+23,1877.1621580193864,7406.0,303.0,107206000000000.0,303958000000000.0,54246185353.93311,NVIDIA A800,Training cost,1,4000000,788306.6236983632,7583744.0,6.381309523809525e-09,310272.0,1010950154.8552281,397902944.1306122,"Qwen1.5 is the beta version of Qwen2, a transformer-based decoder-only language model pretrained on a large amount of data. In comparison with the previous released Qwen, the improvements include:

8 model sizes, including 0.5B, 1.8B, 4B, 7B, 14B, 32B and 72B dense models, and an MoE model of 14B with 2.7B activated;
Significant performance improvement in human preference for chat models;
Multilingual support of both base and chat models;
Stable support of 32K context length for models of all sizes
No need of trust_remote_code.",https://huggingface.co/Qwen/Qwen1.5-7B,Introducing Qwen1.5
Qwen2-57B-A14B,Language,China,Alibaba,2024-06-07,Industry,Chat,Confident,128.0,Open weights (unrestricted),Unspecified unreleased,Unreleased,57000000000.0,4500000000000.0,4096.0,3.7800000000001e+23,1877.1621580193864,7406.0,303.0,107206000000000.0,303958000000000.0,54246185353.93311,NVIDIA A100,Training cost,1,4000000,197336.5951439333,947968.0,2.836137566137491e-09,38784.0,808290693.7095507,158859264.0,"This report introduces the Qwen2 series, the latest addition to our large language models and large multimodal models. We release a comprehensive suite of
foundational and instruction-tuned language models, encompassing a parameter
range from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts
model. Qwen2 surpasses most prior open-weight models, including its predecessor
Qwen1.5, and exhibits competitive performance relative to proprietary models
across diverse benchmarks on language understanding, generation, multilingual
proficiency, coding, mathematics, and reasoning.
The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on
MMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as
a base language model. The instruction-tuned variant, Qwen2-72B-Instruct, attains
9.1 on MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover,
Qwen2 demonstrates robust multilingual capabilities, proficient in approximately
30 languages, spanning English, Chinese, Spanish, French, German, Arabic, Russian, Korean, Japanese, Thai, Vietnamese, and more, underscoring its versatility
and global reach.
To foster community innovation and accessibility, we have made the Qwen2 model
weights openly available on Hugging Face1 and ModelScope2, and the supplementary materials including example code on GitHub3. These platforms also include
resources for quantization, fine-tuning, and deployment, facilitating a wide range
of applications and research endeavors.","https://qwenlm.github.io/blog/qwen2/ 
https://arxiv.org/abs/2407.10671 ",Hello Qwen2
Qwen2-57B-A14B,Language,China,Alibaba,2024-06-07,Industry,Language modeling/generation,Confident,128.0,Open weights (unrestricted),Unspecified unreleased,Unreleased,57000000000.0,4500000000000.0,4096.0,3.7800000000001e+23,1877.1621580193864,7406.0,303.0,107206000000000.0,303958000000000.0,54246185353.93311,NVIDIA A100,Training cost,1,4000000,197336.5951439333,947968.0,2.836137566137491e-09,38784.0,808290693.7095507,158859264.0,"This report introduces the Qwen2 series, the latest addition to our large language models and large multimodal models. We release a comprehensive suite of
foundational and instruction-tuned language models, encompassing a parameter
range from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts
model. Qwen2 surpasses most prior open-weight models, including its predecessor
Qwen1.5, and exhibits competitive performance relative to proprietary models
across diverse benchmarks on language understanding, generation, multilingual
proficiency, coding, mathematics, and reasoning.
The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on
MMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as
a base language model. The instruction-tuned variant, Qwen2-72B-Instruct, attains
9.1 on MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover,
Qwen2 demonstrates robust multilingual capabilities, proficient in approximately
30 languages, spanning English, Chinese, Spanish, French, German, Arabic, Russian, Korean, Japanese, Thai, Vietnamese, and more, underscoring its versatility
and global reach.
To foster community innovation and accessibility, we have made the Qwen2 model
weights openly available on Hugging Face1 and ModelScope2, and the supplementary materials including example code on GitHub3. These platforms also include
resources for quantization, fine-tuning, and deployment, facilitating a wide range
of applications and research endeavors.","https://qwenlm.github.io/blog/qwen2/ 
https://arxiv.org/abs/2407.10671 ",Hello Qwen2
Qwen2-72B,Language,China,Alibaba,2024-06-07,Industry,Chat,Confident,128.0,Open weights (unrestricted),Unspecified unreleased,Unreleased,72710000000.0,7000000000000.0,4096.0,3.02e+24,1877.1621580193864,7406.0,303.0,107206000000000.0,303958000000000.0,54246185353.93311,NVIDIA A100,Training cost,1,4000000,197336.5951439333,947968.0,3.5498675496688744e-10,38784.0,808290693.7095507,158859264.0,"After months of efforts, we are pleased to announce the evolution from Qwen1.5 to Qwen2. This time, we bring to you:
- Pretrained and instruction-tuned models of 5 sizes, including Qwen2-0.5B, Qwen2-1.5B, Qwen2-7B, Qwen2-57B-A14B, and Qwen2-72B;
- Having been trained on data in 27 additional languages besides English and Chinese;
- State-of-the-art performance in a large number of benchmark evaluations;
- Significantly improved performance in coding and mathematics;
- Extended context length support up to 128K tokens with Qwen2-7B-Instruct and Qwen2-72B-Instruct.

(Technical report to follow)","https://qwenlm.github.io/blog/qwen2/ 
https://arxiv.org/abs/2407.10671 ",Hello Qwen2
Qwen2-72B,Language,China,Alibaba,2024-06-07,Industry,Language modeling/generation,Confident,128.0,Open weights (unrestricted),Unspecified unreleased,Unreleased,72710000000.0,7000000000000.0,4096.0,3.02e+24,1877.1621580193864,7406.0,303.0,107206000000000.0,303958000000000.0,54246185353.93311,NVIDIA A100,Training cost,1,4000000,197336.5951439333,947968.0,3.5498675496688744e-10,38784.0,808290693.7095507,158859264.0,"After months of efforts, we are pleased to announce the evolution from Qwen1.5 to Qwen2. This time, we bring to you:
- Pretrained and instruction-tuned models of 5 sizes, including Qwen2-0.5B, Qwen2-1.5B, Qwen2-7B, Qwen2-57B-A14B, and Qwen2-72B;
- Having been trained on data in 27 additional languages besides English and Chinese;
- State-of-the-art performance in a large number of benchmark evaluations;
- Significantly improved performance in coding and mathematics;
- Extended context length support up to 128K tokens with Qwen2-7B-Instruct and Qwen2-72B-Instruct.

(Technical report to follow)","https://qwenlm.github.io/blog/qwen2/ 
https://arxiv.org/abs/2407.10671 ",Hello Qwen2
Qwen2-7B,Language,China,Alibaba,2024-06-07,Industry,Chat,Confident,128.0,Open weights (unrestricted),Unspecified unreleased,Unreleased,7000000000.0,7000000000000.0,4096.0,2.9400000000001e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Training cost,1,4000000,197336.5951439333,1293528.32,1.9563605442176205e-08,51200.0,808290693.7095507,209715200.0,"This report introduces the Qwen2 series, the latest addition to our large language models and large multimodal models. We release a comprehensive suite of
foundational and instruction-tuned language models, encompassing a parameter
range from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts
model. Qwen2 surpasses most prior open-weight models, including its predecessor
Qwen1.5, and exhibits competitive performance relative to proprietary models
across diverse benchmarks on language understanding, generation, multilingual
proficiency, coding, mathematics, and reasoning.
The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on
MMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as
a base language model. The instruction-tuned variant, Qwen2-72B-Instruct, attains
9.1 on MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover,
Qwen2 demonstrates robust multilingual capabilities, proficient in approximately
30 languages, spanning English, Chinese, Spanish, French, German, Arabic, Russian, Korean, Japanese, Thai, Vietnamese, and more, underscoring its versatility
and global reach.
To foster community innovation and accessibility, we have made the Qwen2 model
weights openly available on Hugging Face1 and ModelScope2, and the supplementary materials including example code on GitHub3. These platforms also include
resources for quantization, fine-tuning, and deployment, facilitating a wide range
of applications and research endeavors.","https://qwenlm.github.io/blog/qwen2/ 
https://arxiv.org/abs/2407.10671 ",Hello Qwen2
Qwen2-7B,Language,China,Alibaba,2024-06-07,Industry,Language modeling/generation,Confident,128.0,Open weights (unrestricted),Unspecified unreleased,Unreleased,7000000000.0,7000000000000.0,4096.0,2.9400000000001e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Training cost,1,4000000,197336.5951439333,1293528.32,1.9563605442176205e-08,51200.0,808290693.7095507,209715200.0,"This report introduces the Qwen2 series, the latest addition to our large language models and large multimodal models. We release a comprehensive suite of
foundational and instruction-tuned language models, encompassing a parameter
range from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts
model. Qwen2 surpasses most prior open-weight models, including its predecessor
Qwen1.5, and exhibits competitive performance relative to proprietary models
across diverse benchmarks on language understanding, generation, multilingual
proficiency, coding, mathematics, and reasoning.
The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on
MMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as
a base language model. The instruction-tuned variant, Qwen2-72B-Instruct, attains
9.1 on MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover,
Qwen2 demonstrates robust multilingual capabilities, proficient in approximately
30 languages, spanning English, Chinese, Spanish, French, German, Arabic, Russian, Korean, Japanese, Thai, Vietnamese, and more, underscoring its versatility
and global reach.
To foster community innovation and accessibility, we have made the Qwen2 model
weights openly available on Hugging Face1 and ModelScope2, and the supplementary materials including example code on GitHub3. These platforms also include
resources for quantization, fine-tuning, and deployment, facilitating a wide range
of applications and research endeavors.","https://qwenlm.github.io/blog/qwen2/ 
https://arxiv.org/abs/2407.10671 ",Hello Qwen2
Qwen2-VL-72B,Language,China,Alibaba,2024-09-18,Industry,Character recognition,Likely,128.0,Open weights (unrestricted),Unspecified unreleased,Not-defined,72000000000.0,1400000000000.0,4096.0,6.048e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Significant use,1,4000000,197336.5951439333,1293528.32,9.51008597883598e-09,51200.0,808290693.7095507,209715200.0,"We present the Qwen2-VL Series, an advanced upgrade of the previous Qwen-VL models that redefines the conventional predetermined-resolution approach in visual processing. Qwen2-VL introduces the Naive Dynamic Resolution mechanism, which enables the model to dynamically process images of varying resolutions into different numbers of visual tokens. This approach allows the model to generate more efficient and accurate visual representations, closely aligning with human perceptual processes. The model also integrates Multimodal Rotary Position Embedding (M-RoPE), facilitating the effective fusion of positional information across text, images, and videos. We employ a unified paradigm for processing both images and videos, enhancing the model's visual perception capabilities. To explore the potential of large multimodal models, Qwen2-VL investigates the scaling laws for large vision-language models (LVLMs). By scaling both the model size-with versions at 2B, 8B, and 72B parameters-and the amount of training data, the Qwen2-VL Series achieves highly competitive performance. Notably, the Qwen2-VL-72B model achieves results comparable to leading models such as GPT-4o and Claude3.5-Sonnet across various multimodal benchmarks, outperforming other generalist models. Code is available at \url{this https URL}.",https://arxiv.org/abs/2409.12191,Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution
Qwen2-VL-72B,Language,China,Alibaba,2024-09-18,Industry,Language modeling/generation,Likely,128.0,Open weights (unrestricted),Unspecified unreleased,Not-defined,72000000000.0,1400000000000.0,4096.0,6.048e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Significant use,1,4000000,197336.5951439333,1293528.32,9.51008597883598e-09,51200.0,808290693.7095507,209715200.0,"We present the Qwen2-VL Series, an advanced upgrade of the previous Qwen-VL models that redefines the conventional predetermined-resolution approach in visual processing. Qwen2-VL introduces the Naive Dynamic Resolution mechanism, which enables the model to dynamically process images of varying resolutions into different numbers of visual tokens. This approach allows the model to generate more efficient and accurate visual representations, closely aligning with human perceptual processes. The model also integrates Multimodal Rotary Position Embedding (M-RoPE), facilitating the effective fusion of positional information across text, images, and videos. We employ a unified paradigm for processing both images and videos, enhancing the model's visual perception capabilities. To explore the potential of large multimodal models, Qwen2-VL investigates the scaling laws for large vision-language models (LVLMs). By scaling both the model size-with versions at 2B, 8B, and 72B parameters-and the amount of training data, the Qwen2-VL Series achieves highly competitive performance. Notably, the Qwen2-VL-72B model achieves results comparable to leading models such as GPT-4o and Claude3.5-Sonnet across various multimodal benchmarks, outperforming other generalist models. Code is available at \url{this https URL}.",https://arxiv.org/abs/2409.12191,Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution
Qwen2-VL-72B,Language,China,Alibaba,2024-09-18,Industry,Quantitative reasoning,Likely,128.0,Open weights (unrestricted),Unspecified unreleased,Not-defined,72000000000.0,1400000000000.0,4096.0,6.048e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Significant use,1,4000000,197336.5951439333,1293528.32,9.51008597883598e-09,51200.0,808290693.7095507,209715200.0,"We present the Qwen2-VL Series, an advanced upgrade of the previous Qwen-VL models that redefines the conventional predetermined-resolution approach in visual processing. Qwen2-VL introduces the Naive Dynamic Resolution mechanism, which enables the model to dynamically process images of varying resolutions into different numbers of visual tokens. This approach allows the model to generate more efficient and accurate visual representations, closely aligning with human perceptual processes. The model also integrates Multimodal Rotary Position Embedding (M-RoPE), facilitating the effective fusion of positional information across text, images, and videos. We employ a unified paradigm for processing both images and videos, enhancing the model's visual perception capabilities. To explore the potential of large multimodal models, Qwen2-VL investigates the scaling laws for large vision-language models (LVLMs). By scaling both the model size-with versions at 2B, 8B, and 72B parameters-and the amount of training data, the Qwen2-VL Series achieves highly competitive performance. Notably, the Qwen2-VL-72B model achieves results comparable to leading models such as GPT-4o and Claude3.5-Sonnet across various multimodal benchmarks, outperforming other generalist models. Code is available at \url{this https URL}.",https://arxiv.org/abs/2409.12191,Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution
Qwen2-VL-72B,Language,China,Alibaba,2024-09-18,Industry,Question answering,Likely,128.0,Open weights (unrestricted),Unspecified unreleased,Not-defined,72000000000.0,1400000000000.0,4096.0,6.048e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Significant use,1,4000000,197336.5951439333,1293528.32,9.51008597883598e-09,51200.0,808290693.7095507,209715200.0,"We present the Qwen2-VL Series, an advanced upgrade of the previous Qwen-VL models that redefines the conventional predetermined-resolution approach in visual processing. Qwen2-VL introduces the Naive Dynamic Resolution mechanism, which enables the model to dynamically process images of varying resolutions into different numbers of visual tokens. This approach allows the model to generate more efficient and accurate visual representations, closely aligning with human perceptual processes. The model also integrates Multimodal Rotary Position Embedding (M-RoPE), facilitating the effective fusion of positional information across text, images, and videos. We employ a unified paradigm for processing both images and videos, enhancing the model's visual perception capabilities. To explore the potential of large multimodal models, Qwen2-VL investigates the scaling laws for large vision-language models (LVLMs). By scaling both the model size-with versions at 2B, 8B, and 72B parameters-and the amount of training data, the Qwen2-VL Series achieves highly competitive performance. Notably, the Qwen2-VL-72B model achieves results comparable to leading models such as GPT-4o and Claude3.5-Sonnet across various multimodal benchmarks, outperforming other generalist models. Code is available at \url{this https URL}.",https://arxiv.org/abs/2409.12191,Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution
Qwen2-VL-72B,Language,China,Alibaba,2024-09-18,Industry,Translation,Likely,128.0,Open weights (unrestricted),Unspecified unreleased,Not-defined,72000000000.0,1400000000000.0,4096.0,6.048e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Significant use,1,4000000,197336.5951439333,1293528.32,9.51008597883598e-09,51200.0,808290693.7095507,209715200.0,"We present the Qwen2-VL Series, an advanced upgrade of the previous Qwen-VL models that redefines the conventional predetermined-resolution approach in visual processing. Qwen2-VL introduces the Naive Dynamic Resolution mechanism, which enables the model to dynamically process images of varying resolutions into different numbers of visual tokens. This approach allows the model to generate more efficient and accurate visual representations, closely aligning with human perceptual processes. The model also integrates Multimodal Rotary Position Embedding (M-RoPE), facilitating the effective fusion of positional information across text, images, and videos. We employ a unified paradigm for processing both images and videos, enhancing the model's visual perception capabilities. To explore the potential of large multimodal models, Qwen2-VL investigates the scaling laws for large vision-language models (LVLMs). By scaling both the model size-with versions at 2B, 8B, and 72B parameters-and the amount of training data, the Qwen2-VL Series achieves highly competitive performance. Notably, the Qwen2-VL-72B model achieves results comparable to leading models such as GPT-4o and Claude3.5-Sonnet across various multimodal benchmarks, outperforming other generalist models. Code is available at \url{this https URL}.",https://arxiv.org/abs/2409.12191,Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution
Qwen2-VL-72B,Language,China,Alibaba,2024-09-18,Industry,Video description,Likely,128.0,Open weights (unrestricted),Unspecified unreleased,Not-defined,72000000000.0,1400000000000.0,4096.0,6.048e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Significant use,1,4000000,197336.5951439333,1293528.32,9.51008597883598e-09,51200.0,808290693.7095507,209715200.0,"We present the Qwen2-VL Series, an advanced upgrade of the previous Qwen-VL models that redefines the conventional predetermined-resolution approach in visual processing. Qwen2-VL introduces the Naive Dynamic Resolution mechanism, which enables the model to dynamically process images of varying resolutions into different numbers of visual tokens. This approach allows the model to generate more efficient and accurate visual representations, closely aligning with human perceptual processes. The model also integrates Multimodal Rotary Position Embedding (M-RoPE), facilitating the effective fusion of positional information across text, images, and videos. We employ a unified paradigm for processing both images and videos, enhancing the model's visual perception capabilities. To explore the potential of large multimodal models, Qwen2-VL investigates the scaling laws for large vision-language models (LVLMs). By scaling both the model size-with versions at 2B, 8B, and 72B parameters-and the amount of training data, the Qwen2-VL Series achieves highly competitive performance. Notably, the Qwen2-VL-72B model achieves results comparable to leading models such as GPT-4o and Claude3.5-Sonnet across various multimodal benchmarks, outperforming other generalist models. Code is available at \url{this https URL}.",https://arxiv.org/abs/2409.12191,Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution
Qwen2-VL-72B,Language,China,Alibaba,2024-09-18,Industry,Visual question answering,Likely,128.0,Open weights (unrestricted),Unspecified unreleased,Not-defined,72000000000.0,1400000000000.0,4096.0,6.048e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Significant use,1,4000000,197336.5951439333,1293528.32,9.51008597883598e-09,51200.0,808290693.7095507,209715200.0,"We present the Qwen2-VL Series, an advanced upgrade of the previous Qwen-VL models that redefines the conventional predetermined-resolution approach in visual processing. Qwen2-VL introduces the Naive Dynamic Resolution mechanism, which enables the model to dynamically process images of varying resolutions into different numbers of visual tokens. This approach allows the model to generate more efficient and accurate visual representations, closely aligning with human perceptual processes. The model also integrates Multimodal Rotary Position Embedding (M-RoPE), facilitating the effective fusion of positional information across text, images, and videos. We employ a unified paradigm for processing both images and videos, enhancing the model's visual perception capabilities. To explore the potential of large multimodal models, Qwen2-VL investigates the scaling laws for large vision-language models (LVLMs). By scaling both the model size-with versions at 2B, 8B, and 72B parameters-and the amount of training data, the Qwen2-VL Series achieves highly competitive performance. Notably, the Qwen2-VL-72B model achieves results comparable to leading models such as GPT-4o and Claude3.5-Sonnet across various multimodal benchmarks, outperforming other generalist models. Code is available at \url{this https URL}.",https://arxiv.org/abs/2409.12191,Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution
Qwen2-VL-72B,Multimodal,China,Alibaba,2024-09-18,Industry,Character recognition,Likely,128.0,Open weights (unrestricted),Unspecified unreleased,Not-defined,72000000000.0,1400000000000.0,4096.0,6.048e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Significant use,1,4000000,197336.5951439333,1293528.32,9.51008597883598e-09,51200.0,808290693.7095507,209715200.0,"We present the Qwen2-VL Series, an advanced upgrade of the previous Qwen-VL models that redefines the conventional predetermined-resolution approach in visual processing. Qwen2-VL introduces the Naive Dynamic Resolution mechanism, which enables the model to dynamically process images of varying resolutions into different numbers of visual tokens. This approach allows the model to generate more efficient and accurate visual representations, closely aligning with human perceptual processes. The model also integrates Multimodal Rotary Position Embedding (M-RoPE), facilitating the effective fusion of positional information across text, images, and videos. We employ a unified paradigm for processing both images and videos, enhancing the model's visual perception capabilities. To explore the potential of large multimodal models, Qwen2-VL investigates the scaling laws for large vision-language models (LVLMs). By scaling both the model size-with versions at 2B, 8B, and 72B parameters-and the amount of training data, the Qwen2-VL Series achieves highly competitive performance. Notably, the Qwen2-VL-72B model achieves results comparable to leading models such as GPT-4o and Claude3.5-Sonnet across various multimodal benchmarks, outperforming other generalist models. Code is available at \url{this https URL}.",https://arxiv.org/abs/2409.12191,Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution
Qwen2-VL-72B,Multimodal,China,Alibaba,2024-09-18,Industry,Language modeling/generation,Likely,128.0,Open weights (unrestricted),Unspecified unreleased,Not-defined,72000000000.0,1400000000000.0,4096.0,6.048e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Significant use,1,4000000,197336.5951439333,1293528.32,9.51008597883598e-09,51200.0,808290693.7095507,209715200.0,"We present the Qwen2-VL Series, an advanced upgrade of the previous Qwen-VL models that redefines the conventional predetermined-resolution approach in visual processing. Qwen2-VL introduces the Naive Dynamic Resolution mechanism, which enables the model to dynamically process images of varying resolutions into different numbers of visual tokens. This approach allows the model to generate more efficient and accurate visual representations, closely aligning with human perceptual processes. The model also integrates Multimodal Rotary Position Embedding (M-RoPE), facilitating the effective fusion of positional information across text, images, and videos. We employ a unified paradigm for processing both images and videos, enhancing the model's visual perception capabilities. To explore the potential of large multimodal models, Qwen2-VL investigates the scaling laws for large vision-language models (LVLMs). By scaling both the model size-with versions at 2B, 8B, and 72B parameters-and the amount of training data, the Qwen2-VL Series achieves highly competitive performance. Notably, the Qwen2-VL-72B model achieves results comparable to leading models such as GPT-4o and Claude3.5-Sonnet across various multimodal benchmarks, outperforming other generalist models. Code is available at \url{this https URL}.",https://arxiv.org/abs/2409.12191,Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution
Qwen2-VL-72B,Multimodal,China,Alibaba,2024-09-18,Industry,Quantitative reasoning,Likely,128.0,Open weights (unrestricted),Unspecified unreleased,Not-defined,72000000000.0,1400000000000.0,4096.0,6.048e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Significant use,1,4000000,197336.5951439333,1293528.32,9.51008597883598e-09,51200.0,808290693.7095507,209715200.0,"We present the Qwen2-VL Series, an advanced upgrade of the previous Qwen-VL models that redefines the conventional predetermined-resolution approach in visual processing. Qwen2-VL introduces the Naive Dynamic Resolution mechanism, which enables the model to dynamically process images of varying resolutions into different numbers of visual tokens. This approach allows the model to generate more efficient and accurate visual representations, closely aligning with human perceptual processes. The model also integrates Multimodal Rotary Position Embedding (M-RoPE), facilitating the effective fusion of positional information across text, images, and videos. We employ a unified paradigm for processing both images and videos, enhancing the model's visual perception capabilities. To explore the potential of large multimodal models, Qwen2-VL investigates the scaling laws for large vision-language models (LVLMs). By scaling both the model size-with versions at 2B, 8B, and 72B parameters-and the amount of training data, the Qwen2-VL Series achieves highly competitive performance. Notably, the Qwen2-VL-72B model achieves results comparable to leading models such as GPT-4o and Claude3.5-Sonnet across various multimodal benchmarks, outperforming other generalist models. Code is available at \url{this https URL}.",https://arxiv.org/abs/2409.12191,Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution
Qwen2-VL-72B,Multimodal,China,Alibaba,2024-09-18,Industry,Question answering,Likely,128.0,Open weights (unrestricted),Unspecified unreleased,Not-defined,72000000000.0,1400000000000.0,4096.0,6.048e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Significant use,1,4000000,197336.5951439333,1293528.32,9.51008597883598e-09,51200.0,808290693.7095507,209715200.0,"We present the Qwen2-VL Series, an advanced upgrade of the previous Qwen-VL models that redefines the conventional predetermined-resolution approach in visual processing. Qwen2-VL introduces the Naive Dynamic Resolution mechanism, which enables the model to dynamically process images of varying resolutions into different numbers of visual tokens. This approach allows the model to generate more efficient and accurate visual representations, closely aligning with human perceptual processes. The model also integrates Multimodal Rotary Position Embedding (M-RoPE), facilitating the effective fusion of positional information across text, images, and videos. We employ a unified paradigm for processing both images and videos, enhancing the model's visual perception capabilities. To explore the potential of large multimodal models, Qwen2-VL investigates the scaling laws for large vision-language models (LVLMs). By scaling both the model size-with versions at 2B, 8B, and 72B parameters-and the amount of training data, the Qwen2-VL Series achieves highly competitive performance. Notably, the Qwen2-VL-72B model achieves results comparable to leading models such as GPT-4o and Claude3.5-Sonnet across various multimodal benchmarks, outperforming other generalist models. Code is available at \url{this https URL}.",https://arxiv.org/abs/2409.12191,Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution
Qwen2-VL-72B,Multimodal,China,Alibaba,2024-09-18,Industry,Translation,Likely,128.0,Open weights (unrestricted),Unspecified unreleased,Not-defined,72000000000.0,1400000000000.0,4096.0,6.048e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Significant use,1,4000000,197336.5951439333,1293528.32,9.51008597883598e-09,51200.0,808290693.7095507,209715200.0,"We present the Qwen2-VL Series, an advanced upgrade of the previous Qwen-VL models that redefines the conventional predetermined-resolution approach in visual processing. Qwen2-VL introduces the Naive Dynamic Resolution mechanism, which enables the model to dynamically process images of varying resolutions into different numbers of visual tokens. This approach allows the model to generate more efficient and accurate visual representations, closely aligning with human perceptual processes. The model also integrates Multimodal Rotary Position Embedding (M-RoPE), facilitating the effective fusion of positional information across text, images, and videos. We employ a unified paradigm for processing both images and videos, enhancing the model's visual perception capabilities. To explore the potential of large multimodal models, Qwen2-VL investigates the scaling laws for large vision-language models (LVLMs). By scaling both the model size-with versions at 2B, 8B, and 72B parameters-and the amount of training data, the Qwen2-VL Series achieves highly competitive performance. Notably, the Qwen2-VL-72B model achieves results comparable to leading models such as GPT-4o and Claude3.5-Sonnet across various multimodal benchmarks, outperforming other generalist models. Code is available at \url{this https URL}.",https://arxiv.org/abs/2409.12191,Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution
Qwen2-VL-72B,Multimodal,China,Alibaba,2024-09-18,Industry,Video description,Likely,128.0,Open weights (unrestricted),Unspecified unreleased,Not-defined,72000000000.0,1400000000000.0,4096.0,6.048e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Significant use,1,4000000,197336.5951439333,1293528.32,9.51008597883598e-09,51200.0,808290693.7095507,209715200.0,"We present the Qwen2-VL Series, an advanced upgrade of the previous Qwen-VL models that redefines the conventional predetermined-resolution approach in visual processing. Qwen2-VL introduces the Naive Dynamic Resolution mechanism, which enables the model to dynamically process images of varying resolutions into different numbers of visual tokens. This approach allows the model to generate more efficient and accurate visual representations, closely aligning with human perceptual processes. The model also integrates Multimodal Rotary Position Embedding (M-RoPE), facilitating the effective fusion of positional information across text, images, and videos. We employ a unified paradigm for processing both images and videos, enhancing the model's visual perception capabilities. To explore the potential of large multimodal models, Qwen2-VL investigates the scaling laws for large vision-language models (LVLMs). By scaling both the model size-with versions at 2B, 8B, and 72B parameters-and the amount of training data, the Qwen2-VL Series achieves highly competitive performance. Notably, the Qwen2-VL-72B model achieves results comparable to leading models such as GPT-4o and Claude3.5-Sonnet across various multimodal benchmarks, outperforming other generalist models. Code is available at \url{this https URL}.",https://arxiv.org/abs/2409.12191,Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution
Qwen2-VL-72B,Multimodal,China,Alibaba,2024-09-18,Industry,Visual question answering,Likely,128.0,Open weights (unrestricted),Unspecified unreleased,Not-defined,72000000000.0,1400000000000.0,4096.0,6.048e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Significant use,1,4000000,197336.5951439333,1293528.32,9.51008597883598e-09,51200.0,808290693.7095507,209715200.0,"We present the Qwen2-VL Series, an advanced upgrade of the previous Qwen-VL models that redefines the conventional predetermined-resolution approach in visual processing. Qwen2-VL introduces the Naive Dynamic Resolution mechanism, which enables the model to dynamically process images of varying resolutions into different numbers of visual tokens. This approach allows the model to generate more efficient and accurate visual representations, closely aligning with human perceptual processes. The model also integrates Multimodal Rotary Position Embedding (M-RoPE), facilitating the effective fusion of positional information across text, images, and videos. We employ a unified paradigm for processing both images and videos, enhancing the model's visual perception capabilities. To explore the potential of large multimodal models, Qwen2-VL investigates the scaling laws for large vision-language models (LVLMs). By scaling both the model size-with versions at 2B, 8B, and 72B parameters-and the amount of training data, the Qwen2-VL Series achieves highly competitive performance. Notably, the Qwen2-VL-72B model achieves results comparable to leading models such as GPT-4o and Claude3.5-Sonnet across various multimodal benchmarks, outperforming other generalist models. Code is available at \url{this https URL}.",https://arxiv.org/abs/2409.12191,Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution
Qwen2-VL-72B,Vision,China,Alibaba,2024-09-18,Industry,Character recognition,Likely,128.0,Open weights (unrestricted),Unspecified unreleased,Not-defined,72000000000.0,1400000000000.0,4096.0,6.048e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Significant use,1,4000000,197336.5951439333,1293528.32,9.51008597883598e-09,51200.0,808290693.7095507,209715200.0,"We present the Qwen2-VL Series, an advanced upgrade of the previous Qwen-VL models that redefines the conventional predetermined-resolution approach in visual processing. Qwen2-VL introduces the Naive Dynamic Resolution mechanism, which enables the model to dynamically process images of varying resolutions into different numbers of visual tokens. This approach allows the model to generate more efficient and accurate visual representations, closely aligning with human perceptual processes. The model also integrates Multimodal Rotary Position Embedding (M-RoPE), facilitating the effective fusion of positional information across text, images, and videos. We employ a unified paradigm for processing both images and videos, enhancing the model's visual perception capabilities. To explore the potential of large multimodal models, Qwen2-VL investigates the scaling laws for large vision-language models (LVLMs). By scaling both the model size-with versions at 2B, 8B, and 72B parameters-and the amount of training data, the Qwen2-VL Series achieves highly competitive performance. Notably, the Qwen2-VL-72B model achieves results comparable to leading models such as GPT-4o and Claude3.5-Sonnet across various multimodal benchmarks, outperforming other generalist models. Code is available at \url{this https URL}.",https://arxiv.org/abs/2409.12191,Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution
Qwen2-VL-72B,Vision,China,Alibaba,2024-09-18,Industry,Language modeling/generation,Likely,128.0,Open weights (unrestricted),Unspecified unreleased,Not-defined,72000000000.0,1400000000000.0,4096.0,6.048e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Significant use,1,4000000,197336.5951439333,1293528.32,9.51008597883598e-09,51200.0,808290693.7095507,209715200.0,"We present the Qwen2-VL Series, an advanced upgrade of the previous Qwen-VL models that redefines the conventional predetermined-resolution approach in visual processing. Qwen2-VL introduces the Naive Dynamic Resolution mechanism, which enables the model to dynamically process images of varying resolutions into different numbers of visual tokens. This approach allows the model to generate more efficient and accurate visual representations, closely aligning with human perceptual processes. The model also integrates Multimodal Rotary Position Embedding (M-RoPE), facilitating the effective fusion of positional information across text, images, and videos. We employ a unified paradigm for processing both images and videos, enhancing the model's visual perception capabilities. To explore the potential of large multimodal models, Qwen2-VL investigates the scaling laws for large vision-language models (LVLMs). By scaling both the model size-with versions at 2B, 8B, and 72B parameters-and the amount of training data, the Qwen2-VL Series achieves highly competitive performance. Notably, the Qwen2-VL-72B model achieves results comparable to leading models such as GPT-4o and Claude3.5-Sonnet across various multimodal benchmarks, outperforming other generalist models. Code is available at \url{this https URL}.",https://arxiv.org/abs/2409.12191,Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution
Qwen2-VL-72B,Vision,China,Alibaba,2024-09-18,Industry,Quantitative reasoning,Likely,128.0,Open weights (unrestricted),Unspecified unreleased,Not-defined,72000000000.0,1400000000000.0,4096.0,6.048e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Significant use,1,4000000,197336.5951439333,1293528.32,9.51008597883598e-09,51200.0,808290693.7095507,209715200.0,"We present the Qwen2-VL Series, an advanced upgrade of the previous Qwen-VL models that redefines the conventional predetermined-resolution approach in visual processing. Qwen2-VL introduces the Naive Dynamic Resolution mechanism, which enables the model to dynamically process images of varying resolutions into different numbers of visual tokens. This approach allows the model to generate more efficient and accurate visual representations, closely aligning with human perceptual processes. The model also integrates Multimodal Rotary Position Embedding (M-RoPE), facilitating the effective fusion of positional information across text, images, and videos. We employ a unified paradigm for processing both images and videos, enhancing the model's visual perception capabilities. To explore the potential of large multimodal models, Qwen2-VL investigates the scaling laws for large vision-language models (LVLMs). By scaling both the model size-with versions at 2B, 8B, and 72B parameters-and the amount of training data, the Qwen2-VL Series achieves highly competitive performance. Notably, the Qwen2-VL-72B model achieves results comparable to leading models such as GPT-4o and Claude3.5-Sonnet across various multimodal benchmarks, outperforming other generalist models. Code is available at \url{this https URL}.",https://arxiv.org/abs/2409.12191,Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution
Qwen2-VL-72B,Vision,China,Alibaba,2024-09-18,Industry,Question answering,Likely,128.0,Open weights (unrestricted),Unspecified unreleased,Not-defined,72000000000.0,1400000000000.0,4096.0,6.048e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Significant use,1,4000000,197336.5951439333,1293528.32,9.51008597883598e-09,51200.0,808290693.7095507,209715200.0,"We present the Qwen2-VL Series, an advanced upgrade of the previous Qwen-VL models that redefines the conventional predetermined-resolution approach in visual processing. Qwen2-VL introduces the Naive Dynamic Resolution mechanism, which enables the model to dynamically process images of varying resolutions into different numbers of visual tokens. This approach allows the model to generate more efficient and accurate visual representations, closely aligning with human perceptual processes. The model also integrates Multimodal Rotary Position Embedding (M-RoPE), facilitating the effective fusion of positional information across text, images, and videos. We employ a unified paradigm for processing both images and videos, enhancing the model's visual perception capabilities. To explore the potential of large multimodal models, Qwen2-VL investigates the scaling laws for large vision-language models (LVLMs). By scaling both the model size-with versions at 2B, 8B, and 72B parameters-and the amount of training data, the Qwen2-VL Series achieves highly competitive performance. Notably, the Qwen2-VL-72B model achieves results comparable to leading models such as GPT-4o and Claude3.5-Sonnet across various multimodal benchmarks, outperforming other generalist models. Code is available at \url{this https URL}.",https://arxiv.org/abs/2409.12191,Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution
Qwen2-VL-72B,Vision,China,Alibaba,2024-09-18,Industry,Translation,Likely,128.0,Open weights (unrestricted),Unspecified unreleased,Not-defined,72000000000.0,1400000000000.0,4096.0,6.048e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Significant use,1,4000000,197336.5951439333,1293528.32,9.51008597883598e-09,51200.0,808290693.7095507,209715200.0,"We present the Qwen2-VL Series, an advanced upgrade of the previous Qwen-VL models that redefines the conventional predetermined-resolution approach in visual processing. Qwen2-VL introduces the Naive Dynamic Resolution mechanism, which enables the model to dynamically process images of varying resolutions into different numbers of visual tokens. This approach allows the model to generate more efficient and accurate visual representations, closely aligning with human perceptual processes. The model also integrates Multimodal Rotary Position Embedding (M-RoPE), facilitating the effective fusion of positional information across text, images, and videos. We employ a unified paradigm for processing both images and videos, enhancing the model's visual perception capabilities. To explore the potential of large multimodal models, Qwen2-VL investigates the scaling laws for large vision-language models (LVLMs). By scaling both the model size-with versions at 2B, 8B, and 72B parameters-and the amount of training data, the Qwen2-VL Series achieves highly competitive performance. Notably, the Qwen2-VL-72B model achieves results comparable to leading models such as GPT-4o and Claude3.5-Sonnet across various multimodal benchmarks, outperforming other generalist models. Code is available at \url{this https URL}.",https://arxiv.org/abs/2409.12191,Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution
Qwen2-VL-72B,Vision,China,Alibaba,2024-09-18,Industry,Video description,Likely,128.0,Open weights (unrestricted),Unspecified unreleased,Not-defined,72000000000.0,1400000000000.0,4096.0,6.048e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Significant use,1,4000000,197336.5951439333,1293528.32,9.51008597883598e-09,51200.0,808290693.7095507,209715200.0,"We present the Qwen2-VL Series, an advanced upgrade of the previous Qwen-VL models that redefines the conventional predetermined-resolution approach in visual processing. Qwen2-VL introduces the Naive Dynamic Resolution mechanism, which enables the model to dynamically process images of varying resolutions into different numbers of visual tokens. This approach allows the model to generate more efficient and accurate visual representations, closely aligning with human perceptual processes. The model also integrates Multimodal Rotary Position Embedding (M-RoPE), facilitating the effective fusion of positional information across text, images, and videos. We employ a unified paradigm for processing both images and videos, enhancing the model's visual perception capabilities. To explore the potential of large multimodal models, Qwen2-VL investigates the scaling laws for large vision-language models (LVLMs). By scaling both the model size-with versions at 2B, 8B, and 72B parameters-and the amount of training data, the Qwen2-VL Series achieves highly competitive performance. Notably, the Qwen2-VL-72B model achieves results comparable to leading models such as GPT-4o and Claude3.5-Sonnet across various multimodal benchmarks, outperforming other generalist models. Code is available at \url{this https URL}.",https://arxiv.org/abs/2409.12191,Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution
Qwen2-VL-72B,Vision,China,Alibaba,2024-09-18,Industry,Visual question answering,Likely,128.0,Open weights (unrestricted),Unspecified unreleased,Not-defined,72000000000.0,1400000000000.0,4096.0,6.048e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Significant use,1,4000000,197336.5951439333,1293528.32,9.51008597883598e-09,51200.0,808290693.7095507,209715200.0,"We present the Qwen2-VL Series, an advanced upgrade of the previous Qwen-VL models that redefines the conventional predetermined-resolution approach in visual processing. Qwen2-VL introduces the Naive Dynamic Resolution mechanism, which enables the model to dynamically process images of varying resolutions into different numbers of visual tokens. This approach allows the model to generate more efficient and accurate visual representations, closely aligning with human perceptual processes. The model also integrates Multimodal Rotary Position Embedding (M-RoPE), facilitating the effective fusion of positional information across text, images, and videos. We employ a unified paradigm for processing both images and videos, enhancing the model's visual perception capabilities. To explore the potential of large multimodal models, Qwen2-VL investigates the scaling laws for large vision-language models (LVLMs). By scaling both the model size-with versions at 2B, 8B, and 72B parameters-and the amount of training data, the Qwen2-VL Series achieves highly competitive performance. Notably, the Qwen2-VL-72B model achieves results comparable to leading models such as GPT-4o and Claude3.5-Sonnet across various multimodal benchmarks, outperforming other generalist models. Code is available at \url{this https URL}.",https://arxiv.org/abs/2409.12191,Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution
Qwen2.5-14B,Language,China,Alibaba,2024-09-19,Industry,Language modeling/generation,Confident,1024.0,Open weights (unrestricted),Unspecified unreleased,Unreleased,14700000000.0,18000000000000.0,1282.4326530612243,1.58760000000001e+24,1877.1621580193864,7406.0,303.0,107206000000000.0,303958000000000.0,54246185353.93311,NVIDIA A800,Training cost,1,4000000,788306.6236983632,7583744.0,6.752708490803687e-10,310272.0,1010950154.8552281,397902944.1306122,"In the past three months since Qwen2’s release, numerous developers have built new models on the Qwen2 language models, providing us with valuable feedback. During this period, we have focused on creating smarter and more knowledgeable language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2.5. We are announcing what might be the largest opensource release in history! Let’s get the party started!

The Qwen2.5-7B model surpasses its predecessors and counterparts in numerous benchmarks, despite having fewer non-embedding parameters. It demonstrates significant improvements across various tasks, achieving 74.2 on general benchmarks like MMLU, 49.8 on math challenges such as MATH, and 57.9 on coding tasks like HumanEval.",https://qwenlm.github.io/blog/qwen2.5/,Qwen2.5: A Party of Foundation Models!
Qwen2.5-14B,Language,China,Alibaba,2024-09-19,Industry,Quantitative reasoning,Confident,1024.0,Open weights (unrestricted),Unspecified unreleased,Unreleased,14700000000.0,18000000000000.0,1282.4326530612243,1.58760000000001e+24,1877.1621580193864,7406.0,303.0,107206000000000.0,303958000000000.0,54246185353.93311,NVIDIA A800,Training cost,1,4000000,788306.6236983632,7583744.0,6.752708490803687e-10,310272.0,1010950154.8552281,397902944.1306122,"In the past three months since Qwen2’s release, numerous developers have built new models on the Qwen2 language models, providing us with valuable feedback. During this period, we have focused on creating smarter and more knowledgeable language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2.5. We are announcing what might be the largest opensource release in history! Let’s get the party started!

The Qwen2.5-7B model surpasses its predecessors and counterparts in numerous benchmarks, despite having fewer non-embedding parameters. It demonstrates significant improvements across various tasks, achieving 74.2 on general benchmarks like MMLU, 49.8 on math challenges such as MATH, and 57.9 on coding tasks like HumanEval.",https://qwenlm.github.io/blog/qwen2.5/,Qwen2.5: A Party of Foundation Models!
Qwen2.5-14B,Language,China,Alibaba,2024-09-19,Industry,Question answering,Confident,1024.0,Open weights (unrestricted),Unspecified unreleased,Unreleased,14700000000.0,18000000000000.0,1282.4326530612243,1.58760000000001e+24,1877.1621580193864,7406.0,303.0,107206000000000.0,303958000000000.0,54246185353.93311,NVIDIA A800,Training cost,1,4000000,788306.6236983632,7583744.0,6.752708490803687e-10,310272.0,1010950154.8552281,397902944.1306122,"In the past three months since Qwen2’s release, numerous developers have built new models on the Qwen2 language models, providing us with valuable feedback. During this period, we have focused on creating smarter and more knowledgeable language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2.5. We are announcing what might be the largest opensource release in history! Let’s get the party started!

The Qwen2.5-7B model surpasses its predecessors and counterparts in numerous benchmarks, despite having fewer non-embedding parameters. It demonstrates significant improvements across various tasks, achieving 74.2 on general benchmarks like MMLU, 49.8 on math challenges such as MATH, and 57.9 on coding tasks like HumanEval.",https://qwenlm.github.io/blog/qwen2.5/,Qwen2.5: A Party of Foundation Models!
Qwen2.5-3B,Language,China,Alibaba,2024-09-19,Industry,Language modeling/generation,Confident,1024.0,Open weights (non-commercial),Unspecified unreleased,Unreleased,3090000000.0,18000000000000.0,1080.0,3.3372e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A800,Training cost,1,4000000,788306.6236983632,10348226.56,1.723510727556035e-08,409600.0,851371153.5942322,442368000.0,"In the past three months since Qwen2’s release, numerous developers have built new models on the Qwen2 language models, providing us with valuable feedback. During this period, we have focused on creating smarter and more knowledgeable language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2.5. We are announcing what might be the largest opensource release in history! Let’s get the party started!",https://qwenlm.github.io/blog/qwen2.5-llm/,Qwen2.5: A Party of Foundation Models!
Qwen2.5-3B,Language,China,Alibaba,2024-09-19,Industry,Quantitative reasoning,Confident,1024.0,Open weights (non-commercial),Unspecified unreleased,Unreleased,3090000000.0,18000000000000.0,1080.0,3.3372e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A800,Training cost,1,4000000,788306.6236983632,10348226.56,1.723510727556035e-08,409600.0,851371153.5942322,442368000.0,"In the past three months since Qwen2’s release, numerous developers have built new models on the Qwen2 language models, providing us with valuable feedback. During this period, we have focused on creating smarter and more knowledgeable language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2.5. We are announcing what might be the largest opensource release in history! Let’s get the party started!",https://qwenlm.github.io/blog/qwen2.5-llm/,Qwen2.5: A Party of Foundation Models!
Qwen2.5-3B,Language,China,Alibaba,2024-09-19,Industry,Question answering,Confident,1024.0,Open weights (non-commercial),Unspecified unreleased,Unreleased,3090000000.0,18000000000000.0,1080.0,3.3372e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A800,Training cost,1,4000000,788306.6236983632,10348226.56,1.723510727556035e-08,409600.0,851371153.5942322,442368000.0,"In the past three months since Qwen2’s release, numerous developers have built new models on the Qwen2 language models, providing us with valuable feedback. During this period, we have focused on creating smarter and more knowledgeable language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2.5. We are announcing what might be the largest opensource release in history! Let’s get the party started!",https://qwenlm.github.io/blog/qwen2.5-llm/,Qwen2.5: A Party of Foundation Models!
Qwen2.5-72B,Language,China,Alibaba,2024-09-19,Industry,Language modeling/generation,Confident,1024.0,Open weights (unrestricted),Unspecified unreleased,Unreleased,72700000000.0,18000000000000.0,1282.4326530612243,7.8e+24,1877.1621580193864,7406.0,303.0,107206000000000.0,303958000000000.0,54246185353.93311,NVIDIA A100,Training cost,1,4000000,788306.6236983632,7583744.0,1.3744358974358974e-10,310272.0,1010950154.8552281,397902944.1306122,"In the past three months since Qwen2’s release, numerous developers have built new models on the Qwen2 language models, providing us with valuable feedback. During this period, we have focused on creating smarter and more knowledgeable language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2.5. We are announcing what might be the largest opensource release in history! Let’s get the party started!",https://qwenlm.github.io/blog/qwen2.5/,Qwen2.5: A Party of Foundation Models!
Qwen2.5-72B,Language,China,Alibaba,2024-09-19,Industry,Quantitative reasoning,Confident,1024.0,Open weights (unrestricted),Unspecified unreleased,Unreleased,72700000000.0,18000000000000.0,1282.4326530612243,7.8e+24,1877.1621580193864,7406.0,303.0,107206000000000.0,303958000000000.0,54246185353.93311,NVIDIA A100,Training cost,1,4000000,788306.6236983632,7583744.0,1.3744358974358974e-10,310272.0,1010950154.8552281,397902944.1306122,"In the past three months since Qwen2’s release, numerous developers have built new models on the Qwen2 language models, providing us with valuable feedback. During this period, we have focused on creating smarter and more knowledgeable language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2.5. We are announcing what might be the largest opensource release in history! Let’s get the party started!",https://qwenlm.github.io/blog/qwen2.5/,Qwen2.5: A Party of Foundation Models!
Qwen2.5-72B,Language,China,Alibaba,2024-09-19,Industry,Question answering,Confident,1024.0,Open weights (unrestricted),Unspecified unreleased,Unreleased,72700000000.0,18000000000000.0,1282.4326530612243,7.8e+24,1877.1621580193864,7406.0,303.0,107206000000000.0,303958000000000.0,54246185353.93311,NVIDIA A100,Training cost,1,4000000,788306.6236983632,7583744.0,1.3744358974358974e-10,310272.0,1010950154.8552281,397902944.1306122,"In the past three months since Qwen2’s release, numerous developers have built new models on the Qwen2 language models, providing us with valuable feedback. During this period, we have focused on creating smarter and more knowledgeable language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2.5. We are announcing what might be the largest opensource release in history! Let’s get the party started!",https://qwenlm.github.io/blog/qwen2.5/,Qwen2.5: A Party of Foundation Models!
Qwen2.5-7B,Language,China,Alibaba,2024-09-19,Industry,Language modeling/generation,Confident,1024.0,Open weights (unrestricted),Unspecified unreleased,Unreleased,7610000000.0,18000000000000.0,1282.4326530612243,8.2188e+23,1877.1621580193864,7406.0,303.0,107206000000000.0,303958000000000.0,54246185353.93311,NVIDIA A800,Training cost,1,4000000,788306.6236983632,7583744.0,1.3043996690514432e-09,310272.0,1010950154.8552281,397902944.1306122,"In the past three months since Qwen2’s release, numerous developers have built new models on the Qwen2 language models, providing us with valuable feedback. During this period, we have focused on creating smarter and more knowledgeable language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2.5. We are announcing what might be the largest opensource release in history! Let’s get the party started!

The Qwen2.5-7B model surpasses its predecessors and counterparts in numerous benchmarks, despite having fewer non-embedding parameters. It demonstrates significant improvements across various tasks, achieving 74.2 on general benchmarks like MMLU, 49.8 on math challenges such as MATH, and 57.9 on coding tasks like HumanEval.",https://qwenlm.github.io/blog/qwen2.5/,Qwen2.5: A Party of Foundation Models!
Qwen2.5-7B,Language,China,Alibaba,2024-09-19,Industry,Quantitative reasoning,Confident,1024.0,Open weights (unrestricted),Unspecified unreleased,Unreleased,7610000000.0,18000000000000.0,1282.4326530612243,8.2188e+23,1877.1621580193864,7406.0,303.0,107206000000000.0,303958000000000.0,54246185353.93311,NVIDIA A800,Training cost,1,4000000,788306.6236983632,7583744.0,1.3043996690514432e-09,310272.0,1010950154.8552281,397902944.1306122,"In the past three months since Qwen2’s release, numerous developers have built new models on the Qwen2 language models, providing us with valuable feedback. During this period, we have focused on creating smarter and more knowledgeable language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2.5. We are announcing what might be the largest opensource release in history! Let’s get the party started!

The Qwen2.5-7B model surpasses its predecessors and counterparts in numerous benchmarks, despite having fewer non-embedding parameters. It demonstrates significant improvements across various tasks, achieving 74.2 on general benchmarks like MMLU, 49.8 on math challenges such as MATH, and 57.9 on coding tasks like HumanEval.",https://qwenlm.github.io/blog/qwen2.5/,Qwen2.5: A Party of Foundation Models!
Qwen2.5-7B,Language,China,Alibaba,2024-09-19,Industry,Question answering,Confident,1024.0,Open weights (unrestricted),Unspecified unreleased,Unreleased,7610000000.0,18000000000000.0,1282.4326530612243,8.2188e+23,1877.1621580193864,7406.0,303.0,107206000000000.0,303958000000000.0,54246185353.93311,NVIDIA A800,Training cost,1,4000000,788306.6236983632,7583744.0,1.3043996690514432e-09,310272.0,1010950154.8552281,397902944.1306122,"In the past three months since Qwen2’s release, numerous developers have built new models on the Qwen2 language models, providing us with valuable feedback. During this period, we have focused on creating smarter and more knowledgeable language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2.5. We are announcing what might be the largest opensource release in history! Let’s get the party started!

The Qwen2.5-7B model surpasses its predecessors and counterparts in numerous benchmarks, despite having fewer non-embedding parameters. It demonstrates significant improvements across various tasks, achieving 74.2 on general benchmarks like MMLU, 49.8 on math challenges such as MATH, and 57.9 on coding tasks like HumanEval.",https://qwenlm.github.io/blog/qwen2.5/,Qwen2.5: A Party of Foundation Models!
Skywork-13B,Language,China,Kunlun Inc.,2023-10-30,Industry,Language modeling,Confident,512.0,Open weights (restricted use),SkyPile,Open (restricted use),13000000000.0,3180000000000.0,940.0,2.5e+23,1877.1621580193864,7406.0,303.0,107206000000000.0,303958000000000.0,54246185353.93311,NVIDIA A800,SOTA improvement,1,16000000,788306.6236983632,3791872.0,0.565,155136.0,741008226.2764614,145827840.0,"In this technical report, we present Skywork-13B, a family of large language models (LLMs) trained on a corpus of over 3.2 trillion tokens drawn from both English and Chinese texts. This bilingual foundation model is the most extensively trained and openly published LLMs of comparable size to date. We introduce a two-stage training methodology using a segmented corpus, targeting general purpose training and then domain-specific enhancement training, respectively. We show that our model not only excels on popular benchmarks, but also achieves state of the art performance in Chinese language modeling on diverse domains. Furthermore, we propose a novel leakage detection method, demonstrating that test data contamination is a pressing issue warranting further investigation by the LLM community. To spur future research, we release Skywork-13B along with checkpoints obtained during intermediate stages of the training process. We are also releasing part of our SkyPile corpus, a collection of over 150 billion tokens of web text, which is the largest high quality open Chinese pre-training corpus to date. We hope Skywork-13B and our open corpus will serve as a valuable open-source resource to democratize access to high-quality LLMs.",https://arxiv.org/abs/2310.19341,Skywork: A More Open Bilingual Foundation Model
Skywork-13B,Language,China,Kunlun Inc.,2023-10-30,Industry,Language modeling/generation,Confident,512.0,Open weights (restricted use),SkyPile,Open (restricted use),13000000000.0,3180000000000.0,940.0,2.5e+23,1877.1621580193864,7406.0,303.0,107206000000000.0,303958000000000.0,54246185353.93311,NVIDIA A800,SOTA improvement,1,16000000,788306.6236983632,3791872.0,0.565,155136.0,741008226.2764614,145827840.0,"In this technical report, we present Skywork-13B, a family of large language models (LLMs) trained on a corpus of over 3.2 trillion tokens drawn from both English and Chinese texts. This bilingual foundation model is the most extensively trained and openly published LLMs of comparable size to date. We introduce a two-stage training methodology using a segmented corpus, targeting general purpose training and then domain-specific enhancement training, respectively. We show that our model not only excels on popular benchmarks, but also achieves state of the art performance in Chinese language modeling on diverse domains. Furthermore, we propose a novel leakage detection method, demonstrating that test data contamination is a pressing issue warranting further investigation by the LLM community. To spur future research, we release Skywork-13B along with checkpoints obtained during intermediate stages of the training process. We are also releasing part of our SkyPile corpus, a collection of over 150 billion tokens of web text, which is the largest high quality open Chinese pre-training corpus to date. We hope Skywork-13B and our open corpus will serve as a valuable open-source resource to democratize access to high-quality LLMs.",https://arxiv.org/abs/2310.19341,Skywork: A More Open Bilingual Foundation Model
Skywork-13B,Language,China,Kunlun Inc.,2023-10-30,Industry,Translation,Confident,512.0,Open weights (restricted use),SkyPile,Open (restricted use),13000000000.0,3180000000000.0,940.0,2.5e+23,1877.1621580193864,7406.0,303.0,107206000000000.0,303958000000000.0,54246185353.93311,NVIDIA A800,SOTA improvement,1,16000000,788306.6236983632,3791872.0,0.565,155136.0,741008226.2764614,145827840.0,"In this technical report, we present Skywork-13B, a family of large language models (LLMs) trained on a corpus of over 3.2 trillion tokens drawn from both English and Chinese texts. This bilingual foundation model is the most extensively trained and openly published LLMs of comparable size to date. We introduce a two-stage training methodology using a segmented corpus, targeting general purpose training and then domain-specific enhancement training, respectively. We show that our model not only excels on popular benchmarks, but also achieves state of the art performance in Chinese language modeling on diverse domains. Furthermore, we propose a novel leakage detection method, demonstrating that test data contamination is a pressing issue warranting further investigation by the LLM community. To spur future research, we release Skywork-13B along with checkpoints obtained during intermediate stages of the training process. We are also releasing part of our SkyPile corpus, a collection of over 150 billion tokens of web text, which is the largest high quality open Chinese pre-training corpus to date. We hope Skywork-13B and our open corpus will serve as a valuable open-source resource to democratize access to high-quality LLMs.",https://arxiv.org/abs/2310.19341,Skywork: A More Open Bilingual Foundation Model
StarCoder 2 15B,Language,Multinational,BigCode,2024-02-29,Industry,Code autocompletion,Confident,512.0,Open weights (restricted use),The Stack v2,Unreleased,15000000000.0,913230000000.0,940.0,3.87e+23,1877.1621580193864,27922.725,550.0,1213583000000000.0,1301500000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,1,16000000,788306.6236983632,14296435.2,0.2272361,281600.0,741008226.2764614,264704000.0,"The BigCode project, an open-scientific collaboration focused on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder2. In partnership with Software Heritage (SWH), we build The Stack v2 on top of the digital commons of their source code archive. Alongside the SWH repositories spanning 619 programming languages, we carefully select other high-quality data sources, such as GitHub pull requests, Kaggle notebooks, and code documentation. This results in a training set that is 4x larger than the first StarCoder dataset. We train StarCoder2 models with 3B, 7B, and 15B parameters on 3.3 to 4.3 trillion tokens and thoroughly evaluate them on a comprehensive set of Code LLM benchmarks. We find that our small model, StarCoder2-3B, outperforms other Code LLMs of similar size on most benchmarks, and also outperforms StarCoderBase-15B. Our large model, StarCoder2- 15B, significantly outperforms other models of comparable size. In addition, it matches or outperforms CodeLlama-34B, a model more than twice its size. Although DeepSeekCoder- 33B is the best-performing model at code completion for high-resource languages, we find that StarCoder2-15B outperforms it on math and code reasoning benchmarks, as well as several low-resource languages. We make the model weights available under an OpenRAIL license and ensure full transparency regarding the training data by releasing the SoftWare Heritage persistent IDentifiers (SWHIDs) of the source code data. ",https://arxiv.org/abs/2402.19173,StarCoder 2 and The Stack v2: The Next Generation
StarCoder 2 15B,Language,Multinational,BigCode,2024-02-29,Industry,Code generation,Confident,512.0,Open weights (restricted use),The Stack v2,Unreleased,15000000000.0,913230000000.0,940.0,3.87e+23,1877.1621580193864,27922.725,550.0,1213583000000000.0,1301500000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,1,16000000,788306.6236983632,14296435.2,0.2272361,281600.0,741008226.2764614,264704000.0,"The BigCode project, an open-scientific collaboration focused on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder2. In partnership with Software Heritage (SWH), we build The Stack v2 on top of the digital commons of their source code archive. Alongside the SWH repositories spanning 619 programming languages, we carefully select other high-quality data sources, such as GitHub pull requests, Kaggle notebooks, and code documentation. This results in a training set that is 4x larger than the first StarCoder dataset. We train StarCoder2 models with 3B, 7B, and 15B parameters on 3.3 to 4.3 trillion tokens and thoroughly evaluate them on a comprehensive set of Code LLM benchmarks. We find that our small model, StarCoder2-3B, outperforms other Code LLMs of similar size on most benchmarks, and also outperforms StarCoderBase-15B. Our large model, StarCoder2- 15B, significantly outperforms other models of comparable size. In addition, it matches or outperforms CodeLlama-34B, a model more than twice its size. Although DeepSeekCoder- 33B is the best-performing model at code completion for high-resource languages, we find that StarCoder2-15B outperforms it on math and code reasoning benchmarks, as well as several low-resource languages. We make the model weights available under an OpenRAIL license and ensure full transparency regarding the training data by releasing the SoftWare Heritage persistent IDentifiers (SWHIDs) of the source code data. ",https://arxiv.org/abs/2402.19173,StarCoder 2 and The Stack v2: The Next Generation
StarCoder 2 15B,Language,Multinational,Hugging Face,2024-02-29,Industry,Code autocompletion,Confident,512.0,Open weights (restricted use),The Stack v2,Unreleased,15000000000.0,913230000000.0,940.0,3.87e+23,1877.1621580193864,27922.725,550.0,1213583000000000.0,1301500000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,1,16000000,788306.6236983632,14296435.2,0.2272361,281600.0,741008226.2764614,264704000.0,"The BigCode project, an open-scientific collaboration focused on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder2. In partnership with Software Heritage (SWH), we build The Stack v2 on top of the digital commons of their source code archive. Alongside the SWH repositories spanning 619 programming languages, we carefully select other high-quality data sources, such as GitHub pull requests, Kaggle notebooks, and code documentation. This results in a training set that is 4x larger than the first StarCoder dataset. We train StarCoder2 models with 3B, 7B, and 15B parameters on 3.3 to 4.3 trillion tokens and thoroughly evaluate them on a comprehensive set of Code LLM benchmarks. We find that our small model, StarCoder2-3B, outperforms other Code LLMs of similar size on most benchmarks, and also outperforms StarCoderBase-15B. Our large model, StarCoder2- 15B, significantly outperforms other models of comparable size. In addition, it matches or outperforms CodeLlama-34B, a model more than twice its size. Although DeepSeekCoder- 33B is the best-performing model at code completion for high-resource languages, we find that StarCoder2-15B outperforms it on math and code reasoning benchmarks, as well as several low-resource languages. We make the model weights available under an OpenRAIL license and ensure full transparency regarding the training data by releasing the SoftWare Heritage persistent IDentifiers (SWHIDs) of the source code data. ",https://arxiv.org/abs/2402.19173,StarCoder 2 and The Stack v2: The Next Generation
StarCoder 2 15B,Language,Multinational,Hugging Face,2024-02-29,Industry,Code generation,Confident,512.0,Open weights (restricted use),The Stack v2,Unreleased,15000000000.0,913230000000.0,940.0,3.87e+23,1877.1621580193864,27922.725,550.0,1213583000000000.0,1301500000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,1,16000000,788306.6236983632,14296435.2,0.2272361,281600.0,741008226.2764614,264704000.0,"The BigCode project, an open-scientific collaboration focused on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder2. In partnership with Software Heritage (SWH), we build The Stack v2 on top of the digital commons of their source code archive. Alongside the SWH repositories spanning 619 programming languages, we carefully select other high-quality data sources, such as GitHub pull requests, Kaggle notebooks, and code documentation. This results in a training set that is 4x larger than the first StarCoder dataset. We train StarCoder2 models with 3B, 7B, and 15B parameters on 3.3 to 4.3 trillion tokens and thoroughly evaluate them on a comprehensive set of Code LLM benchmarks. We find that our small model, StarCoder2-3B, outperforms other Code LLMs of similar size on most benchmarks, and also outperforms StarCoderBase-15B. Our large model, StarCoder2- 15B, significantly outperforms other models of comparable size. In addition, it matches or outperforms CodeLlama-34B, a model more than twice its size. Although DeepSeekCoder- 33B is the best-performing model at code completion for high-resource languages, we find that StarCoder2-15B outperforms it on math and code reasoning benchmarks, as well as several low-resource languages. We make the model weights available under an OpenRAIL license and ensure full transparency regarding the training data by releasing the SoftWare Heritage persistent IDentifiers (SWHIDs) of the source code data. ",https://arxiv.org/abs/2402.19173,StarCoder 2 and The Stack v2: The Next Generation
StarCoder 2 15B,Language,Multinational,NVIDIA,2024-02-29,Industry,Code autocompletion,Confident,512.0,Open weights (restricted use),The Stack v2,Unreleased,15000000000.0,913230000000.0,940.0,3.87e+23,1877.1621580193864,27922.725,550.0,1213583000000000.0,1301500000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,1,16000000,788306.6236983632,14296435.2,0.2272361,281600.0,741008226.2764614,264704000.0,"The BigCode project, an open-scientific collaboration focused on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder2. In partnership with Software Heritage (SWH), we build The Stack v2 on top of the digital commons of their source code archive. Alongside the SWH repositories spanning 619 programming languages, we carefully select other high-quality data sources, such as GitHub pull requests, Kaggle notebooks, and code documentation. This results in a training set that is 4x larger than the first StarCoder dataset. We train StarCoder2 models with 3B, 7B, and 15B parameters on 3.3 to 4.3 trillion tokens and thoroughly evaluate them on a comprehensive set of Code LLM benchmarks. We find that our small model, StarCoder2-3B, outperforms other Code LLMs of similar size on most benchmarks, and also outperforms StarCoderBase-15B. Our large model, StarCoder2- 15B, significantly outperforms other models of comparable size. In addition, it matches or outperforms CodeLlama-34B, a model more than twice its size. Although DeepSeekCoder- 33B is the best-performing model at code completion for high-resource languages, we find that StarCoder2-15B outperforms it on math and code reasoning benchmarks, as well as several low-resource languages. We make the model weights available under an OpenRAIL license and ensure full transparency regarding the training data by releasing the SoftWare Heritage persistent IDentifiers (SWHIDs) of the source code data. ",https://arxiv.org/abs/2402.19173,StarCoder 2 and The Stack v2: The Next Generation
StarCoder 2 15B,Language,Multinational,NVIDIA,2024-02-29,Industry,Code generation,Confident,512.0,Open weights (restricted use),The Stack v2,Unreleased,15000000000.0,913230000000.0,940.0,3.87e+23,1877.1621580193864,27922.725,550.0,1213583000000000.0,1301500000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,1,16000000,788306.6236983632,14296435.2,0.2272361,281600.0,741008226.2764614,264704000.0,"The BigCode project, an open-scientific collaboration focused on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder2. In partnership with Software Heritage (SWH), we build The Stack v2 on top of the digital commons of their source code archive. Alongside the SWH repositories spanning 619 programming languages, we carefully select other high-quality data sources, such as GitHub pull requests, Kaggle notebooks, and code documentation. This results in a training set that is 4x larger than the first StarCoder dataset. We train StarCoder2 models with 3B, 7B, and 15B parameters on 3.3 to 4.3 trillion tokens and thoroughly evaluate them on a comprehensive set of Code LLM benchmarks. We find that our small model, StarCoder2-3B, outperforms other Code LLMs of similar size on most benchmarks, and also outperforms StarCoderBase-15B. Our large model, StarCoder2- 15B, significantly outperforms other models of comparable size. In addition, it matches or outperforms CodeLlama-34B, a model more than twice its size. Although DeepSeekCoder- 33B is the best-performing model at code completion for high-resource languages, we find that StarCoder2-15B outperforms it on math and code reasoning benchmarks, as well as several low-resource languages. We make the model weights available under an OpenRAIL license and ensure full transparency regarding the training data by releasing the SoftWare Heritage persistent IDentifiers (SWHIDs) of the source code data. ",https://arxiv.org/abs/2402.19173,StarCoder 2 and The Stack v2: The Next Generation
StarCoder 2 15B,Language,Multinational,ServiceNow,2024-02-29,Industry,Code autocompletion,Confident,512.0,Open weights (restricted use),The Stack v2,Unreleased,15000000000.0,913230000000.0,940.0,3.87e+23,1877.1621580193864,27922.725,550.0,1213583000000000.0,1301500000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,1,16000000,788306.6236983632,14296435.2,0.2272361,281600.0,741008226.2764614,264704000.0,"The BigCode project, an open-scientific collaboration focused on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder2. In partnership with Software Heritage (SWH), we build The Stack v2 on top of the digital commons of their source code archive. Alongside the SWH repositories spanning 619 programming languages, we carefully select other high-quality data sources, such as GitHub pull requests, Kaggle notebooks, and code documentation. This results in a training set that is 4x larger than the first StarCoder dataset. We train StarCoder2 models with 3B, 7B, and 15B parameters on 3.3 to 4.3 trillion tokens and thoroughly evaluate them on a comprehensive set of Code LLM benchmarks. We find that our small model, StarCoder2-3B, outperforms other Code LLMs of similar size on most benchmarks, and also outperforms StarCoderBase-15B. Our large model, StarCoder2- 15B, significantly outperforms other models of comparable size. In addition, it matches or outperforms CodeLlama-34B, a model more than twice its size. Although DeepSeekCoder- 33B is the best-performing model at code completion for high-resource languages, we find that StarCoder2-15B outperforms it on math and code reasoning benchmarks, as well as several low-resource languages. We make the model weights available under an OpenRAIL license and ensure full transparency regarding the training data by releasing the SoftWare Heritage persistent IDentifiers (SWHIDs) of the source code data. ",https://arxiv.org/abs/2402.19173,StarCoder 2 and The Stack v2: The Next Generation
StarCoder 2 15B,Language,Multinational,ServiceNow,2024-02-29,Industry,Code generation,Confident,512.0,Open weights (restricted use),The Stack v2,Unreleased,15000000000.0,913230000000.0,940.0,3.87e+23,1877.1621580193864,27922.725,550.0,1213583000000000.0,1301500000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,1,16000000,788306.6236983632,14296435.2,0.2272361,281600.0,741008226.2764614,264704000.0,"The BigCode project, an open-scientific collaboration focused on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder2. In partnership with Software Heritage (SWH), we build The Stack v2 on top of the digital commons of their source code archive. Alongside the SWH repositories spanning 619 programming languages, we carefully select other high-quality data sources, such as GitHub pull requests, Kaggle notebooks, and code documentation. This results in a training set that is 4x larger than the first StarCoder dataset. We train StarCoder2 models with 3B, 7B, and 15B parameters on 3.3 to 4.3 trillion tokens and thoroughly evaluate them on a comprehensive set of Code LLM benchmarks. We find that our small model, StarCoder2-3B, outperforms other Code LLMs of similar size on most benchmarks, and also outperforms StarCoderBase-15B. Our large model, StarCoder2- 15B, significantly outperforms other models of comparable size. In addition, it matches or outperforms CodeLlama-34B, a model more than twice its size. Although DeepSeekCoder- 33B is the best-performing model at code completion for high-resource languages, we find that StarCoder2-15B outperforms it on math and code reasoning benchmarks, as well as several low-resource languages. We make the model weights available under an OpenRAIL license and ensure full transparency regarding the training data by releasing the SoftWare Heritage persistent IDentifiers (SWHIDs) of the source code data. ",https://arxiv.org/abs/2402.19173,StarCoder 2 and The Stack v2: The Next Generation
StarCoder 2 15B,Language,USA,BigCode,2024-02-29,Industry,Code autocompletion,Confident,512.0,Open weights (restricted use),The Stack v2,Unreleased,15000000000.0,913230000000.0,940.0,3.87e+23,1877.1621580193864,27922.725,550.0,1213583000000000.0,1301500000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,1,16000000,788306.6236983632,14296435.2,0.2272361,281600.0,741008226.2764614,264704000.0,"The BigCode project, an open-scientific collaboration focused on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder2. In partnership with Software Heritage (SWH), we build The Stack v2 on top of the digital commons of their source code archive. Alongside the SWH repositories spanning 619 programming languages, we carefully select other high-quality data sources, such as GitHub pull requests, Kaggle notebooks, and code documentation. This results in a training set that is 4x larger than the first StarCoder dataset. We train StarCoder2 models with 3B, 7B, and 15B parameters on 3.3 to 4.3 trillion tokens and thoroughly evaluate them on a comprehensive set of Code LLM benchmarks. We find that our small model, StarCoder2-3B, outperforms other Code LLMs of similar size on most benchmarks, and also outperforms StarCoderBase-15B. Our large model, StarCoder2- 15B, significantly outperforms other models of comparable size. In addition, it matches or outperforms CodeLlama-34B, a model more than twice its size. Although DeepSeekCoder- 33B is the best-performing model at code completion for high-resource languages, we find that StarCoder2-15B outperforms it on math and code reasoning benchmarks, as well as several low-resource languages. We make the model weights available under an OpenRAIL license and ensure full transparency regarding the training data by releasing the SoftWare Heritage persistent IDentifiers (SWHIDs) of the source code data. ",https://arxiv.org/abs/2402.19173,StarCoder 2 and The Stack v2: The Next Generation
StarCoder 2 15B,Language,USA,BigCode,2024-02-29,Industry,Code generation,Confident,512.0,Open weights (restricted use),The Stack v2,Unreleased,15000000000.0,913230000000.0,940.0,3.87e+23,1877.1621580193864,27922.725,550.0,1213583000000000.0,1301500000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,1,16000000,788306.6236983632,14296435.2,0.2272361,281600.0,741008226.2764614,264704000.0,"The BigCode project, an open-scientific collaboration focused on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder2. In partnership with Software Heritage (SWH), we build The Stack v2 on top of the digital commons of their source code archive. Alongside the SWH repositories spanning 619 programming languages, we carefully select other high-quality data sources, such as GitHub pull requests, Kaggle notebooks, and code documentation. This results in a training set that is 4x larger than the first StarCoder dataset. We train StarCoder2 models with 3B, 7B, and 15B parameters on 3.3 to 4.3 trillion tokens and thoroughly evaluate them on a comprehensive set of Code LLM benchmarks. We find that our small model, StarCoder2-3B, outperforms other Code LLMs of similar size on most benchmarks, and also outperforms StarCoderBase-15B. Our large model, StarCoder2- 15B, significantly outperforms other models of comparable size. In addition, it matches or outperforms CodeLlama-34B, a model more than twice its size. Although DeepSeekCoder- 33B is the best-performing model at code completion for high-resource languages, we find that StarCoder2-15B outperforms it on math and code reasoning benchmarks, as well as several low-resource languages. We make the model weights available under an OpenRAIL license and ensure full transparency regarding the training data by releasing the SoftWare Heritage persistent IDentifiers (SWHIDs) of the source code data. ",https://arxiv.org/abs/2402.19173,StarCoder 2 and The Stack v2: The Next Generation
StarCoder 2 15B,Language,USA,Hugging Face,2024-02-29,Industry,Code autocompletion,Confident,512.0,Open weights (restricted use),The Stack v2,Unreleased,15000000000.0,913230000000.0,940.0,3.87e+23,1877.1621580193864,27922.725,550.0,1213583000000000.0,1301500000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,1,16000000,788306.6236983632,14296435.2,0.2272361,281600.0,741008226.2764614,264704000.0,"The BigCode project, an open-scientific collaboration focused on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder2. In partnership with Software Heritage (SWH), we build The Stack v2 on top of the digital commons of their source code archive. Alongside the SWH repositories spanning 619 programming languages, we carefully select other high-quality data sources, such as GitHub pull requests, Kaggle notebooks, and code documentation. This results in a training set that is 4x larger than the first StarCoder dataset. We train StarCoder2 models with 3B, 7B, and 15B parameters on 3.3 to 4.3 trillion tokens and thoroughly evaluate them on a comprehensive set of Code LLM benchmarks. We find that our small model, StarCoder2-3B, outperforms other Code LLMs of similar size on most benchmarks, and also outperforms StarCoderBase-15B. Our large model, StarCoder2- 15B, significantly outperforms other models of comparable size. In addition, it matches or outperforms CodeLlama-34B, a model more than twice its size. Although DeepSeekCoder- 33B is the best-performing model at code completion for high-resource languages, we find that StarCoder2-15B outperforms it on math and code reasoning benchmarks, as well as several low-resource languages. We make the model weights available under an OpenRAIL license and ensure full transparency regarding the training data by releasing the SoftWare Heritage persistent IDentifiers (SWHIDs) of the source code data. ",https://arxiv.org/abs/2402.19173,StarCoder 2 and The Stack v2: The Next Generation
StarCoder 2 15B,Language,USA,Hugging Face,2024-02-29,Industry,Code generation,Confident,512.0,Open weights (restricted use),The Stack v2,Unreleased,15000000000.0,913230000000.0,940.0,3.87e+23,1877.1621580193864,27922.725,550.0,1213583000000000.0,1301500000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,1,16000000,788306.6236983632,14296435.2,0.2272361,281600.0,741008226.2764614,264704000.0,"The BigCode project, an open-scientific collaboration focused on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder2. In partnership with Software Heritage (SWH), we build The Stack v2 on top of the digital commons of their source code archive. Alongside the SWH repositories spanning 619 programming languages, we carefully select other high-quality data sources, such as GitHub pull requests, Kaggle notebooks, and code documentation. This results in a training set that is 4x larger than the first StarCoder dataset. We train StarCoder2 models with 3B, 7B, and 15B parameters on 3.3 to 4.3 trillion tokens and thoroughly evaluate them on a comprehensive set of Code LLM benchmarks. We find that our small model, StarCoder2-3B, outperforms other Code LLMs of similar size on most benchmarks, and also outperforms StarCoderBase-15B. Our large model, StarCoder2- 15B, significantly outperforms other models of comparable size. In addition, it matches or outperforms CodeLlama-34B, a model more than twice its size. Although DeepSeekCoder- 33B is the best-performing model at code completion for high-resource languages, we find that StarCoder2-15B outperforms it on math and code reasoning benchmarks, as well as several low-resource languages. We make the model weights available under an OpenRAIL license and ensure full transparency regarding the training data by releasing the SoftWare Heritage persistent IDentifiers (SWHIDs) of the source code data. ",https://arxiv.org/abs/2402.19173,StarCoder 2 and The Stack v2: The Next Generation
StarCoder 2 15B,Language,USA,NVIDIA,2024-02-29,Industry,Code autocompletion,Confident,512.0,Open weights (restricted use),The Stack v2,Unreleased,15000000000.0,913230000000.0,940.0,3.87e+23,1877.1621580193864,27922.725,550.0,1213583000000000.0,1301500000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,1,16000000,788306.6236983632,14296435.2,0.2272361,281600.0,741008226.2764614,264704000.0,"The BigCode project, an open-scientific collaboration focused on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder2. In partnership with Software Heritage (SWH), we build The Stack v2 on top of the digital commons of their source code archive. Alongside the SWH repositories spanning 619 programming languages, we carefully select other high-quality data sources, such as GitHub pull requests, Kaggle notebooks, and code documentation. This results in a training set that is 4x larger than the first StarCoder dataset. We train StarCoder2 models with 3B, 7B, and 15B parameters on 3.3 to 4.3 trillion tokens and thoroughly evaluate them on a comprehensive set of Code LLM benchmarks. We find that our small model, StarCoder2-3B, outperforms other Code LLMs of similar size on most benchmarks, and also outperforms StarCoderBase-15B. Our large model, StarCoder2- 15B, significantly outperforms other models of comparable size. In addition, it matches or outperforms CodeLlama-34B, a model more than twice its size. Although DeepSeekCoder- 33B is the best-performing model at code completion for high-resource languages, we find that StarCoder2-15B outperforms it on math and code reasoning benchmarks, as well as several low-resource languages. We make the model weights available under an OpenRAIL license and ensure full transparency regarding the training data by releasing the SoftWare Heritage persistent IDentifiers (SWHIDs) of the source code data. ",https://arxiv.org/abs/2402.19173,StarCoder 2 and The Stack v2: The Next Generation
StarCoder 2 15B,Language,USA,NVIDIA,2024-02-29,Industry,Code generation,Confident,512.0,Open weights (restricted use),The Stack v2,Unreleased,15000000000.0,913230000000.0,940.0,3.87e+23,1877.1621580193864,27922.725,550.0,1213583000000000.0,1301500000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,1,16000000,788306.6236983632,14296435.2,0.2272361,281600.0,741008226.2764614,264704000.0,"The BigCode project, an open-scientific collaboration focused on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder2. In partnership with Software Heritage (SWH), we build The Stack v2 on top of the digital commons of their source code archive. Alongside the SWH repositories spanning 619 programming languages, we carefully select other high-quality data sources, such as GitHub pull requests, Kaggle notebooks, and code documentation. This results in a training set that is 4x larger than the first StarCoder dataset. We train StarCoder2 models with 3B, 7B, and 15B parameters on 3.3 to 4.3 trillion tokens and thoroughly evaluate them on a comprehensive set of Code LLM benchmarks. We find that our small model, StarCoder2-3B, outperforms other Code LLMs of similar size on most benchmarks, and also outperforms StarCoderBase-15B. Our large model, StarCoder2- 15B, significantly outperforms other models of comparable size. In addition, it matches or outperforms CodeLlama-34B, a model more than twice its size. Although DeepSeekCoder- 33B is the best-performing model at code completion for high-resource languages, we find that StarCoder2-15B outperforms it on math and code reasoning benchmarks, as well as several low-resource languages. We make the model weights available under an OpenRAIL license and ensure full transparency regarding the training data by releasing the SoftWare Heritage persistent IDentifiers (SWHIDs) of the source code data. ",https://arxiv.org/abs/2402.19173,StarCoder 2 and The Stack v2: The Next Generation
StarCoder 2 15B,Language,USA,ServiceNow,2024-02-29,Industry,Code autocompletion,Confident,512.0,Open weights (restricted use),The Stack v2,Unreleased,15000000000.0,913230000000.0,940.0,3.87e+23,1877.1621580193864,27922.725,550.0,1213583000000000.0,1301500000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,1,16000000,788306.6236983632,14296435.2,0.2272361,281600.0,741008226.2764614,264704000.0,"The BigCode project, an open-scientific collaboration focused on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder2. In partnership with Software Heritage (SWH), we build The Stack v2 on top of the digital commons of their source code archive. Alongside the SWH repositories spanning 619 programming languages, we carefully select other high-quality data sources, such as GitHub pull requests, Kaggle notebooks, and code documentation. This results in a training set that is 4x larger than the first StarCoder dataset. We train StarCoder2 models with 3B, 7B, and 15B parameters on 3.3 to 4.3 trillion tokens and thoroughly evaluate them on a comprehensive set of Code LLM benchmarks. We find that our small model, StarCoder2-3B, outperforms other Code LLMs of similar size on most benchmarks, and also outperforms StarCoderBase-15B. Our large model, StarCoder2- 15B, significantly outperforms other models of comparable size. In addition, it matches or outperforms CodeLlama-34B, a model more than twice its size. Although DeepSeekCoder- 33B is the best-performing model at code completion for high-resource languages, we find that StarCoder2-15B outperforms it on math and code reasoning benchmarks, as well as several low-resource languages. We make the model weights available under an OpenRAIL license and ensure full transparency regarding the training data by releasing the SoftWare Heritage persistent IDentifiers (SWHIDs) of the source code data. ",https://arxiv.org/abs/2402.19173,StarCoder 2 and The Stack v2: The Next Generation
StarCoder 2 15B,Language,USA,ServiceNow,2024-02-29,Industry,Code generation,Confident,512.0,Open weights (restricted use),The Stack v2,Unreleased,15000000000.0,913230000000.0,940.0,3.87e+23,1877.1621580193864,27922.725,550.0,1213583000000000.0,1301500000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,1,16000000,788306.6236983632,14296435.2,0.2272361,281600.0,741008226.2764614,264704000.0,"The BigCode project, an open-scientific collaboration focused on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder2. In partnership with Software Heritage (SWH), we build The Stack v2 on top of the digital commons of their source code archive. Alongside the SWH repositories spanning 619 programming languages, we carefully select other high-quality data sources, such as GitHub pull requests, Kaggle notebooks, and code documentation. This results in a training set that is 4x larger than the first StarCoder dataset. We train StarCoder2 models with 3B, 7B, and 15B parameters on 3.3 to 4.3 trillion tokens and thoroughly evaluate them on a comprehensive set of Code LLM benchmarks. We find that our small model, StarCoder2-3B, outperforms other Code LLMs of similar size on most benchmarks, and also outperforms StarCoderBase-15B. Our large model, StarCoder2- 15B, significantly outperforms other models of comparable size. In addition, it matches or outperforms CodeLlama-34B, a model more than twice its size. Although DeepSeekCoder- 33B is the best-performing model at code completion for high-resource languages, we find that StarCoder2-15B outperforms it on math and code reasoning benchmarks, as well as several low-resource languages. We make the model weights available under an OpenRAIL license and ensure full transparency regarding the training data by releasing the SoftWare Heritage persistent IDentifiers (SWHIDs) of the source code data. ",https://arxiv.org/abs/2402.19173,StarCoder 2 and The Stack v2: The Next Generation
Telechat2-115B,Language,China,China Telecom,2024-09-20,Industry,Language modeling,Unverified,1024.0,Open weights (restricted use),Not-defined,Not-defined,115000000000.0,10000000000000.0,347.4,6.899999999999999e+24,1877.1621580193864,6796.58,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,4000000,197336.5951439333,6959697.92,3.985507246376812e-10,196608.0,68554733.15300243,68301619.19999999,,https://huggingface.co/Tele-AI/TeleChat2-115B,TeleChat Technical Report
UL2,Language,Multinational,Google Brain,2022-05-10,Industry,Language modeling/generation,Confident,512.0,Open weights (unrestricted),C4,Not-defined,20000000000.0,1000000000000.0,744.0,1.2e+23,1877.1621580193864,6796.58,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,SOTA improvement,4,65536,218378.65933256023,3479848.96,0.29925187,98304.0,162473722.54342481,73138176.0,"Existing pre-trained models are generally geared towards a particular class of problems. To date, there seems to be still no consensus on what the right architecture and pre-training setup should be. This paper presents a unified framework for pre-training models that are universally effective across datasets and setups. We begin by disentangling architectural archetypes with pre-training objectives -- two concepts that are commonly conflated. Next, we present a generalized and unified perspective for self-supervision in NLP and show how different pre-training objectives can be cast as one another and how interpolating between different objectives can be effective. We then propose Mixture-of-Denoisers (MoD), a pre-training objective that combines diverse pre-training paradigms together. We furthermore introduce a notion of mode switching, wherein downstream fine-tuning is associated with specific pre-training schemes. We conduct extensive ablative experiments to compare multiple pre-training objectives and find that our method pushes the Pareto-frontier by outperforming T5 and/or GPT-like models across multiple diverse setups. Finally, by scaling our model up to 20B parameters, we achieve SOTA performance on 50 well-established supervised NLP tasks ranging from language generation (with automated and human evaluation), language understanding, text classification, question answering, commonsense reasoning, long text reasoning, structured knowledge grounding and information retrieval. Our model also achieve strong results at in-context learning, outperforming 175B GPT-3 on zero-shot SuperGLUE and tripling the performance of T5-XXL on one-shot summarization. We release Flax-based T5X model checkpoints for the 20B model at \url{this https URL}.",https://arxiv.org/abs/2205.05131v1,Unifying Language Learning Paradigms
UL2,Language,Multinational,Google Research,2022-05-10,Industry,Language modeling/generation,Confident,512.0,Open weights (unrestricted),C4,Not-defined,20000000000.0,1000000000000.0,744.0,1.2e+23,1877.1621580193864,6796.58,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,SOTA improvement,4,65536,218378.65933256023,3479848.96,0.29925187,98304.0,162473722.54342481,73138176.0,"Existing pre-trained models are generally geared towards a particular class of problems. To date, there seems to be still no consensus on what the right architecture and pre-training setup should be. This paper presents a unified framework for pre-training models that are universally effective across datasets and setups. We begin by disentangling architectural archetypes with pre-training objectives -- two concepts that are commonly conflated. Next, we present a generalized and unified perspective for self-supervision in NLP and show how different pre-training objectives can be cast as one another and how interpolating between different objectives can be effective. We then propose Mixture-of-Denoisers (MoD), a pre-training objective that combines diverse pre-training paradigms together. We furthermore introduce a notion of mode switching, wherein downstream fine-tuning is associated with specific pre-training schemes. We conduct extensive ablative experiments to compare multiple pre-training objectives and find that our method pushes the Pareto-frontier by outperforming T5 and/or GPT-like models across multiple diverse setups. Finally, by scaling our model up to 20B parameters, we achieve SOTA performance on 50 well-established supervised NLP tasks ranging from language generation (with automated and human evaluation), language understanding, text classification, question answering, commonsense reasoning, long text reasoning, structured knowledge grounding and information retrieval. Our model also achieve strong results at in-context learning, outperforming 175B GPT-3 on zero-shot SuperGLUE and tripling the performance of T5-XXL on one-shot summarization. We release Flax-based T5X model checkpoints for the 20B model at \url{this https URL}.",https://arxiv.org/abs/2205.05131v1,Unifying Language Learning Paradigms
UL2,Language,USA,Google Brain,2022-05-10,Industry,Language modeling/generation,Confident,512.0,Open weights (unrestricted),C4,Not-defined,20000000000.0,1000000000000.0,744.0,1.2e+23,1877.1621580193864,6796.58,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,SOTA improvement,4,65536,218378.65933256023,3479848.96,0.29925187,98304.0,162473722.54342481,73138176.0,"Existing pre-trained models are generally geared towards a particular class of problems. To date, there seems to be still no consensus on what the right architecture and pre-training setup should be. This paper presents a unified framework for pre-training models that are universally effective across datasets and setups. We begin by disentangling architectural archetypes with pre-training objectives -- two concepts that are commonly conflated. Next, we present a generalized and unified perspective for self-supervision in NLP and show how different pre-training objectives can be cast as one another and how interpolating between different objectives can be effective. We then propose Mixture-of-Denoisers (MoD), a pre-training objective that combines diverse pre-training paradigms together. We furthermore introduce a notion of mode switching, wherein downstream fine-tuning is associated with specific pre-training schemes. We conduct extensive ablative experiments to compare multiple pre-training objectives and find that our method pushes the Pareto-frontier by outperforming T5 and/or GPT-like models across multiple diverse setups. Finally, by scaling our model up to 20B parameters, we achieve SOTA performance on 50 well-established supervised NLP tasks ranging from language generation (with automated and human evaluation), language understanding, text classification, question answering, commonsense reasoning, long text reasoning, structured knowledge grounding and information retrieval. Our model also achieve strong results at in-context learning, outperforming 175B GPT-3 on zero-shot SuperGLUE and tripling the performance of T5-XXL on one-shot summarization. We release Flax-based T5X model checkpoints for the 20B model at \url{this https URL}.",https://arxiv.org/abs/2205.05131v1,Unifying Language Learning Paradigms
UL2,Language,USA,Google Research,2022-05-10,Industry,Language modeling/generation,Confident,512.0,Open weights (unrestricted),C4,Not-defined,20000000000.0,1000000000000.0,744.0,1.2e+23,1877.1621580193864,6796.58,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,SOTA improvement,4,65536,218378.65933256023,3479848.96,0.29925187,98304.0,162473722.54342481,73138176.0,"Existing pre-trained models are generally geared towards a particular class of problems. To date, there seems to be still no consensus on what the right architecture and pre-training setup should be. This paper presents a unified framework for pre-training models that are universally effective across datasets and setups. We begin by disentangling architectural archetypes with pre-training objectives -- two concepts that are commonly conflated. Next, we present a generalized and unified perspective for self-supervision in NLP and show how different pre-training objectives can be cast as one another and how interpolating between different objectives can be effective. We then propose Mixture-of-Denoisers (MoD), a pre-training objective that combines diverse pre-training paradigms together. We furthermore introduce a notion of mode switching, wherein downstream fine-tuning is associated with specific pre-training schemes. We conduct extensive ablative experiments to compare multiple pre-training objectives and find that our method pushes the Pareto-frontier by outperforming T5 and/or GPT-like models across multiple diverse setups. Finally, by scaling our model up to 20B parameters, we achieve SOTA performance on 50 well-established supervised NLP tasks ranging from language generation (with automated and human evaluation), language understanding, text classification, question answering, commonsense reasoning, long text reasoning, structured knowledge grounding and information retrieval. Our model also achieve strong results at in-context learning, outperforming 175B GPT-3 on zero-shot SuperGLUE and tripling the performance of T5-XXL on one-shot summarization. We release Flax-based T5X model checkpoints for the 20B model at \url{this https URL}.",https://arxiv.org/abs/2205.05131v1,Unifying Language Learning Paradigms
VLM-4,Language,France,LightOn,2022-04-12,Industry,Language modeling/generation,Unverified,768.0,API access,Not-defined,Unreleased,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,,https://web.archive.org/web/20220506095925/https://lighton.ai/blog/lighton-publicly-launches-muse-an-api-to-vlm-4-large-language-models-trained-natively-in-five-european-languages/,"LightOn publicly launches Muse, an API to VLM-4 Large Language Models trained natively in five European Languages"
VLM-4,Language,France,LightOn,2022-04-12,Industry,Text classification,Unverified,768.0,API access,Not-defined,Unreleased,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,,https://web.archive.org/web/20220506095925/https://lighton.ai/blog/lighton-publicly-launches-muse-an-api-to-vlm-4-large-language-models-trained-natively-in-five-european-languages/,"LightOn publicly launches Muse, an API to VLM-4 Large Language Models trained natively in five European Languages"
VLM-4,Language,France,LightOn,2022-04-12,Industry,Text summarization,Unverified,768.0,API access,Not-defined,Unreleased,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,,https://web.archive.org/web/20220506095925/https://lighton.ai/blog/lighton-publicly-launches-muse-an-api-to-vlm-4-large-language-models-trained-natively-in-five-european-languages/,"LightOn publicly launches Muse, an API to VLM-4 Large Language Models trained natively in five European Languages"
VLM-4,Language,France,LightOn,2022-04-12,Industry,Translation,Unverified,768.0,API access,Not-defined,Unreleased,9000000000.0,10000000000000.0,1440.0,3.6e+25,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,8650752,788306.6236983632,11520000.0,1.5977944444444446e-10,307200.0,1135161538.125643,442368000.0,,https://web.archive.org/web/20220506095925/https://lighton.ai/blog/lighton-publicly-launches-muse-an-api-to-vlm-4-large-language-models-trained-natively-in-five-european-languages/,"LightOn publicly launches Muse, an API to VLM-4 Large Language Models trained natively in five European Languages"
ViT-22B,Vision,USA,Google,2023-02-10,Industry,Image classification,Confident,1024.0,Unreleased,JFT-4B,Unreleased,21743000000.0,4000000000.0,347.4,1.93248e+23,1877.1621580193864,6796.58,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,SOTA improvement,2,2000000,435562.3547730654,6959697.92,0.549,196608.0,151314362.0481629,68301619.19999999,"The scaling of Transformers has driven breakthrough capabilities for language models. At present, the largest large language models (LLMs) contain upwards of 100B parameters. Vision Transformers (ViT) have introduced the same architecture to image and video modelling, but these have not yet been successfully scaled to nearly the same degree; the largest dense ViT contains 4B parameters (Chen et al., 2022). We present a recipe for highly efficient and stable training of a 22B-parameter ViT (ViT-22B) and perform a wide variety of experiments on the resulting model. When evaluated on downstream tasks (often with a lightweight linear model on frozen features), ViT-22B demonstrates increasing performance with scale. We further observe other interesting benefits of scale, including an improved tradeoff between fairness and performance, state-of-the-art alignment to human visual perception in terms of shape/texture bias, and improved robustness. ViT-22B demonstrates the potential for ""LLM-like"" scaling in vision, and provides key steps towards getting there.",https://arxiv.org/abs/2302.05442v1,Scaling Vision Transformers to 22 Billion Parameters
ViT-22B,Vision,USA,Google,2023-02-10,Industry,Object detection,Confident,1024.0,Unreleased,JFT-4B,Unreleased,21743000000.0,4000000000.0,347.4,1.93248e+23,1877.1621580193864,6796.58,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,SOTA improvement,2,2000000,435562.3547730654,6959697.92,0.549,196608.0,151314362.0481629,68301619.19999999,"The scaling of Transformers has driven breakthrough capabilities for language models. At present, the largest large language models (LLMs) contain upwards of 100B parameters. Vision Transformers (ViT) have introduced the same architecture to image and video modelling, but these have not yet been successfully scaled to nearly the same degree; the largest dense ViT contains 4B parameters (Chen et al., 2022). We present a recipe for highly efficient and stable training of a 22B-parameter ViT (ViT-22B) and perform a wide variety of experiments on the resulting model. When evaluated on downstream tasks (often with a lightweight linear model on frozen features), ViT-22B demonstrates increasing performance with scale. We further observe other interesting benefits of scale, including an improved tradeoff between fairness and performance, state-of-the-art alignment to human visual perception in terms of shape/texture bias, and improved robustness. ViT-22B demonstrates the potential for ""LLM-like"" scaling in vision, and provides key steps towards getting there.",https://arxiv.org/abs/2302.05442v1,Scaling Vision Transformers to 22 Billion Parameters
Viking,Language,Finland,Silo AI,2024-04-04,Academia,Language generation,Confident,1024.0,Open weights (unrestricted),Not-defined,Open source,33000000000.0,2000000000000.0,1282.4326530612243,2.574e+23,1877.1621580193864,14500.0,500.0,957440000000000.0,383000000000000.0,128000000000.0,AMD Radeon Instinct MI250X,None,4,6472188,91700208.69015627,14848000.0,3.71965811965812e-08,512000.0,117599341916.78503,656605518.3673469,,https://huggingface.co/LumiOpen/Viking-33B,"Viking 33B is a 33B parameter decoder-only transformer pretrained on Finnish, English, Swedish, Danish, Norwegian, Icelandic and code. It is being trained on 2 trillion tokens (700B billion as of this release). Viking 33B is a fully open source model and is made available under the Apache 2.0 License."
Viking,Language,Finland,Silo AI,2024-04-04,Academia,Language modeling/generation,Confident,1024.0,Open weights (unrestricted),Not-defined,Open source,33000000000.0,2000000000000.0,1282.4326530612243,2.574e+23,1877.1621580193864,14500.0,500.0,957440000000000.0,383000000000000.0,128000000000.0,AMD Radeon Instinct MI250X,None,4,6472188,91700208.69015627,14848000.0,3.71965811965812e-08,512000.0,117599341916.78503,656605518.3673469,,https://huggingface.co/LumiOpen/Viking-33B,"Viking 33B is a 33B parameter decoder-only transformer pretrained on Finnish, English, Swedish, Danish, Norwegian, Icelandic and code. It is being trained on 2 trillion tokens (700B billion as of this release). Viking 33B is a fully open source model and is made available under the Apache 2.0 License."
Viking,Language,Finland,Silo AI,2024-04-04,Academia,Translation,Confident,1024.0,Open weights (unrestricted),Not-defined,Open source,33000000000.0,2000000000000.0,1282.4326530612243,2.574e+23,1877.1621580193864,14500.0,500.0,957440000000000.0,383000000000000.0,128000000000.0,AMD Radeon Instinct MI250X,None,4,6472188,91700208.69015627,14848000.0,3.71965811965812e-08,512000.0,117599341916.78503,656605518.3673469,,https://huggingface.co/LumiOpen/Viking-33B,"Viking 33B is a 33B parameter decoder-only transformer pretrained on Finnish, English, Swedish, Danish, Norwegian, Icelandic and code. It is being trained on 2 trillion tokens (700B billion as of this release). Viking 33B is a fully open source model and is made available under the Apache 2.0 License."
Viking,Language,Finland,Silo AI,2024-04-04,Industry,Language generation,Confident,1024.0,Open weights (unrestricted),Not-defined,Open source,33000000000.0,2000000000000.0,1282.4326530612243,2.574e+23,1877.1621580193864,14500.0,500.0,957440000000000.0,383000000000000.0,128000000000.0,AMD Radeon Instinct MI250X,None,4,6472188,91700208.69015627,14848000.0,3.71965811965812e-08,512000.0,117599341916.78503,656605518.3673469,,https://huggingface.co/LumiOpen/Viking-33B,"Viking 33B is a 33B parameter decoder-only transformer pretrained on Finnish, English, Swedish, Danish, Norwegian, Icelandic and code. It is being trained on 2 trillion tokens (700B billion as of this release). Viking 33B is a fully open source model and is made available under the Apache 2.0 License."
Viking,Language,Finland,Silo AI,2024-04-04,Industry,Language modeling/generation,Confident,1024.0,Open weights (unrestricted),Not-defined,Open source,33000000000.0,2000000000000.0,1282.4326530612243,2.574e+23,1877.1621580193864,14500.0,500.0,957440000000000.0,383000000000000.0,128000000000.0,AMD Radeon Instinct MI250X,None,4,6472188,91700208.69015627,14848000.0,3.71965811965812e-08,512000.0,117599341916.78503,656605518.3673469,,https://huggingface.co/LumiOpen/Viking-33B,"Viking 33B is a 33B parameter decoder-only transformer pretrained on Finnish, English, Swedish, Danish, Norwegian, Icelandic and code. It is being trained on 2 trillion tokens (700B billion as of this release). Viking 33B is a fully open source model and is made available under the Apache 2.0 License."
Viking,Language,Finland,Silo AI,2024-04-04,Industry,Translation,Confident,1024.0,Open weights (unrestricted),Not-defined,Open source,33000000000.0,2000000000000.0,1282.4326530612243,2.574e+23,1877.1621580193864,14500.0,500.0,957440000000000.0,383000000000000.0,128000000000.0,AMD Radeon Instinct MI250X,None,4,6472188,91700208.69015627,14848000.0,3.71965811965812e-08,512000.0,117599341916.78503,656605518.3673469,,https://huggingface.co/LumiOpen/Viking-33B,"Viking 33B is a 33B parameter decoder-only transformer pretrained on Finnish, English, Swedish, Danish, Norwegian, Icelandic and code. It is being trained on 2 trillion tokens (700B billion as of this release). Viking 33B is a fully open source model and is made available under the Apache 2.0 License."
Viking,Language,Finland,University of Turku,2024-04-04,Academia,Language generation,Confident,1024.0,Open weights (unrestricted),Not-defined,Open source,33000000000.0,2000000000000.0,1282.4326530612243,2.574e+23,1877.1621580193864,14500.0,500.0,957440000000000.0,383000000000000.0,128000000000.0,AMD Radeon Instinct MI250X,None,4,6472188,91700208.69015627,14848000.0,3.71965811965812e-08,512000.0,117599341916.78503,656605518.3673469,,https://huggingface.co/LumiOpen/Viking-33B,"Viking 33B is a 33B parameter decoder-only transformer pretrained on Finnish, English, Swedish, Danish, Norwegian, Icelandic and code. It is being trained on 2 trillion tokens (700B billion as of this release). Viking 33B is a fully open source model and is made available under the Apache 2.0 License."
Viking,Language,Finland,University of Turku,2024-04-04,Academia,Language modeling/generation,Confident,1024.0,Open weights (unrestricted),Not-defined,Open source,33000000000.0,2000000000000.0,1282.4326530612243,2.574e+23,1877.1621580193864,14500.0,500.0,957440000000000.0,383000000000000.0,128000000000.0,AMD Radeon Instinct MI250X,None,4,6472188,91700208.69015627,14848000.0,3.71965811965812e-08,512000.0,117599341916.78503,656605518.3673469,,https://huggingface.co/LumiOpen/Viking-33B,"Viking 33B is a 33B parameter decoder-only transformer pretrained on Finnish, English, Swedish, Danish, Norwegian, Icelandic and code. It is being trained on 2 trillion tokens (700B billion as of this release). Viking 33B is a fully open source model and is made available under the Apache 2.0 License."
Viking,Language,Finland,University of Turku,2024-04-04,Academia,Translation,Confident,1024.0,Open weights (unrestricted),Not-defined,Open source,33000000000.0,2000000000000.0,1282.4326530612243,2.574e+23,1877.1621580193864,14500.0,500.0,957440000000000.0,383000000000000.0,128000000000.0,AMD Radeon Instinct MI250X,None,4,6472188,91700208.69015627,14848000.0,3.71965811965812e-08,512000.0,117599341916.78503,656605518.3673469,,https://huggingface.co/LumiOpen/Viking-33B,"Viking 33B is a 33B parameter decoder-only transformer pretrained on Finnish, English, Swedish, Danish, Norwegian, Icelandic and code. It is being trained on 2 trillion tokens (700B billion as of this release). Viking 33B is a fully open source model and is made available under the Apache 2.0 License."
Viking,Language,Finland,University of Turku,2024-04-04,Industry,Language generation,Confident,1024.0,Open weights (unrestricted),Not-defined,Open source,33000000000.0,2000000000000.0,1282.4326530612243,2.574e+23,1877.1621580193864,14500.0,500.0,957440000000000.0,383000000000000.0,128000000000.0,AMD Radeon Instinct MI250X,None,4,6472188,91700208.69015627,14848000.0,3.71965811965812e-08,512000.0,117599341916.78503,656605518.3673469,,https://huggingface.co/LumiOpen/Viking-33B,"Viking 33B is a 33B parameter decoder-only transformer pretrained on Finnish, English, Swedish, Danish, Norwegian, Icelandic and code. It is being trained on 2 trillion tokens (700B billion as of this release). Viking 33B is a fully open source model and is made available under the Apache 2.0 License."
Viking,Language,Finland,University of Turku,2024-04-04,Industry,Language modeling/generation,Confident,1024.0,Open weights (unrestricted),Not-defined,Open source,33000000000.0,2000000000000.0,1282.4326530612243,2.574e+23,1877.1621580193864,14500.0,500.0,957440000000000.0,383000000000000.0,128000000000.0,AMD Radeon Instinct MI250X,None,4,6472188,91700208.69015627,14848000.0,3.71965811965812e-08,512000.0,117599341916.78503,656605518.3673469,,https://huggingface.co/LumiOpen/Viking-33B,"Viking 33B is a 33B parameter decoder-only transformer pretrained on Finnish, English, Swedish, Danish, Norwegian, Icelandic and code. It is being trained on 2 trillion tokens (700B billion as of this release). Viking 33B is a fully open source model and is made available under the Apache 2.0 License."
Viking,Language,Finland,University of Turku,2024-04-04,Industry,Translation,Confident,1024.0,Open weights (unrestricted),Not-defined,Open source,33000000000.0,2000000000000.0,1282.4326530612243,2.574e+23,1877.1621580193864,14500.0,500.0,957440000000000.0,383000000000000.0,128000000000.0,AMD Radeon Instinct MI250X,None,4,6472188,91700208.69015627,14848000.0,3.71965811965812e-08,512000.0,117599341916.78503,656605518.3673469,,https://huggingface.co/LumiOpen/Viking-33B,"Viking 33B is a 33B parameter decoder-only transformer pretrained on Finnish, English, Swedish, Danish, Norwegian, Icelandic and code. It is being trained on 2 trillion tokens (700B billion as of this release). Viking 33B is a fully open source model and is made available under the Apache 2.0 License."
Wu Dao Aquila-33B,Language,China,Beijing Academy of Artificial Intelligence / BAAI,2023-06-10,Academia,Chat,Confident,128.0,Unreleased,Not-defined,Not-defined,33000000000.0,2000000000000.0,500.0,3.7800000000001e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,SOTA improvement,1,4000000,197336.5951439333,1293528.32,1.5216137566137164e-08,51200.0,98668297.57196665,25600000.0,"Who said all large-language models (LLMs) necessarily need to be large? In China’s case, LLMs are currently downsizing in their size and number of parameters. According to sources, this is because the country is now focusing on enabling Chinese startups and smaller entities to build their own generative AI applications. As part of this downscaling trend, in June the Beijing Academy of Artificial Intelligence (BAAI) introduced Wu Dao 3.0, a series of open-source LLMs.

Based on interviews with high-ranking, anonymous sources involved in the project, IEEE Spectrum can report that Wu Dao 3.0 builds on the academy’s work with Wu Dao 2.0, a sparse, multimodal generative AI model—as has been widely reported about version 2.0—with 1.75 trillion parameters. Although there is no single set of parameters for Wu Dao 3.0 (it’s a range of models with a variety of parameter counts) all are well below the 1.75 trillion high-water mark that version 2.0 set.","https://spectrum.ieee.org/china-chatgpt-wu-dao

https://huggingface.co/BAAI/Aquila-7B",
Wu Dao Aquila-33B,Language,China,Beijing Academy of Artificial Intelligence / BAAI,2023-06-10,Academia,Code generation,Confident,128.0,Unreleased,Not-defined,Not-defined,33000000000.0,2000000000000.0,500.0,3.7800000000001e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,SOTA improvement,1,4000000,197336.5951439333,1293528.32,1.5216137566137164e-08,51200.0,98668297.57196665,25600000.0,"Who said all large-language models (LLMs) necessarily need to be large? In China’s case, LLMs are currently downsizing in their size and number of parameters. According to sources, this is because the country is now focusing on enabling Chinese startups and smaller entities to build their own generative AI applications. As part of this downscaling trend, in June the Beijing Academy of Artificial Intelligence (BAAI) introduced Wu Dao 3.0, a series of open-source LLMs.

Based on interviews with high-ranking, anonymous sources involved in the project, IEEE Spectrum can report that Wu Dao 3.0 builds on the academy’s work with Wu Dao 2.0, a sparse, multimodal generative AI model—as has been widely reported about version 2.0—with 1.75 trillion parameters. Although there is no single set of parameters for Wu Dao 3.0 (it’s a range of models with a variety of parameter counts) all are well below the 1.75 trillion high-water mark that version 2.0 set.","https://spectrum.ieee.org/china-chatgpt-wu-dao

https://huggingface.co/BAAI/Aquila-7B",
Wu Dao Aquila-7B,Language,China,Beijing Academy of Artificial Intelligence / BAAI,2023-06-10,Academia,Chat,Confident,128.0,Open weights (restricted use),Not-defined,Open source,7000000000.0,2400000000000.0,500.0,1.01e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,SOTA improvement,1,4000000,197336.5951439333,1293528.32,5.6947524752475247e-08,51200.0,98668297.57196665,25600000.0,"Who said all large-language models (LLMs) necessarily need to be large? In China’s case, LLMs are currently downsizing in their size and number of parameters. According to sources, this is because the country is now focusing on enabling Chinese startups and smaller entities to build their own generative AI applications. As part of this downscaling trend, in June the Beijing Academy of Artificial Intelligence (BAAI) introduced Wu Dao 3.0, a series of open-source LLMs.

Based on interviews with high-ranking, anonymous sources involved in the project, IEEE Spectrum can report that Wu Dao 3.0 builds on the academy’s work with Wu Dao 2.0, a sparse, multimodal generative AI model—as has been widely reported about version 2.0—with 1.75 trillion parameters. Although there is no single set of parameters for Wu Dao 3.0 (it’s a range of models with a variety of parameter counts) all are well below the 1.75 trillion high-water mark that version 2.0 set.","https://spectrum.ieee.org/china-chatgpt-wu-dao

https://huggingface.co/BAAI/Aquila-7B

https://github.com/FlagAI-Open/FlagAI/blob/master/examples/Aquila/README_en.md",
Wu Dao Aquila-7B,Language,China,Beijing Academy of Artificial Intelligence / BAAI,2023-06-10,Academia,Code generation,Confident,128.0,Open weights (restricted use),Not-defined,Open source,7000000000.0,2400000000000.0,500.0,1.01e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,SOTA improvement,1,4000000,197336.5951439333,1293528.32,5.6947524752475247e-08,51200.0,98668297.57196665,25600000.0,"Who said all large-language models (LLMs) necessarily need to be large? In China’s case, LLMs are currently downsizing in their size and number of parameters. According to sources, this is because the country is now focusing on enabling Chinese startups and smaller entities to build their own generative AI applications. As part of this downscaling trend, in June the Beijing Academy of Artificial Intelligence (BAAI) introduced Wu Dao 3.0, a series of open-source LLMs.

Based on interviews with high-ranking, anonymous sources involved in the project, IEEE Spectrum can report that Wu Dao 3.0 builds on the academy’s work with Wu Dao 2.0, a sparse, multimodal generative AI model—as has been widely reported about version 2.0—with 1.75 trillion parameters. Although there is no single set of parameters for Wu Dao 3.0 (it’s a range of models with a variety of parameter counts) all are well below the 1.75 trillion high-water mark that version 2.0 set.","https://spectrum.ieee.org/china-chatgpt-wu-dao

https://huggingface.co/BAAI/Aquila-7B

https://github.com/FlagAI-Open/FlagAI/blob/master/examples/Aquila/README_en.md",
XVERSE-13B-2,Language,China,Shenzhen Yuanxiang Technology,2023-11-06,Industry,Language generation,Likely,1024.0,Open weights (restricted use),Not-defined,Open source,13000000000.0,3200000000000.0,4096.0,1.24800000000001e+24,1877.1621580193864,6796.58,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,16000000,197336.5951439333,6959697.92,2.2035256410256235e-09,196608.0,808290693.7095507,805306368.0,"XVERSE-13B is a multilingual large language model, independently developed by Shenzhen Yuanxiang Technology. Its key features are as follows:

Model Structure: XVERSE-13B uses the mainstream Decoder-only Transformer network structure, supports 8k context length, the longest one among models of the same size, which can meet the need of longer multi-round dialogues, knowledge question-answering, and summarization. This makes the model more versatile in application scenarios.
Training Data: The model has been thoroughly trained on a diversified and high-quality dataset consisting of 3.2 trillion of tokens, including more than 40 languages such as Chinese, English, Russian, and Spanish. The sampling ratio of different types of data is finely set, which makes the performance of Chinese and English excellent, and also takes into account the effect of other languages.
Tokenization: Based on the BPE (Byte-Pair Encoding) algorithm, a tokenizer with a vocabulary size of 100,534 has been trained using hundreds of gigabytes of language data. This tokenizer is capable of supporting multilingual without the need for additional vocabulary expansion.
Training Framework: Several key technologies have also been independently developed, including efficient operators, memory optimization, parallel scheduling strategies, overlap of data-computation-communication, and synergy between platforms and frameworks. These advancements enhance training efficiency and model stability. With these technologies, the peak computational power utilization rate on a thousand-card cluster can reach 58.5%, ranking at the forefront of the industry.",https://huggingface.co/xverse/XVERSE-13B,
XVERSE-13B-2,Language,China,Shenzhen Yuanxiang Technology,2023-11-06,Industry,Language modeling/generation,Likely,1024.0,Open weights (restricted use),Not-defined,Open source,13000000000.0,3200000000000.0,4096.0,1.24800000000001e+24,1877.1621580193864,6796.58,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,16000000,197336.5951439333,6959697.92,2.2035256410256235e-09,196608.0,808290693.7095507,805306368.0,"XVERSE-13B is a multilingual large language model, independently developed by Shenzhen Yuanxiang Technology. Its key features are as follows:

Model Structure: XVERSE-13B uses the mainstream Decoder-only Transformer network structure, supports 8k context length, the longest one among models of the same size, which can meet the need of longer multi-round dialogues, knowledge question-answering, and summarization. This makes the model more versatile in application scenarios.
Training Data: The model has been thoroughly trained on a diversified and high-quality dataset consisting of 3.2 trillion of tokens, including more than 40 languages such as Chinese, English, Russian, and Spanish. The sampling ratio of different types of data is finely set, which makes the performance of Chinese and English excellent, and also takes into account the effect of other languages.
Tokenization: Based on the BPE (Byte-Pair Encoding) algorithm, a tokenizer with a vocabulary size of 100,534 has been trained using hundreds of gigabytes of language data. This tokenizer is capable of supporting multilingual without the need for additional vocabulary expansion.
Training Framework: Several key technologies have also been independently developed, including efficient operators, memory optimization, parallel scheduling strategies, overlap of data-computation-communication, and synergy between platforms and frameworks. These advancements enhance training efficiency and model stability. With these technologies, the peak computational power utilization rate on a thousand-card cluster can reach 58.5%, ranking at the forefront of the industry.",https://huggingface.co/xverse/XVERSE-13B,
XVERSE-13B-2,Language,China,Shenzhen Yuanxiang Technology,2023-11-06,Industry,Question answering,Likely,1024.0,Open weights (restricted use),Not-defined,Open source,13000000000.0,3200000000000.0,4096.0,1.24800000000001e+24,1877.1621580193864,6796.58,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,16000000,197336.5951439333,6959697.92,2.2035256410256235e-09,196608.0,808290693.7095507,805306368.0,"XVERSE-13B is a multilingual large language model, independently developed by Shenzhen Yuanxiang Technology. Its key features are as follows:

Model Structure: XVERSE-13B uses the mainstream Decoder-only Transformer network structure, supports 8k context length, the longest one among models of the same size, which can meet the need of longer multi-round dialogues, knowledge question-answering, and summarization. This makes the model more versatile in application scenarios.
Training Data: The model has been thoroughly trained on a diversified and high-quality dataset consisting of 3.2 trillion of tokens, including more than 40 languages such as Chinese, English, Russian, and Spanish. The sampling ratio of different types of data is finely set, which makes the performance of Chinese and English excellent, and also takes into account the effect of other languages.
Tokenization: Based on the BPE (Byte-Pair Encoding) algorithm, a tokenizer with a vocabulary size of 100,534 has been trained using hundreds of gigabytes of language data. This tokenizer is capable of supporting multilingual without the need for additional vocabulary expansion.
Training Framework: Several key technologies have also been independently developed, including efficient operators, memory optimization, parallel scheduling strategies, overlap of data-computation-communication, and synergy between platforms and frameworks. These advancements enhance training efficiency and model stability. With these technologies, the peak computational power utilization rate on a thousand-card cluster can reach 58.5%, ranking at the forefront of the industry.",https://huggingface.co/xverse/XVERSE-13B,
XVERSE-13B-2,Language,China,Shenzhen Yuanxiang Technology,2023-11-06,Industry,Text summarization,Likely,1024.0,Open weights (restricted use),Not-defined,Open source,13000000000.0,3200000000000.0,4096.0,1.24800000000001e+24,1877.1621580193864,6796.58,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,16000000,197336.5951439333,6959697.92,2.2035256410256235e-09,196608.0,808290693.7095507,805306368.0,"XVERSE-13B is a multilingual large language model, independently developed by Shenzhen Yuanxiang Technology. Its key features are as follows:

Model Structure: XVERSE-13B uses the mainstream Decoder-only Transformer network structure, supports 8k context length, the longest one among models of the same size, which can meet the need of longer multi-round dialogues, knowledge question-answering, and summarization. This makes the model more versatile in application scenarios.
Training Data: The model has been thoroughly trained on a diversified and high-quality dataset consisting of 3.2 trillion of tokens, including more than 40 languages such as Chinese, English, Russian, and Spanish. The sampling ratio of different types of data is finely set, which makes the performance of Chinese and English excellent, and also takes into account the effect of other languages.
Tokenization: Based on the BPE (Byte-Pair Encoding) algorithm, a tokenizer with a vocabulary size of 100,534 has been trained using hundreds of gigabytes of language data. This tokenizer is capable of supporting multilingual without the need for additional vocabulary expansion.
Training Framework: Several key technologies have also been independently developed, including efficient operators, memory optimization, parallel scheduling strategies, overlap of data-computation-communication, and synergy between platforms and frameworks. These advancements enhance training efficiency and model stability. With these technologies, the peak computational power utilization rate on a thousand-card cluster can reach 58.5%, ranking at the forefront of the industry.",https://huggingface.co/xverse/XVERSE-13B,
XVERSE-13B-2,Language,China,Shenzhen Yuanxiang Technology,2023-11-06,Industry,Translation,Likely,1024.0,Open weights (restricted use),Not-defined,Open source,13000000000.0,3200000000000.0,4096.0,1.24800000000001e+24,1877.1621580193864,6796.58,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,16000000,197336.5951439333,6959697.92,2.2035256410256235e-09,196608.0,808290693.7095507,805306368.0,"XVERSE-13B is a multilingual large language model, independently developed by Shenzhen Yuanxiang Technology. Its key features are as follows:

Model Structure: XVERSE-13B uses the mainstream Decoder-only Transformer network structure, supports 8k context length, the longest one among models of the same size, which can meet the need of longer multi-round dialogues, knowledge question-answering, and summarization. This makes the model more versatile in application scenarios.
Training Data: The model has been thoroughly trained on a diversified and high-quality dataset consisting of 3.2 trillion of tokens, including more than 40 languages such as Chinese, English, Russian, and Spanish. The sampling ratio of different types of data is finely set, which makes the performance of Chinese and English excellent, and also takes into account the effect of other languages.
Tokenization: Based on the BPE (Byte-Pair Encoding) algorithm, a tokenizer with a vocabulary size of 100,534 has been trained using hundreds of gigabytes of language data. This tokenizer is capable of supporting multilingual without the need for additional vocabulary expansion.
Training Framework: Several key technologies have also been independently developed, including efficient operators, memory optimization, parallel scheduling strategies, overlap of data-computation-communication, and synergy between platforms and frameworks. These advancements enhance training efficiency and model stability. With these technologies, the peak computational power utilization rate on a thousand-card cluster can reach 58.5%, ranking at the forefront of the industry.",https://huggingface.co/xverse/XVERSE-13B,
XVERSE-13B-2,Language,China,XVERSE Technology,2023-11-06,Industry,Language generation,Likely,1024.0,Open weights (restricted use),Not-defined,Open source,13000000000.0,3200000000000.0,4096.0,1.24800000000001e+24,1877.1621580193864,6796.58,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,16000000,197336.5951439333,6959697.92,2.2035256410256235e-09,196608.0,808290693.7095507,805306368.0,"XVERSE-13B is a multilingual large language model, independently developed by Shenzhen Yuanxiang Technology. Its key features are as follows:

Model Structure: XVERSE-13B uses the mainstream Decoder-only Transformer network structure, supports 8k context length, the longest one among models of the same size, which can meet the need of longer multi-round dialogues, knowledge question-answering, and summarization. This makes the model more versatile in application scenarios.
Training Data: The model has been thoroughly trained on a diversified and high-quality dataset consisting of 3.2 trillion of tokens, including more than 40 languages such as Chinese, English, Russian, and Spanish. The sampling ratio of different types of data is finely set, which makes the performance of Chinese and English excellent, and also takes into account the effect of other languages.
Tokenization: Based on the BPE (Byte-Pair Encoding) algorithm, a tokenizer with a vocabulary size of 100,534 has been trained using hundreds of gigabytes of language data. This tokenizer is capable of supporting multilingual without the need for additional vocabulary expansion.
Training Framework: Several key technologies have also been independently developed, including efficient operators, memory optimization, parallel scheduling strategies, overlap of data-computation-communication, and synergy between platforms and frameworks. These advancements enhance training efficiency and model stability. With these technologies, the peak computational power utilization rate on a thousand-card cluster can reach 58.5%, ranking at the forefront of the industry.",https://huggingface.co/xverse/XVERSE-13B,
XVERSE-13B-2,Language,China,XVERSE Technology,2023-11-06,Industry,Language modeling/generation,Likely,1024.0,Open weights (restricted use),Not-defined,Open source,13000000000.0,3200000000000.0,4096.0,1.24800000000001e+24,1877.1621580193864,6796.58,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,16000000,197336.5951439333,6959697.92,2.2035256410256235e-09,196608.0,808290693.7095507,805306368.0,"XVERSE-13B is a multilingual large language model, independently developed by Shenzhen Yuanxiang Technology. Its key features are as follows:

Model Structure: XVERSE-13B uses the mainstream Decoder-only Transformer network structure, supports 8k context length, the longest one among models of the same size, which can meet the need of longer multi-round dialogues, knowledge question-answering, and summarization. This makes the model more versatile in application scenarios.
Training Data: The model has been thoroughly trained on a diversified and high-quality dataset consisting of 3.2 trillion of tokens, including more than 40 languages such as Chinese, English, Russian, and Spanish. The sampling ratio of different types of data is finely set, which makes the performance of Chinese and English excellent, and also takes into account the effect of other languages.
Tokenization: Based on the BPE (Byte-Pair Encoding) algorithm, a tokenizer with a vocabulary size of 100,534 has been trained using hundreds of gigabytes of language data. This tokenizer is capable of supporting multilingual without the need for additional vocabulary expansion.
Training Framework: Several key technologies have also been independently developed, including efficient operators, memory optimization, parallel scheduling strategies, overlap of data-computation-communication, and synergy between platforms and frameworks. These advancements enhance training efficiency and model stability. With these technologies, the peak computational power utilization rate on a thousand-card cluster can reach 58.5%, ranking at the forefront of the industry.",https://huggingface.co/xverse/XVERSE-13B,
XVERSE-13B-2,Language,China,XVERSE Technology,2023-11-06,Industry,Question answering,Likely,1024.0,Open weights (restricted use),Not-defined,Open source,13000000000.0,3200000000000.0,4096.0,1.24800000000001e+24,1877.1621580193864,6796.58,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,16000000,197336.5951439333,6959697.92,2.2035256410256235e-09,196608.0,808290693.7095507,805306368.0,"XVERSE-13B is a multilingual large language model, independently developed by Shenzhen Yuanxiang Technology. Its key features are as follows:

Model Structure: XVERSE-13B uses the mainstream Decoder-only Transformer network structure, supports 8k context length, the longest one among models of the same size, which can meet the need of longer multi-round dialogues, knowledge question-answering, and summarization. This makes the model more versatile in application scenarios.
Training Data: The model has been thoroughly trained on a diversified and high-quality dataset consisting of 3.2 trillion of tokens, including more than 40 languages such as Chinese, English, Russian, and Spanish. The sampling ratio of different types of data is finely set, which makes the performance of Chinese and English excellent, and also takes into account the effect of other languages.
Tokenization: Based on the BPE (Byte-Pair Encoding) algorithm, a tokenizer with a vocabulary size of 100,534 has been trained using hundreds of gigabytes of language data. This tokenizer is capable of supporting multilingual without the need for additional vocabulary expansion.
Training Framework: Several key technologies have also been independently developed, including efficient operators, memory optimization, parallel scheduling strategies, overlap of data-computation-communication, and synergy between platforms and frameworks. These advancements enhance training efficiency and model stability. With these technologies, the peak computational power utilization rate on a thousand-card cluster can reach 58.5%, ranking at the forefront of the industry.",https://huggingface.co/xverse/XVERSE-13B,
XVERSE-13B-2,Language,China,XVERSE Technology,2023-11-06,Industry,Text summarization,Likely,1024.0,Open weights (restricted use),Not-defined,Open source,13000000000.0,3200000000000.0,4096.0,1.24800000000001e+24,1877.1621580193864,6796.58,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,16000000,197336.5951439333,6959697.92,2.2035256410256235e-09,196608.0,808290693.7095507,805306368.0,"XVERSE-13B is a multilingual large language model, independently developed by Shenzhen Yuanxiang Technology. Its key features are as follows:

Model Structure: XVERSE-13B uses the mainstream Decoder-only Transformer network structure, supports 8k context length, the longest one among models of the same size, which can meet the need of longer multi-round dialogues, knowledge question-answering, and summarization. This makes the model more versatile in application scenarios.
Training Data: The model has been thoroughly trained on a diversified and high-quality dataset consisting of 3.2 trillion of tokens, including more than 40 languages such as Chinese, English, Russian, and Spanish. The sampling ratio of different types of data is finely set, which makes the performance of Chinese and English excellent, and also takes into account the effect of other languages.
Tokenization: Based on the BPE (Byte-Pair Encoding) algorithm, a tokenizer with a vocabulary size of 100,534 has been trained using hundreds of gigabytes of language data. This tokenizer is capable of supporting multilingual without the need for additional vocabulary expansion.
Training Framework: Several key technologies have also been independently developed, including efficient operators, memory optimization, parallel scheduling strategies, overlap of data-computation-communication, and synergy between platforms and frameworks. These advancements enhance training efficiency and model stability. With these technologies, the peak computational power utilization rate on a thousand-card cluster can reach 58.5%, ranking at the forefront of the industry.",https://huggingface.co/xverse/XVERSE-13B,
XVERSE-13B-2,Language,China,XVERSE Technology,2023-11-06,Industry,Translation,Likely,1024.0,Open weights (restricted use),Not-defined,Open source,13000000000.0,3200000000000.0,4096.0,1.24800000000001e+24,1877.1621580193864,6796.58,192.0,275000000000000.0,275000000000000.0,32000000000.0,Google TPU v4,Training cost,1,16000000,197336.5951439333,6959697.92,2.2035256410256235e-09,196608.0,808290693.7095507,805306368.0,"XVERSE-13B is a multilingual large language model, independently developed by Shenzhen Yuanxiang Technology. Its key features are as follows:

Model Structure: XVERSE-13B uses the mainstream Decoder-only Transformer network structure, supports 8k context length, the longest one among models of the same size, which can meet the need of longer multi-round dialogues, knowledge question-answering, and summarization. This makes the model more versatile in application scenarios.
Training Data: The model has been thoroughly trained on a diversified and high-quality dataset consisting of 3.2 trillion of tokens, including more than 40 languages such as Chinese, English, Russian, and Spanish. The sampling ratio of different types of data is finely set, which makes the performance of Chinese and English excellent, and also takes into account the effect of other languages.
Tokenization: Based on the BPE (Byte-Pair Encoding) algorithm, a tokenizer with a vocabulary size of 100,534 has been trained using hundreds of gigabytes of language data. This tokenizer is capable of supporting multilingual without the need for additional vocabulary expansion.
Training Framework: Several key technologies have also been independently developed, including efficient operators, memory optimization, parallel scheduling strategies, overlap of data-computation-communication, and synergy between platforms and frameworks. These advancements enhance training efficiency and model stability. With these technologies, the peak computational power utilization rate on a thousand-card cluster can reach 58.5%, ranking at the forefront of the industry.",https://huggingface.co/xverse/XVERSE-13B,
XVERSE-65B-2,Language,China,Shenzhen Yuanxiang Technology,2023-12-08,Industry,Chat,Confident,128.0,Open weights (restricted use),Not-defined,Open source,65000000000.0,3200000000000.0,4096.0,1.24800000000001e+24,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Training cost,1,4000000,197336.5951439333,1293528.32,4.608733974358938e-09,51200.0,808290693.7095507,209715200.0,,https://github.com/xverse-ai/XVERSE-65B/blob/main/README_EN.md,
XVERSE-65B-2,Language,China,Shenzhen Yuanxiang Technology,2023-12-08,Industry,Language modeling/generation,Confident,128.0,Open weights (restricted use),Not-defined,Open source,65000000000.0,3200000000000.0,4096.0,1.24800000000001e+24,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Training cost,1,4000000,197336.5951439333,1293528.32,4.608733974358938e-09,51200.0,808290693.7095507,209715200.0,,https://github.com/xverse-ai/XVERSE-65B/blob/main/README_EN.md,
XVERSE-65B-2,Language,China,XVERSE Technology,2023-12-08,Industry,Chat,Confident,128.0,Open weights (restricted use),Not-defined,Open source,65000000000.0,3200000000000.0,4096.0,1.24800000000001e+24,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Training cost,1,4000000,197336.5951439333,1293528.32,4.608733974358938e-09,51200.0,808290693.7095507,209715200.0,,https://github.com/xverse-ai/XVERSE-65B/blob/main/README_EN.md,
XVERSE-65B-2,Language,China,XVERSE Technology,2023-12-08,Industry,Language modeling/generation,Confident,128.0,Open weights (restricted use),Not-defined,Open source,65000000000.0,3200000000000.0,4096.0,1.24800000000001e+24,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Training cost,1,4000000,197336.5951439333,1293528.32,4.608733974358938e-09,51200.0,808290693.7095507,209715200.0,,https://github.com/xverse-ai/XVERSE-65B/blob/main/README_EN.md,
YaLM,Language,Russia,Yandex,2022-06-23,Industry,Chat,Likely,800.0,Open weights (unrestricted),"The Pile,YaLM Russian Dataset",Unreleased,100000000000.0,300000000000.0,1560.0,2.2e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,SOTA improvement,1,2000000,788306.6236983632,8084552.0,2.6144090909090907e-08,320000.0,1229758332.9694467,499200000.0,,https://medium.com/yandex/yandex-publishes-yalm-100b-its-the-largest-gpt-like-neural-network-in-open-source-d1df53d0e9a6,Yandex Publishes YaLM 100B. It’s the Largest GPT-Like Neural Network in Open Source
YaLM,Language,Russia,Yandex,2022-06-23,Industry,Language modeling,Likely,800.0,Open weights (unrestricted),"The Pile,YaLM Russian Dataset",Unreleased,100000000000.0,300000000000.0,1560.0,2.2e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,SOTA improvement,1,2000000,788306.6236983632,8084552.0,2.6144090909090907e-08,320000.0,1229758332.9694467,499200000.0,,https://medium.com/yandex/yandex-publishes-yalm-100b-its-the-largest-gpt-like-neural-network-in-open-source-d1df53d0e9a6,Yandex Publishes YaLM 100B. It’s the Largest GPT-Like Neural Network in Open Source
YaYi 2.0,Not-defined,China,Yayi (Wenge),2023-12-22,Industry,Not-defined,Likely,1000.0,Not-defined,Not-defined,Not-defined,30000000000.0,2650000000000.0,1282.4326530612243,4.77e+23,1877.1621580193864,7406.0,303.0,107206000000000.0,303958000000000.0,54246185353.93311,NVIDIA A800,None,4,6472188,91700208.69015627,7406000.0,2.247505241090147e-09,303000.0,117599341916.78503,388577093.87755096,"In this technical report, we propose YAYI 2, including both base and chat models, with 30 billion parameters. YAYI 2 is pre-trained from scratch on a multilingual corpus which contains 2.65 trillion tokens filtered by our pre-training data processing pipeline.",https://arxiv.org/abs/2312.14862v1,"YAYI 2: Multilingual Open-Source Large Language Models
"
Yi 6B,Language,China,01.AI,2023-11-02,Industry,Chat,Confident,128.0,Open weights (restricted use),Unspecified unreleased,Unreleased,6000000000.0,3100000000000.0,720.0,1.26e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Training cost,1,4000000,788306.6236983632,1293528.32,4.5648412698412704e-08,51200.0,567580769.0628215,36864000.0,The Yi series models are large language models trained from scratch by developers at 01.AI.,https://arxiv.org/abs/2403.04652,Yi: Open Foundation Models by 01.AI
Yi 6B,Language,China,01.AI,2023-11-02,Industry,Code generation,Confident,128.0,Open weights (restricted use),Unspecified unreleased,Unreleased,6000000000.0,3100000000000.0,720.0,1.26e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Training cost,1,4000000,788306.6236983632,1293528.32,4.5648412698412704e-08,51200.0,567580769.0628215,36864000.0,The Yi series models are large language models trained from scratch by developers at 01.AI.,https://arxiv.org/abs/2403.04652,Yi: Open Foundation Models by 01.AI
Yi 6B,Language,China,01.AI,2023-11-02,Industry,Language modeling/generation,Confident,128.0,Open weights (restricted use),Unspecified unreleased,Unreleased,6000000000.0,3100000000000.0,720.0,1.26e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Training cost,1,4000000,788306.6236983632,1293528.32,4.5648412698412704e-08,51200.0,567580769.0628215,36864000.0,The Yi series models are large language models trained from scratch by developers at 01.AI.,https://arxiv.org/abs/2403.04652,Yi: Open Foundation Models by 01.AI
Yi 6B,Language,China,01.AI,2023-11-02,Industry,Translation,Confident,128.0,Open weights (restricted use),Unspecified unreleased,Unreleased,6000000000.0,3100000000000.0,720.0,1.26e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Training cost,1,4000000,788306.6236983632,1293528.32,4.5648412698412704e-08,51200.0,567580769.0628215,36864000.0,The Yi series models are large language models trained from scratch by developers at 01.AI.,https://arxiv.org/abs/2403.04652,Yi: Open Foundation Models by 01.AI
Yi-1.5-34B,Language,China,01.AI,2024-05-13,Industry,Chat,Confident,128.0,Open weights (restricted use),Unspecified unreleased,Unreleased,34000000000.0,3600000000000.0,720.0,7.344e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Training cost,1,4000000,788306.6236983632,1293528.32,7.83183551198257e-09,51200.0,567580769.0628215,36864000.0,"Yi-1.5 is an upgraded version of Yi. It is continuously pre-trained on Yi with a high-quality corpus of 500B tokens and fine-tuned on 3M diverse fine-tuning samples.

Compared with Yi, Yi-1.5 delivers stronger performance in coding, math, reasoning, and instruction-following capability, while still maintaining excellent capabilities in language understanding, commonsense reasoning, and reading comprehension.

Yi-1.5 comes in 3 model sizes: 34B, 9B, and 6B. For model details and benchmarks, see Model Card.",https://huggingface.co/01-ai/Yi-1.5-34B,"Yi-1.5 is an upgraded version of Yi, delivering stronger performance in coding, math, reasoning, and instruction-following capability."
Yi-1.5-34B,Language,China,01.AI,2024-05-13,Industry,Code generation,Confident,128.0,Open weights (restricted use),Unspecified unreleased,Unreleased,34000000000.0,3600000000000.0,720.0,7.344e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Training cost,1,4000000,788306.6236983632,1293528.32,7.83183551198257e-09,51200.0,567580769.0628215,36864000.0,"Yi-1.5 is an upgraded version of Yi. It is continuously pre-trained on Yi with a high-quality corpus of 500B tokens and fine-tuned on 3M diverse fine-tuning samples.

Compared with Yi, Yi-1.5 delivers stronger performance in coding, math, reasoning, and instruction-following capability, while still maintaining excellent capabilities in language understanding, commonsense reasoning, and reading comprehension.

Yi-1.5 comes in 3 model sizes: 34B, 9B, and 6B. For model details and benchmarks, see Model Card.",https://huggingface.co/01-ai/Yi-1.5-34B,"Yi-1.5 is an upgraded version of Yi, delivering stronger performance in coding, math, reasoning, and instruction-following capability."
Yi-1.5-34B,Language,China,01.AI,2024-05-13,Industry,Language modeling/generation,Confident,128.0,Open weights (restricted use),Unspecified unreleased,Unreleased,34000000000.0,3600000000000.0,720.0,7.344e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Training cost,1,4000000,788306.6236983632,1293528.32,7.83183551198257e-09,51200.0,567580769.0628215,36864000.0,"Yi-1.5 is an upgraded version of Yi. It is continuously pre-trained on Yi with a high-quality corpus of 500B tokens and fine-tuned on 3M diverse fine-tuning samples.

Compared with Yi, Yi-1.5 delivers stronger performance in coding, math, reasoning, and instruction-following capability, while still maintaining excellent capabilities in language understanding, commonsense reasoning, and reading comprehension.

Yi-1.5 comes in 3 model sizes: 34B, 9B, and 6B. For model details and benchmarks, see Model Card.",https://huggingface.co/01-ai/Yi-1.5-34B,"Yi-1.5 is an upgraded version of Yi, delivering stronger performance in coding, math, reasoning, and instruction-following capability."
Yi-1.5-34B,Language,China,01.AI,2024-05-13,Industry,Translation,Confident,128.0,Open weights (restricted use),Unspecified unreleased,Unreleased,34000000000.0,3600000000000.0,720.0,7.344e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Training cost,1,4000000,788306.6236983632,1293528.32,7.83183551198257e-09,51200.0,567580769.0628215,36864000.0,"Yi-1.5 is an upgraded version of Yi. It is continuously pre-trained on Yi with a high-quality corpus of 500B tokens and fine-tuned on 3M diverse fine-tuning samples.

Compared with Yi, Yi-1.5 delivers stronger performance in coding, math, reasoning, and instruction-following capability, while still maintaining excellent capabilities in language understanding, commonsense reasoning, and reading comprehension.

Yi-1.5 comes in 3 model sizes: 34B, 9B, and 6B. For model details and benchmarks, see Model Card.",https://huggingface.co/01-ai/Yi-1.5-34B,"Yi-1.5 is an upgraded version of Yi, delivering stronger performance in coding, math, reasoning, and instruction-following capability."
Yi-34B,Language,China,01.AI,2023-11-02,Industry,Chat,Confident,128.0,Open weights (restricted use),Unspecified unreleased,Unreleased,34000000000.0,3100000000000.0,347.4,6.1e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Significant use,1,4000000,113138.0969599182,1293528.32,9.429016393442623e-09,51200.0,39304174.88387558,17786880.0,The Yi series models are large language models trained from scratch by developers at 01.AI.,https://arxiv.org/abs/2403.04652,Yi: Open Foundation Models by 01.AI
Yi-34B,Language,China,01.AI,2023-11-02,Industry,Code generation,Confident,128.0,Open weights (restricted use),Unspecified unreleased,Unreleased,34000000000.0,3100000000000.0,347.4,6.1e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Significant use,1,4000000,113138.0969599182,1293528.32,9.429016393442623e-09,51200.0,39304174.88387558,17786880.0,The Yi series models are large language models trained from scratch by developers at 01.AI.,https://arxiv.org/abs/2403.04652,Yi: Open Foundation Models by 01.AI
Yi-34B,Language,China,01.AI,2023-11-02,Industry,Language modeling/generation,Confident,128.0,Open weights (restricted use),Unspecified unreleased,Unreleased,34000000000.0,3100000000000.0,347.4,6.1e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Significant use,1,4000000,113138.0969599182,1293528.32,9.429016393442623e-09,51200.0,39304174.88387558,17786880.0,The Yi series models are large language models trained from scratch by developers at 01.AI.,https://arxiv.org/abs/2403.04652,Yi: Open Foundation Models by 01.AI
Yi-34B,Language,China,01.AI,2023-11-02,Industry,Translation,Confident,128.0,Open weights (restricted use),Unspecified unreleased,Unreleased,34000000000.0,3100000000000.0,347.4,6.1e+23,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA A100,Significant use,1,4000000,113138.0969599182,1293528.32,9.429016393442623e-09,51200.0,39304174.88387558,17786880.0,The Yi series models are large language models trained from scratch by developers at 01.AI.,https://arxiv.org/abs/2403.04652,Yi: Open Foundation Models by 01.AI
Yi-Large,Language,China,01.AI,2024-05-13,Industry,Chat,Speculative,2000.0,API access,Not-defined,Unreleased,100000000000.0,3000000000000.0,720.0,1.8e+24,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA H100 SXM5 80GB,Training cost,1,4000000,788306.6236983632,20211380.0,3.195388888888889e-09,800000.0,567580769.0628215,576000000.0,,,
Yi-Large,Language,China,01.AI,2024-05-13,Industry,Language modeling/generation,Speculative,2000.0,API access,Not-defined,Unreleased,100000000000.0,3000000000000.0,720.0,1.8e+24,1877.1621580193864,10105.69,400.0,575170000000000.0,624000000000000.0,40000000000.0,NVIDIA H100 SXM5 80GB,Training cost,1,4000000,788306.6236983632,20211380.0,3.195388888888889e-09,800000.0,567580769.0628215,576000000.0,,,
Yi-Lightning,Language,China,01.AI,2024-10-18,Industry,Language modeling/generation,Confident,2000.0,API access,Unspecified unreleased,Unreleased,100000000000.0,3000000000000.0,720.0,1.5e+24,1877.1621580193864,44489.88,700.0,1851960000000000.0,1979000000000000.0,80000000000.0,NVIDIA H100 SXM5 80GB,Training cost,1,4000000,3083497.308288588,88979760.0,1.23464e-08,1400000.0,2220118061.9677835,1008000000.0,,https://www.lingyiwanwu.com/en https://platform.lingyiwanwu.com/,Yi-Lightning
Yuan 1.0,Language,China,Inspur,2021-10-12,Industry,Language modeling,Confident,2128.0,API access,"Common Crawl,Wikipedia,Sogue News",Unreleased,245730000000.0,1000000000000.0,1282.4326530612243,3.5380000000001e+23,1877.1621580193864,17021.255373450655,365.5062477689985,724138441571930.2,835942301719094.5,51738659949.274826,Not-defined,SOTA improvement,4,6881280,788306.6236983632,36221231.43470299,0.45,777797.2952524287,1010950154.8552281,997472648.8944166,"Recent work like GPT-3 has demonstrated excellent performance of Zero-Shot and Few-Shot learning on many natural language processing (NLP) tasks by scaling up model size, dataset size and the amount of computation. However, training a model like GPT-3 requires huge amount of computational resources which makes it challengeable to researchers. In this work, we propose a method that incorporates large-scale distributed training performance into model architecture design. With this method, Yuan 1.0, the current largest singleton language model with 245B parameters, achieves excellent performance on thousands GPUs during training, and the state-of-the-art results on NLP tasks. A data processing method is designed to efficiently filter massive amount of raw data. The current largest high-quality Chinese corpus with 5TB high quality texts is built based on this method. In addition, a calibration and label expansion method is proposed to improve the Zero-Shot and Few-Shot performance, and steady improvement is observed on the accuracy of various tasks. Yuan 1.0 presents strong capacity of natural language generation, and the generated articles are difficult to distinguish from the human-written ones.",https://arxiv.org/abs/2110.04725,Yuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning
Yuan 2.0,Language,China,Inspur,2023-11-27,Industry,Code generation,Confident,2128.0,Open weights (restricted use),Not-defined,Not-defined,102600000000.0,288000000000.0,1282.4326530612243,1.78e+23,1877.1621580193864,17021.255373450655,365.5062477689985,724138441571930.2,835942301719094.5,51738659949.274826,Not-defined,Training cost,4,6881280,788306.6236983632,36221231.43470299,0.45,777797.2952524287,1010950154.8552281,997472648.8944166,"In this work, the Localized Filtering-based Attention (LFA) is introduced to incorporate prior knowledge of local dependencies of natural language into Attention. Based on LFA, we develop and release Yuan 2.0, a large language model with parameters ranging from 2.1 billion to 102.6 billion. A data filtering and generation method is presented to build pretraining and fine-tuning dataset in high quality. A distributed training method with non-uniform pipeline parallel, data parallel, and optimizer parallel is proposed, which greatly reduces the bandwidth requirements of intra-node communication, and achieves good performance in large-scale distributed training. Yuan 2.0 models display impressive ability in code generation, math problem-solving, and chat compared with existing models. The latest version of YUAN 2.0, including model weights and source code, is accessible at Github.",https://arxiv.org/abs/2311.15786v1,YUAN 2.0: A Large Language Model with Localized Filtering-based Attention
Yuan 2.0,Language,China,Inspur,2023-11-27,Industry,Language modeling/generation,Confident,2128.0,Open weights (restricted use),Not-defined,Not-defined,102600000000.0,288000000000.0,1282.4326530612243,1.78e+23,1877.1621580193864,17021.255373450655,365.5062477689985,724138441571930.2,835942301719094.5,51738659949.274826,Not-defined,Training cost,4,6881280,788306.6236983632,36221231.43470299,0.45,777797.2952524287,1010950154.8552281,997472648.8944166,"In this work, the Localized Filtering-based Attention (LFA) is introduced to incorporate prior knowledge of local dependencies of natural language into Attention. Based on LFA, we develop and release Yuan 2.0, a large language model with parameters ranging from 2.1 billion to 102.6 billion. A data filtering and generation method is presented to build pretraining and fine-tuning dataset in high quality. A distributed training method with non-uniform pipeline parallel, data parallel, and optimizer parallel is proposed, which greatly reduces the bandwidth requirements of intra-node communication, and achieves good performance in large-scale distributed training. Yuan 2.0 models display impressive ability in code generation, math problem-solving, and chat compared with existing models. The latest version of YUAN 2.0, including model weights and source code, is accessible at Github.",https://arxiv.org/abs/2311.15786v1,YUAN 2.0: A Large Language Model with Localized Filtering-based Attention
Yuan 2.0,Language,China,Inspur,2023-11-27,Industry,Translation,Confident,2128.0,Open weights (restricted use),Not-defined,Not-defined,102600000000.0,288000000000.0,1282.4326530612243,1.78e+23,1877.1621580193864,17021.255373450655,365.5062477689985,724138441571930.2,835942301719094.5,51738659949.274826,Not-defined,Training cost,4,6881280,788306.6236983632,36221231.43470299,0.45,777797.2952524287,1010950154.8552281,997472648.8944166,"In this work, the Localized Filtering-based Attention (LFA) is introduced to incorporate prior knowledge of local dependencies of natural language into Attention. Based on LFA, we develop and release Yuan 2.0, a large language model with parameters ranging from 2.1 billion to 102.6 billion. A data filtering and generation method is presented to build pretraining and fine-tuning dataset in high quality. A distributed training method with non-uniform pipeline parallel, data parallel, and optimizer parallel is proposed, which greatly reduces the bandwidth requirements of intra-node communication, and achieves good performance in large-scale distributed training. Yuan 2.0 models display impressive ability in code generation, math problem-solving, and chat compared with existing models. The latest version of YUAN 2.0, including model weights and source code, is accessible at Github.",https://arxiv.org/abs/2311.15786v1,YUAN 2.0: A Large Language Model with Localized Filtering-based Attention
phi-3-medium 14B,Language,USA,Microsoft,2024-04-23,Industry,Chat,Likely,1920.0,Open weights (unrestricted),Phi-3 Dataset,Unreleased,14000000000.0,4800000000000.0,504.0,4.032e+23,1877.1621580193864,29744.94,550.0,1213583000000000.0,1301500000000000.0,60000000000.0,NVIDIA H100 SXM5 80GB,SOTA improvement,1,4000000,904992.9349364166,57110284.8,3.0098784722222227e-08,1056000.0,456116439.20795393,532224000.0,"We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone. The innovation lies entirely in our dataset for training, a scaled-up version of the one used for phi-2, composed of heavily filtered web data and synthetic data. The model is also further aligned for robustness, safety, and chat format. We also provide some initial parameter-scaling results with a 7B and 14B models trained for 4.8T tokens, called phi-3-small and phi-3-medium, both significantly more capable than phi-3-mini (e.g., respectively 75% and 78% on MMLU, and 8.7 and 8.9 on MT-bench).",https://arxiv.org/abs/2404.14219,Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone
phi-3-medium 14B,Language,USA,Microsoft,2024-04-23,Industry,Language modeling/generation,Likely,1920.0,Open weights (unrestricted),Phi-3 Dataset,Unreleased,14000000000.0,4800000000000.0,504.0,4.032e+23,1877.1621580193864,29744.94,550.0,1213583000000000.0,1301500000000000.0,60000000000.0,NVIDIA H100 SXM5 80GB,SOTA improvement,1,4000000,904992.9349364166,57110284.8,3.0098784722222227e-08,1056000.0,456116439.20795393,532224000.0,"We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone. The innovation lies entirely in our dataset for training, a scaled-up version of the one used for phi-2, composed of heavily filtered web data and synthetic data. The model is also further aligned for robustness, safety, and chat format. We also provide some initial parameter-scaling results with a 7B and 14B models trained for 4.8T tokens, called phi-3-small and phi-3-medium, both significantly more capable than phi-3-mini (e.g., respectively 75% and 78% on MMLU, and 8.7 and 8.9 on MT-bench).",https://arxiv.org/abs/2404.14219,Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone
phi-3-small 7.4B,Language,USA,Microsoft,2024-04-23,Industry,Chat,Confident,512.0,Not-defined,Not-defined,Not-defined,7400000000.0,4800000000000.0,552.0,2.1312e+23,1877.1621580193864,29744.94,550.0,1213583000000000.0,1301500000000000.0,60000000000.0,NVIDIA H100 SXM5 80GB,None,4,6472188,904992.9349364166,15229409.28,5.694364677177177e-08,281600.0,499556100.0849019,155443200.0,"We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone. The innovation lies entirely in our dataset for training, a scaled-up version of the one used for phi-2, composed of heavily filtered web data and synthetic data. The model is also further aligned for robustness, safety, and chat format. We also provide some initial parameter-scaling results with a 7B and 14B models trained for 4.8T tokens, called phi-3-small and phi-3-medium, both significantly more capable than phi-3-mini (e.g., respectively 75% and 78% on MMLU, and 8.7 and 8.9 on MT-bench).",https://arxiv.org/abs/2404.14219,Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone
phi-3-small 7.4B,Language,USA,Microsoft,2024-04-23,Industry,Language modeling/generation,Confident,512.0,Not-defined,Not-defined,Not-defined,7400000000.0,4800000000000.0,552.0,2.1312e+23,1877.1621580193864,29744.94,550.0,1213583000000000.0,1301500000000000.0,60000000000.0,NVIDIA H100 SXM5 80GB,None,4,6472188,904992.9349364166,15229409.28,5.694364677177177e-08,281600.0,499556100.0849019,155443200.0,"We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone. The innovation lies entirely in our dataset for training, a scaled-up version of the one used for phi-2, composed of heavily filtered web data and synthetic data. The model is also further aligned for robustness, safety, and chat format. We also provide some initial parameter-scaling results with a 7B and 14B models trained for 4.8T tokens, called phi-3-small and phi-3-medium, both significantly more capable than phi-3-mini (e.g., respectively 75% and 78% on MMLU, and 8.7 and 8.9 on MT-bench).",https://arxiv.org/abs/2404.14219,Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone
ruGPT-3.5 13B,Language,Russia,Sber,2023-04-24,Government,Chat,Confident,512.0,Open weights (unrestricted),Not-defined,Unreleased,13000000000.0,300000000000.0,1080.0,1.0699776e+23,1877.1621580193864,10802.845,350.0,377485000000000.0,429328500000000.0,28000000000.0,"NVIDIA A100,NVIDIA Tesla V100 SXM2",Training cost,1,4000000,788306.6236983632,5531056.64,3.527971052851948e-08,179200.0,851371153.5942322,193536000.0,,https://huggingface.co/ai-forever/ruGPT-3.5-13B,ruGPT-3.5 13B
ruGPT-3.5 13B,Language,Russia,Sber,2023-04-24,Government,Language modeling/generation,Confident,512.0,Open weights (unrestricted),Not-defined,Unreleased,13000000000.0,300000000000.0,1080.0,1.0699776e+23,1877.1621580193864,10802.845,350.0,377485000000000.0,429328500000000.0,28000000000.0,"NVIDIA A100,NVIDIA Tesla V100 SXM2",Training cost,1,4000000,788306.6236983632,5531056.64,3.527971052851948e-08,179200.0,851371153.5942322,193536000.0,,https://huggingface.co/ai-forever/ruGPT-3.5-13B,ruGPT-3.5 13B
ruGPT-3.5 13B,Language,Russia,Sber,2023-04-24,Industry,Chat,Confident,512.0,Open weights (unrestricted),Not-defined,Unreleased,13000000000.0,300000000000.0,1080.0,1.0699776e+23,1877.1621580193864,10802.845,350.0,377485000000000.0,429328500000000.0,28000000000.0,"NVIDIA A100,NVIDIA Tesla V100 SXM2",Training cost,1,4000000,788306.6236983632,5531056.64,3.527971052851948e-08,179200.0,851371153.5942322,193536000.0,,https://huggingface.co/ai-forever/ruGPT-3.5-13B,ruGPT-3.5 13B
ruGPT-3.5 13B,Language,Russia,Sber,2023-04-24,Industry,Language modeling/generation,Confident,512.0,Open weights (unrestricted),Not-defined,Unreleased,13000000000.0,300000000000.0,1080.0,1.0699776e+23,1877.1621580193864,10802.845,350.0,377485000000000.0,429328500000000.0,28000000000.0,"NVIDIA A100,NVIDIA Tesla V100 SXM2",Training cost,1,4000000,788306.6236983632,5531056.64,3.527971052851948e-08,179200.0,851371153.5942322,193536000.0,,https://huggingface.co/ai-forever/ruGPT-3.5-13B,ruGPT-3.5 13B
tsuzumi 7B,Language,Japan,NTT Communication Science Laboratories,2023-12-01,Industry,Chat,Confident,480.0,Not-defined,Not-defined,Not-defined,7000000000.0,1500000000000.0,720.0,1.17e+23,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Significant use,1,2000000,113138.0969599182,7200000.0,4.916290598290598e-08,192000.0,81459429.8111411,138240000.0,,https://group.ntt/en/magazine/blog/tsuzumi/,"NTT's Large Language Model ""tsuzumi"" is Here!"
tsuzumi 7B,Language,Japan,NTT Communication Science Laboratories,2023-12-01,Industry,Language modeling/generation,Confident,480.0,Not-defined,Not-defined,Not-defined,7000000000.0,1500000000000.0,720.0,1.17e+23,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Significant use,1,2000000,113138.0969599182,7200000.0,4.916290598290598e-08,192000.0,81459429.8111411,138240000.0,,https://group.ntt/en/magazine/blog/tsuzumi/,"NTT's Large Language Model ""tsuzumi"" is Here!"
xTrimoPGLM -100B,Biology,China,BioMap Research,2023-07-06,Academia,Protein or nucleotide language model (pLM/nLM),Confident,768.0,Unreleased,UniRef50,Unreleased,100000000000.0,2600000000000.0,3912.0,6.2e+23,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,1,2097152,679602.2897189565,11520000.0,9.277516129032258e-09,307200.0,2658604157.3805575,1201766400.0,"Protein language models have shown remarkable success in learning biological information from protein sequences. However, most existing models are limited by either autoencoding or autoregressive pre-training objectives, which makes them struggle to handle protein understanding and generation tasks concurrently. This paper proposes a unified protein language model, xTrimoPGLM, to address these two types of tasks simultaneously through an innovative pre-training framework. Our key technical contribution is an exploration of the compatibility and the potential for joint optimization of the two types of objectives, which has led to a strategy for training xTrimoPGLM at an unprecedented scale of 100 billion parameters and 1 trillion training tokens. Our extensive experiments reveal that xTrimoPGLM significantly outperforms other advanced baselines in diverse protein understanding tasks (13 out of 15 tasks across four categories) and generates novel protein sequences which are structurally similar to natural ones. Furthermore, using the same xTrimoPGLM framework, we train an antibody-specific model (xTrimoPGLM-Ab) using 1 billion parameters. This model set a new record in predicting antibody naturalness and structures, both essential to the field of antibody-based drug design, and demonstrated a significantly faster inference speed than AlphaFold2. These results highlight the substantial capability and versatility of xTrimoPGLM in understanding and generating protein sequences.",https://www.biorxiv.org/content/10.1101/2023.07.05.547496v4,xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein
xTrimoPGLM -100B,Biology,China,BioMap Research,2023-07-06,Academia,Protein or nucleotide language model (pLM/nLM),Confident,768.0,Unreleased,UniRef50,Unreleased,100000000000.0,2600000000000.0,3912.0,6.2e+23,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,2097152,679602.2897189565,11520000.0,9.277516129032258e-09,307200.0,2658604157.3805575,1201766400.0,"Protein language models have shown remarkable success in learning biological information from protein sequences. However, most existing models are limited by either autoencoding or autoregressive pre-training objectives, which makes them struggle to handle protein understanding and generation tasks concurrently. This paper proposes a unified protein language model, xTrimoPGLM, to address these two types of tasks simultaneously through an innovative pre-training framework. Our key technical contribution is an exploration of the compatibility and the potential for joint optimization of the two types of objectives, which has led to a strategy for training xTrimoPGLM at an unprecedented scale of 100 billion parameters and 1 trillion training tokens. Our extensive experiments reveal that xTrimoPGLM significantly outperforms other advanced baselines in diverse protein understanding tasks (13 out of 15 tasks across four categories) and generates novel protein sequences which are structurally similar to natural ones. Furthermore, using the same xTrimoPGLM framework, we train an antibody-specific model (xTrimoPGLM-Ab) using 1 billion parameters. This model set a new record in predicting antibody naturalness and structures, both essential to the field of antibody-based drug design, and demonstrated a significantly faster inference speed than AlphaFold2. These results highlight the substantial capability and versatility of xTrimoPGLM in understanding and generating protein sequences.",https://www.biorxiv.org/content/10.1101/2023.07.05.547496v4,xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein
xTrimoPGLM -100B,Biology,China,BioMap Research,2023-07-06,Academia,Proteins,Confident,768.0,Unreleased,UniRef50,Unreleased,100000000000.0,2600000000000.0,3912.0,6.2e+23,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,1,2097152,679602.2897189565,11520000.0,9.277516129032258e-09,307200.0,2658604157.3805575,1201766400.0,"Protein language models have shown remarkable success in learning biological information from protein sequences. However, most existing models are limited by either autoencoding or autoregressive pre-training objectives, which makes them struggle to handle protein understanding and generation tasks concurrently. This paper proposes a unified protein language model, xTrimoPGLM, to address these two types of tasks simultaneously through an innovative pre-training framework. Our key technical contribution is an exploration of the compatibility and the potential for joint optimization of the two types of objectives, which has led to a strategy for training xTrimoPGLM at an unprecedented scale of 100 billion parameters and 1 trillion training tokens. Our extensive experiments reveal that xTrimoPGLM significantly outperforms other advanced baselines in diverse protein understanding tasks (13 out of 15 tasks across four categories) and generates novel protein sequences which are structurally similar to natural ones. Furthermore, using the same xTrimoPGLM framework, we train an antibody-specific model (xTrimoPGLM-Ab) using 1 billion parameters. This model set a new record in predicting antibody naturalness and structures, both essential to the field of antibody-based drug design, and demonstrated a significantly faster inference speed than AlphaFold2. These results highlight the substantial capability and versatility of xTrimoPGLM in understanding and generating protein sequences.",https://www.biorxiv.org/content/10.1101/2023.07.05.547496v4,xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein
xTrimoPGLM -100B,Biology,China,BioMap Research,2023-07-06,Academia,Proteins,Confident,768.0,Unreleased,UniRef50,Unreleased,100000000000.0,2600000000000.0,3912.0,6.2e+23,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,2097152,679602.2897189565,11520000.0,9.277516129032258e-09,307200.0,2658604157.3805575,1201766400.0,"Protein language models have shown remarkable success in learning biological information from protein sequences. However, most existing models are limited by either autoencoding or autoregressive pre-training objectives, which makes them struggle to handle protein understanding and generation tasks concurrently. This paper proposes a unified protein language model, xTrimoPGLM, to address these two types of tasks simultaneously through an innovative pre-training framework. Our key technical contribution is an exploration of the compatibility and the potential for joint optimization of the two types of objectives, which has led to a strategy for training xTrimoPGLM at an unprecedented scale of 100 billion parameters and 1 trillion training tokens. Our extensive experiments reveal that xTrimoPGLM significantly outperforms other advanced baselines in diverse protein understanding tasks (13 out of 15 tasks across four categories) and generates novel protein sequences which are structurally similar to natural ones. Furthermore, using the same xTrimoPGLM framework, we train an antibody-specific model (xTrimoPGLM-Ab) using 1 billion parameters. This model set a new record in predicting antibody naturalness and structures, both essential to the field of antibody-based drug design, and demonstrated a significantly faster inference speed than AlphaFold2. These results highlight the substantial capability and versatility of xTrimoPGLM in understanding and generating protein sequences.",https://www.biorxiv.org/content/10.1101/2023.07.05.547496v4,xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein
xTrimoPGLM -100B,Biology,China,BioMap Research,2023-07-06,Industry,Protein or nucleotide language model (pLM/nLM),Confident,768.0,Unreleased,UniRef50,Unreleased,100000000000.0,2600000000000.0,3912.0,6.2e+23,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,1,2097152,679602.2897189565,11520000.0,9.277516129032258e-09,307200.0,2658604157.3805575,1201766400.0,"Protein language models have shown remarkable success in learning biological information from protein sequences. However, most existing models are limited by either autoencoding or autoregressive pre-training objectives, which makes them struggle to handle protein understanding and generation tasks concurrently. This paper proposes a unified protein language model, xTrimoPGLM, to address these two types of tasks simultaneously through an innovative pre-training framework. Our key technical contribution is an exploration of the compatibility and the potential for joint optimization of the two types of objectives, which has led to a strategy for training xTrimoPGLM at an unprecedented scale of 100 billion parameters and 1 trillion training tokens. Our extensive experiments reveal that xTrimoPGLM significantly outperforms other advanced baselines in diverse protein understanding tasks (13 out of 15 tasks across four categories) and generates novel protein sequences which are structurally similar to natural ones. Furthermore, using the same xTrimoPGLM framework, we train an antibody-specific model (xTrimoPGLM-Ab) using 1 billion parameters. This model set a new record in predicting antibody naturalness and structures, both essential to the field of antibody-based drug design, and demonstrated a significantly faster inference speed than AlphaFold2. These results highlight the substantial capability and versatility of xTrimoPGLM in understanding and generating protein sequences.",https://www.biorxiv.org/content/10.1101/2023.07.05.547496v4,xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein
xTrimoPGLM -100B,Biology,China,BioMap Research,2023-07-06,Industry,Protein or nucleotide language model (pLM/nLM),Confident,768.0,Unreleased,UniRef50,Unreleased,100000000000.0,2600000000000.0,3912.0,6.2e+23,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,2097152,679602.2897189565,11520000.0,9.277516129032258e-09,307200.0,2658604157.3805575,1201766400.0,"Protein language models have shown remarkable success in learning biological information from protein sequences. However, most existing models are limited by either autoencoding or autoregressive pre-training objectives, which makes them struggle to handle protein understanding and generation tasks concurrently. This paper proposes a unified protein language model, xTrimoPGLM, to address these two types of tasks simultaneously through an innovative pre-training framework. Our key technical contribution is an exploration of the compatibility and the potential for joint optimization of the two types of objectives, which has led to a strategy for training xTrimoPGLM at an unprecedented scale of 100 billion parameters and 1 trillion training tokens. Our extensive experiments reveal that xTrimoPGLM significantly outperforms other advanced baselines in diverse protein understanding tasks (13 out of 15 tasks across four categories) and generates novel protein sequences which are structurally similar to natural ones. Furthermore, using the same xTrimoPGLM framework, we train an antibody-specific model (xTrimoPGLM-Ab) using 1 billion parameters. This model set a new record in predicting antibody naturalness and structures, both essential to the field of antibody-based drug design, and demonstrated a significantly faster inference speed than AlphaFold2. These results highlight the substantial capability and versatility of xTrimoPGLM in understanding and generating protein sequences.",https://www.biorxiv.org/content/10.1101/2023.07.05.547496v4,xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein
xTrimoPGLM -100B,Biology,China,BioMap Research,2023-07-06,Industry,Proteins,Confident,768.0,Unreleased,UniRef50,Unreleased,100000000000.0,2600000000000.0,3912.0,6.2e+23,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,1,2097152,679602.2897189565,11520000.0,9.277516129032258e-09,307200.0,2658604157.3805575,1201766400.0,"Protein language models have shown remarkable success in learning biological information from protein sequences. However, most existing models are limited by either autoencoding or autoregressive pre-training objectives, which makes them struggle to handle protein understanding and generation tasks concurrently. This paper proposes a unified protein language model, xTrimoPGLM, to address these two types of tasks simultaneously through an innovative pre-training framework. Our key technical contribution is an exploration of the compatibility and the potential for joint optimization of the two types of objectives, which has led to a strategy for training xTrimoPGLM at an unprecedented scale of 100 billion parameters and 1 trillion training tokens. Our extensive experiments reveal that xTrimoPGLM significantly outperforms other advanced baselines in diverse protein understanding tasks (13 out of 15 tasks across four categories) and generates novel protein sequences which are structurally similar to natural ones. Furthermore, using the same xTrimoPGLM framework, we train an antibody-specific model (xTrimoPGLM-Ab) using 1 billion parameters. This model set a new record in predicting antibody naturalness and structures, both essential to the field of antibody-based drug design, and demonstrated a significantly faster inference speed than AlphaFold2. These results highlight the substantial capability and versatility of xTrimoPGLM in understanding and generating protein sequences.",https://www.biorxiv.org/content/10.1101/2023.07.05.547496v4,xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein
xTrimoPGLM -100B,Biology,China,BioMap Research,2023-07-06,Industry,Proteins,Confident,768.0,Unreleased,UniRef50,Unreleased,100000000000.0,2600000000000.0,3912.0,6.2e+23,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,2097152,679602.2897189565,11520000.0,9.277516129032258e-09,307200.0,2658604157.3805575,1201766400.0,"Protein language models have shown remarkable success in learning biological information from protein sequences. However, most existing models are limited by either autoencoding or autoregressive pre-training objectives, which makes them struggle to handle protein understanding and generation tasks concurrently. This paper proposes a unified protein language model, xTrimoPGLM, to address these two types of tasks simultaneously through an innovative pre-training framework. Our key technical contribution is an exploration of the compatibility and the potential for joint optimization of the two types of objectives, which has led to a strategy for training xTrimoPGLM at an unprecedented scale of 100 billion parameters and 1 trillion training tokens. Our extensive experiments reveal that xTrimoPGLM significantly outperforms other advanced baselines in diverse protein understanding tasks (13 out of 15 tasks across four categories) and generates novel protein sequences which are structurally similar to natural ones. Furthermore, using the same xTrimoPGLM framework, we train an antibody-specific model (xTrimoPGLM-Ab) using 1 billion parameters. This model set a new record in predicting antibody naturalness and structures, both essential to the field of antibody-based drug design, and demonstrated a significantly faster inference speed than AlphaFold2. These results highlight the substantial capability and versatility of xTrimoPGLM in understanding and generating protein sequences.",https://www.biorxiv.org/content/10.1101/2023.07.05.547496v4,xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein
xTrimoPGLM -100B,Biology,China,Tsinghua University,2023-07-06,Academia,Protein or nucleotide language model (pLM/nLM),Confident,768.0,Unreleased,UniRef50,Unreleased,100000000000.0,2600000000000.0,3912.0,6.2e+23,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,1,2097152,679602.2897189565,11520000.0,9.277516129032258e-09,307200.0,2658604157.3805575,1201766400.0,"Protein language models have shown remarkable success in learning biological information from protein sequences. However, most existing models are limited by either autoencoding or autoregressive pre-training objectives, which makes them struggle to handle protein understanding and generation tasks concurrently. This paper proposes a unified protein language model, xTrimoPGLM, to address these two types of tasks simultaneously through an innovative pre-training framework. Our key technical contribution is an exploration of the compatibility and the potential for joint optimization of the two types of objectives, which has led to a strategy for training xTrimoPGLM at an unprecedented scale of 100 billion parameters and 1 trillion training tokens. Our extensive experiments reveal that xTrimoPGLM significantly outperforms other advanced baselines in diverse protein understanding tasks (13 out of 15 tasks across four categories) and generates novel protein sequences which are structurally similar to natural ones. Furthermore, using the same xTrimoPGLM framework, we train an antibody-specific model (xTrimoPGLM-Ab) using 1 billion parameters. This model set a new record in predicting antibody naturalness and structures, both essential to the field of antibody-based drug design, and demonstrated a significantly faster inference speed than AlphaFold2. These results highlight the substantial capability and versatility of xTrimoPGLM in understanding and generating protein sequences.",https://www.biorxiv.org/content/10.1101/2023.07.05.547496v4,xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein
xTrimoPGLM -100B,Biology,China,Tsinghua University,2023-07-06,Academia,Protein or nucleotide language model (pLM/nLM),Confident,768.0,Unreleased,UniRef50,Unreleased,100000000000.0,2600000000000.0,3912.0,6.2e+23,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,2097152,679602.2897189565,11520000.0,9.277516129032258e-09,307200.0,2658604157.3805575,1201766400.0,"Protein language models have shown remarkable success in learning biological information from protein sequences. However, most existing models are limited by either autoencoding or autoregressive pre-training objectives, which makes them struggle to handle protein understanding and generation tasks concurrently. This paper proposes a unified protein language model, xTrimoPGLM, to address these two types of tasks simultaneously through an innovative pre-training framework. Our key technical contribution is an exploration of the compatibility and the potential for joint optimization of the two types of objectives, which has led to a strategy for training xTrimoPGLM at an unprecedented scale of 100 billion parameters and 1 trillion training tokens. Our extensive experiments reveal that xTrimoPGLM significantly outperforms other advanced baselines in diverse protein understanding tasks (13 out of 15 tasks across four categories) and generates novel protein sequences which are structurally similar to natural ones. Furthermore, using the same xTrimoPGLM framework, we train an antibody-specific model (xTrimoPGLM-Ab) using 1 billion parameters. This model set a new record in predicting antibody naturalness and structures, both essential to the field of antibody-based drug design, and demonstrated a significantly faster inference speed than AlphaFold2. These results highlight the substantial capability and versatility of xTrimoPGLM in understanding and generating protein sequences.",https://www.biorxiv.org/content/10.1101/2023.07.05.547496v4,xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein
xTrimoPGLM -100B,Biology,China,Tsinghua University,2023-07-06,Academia,Proteins,Confident,768.0,Unreleased,UniRef50,Unreleased,100000000000.0,2600000000000.0,3912.0,6.2e+23,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,1,2097152,679602.2897189565,11520000.0,9.277516129032258e-09,307200.0,2658604157.3805575,1201766400.0,"Protein language models have shown remarkable success in learning biological information from protein sequences. However, most existing models are limited by either autoencoding or autoregressive pre-training objectives, which makes them struggle to handle protein understanding and generation tasks concurrently. This paper proposes a unified protein language model, xTrimoPGLM, to address these two types of tasks simultaneously through an innovative pre-training framework. Our key technical contribution is an exploration of the compatibility and the potential for joint optimization of the two types of objectives, which has led to a strategy for training xTrimoPGLM at an unprecedented scale of 100 billion parameters and 1 trillion training tokens. Our extensive experiments reveal that xTrimoPGLM significantly outperforms other advanced baselines in diverse protein understanding tasks (13 out of 15 tasks across four categories) and generates novel protein sequences which are structurally similar to natural ones. Furthermore, using the same xTrimoPGLM framework, we train an antibody-specific model (xTrimoPGLM-Ab) using 1 billion parameters. This model set a new record in predicting antibody naturalness and structures, both essential to the field of antibody-based drug design, and demonstrated a significantly faster inference speed than AlphaFold2. These results highlight the substantial capability and versatility of xTrimoPGLM in understanding and generating protein sequences.",https://www.biorxiv.org/content/10.1101/2023.07.05.547496v4,xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein
xTrimoPGLM -100B,Biology,China,Tsinghua University,2023-07-06,Academia,Proteins,Confident,768.0,Unreleased,UniRef50,Unreleased,100000000000.0,2600000000000.0,3912.0,6.2e+23,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,2097152,679602.2897189565,11520000.0,9.277516129032258e-09,307200.0,2658604157.3805575,1201766400.0,"Protein language models have shown remarkable success in learning biological information from protein sequences. However, most existing models are limited by either autoencoding or autoregressive pre-training objectives, which makes them struggle to handle protein understanding and generation tasks concurrently. This paper proposes a unified protein language model, xTrimoPGLM, to address these two types of tasks simultaneously through an innovative pre-training framework. Our key technical contribution is an exploration of the compatibility and the potential for joint optimization of the two types of objectives, which has led to a strategy for training xTrimoPGLM at an unprecedented scale of 100 billion parameters and 1 trillion training tokens. Our extensive experiments reveal that xTrimoPGLM significantly outperforms other advanced baselines in diverse protein understanding tasks (13 out of 15 tasks across four categories) and generates novel protein sequences which are structurally similar to natural ones. Furthermore, using the same xTrimoPGLM framework, we train an antibody-specific model (xTrimoPGLM-Ab) using 1 billion parameters. This model set a new record in predicting antibody naturalness and structures, both essential to the field of antibody-based drug design, and demonstrated a significantly faster inference speed than AlphaFold2. These results highlight the substantial capability and versatility of xTrimoPGLM in understanding and generating protein sequences.",https://www.biorxiv.org/content/10.1101/2023.07.05.547496v4,xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein
xTrimoPGLM -100B,Biology,China,Tsinghua University,2023-07-06,Industry,Protein or nucleotide language model (pLM/nLM),Confident,768.0,Unreleased,UniRef50,Unreleased,100000000000.0,2600000000000.0,3912.0,6.2e+23,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,1,2097152,679602.2897189565,11520000.0,9.277516129032258e-09,307200.0,2658604157.3805575,1201766400.0,"Protein language models have shown remarkable success in learning biological information from protein sequences. However, most existing models are limited by either autoencoding or autoregressive pre-training objectives, which makes them struggle to handle protein understanding and generation tasks concurrently. This paper proposes a unified protein language model, xTrimoPGLM, to address these two types of tasks simultaneously through an innovative pre-training framework. Our key technical contribution is an exploration of the compatibility and the potential for joint optimization of the two types of objectives, which has led to a strategy for training xTrimoPGLM at an unprecedented scale of 100 billion parameters and 1 trillion training tokens. Our extensive experiments reveal that xTrimoPGLM significantly outperforms other advanced baselines in diverse protein understanding tasks (13 out of 15 tasks across four categories) and generates novel protein sequences which are structurally similar to natural ones. Furthermore, using the same xTrimoPGLM framework, we train an antibody-specific model (xTrimoPGLM-Ab) using 1 billion parameters. This model set a new record in predicting antibody naturalness and structures, both essential to the field of antibody-based drug design, and demonstrated a significantly faster inference speed than AlphaFold2. These results highlight the substantial capability and versatility of xTrimoPGLM in understanding and generating protein sequences.",https://www.biorxiv.org/content/10.1101/2023.07.05.547496v4,xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein
xTrimoPGLM -100B,Biology,China,Tsinghua University,2023-07-06,Industry,Protein or nucleotide language model (pLM/nLM),Confident,768.0,Unreleased,UniRef50,Unreleased,100000000000.0,2600000000000.0,3912.0,6.2e+23,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,2097152,679602.2897189565,11520000.0,9.277516129032258e-09,307200.0,2658604157.3805575,1201766400.0,"Protein language models have shown remarkable success in learning biological information from protein sequences. However, most existing models are limited by either autoencoding or autoregressive pre-training objectives, which makes them struggle to handle protein understanding and generation tasks concurrently. This paper proposes a unified protein language model, xTrimoPGLM, to address these two types of tasks simultaneously through an innovative pre-training framework. Our key technical contribution is an exploration of the compatibility and the potential for joint optimization of the two types of objectives, which has led to a strategy for training xTrimoPGLM at an unprecedented scale of 100 billion parameters and 1 trillion training tokens. Our extensive experiments reveal that xTrimoPGLM significantly outperforms other advanced baselines in diverse protein understanding tasks (13 out of 15 tasks across four categories) and generates novel protein sequences which are structurally similar to natural ones. Furthermore, using the same xTrimoPGLM framework, we train an antibody-specific model (xTrimoPGLM-Ab) using 1 billion parameters. This model set a new record in predicting antibody naturalness and structures, both essential to the field of antibody-based drug design, and demonstrated a significantly faster inference speed than AlphaFold2. These results highlight the substantial capability and versatility of xTrimoPGLM in understanding and generating protein sequences.",https://www.biorxiv.org/content/10.1101/2023.07.05.547496v4,xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein
xTrimoPGLM -100B,Biology,China,Tsinghua University,2023-07-06,Industry,Proteins,Confident,768.0,Unreleased,UniRef50,Unreleased,100000000000.0,2600000000000.0,3912.0,6.2e+23,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,SOTA improvement,1,2097152,679602.2897189565,11520000.0,9.277516129032258e-09,307200.0,2658604157.3805575,1201766400.0,"Protein language models have shown remarkable success in learning biological information from protein sequences. However, most existing models are limited by either autoencoding or autoregressive pre-training objectives, which makes them struggle to handle protein understanding and generation tasks concurrently. This paper proposes a unified protein language model, xTrimoPGLM, to address these two types of tasks simultaneously through an innovative pre-training framework. Our key technical contribution is an exploration of the compatibility and the potential for joint optimization of the two types of objectives, which has led to a strategy for training xTrimoPGLM at an unprecedented scale of 100 billion parameters and 1 trillion training tokens. Our extensive experiments reveal that xTrimoPGLM significantly outperforms other advanced baselines in diverse protein understanding tasks (13 out of 15 tasks across four categories) and generates novel protein sequences which are structurally similar to natural ones. Furthermore, using the same xTrimoPGLM framework, we train an antibody-specific model (xTrimoPGLM-Ab) using 1 billion parameters. This model set a new record in predicting antibody naturalness and structures, both essential to the field of antibody-based drug design, and demonstrated a significantly faster inference speed than AlphaFold2. These results highlight the substantial capability and versatility of xTrimoPGLM in understanding and generating protein sequences.",https://www.biorxiv.org/content/10.1101/2023.07.05.547496v4,xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein
xTrimoPGLM -100B,Biology,China,Tsinghua University,2023-07-06,Industry,Proteins,Confident,768.0,Unreleased,UniRef50,Unreleased,100000000000.0,2600000000000.0,3912.0,6.2e+23,1877.1621580193864,15000.0,400.0,575206000000000.0,624000000000000.0,40000000000.0,NVIDIA A100 SXM4 40 GB,Training cost,1,2097152,679602.2897189565,11520000.0,9.277516129032258e-09,307200.0,2658604157.3805575,1201766400.0,"Protein language models have shown remarkable success in learning biological information from protein sequences. However, most existing models are limited by either autoencoding or autoregressive pre-training objectives, which makes them struggle to handle protein understanding and generation tasks concurrently. This paper proposes a unified protein language model, xTrimoPGLM, to address these two types of tasks simultaneously through an innovative pre-training framework. Our key technical contribution is an exploration of the compatibility and the potential for joint optimization of the two types of objectives, which has led to a strategy for training xTrimoPGLM at an unprecedented scale of 100 billion parameters and 1 trillion training tokens. Our extensive experiments reveal that xTrimoPGLM significantly outperforms other advanced baselines in diverse protein understanding tasks (13 out of 15 tasks across four categories) and generates novel protein sequences which are structurally similar to natural ones. Furthermore, using the same xTrimoPGLM framework, we train an antibody-specific model (xTrimoPGLM-Ab) using 1 billion parameters. This model set a new record in predicting antibody naturalness and structures, both essential to the field of antibody-based drug design, and demonstrated a significantly faster inference speed than AlphaFold2. These results highlight the substantial capability and versatility of xTrimoPGLM in understanding and generating protein sequences.",https://www.biorxiv.org/content/10.1101/2023.07.05.547496v4,xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein
