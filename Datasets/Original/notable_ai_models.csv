Model,Publication date,Notability criteria,Parameters,Training compute (FLOP),Training dataset size (datapoints),Epochs,Training time (hours),Hardware quantity,Hardware utilization,Training compute cost (2023 USD),Training hardware,Training dataset,Confidence,Base model,Finetune compute (FLOP),Batch size,Model accessibility,Training code accessibility,Inference code accessibility,Frontier model,Training power draw (W),Training compute estimation method,Notability criteria notes,Parameters notes,Training compute notes,Training dataset notes,Dataset size notes,Training time notes,Compute cost notes,Finetune compute notes,Batch size notes,Accessibility notes
EXAONE Deep 32B,3/16/2025,Training cost,32000000000,1.26e+24,12000000000,,2160,512,0.3156,,NVIDIA H100 SXM5 80GB,Unspecified unreleased,Confident,EXAONE 3.5 32B,7.04e+21,,Open weights (non-commercial),Unreleased,,,788306.6236983632,"Reported,Operation counting,Hardware","512 H100 for 3 months

Math – EXAONE Deep 32B Outperforms Competitors in High-Difficulty Math Benchmarks Even at Just 5% of Their Size

MMLU – EXAONE Deep 32B Achieves 83.0 score, Proving the Best Performance Among Domestic Models",32B,"1.25 × 10^24 (base model reported training compute) + 7.04 × 10^21 (finetune compute) = 1.26 × 10^24 FLOP

Table 1",,"""To enhance the reasoning capabilities of language models, we have utilized 1.6M instances for SFT and 20K instances of preference data for DPO. The SFT dataset contains approximately 12B tokens""",512 H100 GPUs were used for three months,,"Table 1 (reported): 7.04 × 10^21 FLOP

6ND = 6*32B parameters * 12B tokens = 2.304e+21 FLOP",,"https://huggingface.co/LGAI-EXAONE/EXAONE-Deep-32B

Exaone License"
QwQ-32B,3/6/2025,SOTA improvement,32500000000,3.51e+24,,,,,,,,Unspecified unreleased,Speculative,Qwen2.5-Coder (32B),,,Open weights (unrestricted),Unreleased,,,,,Blog (https://qwenlm.github.io/blog/qwq-32b-preview/) lists AIME and MATH-500 scores superior to o1-preview,"Architecture: transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias
Number of Parameters: 32.5B
Number of Paramaters (Non-Embedding): 31.0B
Number of Layers: 64
Number of Attention Heads (GQA): 40 for Q and 8 for KV","Assuming the same dataset size as for Qwen2.5 training (18T tokens):

6ND = 6 * 32500000000 parameters * 18 * 10^12 tokens =  3.51 × 10^24

'Speculative' confidence",,Speculativley might be similar to Qwen2.5 models (18T tokens),,,,,"https://huggingface.co/Qwen/QwQ-32B
Apache 2"
GPT-4.5,2/27/2025,Training cost,,,,,,,,,,Unspecified unreleased,Unknown,,,,API access,Unreleased,,,,,"Described by OpenAI as a ""new order of magnitude of compute""

https://openai.com/index/introducing-gpt-4-5/",,,"""GPT-4.5 was pre-trained and post-trained on diverse datasets, including a mix of publicly available
data, proprietary data from data partnerships, and custom datasets developed in-house, which
collectively contribute to the model’s robust conversational capabilities and world knowledge.""",,,,,,
Claude 3.7 Sonnet,2/24/2025,Training cost,,3.3499999999999998e+25,,,,,,,,Unspecified unreleased,Likely,,,,API access,Unreleased,,,,,,,https://docs.google.com/spreadsheets/d/10bhwdVrfHI8tysVIz62ZxtvQ30L-HojYvmU18_b-WIM/edit?gid=0#gid=0,"""Claude 3.7 Sonnet is trained on a proprietary mix of publicly available information on the Internet, as well as non-public data from third parties, data provided by data labeling services and paid contractors, and data we
generate internally. While trained on publicly available information on the internet through November 2024, Claude 3.7 Sonnet’s knowledge cut-off date is the end of October 2024. This means the model’s knowledge base is most extensive and reliable on information and events up to October 2024.""",,,,,,
Grok-3,2/17/2025,Training cost,,4.64e+26,,,2400,100000,,,NVIDIA H100 SXM5 80GB,Unspecified unreleased,Confident,,,,Hosted access (no API),Unreleased,,,154003694.78442368,"Hardware,Comparison with other models",,,"Estimate based on training time for a cluster of 100,000 H100s, and xAI's statement that Grok 2 was trained on more compute than GPT-4 (2.1e25) and that Grok 3 was trained on around 15 times more compute than Grok 2. 

Full estimate here: https://docs.google.com/document/d/1C_dABuZrAqYE_ui4_GZ4bRLtq3TBjIGoBSktaPElhEU/edit?usp=sharing",,,Estimated to be between 3 and 4 months. We use 100 days in our estimate,,,,
o3-mini,1/31/2025,Significant use,,,,,,,,,,Unspecified unreleased,Unknown,,,,API access,Unreleased,,,,,,,,,,,,,,
Computer-Using Agent (CUA),1/23/2025,SOTA improvement,,,,,,,,,,Unspecified unreleased,Unknown,,,,Hosted access (no API),Unreleased,,,,,SOTA at OSWorld,,,,,,,,,
Kimi k1.5,1/22/2025,SOTA improvement,,,,,,,,,,Unspecified unreleased,Unknown,,,,API access,Unreleased,,,,,"""Sota short-CoT performance, outperforming GPT-4o and Claude Sonnet 3.5 on AIME,  MATH-500, LiveCodeBench by a large margin (up to +550%)""",,,"""The Kimi k1.5 base model is trained on a diverse, high-quality multimodal corpus. The language data covers five domains: English, Chinese, Code, Mathematics Reasoning, and Knowledge. Multimodal data, including Captioning, Image-text Interleaving, OCR, Knowledge, and QA datasets, enables our model to acquire vision-language capabilities.
Rigorous quality control ensures relevance, diversity, and balance in the overall pretrain dataset.""",,,,,,
Doubao-1.5-pro,1/22/2025,Training cost,,,9E+12,1,,,,,,,Unknown,,,,Hosted access (no API),Unreleased,,,,,,"Not directly reported. We are told it is a MoE model, and that it matches the performance of a dense model trained on the same data, while using 1/7th of the activated parameters. Additionally they say ""The number of parameters of the Doubao dense model is also much smaller than that of Llama3.1-405B"", which suggests that the number of activated parameters on the forward pass is ""much less"" than 405B/7 = 58B parameters.","The model was trained on 9T tokens; since we know the MoE model uses ""much less"" than 58B parameters (see parameter notes), training compute is likely to be less than 6 * 9T * 58B = 3.132e24",,9T tokens,,,,,
DeepSeek-R1,1/20/2025,"Training cost,SOTA improvement",6.71E+11,5.17e+24,,,,,,6770000,,Unspecified unreleased,Confident,DeepSeek-V3,6.1e+23,,Open weights (unrestricted),Unreleased,,,,Operation counting,"Best score on SuperCLUE Math6o in Jan 2025
https://www.superclueai.com/","671B total
37B activated
https://github.com/deepseek-ai/DeepSeek-R1/tree/main",4.56e+24 FLOP (estimated base model Deepseek V3 training compute) + 6.1e23 FLOP = 5.17e+24 FLOP,"RL + SFT

When reasoning-oriented RL converges, we utilize the resulting checkpoint to collect SFT
(Supervised Fine-Tuning) data for the subsequent round. Unlike the initial cold-start data, which
primarily focuses on reasoning, this stage incorporates data from other domains to enhance the
model’s capabilities in writing, role-playing, and other general-purpose tasks.",,,"$5.576M 2024 USD (estimated training cost of Deepseek v3) + $1M (estimated training cost of RL) = $6.576M 2024 USD = $6.77M 2023 USD

https://www.usinflationcalculator.com/

https://epoch.ai/gradient-updates/what-went-into-training-deepseek-r1",6.1e23 FLOP from these estimations: https://epoch.ai/gradient-updates/what-went-into-training-deepseek-r1,,MIT licensed
DeepSeek-V3,12/24/2024,Training cost,6.71E+11,4.56e+24,1.48E+13,,,2048,,5390000,NVIDIA H800 SXM5,,Confident,,,,Open weights (restricted use),,,,3155569.9410964646,"Operation counting,Hardware",training cost was $5.3million USD (Table 1),Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token.,"6*37B*14.8T = 3.2856e+24

Alternatively, they say: ""DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training."" and we know they trained in FP8. H800s get 1.513e15 FLOP/s in FP8:
2.788M * 3600 * 1.513e15 * 0.3 = 4.56e24

Utilization may be somewhat lower for FP8.

Upper bound estimate: 50% utilization would mean 7.59e24",,"""We pre-train DeepSeek-V3 on 14.8 trillion diverse and
high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities""","""DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training""","Table 1
$5.576M 2024 USD = $5.39M 2023 USD (https://www.usinflationcalculator.com/)",,,"MIT and deepseek license
https://github.com/deepseek-ai/DeepSeek-V3?tab=readme-ov-file"
o3,12/20/2024,Significant use,,,,,,,,,,Unspecified unreleased,Unknown,,,,Unreleased,Unreleased,,,,,,,,,,,,,,
Veo 2,12/16/2024,SOTA improvement,,,,,,,,,,Unspecified unreleased,Unknown,,,,API access,,,,,,"""Veo has achieved state of the art results in head-to-head comparisons of outputs by human raters over top video generation models.

Participants viewed 1003 prompts and respective videos on MovieGenBench, a benchmark dataset released by Meta. Veo 2 performs best on overall preference, and for its capability to follow prompts accurately.""

SOTA qualification is unclear solely from MovieGenBench, which is subjective and depends on human raters. But Veo 2 seems to be SOTA over Meta Movie Gen, Kling, Minimax, and Sora Turbo.
Updated Sora could be better, but was released later this same month.",,,,,,,,,
Gemini 2.0 Pro,12/11/2024,Training cost,,,,,,,,,,Unspecified unreleased,Unknown,,,,Hosted access (no API),Unreleased,,,,,,,,,,,,,,
EXAONE 3.5 32B,12/9/2024,Training cost,32000000000,1.25e+24,6.5E+12,,,,,,,Unspecified unreleased,Confident,,,,Open weights (non-commercial),Unreleased,,,,Reported,,32B,1.25 × 10^24 (Table 2) ,,6.5T tokens (Table 2),,,,,Exaone license (allows only non-commercial usage)
Llama 3.3,12/6/2024,Training cost,70000000000,6.8649768e+24,1.5E+13,,,,,,NVIDIA H100 SXM5 80GB,Unspecified unreleased,Confident,,,,Open weights (restricted use),Unreleased,,,,"Operation counting,Hardware",,70B,"6ND = 6*70*10^9*15*10^12= 6.3e+24

7000000*3600*989500000000000*0.3= 7.48062e+24

sqrt(7.48062e+24*6.3e+24) = 6.8649768e+24","""A new mix of publicly available online data.""","""Overview: Llama 3.3 was pretrained on ~15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 25M synthetically generated examples.

Data Freshness: The pretraining data has a cutoff of December 2023.""","""Training utilized a cumulative of 39.3M GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency.""

Llama 3.3 70B: Training Time (GPU hours): 7M
",,,,"License A custom commercial license, the Llama 3.3 Community License Agreement, is available at: https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/LICENSE

""Llama 3.3 is intended for commercial and research use in multiple languages."""
o1,12/5/2024,"SOTA improvement,Significant use",,,,,,,,,,Unspecified unreleased,Unknown,,,,API access,Unreleased,,,,,SOTA in GPQA among others: https://openai.com/index/learning-to-reason-with-llms/ ,,,"""Both models were trained on a variety of publicly available datasets, including web data and open-source datasets. Key components include reasoning data and scientific literature. <..> To further enhance the capabilities of o1-preview and o1-mini, we formed partnerships to access high-value non-public datasets. These proprietary data sources include paywalled content, specialized archives, and other domain-specific datasets that provide deeper insights into industry-specific knowledge and use cases.""",,,,,,
Amazon Nova Pro,12/3/2024,SOTA improvement,,6.000010000000001e+24,,,,,,,,,Speculative,,,,API access,,,,,Comparison with other models,"""It achieves state-of-the-art performance on key benchmarks including visual question answering (TextVQA) and video understanding (VATEX).""",,"""probably just below 1e25 stemming from the Llama 70B serving speed.  If Llama 70B is trained proportionally to 405B, then it's at ~ 6.6e24. Nova Pro is served at 100tk/s, while Llama 70B is served at 70tk/s on average, and 100tk/s by together.ai at FP8. So Nova Pro would be >1e25 if they roughly 2x the amount of training compared to Llama 70B which [seems unlikely]""",,,,,,,
Fugatto 1,11/25/2024,SOTA improvement,2500000000,,,,,32,,,NVIDIA A100,,Confident,,,,Unreleased,,,,28182.178972509773,,"""We showcase Fugatto’s performance on traditional TTA benchmarks that
measure a model’s ability to synthesize general sounds (AudioCAPS) and music (MusicCAPS) that follow instructions provided in text. We use the metrics (FD, FAD, and IS) and data splits (train, test) used in Kong et al. (2024b). Results in Table 3a and Table 3b shows that our model achieves strictly better scores than existing generalist models, while occasionally outperforming expert models""",,,,"""our dataset is comprised of at least 50,000 hours of audio""",,,,,
Suno v4,11/19/2024,Significant use,,,,,,,,,,,Unknown,,,,API access,,,,,,https://www.semrush.com/website/suno.ai/overview/,,,,,,,,,
Pixtral Large,11/18/2024,"Significant use,SOTA improvement",1.24E+11,,,,,,,,,,Confident,Mistral Large 2,,,Open weights (restricted use),,,,,,Number of downloads not visible,"123B multimodal decoder, 1B parameter vision encoder",,,,,,,,
Hunyuan-Large,11/6/2024,Training cost,3.89E+11,3.49237e+24,7E+12,,,,,,,Unspecified unreleased,Confident,,,,Open weights (restricted use),Open (restricted use),,,,Operation counting,,"""a total of 389 billion parameters and 52 billion activation parameters""","52B activated parameters

6ND = 6*52*10^9*7*10^12 = 2.184 × 10^24

They also suggest more precise formula to calculate MoE compute budget:

9.59ND + 2.3 × 10^8D = 9.59*52*10^9*7*10^12 + 2.3 × 10^8 ×  7*10^12 = 3.49237×10^24

which seems closer to projected compute on Figure 3",,"""# Trained Tokens 7T""  Table 1",,,,,"the license doesn't regulate usage in the EU
also requires additional licensing in case of massive commercial use"
Doubao-pro,10/28/2024,Training cost,5E+11,2.505e+25,8.35E+12,,,,,,,Unspecified unreleased,Speculative,,,,API access,Unreleased,,,,Operation counting,,"[Speculative] Doubao's large language model has scaled up from 35 billion parameters to 800 billion, with 500 billion and 800 billion parameter models currently under training.
https://xueqiu.com/9637001584/309910396?md5__1038=7qmx2DyDuie4cDBqDTQEWqDtMvO4iTphD
",6ND = 6 * 500*10^9 * 8350*10^9 = 2.505e+25,"Doubao's data sources primarily rely on proprietary business data, accounting for 50-60%; externally sourced data comprises 15-20%; and synthetic data has been used since June of this year, although Doubao is cautious in feeding synthetic data due to its uncertain quality. ","[Speculative] Doubao's pre-training data volume is approximately 500TB, with only about 10% of this data actually used for training. The current version employs a non-Mixture-of-Experts (MoE) architecture. In the future, MoE architecture may be introduced to increase parameter count and performance, while also integrating multimodal data solutions.

So this model is dense, and the training data is probably all text tokens, not multimodal.

50TB * 167M tokens/GB ~= 8.35 trillion tokens
",,,,,
NVLM-X 72B,10/22/2024,Training cost,72000000000,3.0398181e+24,45875200000,1,,128,,,NVIDIA H100 SXM5 80GB,"COCO,Conceptual Captions (CC3M),SBU,VQAv2,VisualGenome,TextVQA,OCR-VQA",Likely,"Qwen2-72B,InternViT-6B",1.9818086e+22,,Open weights (non-commercial),,,,197336.5951439333,Operation counting,,72B,3.02e24 FLOP (Qwen2-72B compute) + 19818086000000000000000 = 3.0398181e+24,"Captioning COCO [72], CC3M [127], SBU [114], LAION-115M (sanitized) [123; 66]
VQA (natural image) VQAv2 [38], Visual Genome [59]
Chart DVQA [51]
Document Docmatix [90]
OCR /
Scene-Text
OCR-VQA [98], COCO-Text [144], TextOCR [132], ReCTs [170], RRC-ArT [22], RRC-LSVT [134]
RCTW [128], synthdog-en [57], pdfa-eng-wds [117]
Math CLEVR-Math [73]","Pre-training
Global batch size 2,048
Sequence length in the LLM decoder 512
Downsampling of visual tokens 1024->256
# of visual token per tile 256
# of tiles 1
# of training steps 20K

2048 * (512 + 256 * 1) * 20000 = 31,457,280,000

SFT:
Global batch size 256
Sequence length in the LLM decoder  1,024
# of visual token per tile 256
# of tiles 6+1
# of training steps 20K

256 * (1,024 + 256*7) * 20000 = 14417920000

31,457,280,000 +14417920000 = 45875200000",,Model uses ,6*72B*45875200000 = 1.9818086e+22,,
NVLM-H 72B,10/22/2024,Training cost,72000000000,3.02e+24,1.25829E+11,1,,128,,,NVIDIA H100 SXM5 80GB,"COCO,Conceptual Captions (CC3M),SBU,VQAv2,VisualGenome,TextVQA,OCR-VQA",Likely,"Qwen2-72B,InternViT-6B",5.436e+22,,Open weights (non-commercial),,,,197336.5951439333,Operation counting,,72B,Additional compute in this paper is negligible relative to the compute used to train the language model backbone (Qwen2-72B at 3.02e24 FLOP),"Captioning COCO [72], CC3M [127], SBU [114], LAION-115M (sanitized) [123; 66]
VQA (natural image) VQAv2 [38], Visual Genome [59]
Chart DVQA [51]
Document Docmatix [90]
OCR /
Scene-Text
OCR-VQA [98], COCO-Text [144], TextOCR [132], ReCTs [170], RRC-ArT [22], RRC-LSVT [134]
RCTW [128], synthdog-en [57], pdfa-eng-wds [117]
Math CLEVR-Math [73]","Pre-training:
Global batch size 2,048
Sequence length in the LLM decoder 512
Downsampling of visual tokens 1024->256
# of visual token per tile 256
# of tiles 6+1
# of training steps 20K

2048 * (512+256*7) * 20000 = 94,371,840,000

SFT:
Global batch size  256
Sequence length in the LLM decoder  1,280
# of visual token per tile 256
# of tiles 6+1
# of training steps 40K

256*(1280+256*7)*40000 = 31,457,280,000

94,371,840,000 + 31,457,280,000 = 125,829,120,000",,,"6ND = 6*125,829,120,000*72000000000.00 = 5.436e22
",,
NVLM-D 72B,10/22/2024,SOTA improvement,72000000000,3.02e+24,57016320000,1,,128,,,NVIDIA H100 SXM5 80GB,"COCO,Conceptual Captions (CC3M),SBU,VQAv2,VisualGenome,TextVQA,OCR-VQA",Confident,"Qwen2-72B,InternViT-6B",2.463e+22,,Open weights (non-commercial),Open (non-commercial),,,197336.5951439333,Operation counting,SOTA on OCRBench and VQAv2,72B,"Uses Qwen2-72B as a backbone, which trained with 3.02e24 FLOP, as well as InternViT-6B. It's unclear how many FLOP were spent training but probably negligible; e.g. PaLI trained ViT-e with ~4B parameters using 1.07e23 FLOP.

Fine-tuning FLOPs:
57,016,320,000 image/text tokens over all stages
6 * 72B * 57,016,320,000 = 2.463e22
","Captioning COCO [72], CC3M [127], SBU [114], LAION-115M (sanitized) [123; 66]
VQA (natural image) VQAv2 [38], Visual Genome [59]
Chart DVQA [51]
Document Docmatix [90]
OCR /
Scene-Text
OCR-VQA [98], COCO-Text [144], TextOCR [132], ReCTs [170], RRC-ArT [22], RRC-LSVT [134]
RCTW [128], synthdog-en [57], pdfa-eng-wds [117]
Math CLEVR-Math [73]","Pre-training
Global batch size 2,048
Sequence length in the LLM decoder 512
Downsampling of visual tokens 1024->256
# of visual token per tile 256
# of tiles 1
# of training steps 20K

2048 * (512 + 256 * 1) * 20000 = 31,457,280,000

SFT:
Global batch size 128
Sequence length in the LLM decoder 3,200
# of visual token per tile 256
# of tiles 6+1
# of training steps 40K

128 * (3200 + 256*7) * 40000 = 25,559,040,000

31,457,280,000 + 25,559,040,000 = 57,016,320,000",,Model uses ,"Fine-tuning FLOPs:
57,016,320,000 image/text tokens over all stages
6 * 72B * 57,016,320,000 = 2.463e22",,"https://huggingface.co/nvidia/NVLM-D-72B
Creative Commons Attribution: Non-Commercial 4.0 International

*training code ""coming soon"""
Yi-Lightning,10/18/2024,Training cost,,1.5e+24,,,720,2000,,,NVIDIA H100 SXM5 80GB,Unspecified unreleased,Confident,,,,API access,Unreleased,,,3083497.308288588,Hardware,"On the blind test list LMSYS, Yi-Lightning surpassed GPT-4o-2024-05-13 released by OpenAI and Anthropic, as well as Claude 3.5 Sonnet, ranking sixth in the world and first in China.",,"The CEO of 01.AI tweeted that Yi-Lightning was trained for 1 month on 2000 H100s: https://x.com/kaifulee/status/1846310645849047524
Assuming this is accurate:
(9.9e14 * 2000) FLOP/s * 1 month * 30.5 days/month * 24hr/day * 3600 s/hr * 0.3 utilization assumption = 1.565e24",,,"https://x.com/kaifulee/status/1846310645849047524
""it was trained on 2000 H100s for 1 month""",,,,https://platform.lingyiwanwu.com/
CHAI-1,10/15/2024,SOTA improvement,,7.7605724e+21,,,720,128,,,NVIDIA A100,"PDB (Protein Data Bank), AlphaFold database (AFDB)",Confident,,,128,Open weights (non-commercial),Open (non-commercial),,,112771.00249279808,Hardware,Matches or beats AF3 on Ligand PoseBusters,,"From paper: 128 A100s for 30 days; assumptions: 30% utilization rate, FP16 precision",,,Taken from paper: 128 A100s for 30 days,,,Taken from paper,https://github.com/chaidiscovery/chai-lab?tab=License-1-ov-file
Palmyra X 004,10/9/2024,SOTA improvement,1.5E+11,,,,,,,,,,,,,,API access,,,,,,SOTA on Berkeley’s Tool Calling Leaderboard,Source: https://venturebeat.com/ai/writers-palmyra-x-004-takes-the-lead-in-ai-function-calling-surpassing-tech-giants/,,,,,,,,
Movie Gen Video,10/4/2024,Training cost,30000000000,1.65e+24,26600000000,,331,6144,,,NVIDIA H100 SXM5 80GB,,Confident,,,,Unreleased,,,,9473720.068741636,Operation counting,BOTE estimate of cost is ~$3 million,30B,"Model size = 30B
Broken down by training stage (table 3):
256px T2I: samples seen = 1.94E9; sample token length = 256; flops = 6ND = 8.94E22
256px T2I/V: samples seen = 3.95E8; sample token length = 8192; flops = 6ND = 5.82E23
768px T2I/V: samples seen = 7.38E7; sample token length = 73,728; flops = 6ND = 9.79E23
Total flops = 1.65E24",,"O(1B) images
O(100M) videos, each with 256 frames ~= 25M images","54 hours for 256px T2I
128 hours for 256px T2I/V
149 hours for 768px T2I/V",,,,
Qwen2.5-72B,9/19/2024,Training cost,72700000000,7.8e+24,1.8E+13,1,,,,,,Unspecified unreleased,Confident,,,,Open weights (unrestricted),Unreleased,,,,Operation counting,"High compute, near 1e25",72.7B,"Training dataset size was 18 trillion

6ND = 6 * 72.7 billion parameters * 18 trillion tokens = 7.8e24","""In terms of Qwen2.5, the language models, all models are pretrained on our latest large-scale dataset, encompassing up to 18 trillion tokens.""","""In terms of Qwen2.5, the language models, all models are pretrained on our latest large-scale dataset, encompassing up to 18 trillion tokens""",,,,,"license: allows commercial. weights only
https://huggingface.co/Qwen/Qwen2.5-72B/blob/main/LICENSE "
Qwen2.5 Instruct (72B),9/19/2024,Training cost,72700000000,7.8516e+24,1.8E+13,,,,,,,Unspecified unreleased,Confident,Qwen2.5-72B,,,Open weights (restricted use),,,,,Operation counting,,"Number of Parameters: 72.7B
Number of Paramaters (Non-Embedding): 70.0B",6ND = 6*72700000000 parameters *18000000000000 tokens = 7.8516e+24,,"""In terms of Qwen2.5, the language models, all models are pretrained on our latest large-scale dataset, encompassing up to 18 trillion tokens.""",,,,,"requires permission to use in applications with 100K+ users

https://huggingface.co/Qwen/Qwen2.5-72B-Instruct"
Qwen2.5-32B,9/17/2024,Training cost,32500000000,3.51e+24,1.8E+13,,,,,,,Unspecified unreleased,Confident,,,,Open weights (unrestricted),Unreleased,,,,Operation counting,,32.5B,6 * 32.5B parameters * 18 trillion tokens = 3.51 × 10^24,,"""In terms of Qwen2.5, the language models, all models are pretrained on our latest large-scale dataset, encompassing up to 18 trillion tokens""",,,,,"Apache 2.0
https://huggingface.co/Qwen/Qwen2.5-32B"
o1-preview,9/12/2024,"SOTA improvement,Significant use",,,,,,,,,,Unspecified unreleased,Unknown,,,,API access,Unreleased,,,,,SOTA in GPQA among others: https://openai.com/index/learning-to-reason-with-llms/ ,,,"""Both models were trained on a variety of publicly available datasets, including web data and open-source datasets. Key components include reasoning data and scientific literature. <..> To further enhance the capabilities of o1-preview and o1-mini, we formed partnerships to access high-value non-public datasets. These proprietary data sources include paywalled content, specialized archives, and other domain-specific datasets that provide deeper insights into industry-specific knowledge and use cases.""",,,,,,
o1-mini,9/12/2024,Significant use,,,,,,,,,,Unspecified unreleased,Unknown,,,,API access,Unreleased,,,,,"Model available in ChatGPT, likely widely used ",,,"""Both models were trained on a variety of publicly available datasets, including web data and open-source datasets. Key components include reasoning data and scientific literature. <..> To further enhance the capabilities of o1-preview and o1-mini, we formed partnerships to access high-value non-public datasets. These proprietary data sources include paywalled content, specialized archives, and other domain-specific datasets that provide deeper insights into industry-specific knowledge and use cases.""",,,,,,
DeepSeek-V2.5,9/6/2024,Training cost,2.36E+11,1.7892e+24,,,,,,,,"GitHub,Common Crawl",Confident,,,36864000,Open weights (restricted use),Unreleased,,,,Operation counting,,"21B active params, 236B total","V2.5 is a merge of V2-coder and V2-chat
V2-coder is trained for 6T additional tokens from an intermediate checkpoint of V2, which had been trained for 4.2T tokens. Total: 10.2T
V2-chat is fine-tuned from V2, saw 8.2T tokens in pre-training
Unique steps: 8.2T + 6T = 14.2T
FLOPs: 6 * 21B * 14.2T = 1.7892e24",,"The original V2 had a dataset of 8.1T unique tokens, and coder-V2 added an additional 1.391T unique tokens of code and math. But it appears no additional training was done to combine them into this model.",,,,"Maximum batch size comes from training of V2-coder, which used long context training with 288 batches of 128k tokens = 36,864,000",
Hunyuan Turbo,9/5/2024,SOTA improvement,,,,,,,,,,Unspecified unreleased,Unknown,,,,,,,,,,"Best score on SuperCLUE总排行榜（2024年8月）- SuperCLUE general benchmak from Aug 2024 (https://www.superclueai.com/) in terms of ""science"" and ""liberal arts"" evaluation.",,,,,,,,,
AlphaProteo,9/5/2024,Historical significance,,,,,,,,,,PDB (Protein Data Bank),Unknown,,,,Unreleased,Unreleased,,,,,Economic impacts from development of commercially and socially valuable protein designs and materials,,,Trained on large amounts of protein data from the Protein Data Bank (PDB) and more than 100 million predicted structures from AlphaFold,,,,,,
GLM-4-Plus,8/29/2024,Training cost,,3.5999999999999997e+25,,,,,,,,,Unknown,,,,API access,,,,,Benchmarks,,,Estimated using benchmark imputation,,,,,,,
Jamba 1.5-Large,8/22/2024,Training cost,3.98E+11,,,,,,,,NVIDIA H100 SXM5 80GB,Unspecified unreleased,Confident,,,,Open weights (restricted use),Unreleased,,,,,,94B active/398B total,,,,,,,,Commercial use allowed up to $50M USD annual revenue.
Grok-2,8/13/2024,Training cost,,2.9599999999999996e+25,,,,,,,NVIDIA H100 SXM5 80GB,Unspecified unreleased,Confident,,,,Hosted access (no API),Unreleased,,,,"Comparison with other models,Reported",,,Estimate based on xAI statements comparing Grok-2 compute to GPT-4 and Grok-3. Full estimate here: https://docs.google.com/document/d/1C_dABuZrAqYE_ui4_GZ4bRLtq3TBjIGoBSktaPElhEU/edit?usp=sharing,,,,,,,
Table Tennis Agent,8/7/2024,SOTA improvement,185000,,,,,,,,,,Likely,,,,Unreleased,Unreleased,,,,,"""first learned robot agent that reaches amateur human-level performance in competitive table tennis""","17 low level controllers with 10k parameters each: 
""Each policy is a dilated-gated CNN
[22] following the architecture in [23] with 10k parameters... The final
system contained 17 LLCs""

One high-level controller with 4.5k parameters: ""The style policy architecture, similar to the LLC but with
only 4.5k parameters, has a (8, 128) observation space""

spin classifier that is a 2-layer MLP of hidden sizes (128, 64) and input size 18, which is 10k parameters per o1 and Claude.

So ~185k parameters total",unclear,"Trained in simulation via reinforcement learning with feedback from real world

""This iterative cycle of training models in simulation on
the latest dataset, evaluating it in the real world, and using
the annotated evaluation data to extend the dataset, can be
repeated as many times as needed. We completed 7 cycles
for rally balls and 2 cycles for serving balls over the course
of 3 months with over 50 different human opponents, leading
to a final dataset size of 14.2k initial ball states for rallies
and 3.4k for serves. A summary of the dataset evolution is
presented in Table I and Figure 6.""","~18k ball states

""This iterative cycle of training models in simulation on
the latest dataset, evaluating it in the real world, and using
the annotated evaluation data to extend the dataset, can be
repeated as many times as needed. We completed 7 cycles
for rally balls and 2 cycles for serving balls over the course
of 3 months with over 50 different human opponents, leading
to a final dataset size of 14.2k initial ball states for rallies
and 3.4k for serves. A summary of the dataset evolution is
presented in Table I and Figure 6.""",,,,,
AFM-server,7/29/2024,Significant use,,4.3e+24,7.4E+12,1,,8192,0.52,,Google TPU v4,,Likely,,,18949752.58,Hosted access (no API),Unreleased,,,3466813.5881738467,"Operation counting,Hardware","Currently in beta access only, but will be integrated into millions or billions of iPhones.",,"""The AFM base models are dense decoder-only models that build on the
Transformer architecture""

""We train AFM-server from scratch for 6.3T tokens on 8192
TPUv4 chips, using a sequence length of 4096 and a batch-size of 4096 sequences.""

""For both models we perform continued pre-training at a sequence length of
8192, with another 1T tokens from a mixture that upweights math and code,
and down-weights the bulk web-crawl.""

""The sustained model-flop-utilization (MFU) for this training run was approximately 52%.""

Parameter count is not specified other than it being ""larger"" than 3 billion.

Counting FLOP: Chinchilla scaling laws would suggest 7.3T / 20 = 365B parameters. 

365B parameters * 7.3T tokens * 6 ~= 1.6e25 FLOP.

However, the attention to inference optimization in the technical report suggests a smaller size, even for this ""server"" model. One point of reference is Llama 3 70B being overtrained by a factor of 10. If this is true of AFM-server, the parameter count would be ~37B and training compute would be 1.6e24 FLOP.

GPU-time: assume a wall-clock training time of 30 days based on the current trend value for notable models.

8192 chips * 275e12 FLOP/s per chip * 0.52 utilization * 30 * 24 * 60 * 60 s ~= 3.0e24 FLOP

The geometric mean of these three estimates is 4.3e24 FLOP.","6.3T tokens of web text, code, and math, plus another 1T in the second stage and 100B in the third. See section 3.1 for details.","Not explicitly mentioned, but I assume the 7.4T tokens do not involve multiple epochs.",,,,"Main pretraining uses sequence length of 4096 tokens; 4096 sequences per batch. During the ""continued"" pre-training stage, sequence length is upped to 8192 while batch size remains 4096. During context lengthening, sequence length is upped to 32768 while ""the recipe is similar to that used for continued pre-training"" implies same batch size of 4096.

Weighting batch sizes by number of tokens seen in each stage:

exp((6.3T * ln(4096 * 4096) + 1T * ln(8192 * 4096) + 100B * ln(32768 * 4096))/ 7.4T) = 18,949,753",
AFM-on-device,7/29/2024,Significant use,2730000000,4.5126e+23,7.588E+12,1,,2048,0.52,,Google TPU v5p,,Confident,,,18949752.58,Hosted access (no API),Unreleased,,,,Operation counting,"Currently in beta access only, but will be integrated into millions or billions of iPhones.","Table 1, sum of non-embedding and embedding parameters","Model was initialized from a pruned version of a 6.4B parameter model trained using the same recipe as AFM-server. Assuming ""same recipe"" involves training for the full 6.3T tokens, this implies 6 * 6.3T * 6.4B = 2.42e23 FLOP. 

The pruning masks are learned by training over 188B tokens, which suggests 6 * 188B * 6.4B = 7.22e21 FLOPs.

Pretraining is then run over 6.3T tokens; however, labels are a convex combination of true labels and the predicted labels from the unpruned 6.4B model. Since this involves running the 6.3T tokens forward through both the 6.4B and the 2.73B model, but only calculating gradients for the smaller model, FLOPs here are equal to (6 * 6.3T * 2.73B) + (2 * 6.3T * 6.4B) = 1.84e23. 

Finally, there is a 1T ""continuation"" pretraining stage without distillation loss, for 6 * 1T * 2.73B = 1.64e22 FLOP, and a 100B context-lengthening stage for another 6 * 100B * 2.73B = 1.64e21 FLOP

In total: 2.42e23 + 7.22e21 + 1.84e23 + 1.64e22 + 1.64e21 = 4.51e23 FLOP","188B of tokens are used to train a pruning mask to reduce a 6.4B model to the 2.73B used for AFM-on-device. Main pre-training data is 6.3T tokens of web text, code, and math, plus another 1T in the second pre-training stage and 100B in the third. See section 3.1 for details. Post-training details do not give details on dataset size.","Not explicitly mentioned, but I assume the 7.588T tokens do not involve multiple epochs.","Trained on ""one slice of 2048 TPUv5p chips""; wall-time not given.",,,"Main pretraining uses sequence length of 4096 tokens; 4096 sequences per batch. During the ""continued"" pre-training stage, sequence length is upped to 8192 while batch size remains 4096. During context lengthening, sequence length is upped to 32768 while ""the recipe is similar to that used for continued pre-training"" implies same batch size of 4096.

Weighting batch sizes by number of tokens seen in each stage:

exp((6.3T * ln(4096 * 4096) + 1T * ln(8192 * 4096) + 100B * ln(32768 * 4096))/ 7.4T) = 18,949,753",
Mistral Large 2,7/24/2024,Training cost,1.23E+11,2.13e+25,,,,,,,,Unspecified unreleased,Likely,,,,Open weights (non-commercial),Unreleased,,TRUE,,"Hardware,Cost,Benchmarks",likely high training cost since previous Mistral Large cost around 20 million,,"Details are sparse, but we can hazard a guess based on evidence about the training cluster they may have used, the scale up in compute they likely would have used relative to Mistral Large 1, and from the model's MMLU score. Extended reasoning given here: https://docs.google.com/document/d/1I2ZWBLFMpRZYcdMMUfKAGZFJrOJpduNDS9ZeVFIHnd8/edit?usp=sharing",,,,,,,"""We are releasing Mistral Large 2 under the Mistral Research License, that allows usage and modification for research and non-commercial usages. For commercial usage of Mistral Large 2 requiring self-deployment, a Mistral Commercial License must be acquired by contacting us."""
Llama 3.1-405B,7/23/2024,"SOTA improvement,Training cost",4.05E+11,3.8e+25,1.56E+13,1,2142,16384,0.4042,,NVIDIA H100 SXM5 80GB,Llama 3 dataset,Confident,,,16000000,Open weights (restricted use),Open (restricted use),Open access (restricted use),TRUE,25280251.591531232,"Reported,Operation counting","High training compute, exceeds 4o and Claude 3.5 on some benchmarks:

https://ai.meta.com/blog/meta-llama-3-1/ ",405B,"Stated in paper.

Also, 6 * 405B * 15.6T training tokens = 3.8e25",,15.6T tokens,"Trained on 30.84M GPU hours (https://huggingface.co/blog/llama31) and used ""up to 16K H100 GPU[s]"" so training took at least
30.84M / 16k = 1927.5 hours or ~80 days. 

Section 3.3.4 gives reliability details over a 54 day period during training, for which they had ""higher than 90% effective training time""
1927.5 / 0.9 = 2142 hours

Probably, full training time is somewhat longer, since it sounds like there were periods where not all 16k H100s were running.",,,,"Llama 3.1 model license:

https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/blob/main/LICENSE 

must seek separate license if over 700m monthly users, acceptable use restrictions

training code here: https://github.com/meta-llama/llama-recipes/blob/main/src/llama_recipes/utils/train_utils.py#L70 
"
GPT-4o mini,7/18/2024,Significant use,,7.360010000000001e+24,,,,,,,,Unspecified unreleased,Speculative,,,,API access,Unreleased,,,,Benchmarks,"No public breakdown of GPT-4o mini users, but as of late 2024, it is one of the few main models available in ChatGPT and OpenAI's cheapest model. OpenAI has hundreds of millions of users.",,"Training compute estimated from benchmark scores.

90% CI [3.23e+24, 2.05e+25]",,,,,,,
ESM3 (98B),6/25/2024,Historical significance,98500000000,1.07e+24,7.71E+11,2.3,,,,,,ESM3 Dataset,Confident,,,4194304,Unreleased,Unreleased,,,,Reported,"Largest (in compute) biology and protein model to date, was able to discover novel green fluorescent proteins",98.5 billion (Table S1),"""ESM3 at its largest scale was trained with 1.07×10^24 FLOPs on 2.78 billion proteins and 771 billion unique tokens, and has 98 billion parameters.""

per Table 1, trained 98B model on 1.8T training tokens. 98 billion * 1800 billion * 6 = 1.06e24. Likely some rounding, so will go with developer's reported count.",, 771 billion tokens,,,,Table S1,only small version released
Claude 3.5 Sonnet,6/20/2024,"Significant use,SOTA improvement",,3.650001e+25,,,,,,,,Unspecified unreleased,Speculative,,,,API access,Unreleased,,,,Benchmarks,"""It also sets new performance standards in evaluations of graduate
level science knowledge (GPQA) [1], general reasoning (MMLU) [2], and coding proficiency (HumanEval)
[3].""",,"Training compute estimated from benchmark scores.

Blog post by Dario Amodei includes some info on 3.5 Sonnet compute: https://darioamodei.com/on-deepseek-and-export-controls
""Claude 3.5 Sonnet is a mid-sized model that cost a few $10M's to train (I won't give an exact number). Also, 3.5 Sonnet was not trained in any way that involved a larger or more expensive model (contrary to some rumors).""",Training data cutoff Apr 2024,,,,,,
DeepSeek-Coder-V2 236B,6/17/2024,SOTA improvement,2.36E+11,1.2852e+24,3.191E+12,,,,,,,"GitHub,Common Crawl",Confident,DeepSeek-V2 (MoE-236B),,36864000,Open weights (restricted use),Unreleased,,,,Operation counting,"New SOTA on Aider, AIME 2024, and Math Odyssey benchmarks (including against proprietary models such as Claude 3 Opus, GPT-4o and GPT-4 Turbo).
Note that Figure 1 appears to show the new model getting SOTA for several other benchmarks, but omits results from GPT-4o which wins in most cases.",Mixture of experts model. 21B parameters activated per token.,"Trained on a total of 10.2T tokens
6NC: 6 * 10.2T * 21B active parameters = 1.285e24","See Section 2. ""In the pre-training phase, the dataset of DeepSeek-Coder-V2 is created with a composition of 60% source code, 10% math corpus, and 30% natural language corpus""","""In the pre-training phase, the dataset of DeepSeek-Coder-V2 is created with a composition of 60% source code, 10% math corpus, and 30% natural language corpus ... The source code consists of 1,170B code-related tokens sourced from GitHub and CommonCrawl... For the math corpus, we collect 221B math-related tokens sourced from CommonCrawl... In total, DeepSeek-Coder-V2 has been exposed to 10.2T training tokens, where 4.2 trillion tokens originate from the DeepSeek V2 dataset, while the remaining 6 trillion tokens come from the DeepSeek-Coder-V2 dataset""

Total of 1.391T tokens in the new data.

From the DeepSeek-V2 paper: ""our tokenized pretraining corpus contains 8.1T tokens""

So some tokens are trained over for multiple epochs:
- 6T * 0.6 / 1.17T = 3.1 epochs on the code corpus
- 6T * 0.1 / 221B = 2.7 epochs on the math corpus
- 6T * 0.3 / 8.1T = 0.22 epochs on the natural language corpus

Total unique tokens seen is likely 1.17T + 221B + (6T*0.3) = 3.191T",,,,"Most training is done at batch size of 36,864. They do long context training: ""In the first stage, we utilize a sequence length of 32K and a batch size of 1152 for 1000 steps. In the second stage, we train the model for an additional 1000 steps, employing a sequence length of 128K and a batch size of 288 sequences"" 128k*288 = 36,864,000","license has some harmful use restrictions: https://github.com/deepseek-ai/DeepSeek-Coder-V2/blob/main/LICENSE-MODEL 

no training code"
Nemotron-4 340B,6/14/2024,Training cost,3.4E+11,1.7999999999999999e+25,6.75E+12,,2200,6144,0.410675,,NVIDIA H100 SXM5 80GB,Unspecified unreleased,Confident,,,,Open weights (unrestricted),Unreleased,,TRUE,9483521.861161152,"Operation counting,Hardware","~2e25 FLOP, so high training cost, likely >5M",340B,"9 trillion tokens for training
6 * 340B * 9T = 1.8E25

alternatively, can do a hardware estimate with a few extra steps:

According to the technical report, Nemotron-4 340B was trained using up to 6144 H100 GPUs. Helpfully, they also report the model FLOP utilization (MFU), which was 41-42% (Table 2). This is the ratio of the actual output of their GPUs, in FLOP used for training, relative to their theoretical max of 989 teraFLOP/s per GPU. 
Unfortunately, the report omits the last ingredient, which is the duration of the training run. However, in Table 2 they report some relevant data that we can use to infer the training time. 
Nemotron-4 was trained in several stages, but the largest stage used all 6144 GPUs with a batch size of 2304 and an iteration time (time per batch) of 8.0 seconds. This stage involved 7.6T tokens, so it makes up the majority of training. 
A batch size of 2304 means that each batch consists of 2304 sequences, and they report that the sequence length used for training was 4096 tokens. This means that each batch contained 4096 * 2304 = 9,437,184 tokens. 
So, during this stage, it took 8 seconds to train the model on 9.4m tokens. Extrapolating to the entire 9T token dataset, this implies the training run would have taken 7,659,574 seconds, or 89 days. (it actually took longer because they didn't use all their GPUs for the whole run) 
Multiplying 7,659,574 seconds by 41% MFU, 989 peak teraFLOP/s for each H100, and 6144 H100s, we get ~1.9e25 FLOP. This is very close to our first estimate. 
","The technical report for the 340B model cites the report for the 15B version (https://arxiv.org/pdf/2402.16819 )

from that paper:

""We train Nemotron-4 15B on a pre-training dataset consisting of 8 trillion tokens. At a high-level,
the data blend is split into three different types of data: English natural language data (70%), multilingual
natural language data (15%), and source-code data (15%).
The English corpus consists of curated documents from a variety of sources and domains including web
documents, news articles, scientific papers, books, etc and the distribution used in our pre-training set is
highlighted in Figure 2. The code and multilingual data consists of a diverse set of natural and programming
languages. We find that appropriately sampling tokens from these languages is key to strong accuracies in
these domains. We share the distributions used for both code and multilingual tokens in our pre-training
dataset in Figure 3 and Figure 4 respectively.
In constructing the pre-training corpus, we remove any possible duplicates via document-level exact and
near-deduplication (Jennings et al., 2023). We additionally applied document-level quality filtering across
our corpus using a language-model based filtering approach similar to (Wenzek et al., 2019) in addition to a
series of heuristic filters as described in (Rae et al., 2022) and (Raffel et al., 2020).""","9T training tokens.

They first train on an 8T token dataset and then an additional 1T tokens, it's slightly unclear if that's more data or a partial second epoch

6.75T words using 1 token = 0.75 words","see training compute notes, this is an inferred estimate",,,,Permissive commercial license: https://developer.download.nvidia.com/licenses/nvidia-open-model-license-agreement-june-2024.pdf 
OpenVLA,6/13/2024,SOTA improvement,7188100000,1.1e+23,970000,27,336,64,,,NVIDIA A100,Open X-Embodiment,Confident,Llama 2-7B,9.66e+21,2048,Open weights (unrestricted),Open source,Open source,,56450.05922,Hardware,"""OpenVLA outperforms the 55B-parameter RT-2-X model [1, 7], the prior state-of-the-art VLA, by 16.5% absolute success rate across 29 evaluation tasks on the WidowX and Google Robot embodiments.""","Based on a Prismatic-7B VLM backbone, which itself is comprised of 600M parameter vision encoder (DinoV2 + SigLIP) plus Llama-2 7B. Table 1 indicates 7.1881 billion trainable parameters","Majority of compute is from VLA pre-training embedded in Prismatic-7B and it's constituent models. 

The fine-tuning compute used in this paper is ""64 A100 GPUs for 14 days, or a total of 21,500 A100-hours""
21500 * 3600 * 3.12e14 * 0.4 = 9.66e21

Prismatic-7B training took ""less than 9 hours"" on 8 A100s: 9 * 3600 * 8 * 3.12e14 * 0.4 = 3.23e19

Add in the pre-trained components:
- DinoV2 = 7.42e21, per our database
- The SigLIP model in question is SoViT-400m/14 from the cited Alabdulmohsin et al., 2023) and ""is pretrained on 40 billion examples, which amounts to 9T GFLOPs and 230K TPUv3 core-hours"" = 9e21
- Llama 2-7B = 8.4e22, per our database

Total
9.66e21 + 3.23e19 + 7.42e21 + 9e21 + 8.4e22 = 1.10e23","""The full OpenX dataset, at the time of writing, consists of more than 70 individual robot datasets, with more than 2M robot trajectories [...] we apply multiple steps of data curation to the raw dataset.""","""OpenVLA consists of a pretrained visually-conditioned language model backbone that captures visual features at multiple granularities, fine-tuned on a large, diverse dataset of 970k robot manipulation trajectories from the Open-X Embodiment [1] dataset""
Filtered from 2M total in OpenX.","""The final OpenVLA model is trained on a cluster of 64 A100 GPUs for 14 days""
14 days * 24 hr/day = 336 hours",,"""64 A100 GPUs for 14 days, or a total of 21,500 A100-hours""
21500 * 3600 * 3.12e14 * 0.4 = 9.66e21",,"""OpenVLA uses multiple pretrained model components: SigLIP [9] and DinoV2 [25] vision encoders and a Llama 2 [10] language model backbone. For all three models, weights are open, but not their training data or code. We release training data, code and model weights for reproducing OpenVLA on top of these components.""
All published material is on an MIT license.

train code: https://github.com/openvla/openvla/blob/main/scripts/pretrain.py "
Qwen2-72B,6/7/2024,Training cost,72710000000,3.02e+24,7E+12,,,,,,,Unspecified unreleased,Confident,,,,Open weights (unrestricted),Unreleased,,,,Operation counting,"SOTA claims are against open source models within a parameter class.

Possibly high training cost, at 3e24 FLOP seems borderline.","72.71B parameters in total, of which 70.21B are non-embedding parameters","72 billion params, 7 trillion tokens

6 * 72 billion * 7 trillion ~= 3.02e24","""All models were pre-trained on a high-quality, large-scale dataset comprising over 7 trillion tokens,
covering a wide range of domains and languages. Compared to previous editions of Qwen, Qwen2
includes a broader spectrum of linguistic data, enhancing the quantity and quality of code and mathematics content. ""","""All models were pre-trained on a high-quality, large-scale dataset comprising over 7 trillion tokens, covering a wide range of domains and languages.""",,,,,Apache 2.0
GLM-4 (0520),5/20/2024,Training cost,,,1E+13,,,,,,,,Likely,GLM-4 (0116),,,API access,,,TRUE,,Operation counting,"Trained on 10T tokens with similar architecture to GPT-4, probably >$1M compute cost.",,"- “the GLM-4 models are pre-trained on ten trillions of tokens”
- I did not find any information about parameters or compute. Here they speculatively estimate GLM-4 to be 200B parameters (which seems plausible), though no source provided: https://lifearchitect.ai/models-table/
- “GLM-4 gets close to the state-of-the-art models (GPT-4-Turbo, Gemini 1.5 Pro, and Claude 3 Opus)”  none of these models has parameters disclosed or compute estimation.

6*10000000000000*200000000000 = 1.2e+25 FLOPs with “Likely” confidence (+/- 1 OOM)","""To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage""",,,,,,"the GLM-4 API at
https://bigmodel.cn"
Yi-Large,5/13/2024,Training cost,1E+11,1.8e+24,3E+12,,,,,,,,Speculative,,,,API access,Unreleased,,,,Operation counting,,"""Yi-Large is a software over-the-air-driven closed-source large model with a parameter of over 100 billion tokens."" from https://www.chinadaily.com.cn/a/202405/13/WS6641abd1a31082fc043c6ccd.html","6ND = 6*100000000000*3000000000000=1.8e+24

(speculative confidence because training dataset size is very uncertain)",,"3T tokens for previous Yi models: ""Targeted as a bilingual language model and trained on 3T multilingual corpus, the Yi series models become one of the strongest LLM worldwide, showing promise in language understanding, commonsense reasoning, reading comprehension, and more.""
",,,,,
GPT-4o,5/13/2024,"SOTA improvement,Significant use",,3.810001e+25,,,,,,,,Unspecified unreleased,Speculative,,,,API access,Unreleased,,,,Benchmarks,"Outperforms GPT-4 Turbo and other models on text and especially on multimodal benchmarks, such as MMLU, GPQA, HumanEval, MMMU, etc See Model Evaluations: https://openai.com/index/hello-gpt-4o/ 

GPT-4o is now the default model in ChatGPT, so it's one of the most widely used models.","Not known.

Inference costs in the API are 2x cheaper than GPT-4 Turbo",Training compute estimated from benchmark scores.,"""With GPT-4o, we trained a single new model end-to-end across text, vision, and audio.""",,,,"Definitely a new model, not a GPT-4 finetune",,
Llama 3-70B,4/18/2024,Significant use,70000000000,7.861e+24,1.5E+13,,,,,,NVIDIA H100 SXM5 80GB,Llama 3 dataset,Confident,,,,Open weights (restricted use),Unreleased,,TRUE,,"Operation counting,Hardware","Will almost certainly be very influential and widely used in the open access AI industry, as with the previous Llama generations.",,"Arithmetic calculation:
6 * 15T tokens * 70B parameters = 6.3e24

GPU calculation:
https://huggingface.co/meta-llama/Meta-Llama-3-70B indicates training took 6.4M GPU-hours
We also know their larger scale training runs for 405B were getting between 0.38-0.41 MFU. Presumably the 70B model gets at least 0.43 utilization (405B has to be split across two nodes, while 70B should fit on one).
990 TFLOPS per GPU * 6.4 million GPU hours * 3600s * 0.43 = 9.808e24

Geometric mean: sqrt(6.3e24 * 9.808e24) = 7.861e24","""Llama 3 is pretrained on over 15T tokens that were all collected from publicly available sources. Our training dataset is seven times larger than that used for Llama 2, and it includes four times more code. To prepare for upcoming multilingual use cases, over 5% of the Llama 3 pretraining dataset consists of high-quality non-English data that covers over 30 languages.""",,,,,,"https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md

License A custom commercial license is available at: https://llama.meta.com/llama3/license"
Reka Core,4/15/2024,Training cost,67000000000,8.400010000000001e+24,,,,,,,"NVIDIA A100,NVIDIA H100 SXM5 80GB","Wikipedia,Unspecified unreleased",Speculative,,,,API access,Unreleased,,TRUE,,Hardware,,,"No direct information about Reka Core model (""Reka Core has not finished training and is still improving."")

The smaller dense model Reka Flash has 21B parameters and was trained on 5 trillion language tokens.

There is information about compute: ""Our setup comprises of clusters from a mixture of vendors with our peak compute being approximately
2.5K H100s and 2.5K A100s.""

If we assume 2 months of training with 2.5k H100s and 2.5k A100s at utilization 0.5 we get 8.4e24 FLOP (2500*9.9e14+2500*3.12e14)*60*60*24*60*0.5.","The training data comprises a mixture of publicly available and proprietary/licensed datasets with a dataset knowledge cutoff of November 2023. The dataset ingested by our model comprises of text, images, videos, and audio clips. Reka Flash and Reka Edge were trained on approximately 5 trillion and 4.5 trillion extensively deduplicated and filtered language tokens, respectively. While the classification of corpora is not strictly defined to one class or category, approximately 25% of our pretraining data is code related, and 30% are STEM related. Approximately 25% of the data is web crawl. About 10% of our data has some relation to math.",,,,,,
ReALM,3/29/2024,SOTA improvement,3000000000,,16300,,,,,,,,Confident,Flan-T5 11B,,,Unreleased,,,,,,"""We show that ReaLM outperforms previous approaches, and performs roughly as well as the state-of-the-art LLM today, GPT-4, despite consisting of far fewer parameters.""

""We also benchmark against GPT-3.5 and GPT-4, with our smallest model achieving performance comparable to that of GPT-4, and our larger models substantially outperforming it.""",Fine-tuned FLAN-T5 models ranging from 80M to 3B,Fine-tuned from FLAN-T5,"Mix of synthetic and manually annotated data. 

""Each data point contains the user query and a list of entities, along with the ground-truth entity (or set of entities) that are relevant to the corresponding user query. Each entity, in turn, contains information about its type and other properties such as the name and other textual details associated with the entity (the label and time of an alarm, for example)""",2300 training examples from conversation; 3900 synthetically generated training examples; 10100 training examples using context from a phone screen.,,,No training details given,,
DBRX,3/27/2024,Training cost,1.32E+11,2.6e+24,9E+12,1,,,,,NVIDIA H100 SXM5 80GB,,Confident,,,,Open weights (restricted use),Unreleased,,,,Operation counting,,132B mixture of experts. 36B parameters active per inference,"Mixture of Experts (MoE)

36 billion active params * 12 trillion tokens * 6 ~= 2.6e24
https://www.wolframalpha.com/input?i=6+FLOP+*+36+billion+*+12+trillion

also, it was trained on 3072 NVIDIA H100s, but with an unclear timeframe (end-end process was three months, including evals and red-teaming).","12T tokens, text and code

""It was pre-trained on 12T tokens of text and code data...

DBRX was pretrained on 12T tokens of carefully curated data and a maximum context length of 32k tokens. We estimate that this data is at least 2x better token-for-token than the data we used to pretrain the MPT family of models""

from HF: https://huggingface.co/databricks/dbrx-base

The training mix used for DBRX contains both natural-language and code examples. The vast majority of our training data is in the English language","12T tokens is equivalent to 9T words. Though it includes code data, so not very literally 9T words",,,,,"license: https://www.databricks.com/legal/open-model-license
conditions based on monthly users"
MM1-30B,3/14/2024,SOTA improvement,30000000000,4.86e+23,1.5E+12,,,,,,,"Conceptual Captions (CC3M),Conceptual Captions 12M (CC12M),COYO-700M,Unspecified unreleased,OBELICS",Likely,,,,Unreleased,Unreleased,,,,Operation counting,""" In particular, the pretrained model MM1 is SOTA, performing better than Emu2 [105], Flamingo [3],
and IDEFICS [47] on captioning and visual question answering (VQA) tasks
in few-shot settings, both in small and large size regimes""

Table 4: outperforms Gemini and GPT-4V on VQA",30B,"Pre-trained on ~2B image-text pairs and 2T tokens (Table 2). Each image is 144 tokens, so the images are ~300B tokens.
Then additional multimodal training for 400B tokens, for a total of ~2.7T tokens.

This is the final training recipe: ""We initialize both the image encoder and the underlying LLM decoder weights for MM1 from in-house pre-trained models2. We then perform multimodal pre-training on the above data mix for 200k steps (approx. 400B tokens).""

Compute  = 6ND = 6 * 2.7 trillion * 30 billion = 4.86e23

maybe the size of the visual connector is relevant","Text, captioned images. See Table 2",at least 2T tokens,,,,,
Inflection-2.5,3/7/2024,Significant use,,1.0001e+25,,,,,,,NVIDIA H100 SXM5 80GB,,Speculative,,,,Hosted access (no API),Unreleased,,TRUE,,Comparison with other models,one million daily users; six million monthly,,"""Inflection-1 used approximately 4% the training FLOPs of GPT-4 and, on average, performed at approximately 72% GPT-4 level on a diverse range of IQ-oriented tasks. Inflection-2.5, now powering Pi, achieves more than 94% the average performance of GPT-4 despite using only 40% the training FLOPs.""

This is a weird one - we estimated GPT-4 at 2.1e25 FLOP (which could be off somewhat, or Inflection could believe a different number). 40% of that is ~8e24. But Inflection 2, the previous model, was trained on ~1e25 FLOP per Inflection. Inflection-2.5 also does better on benchmarks than 2. Intuitively Inflection-2.5 would be trained on appreciably more compute. 

1e25 seems like a rough, perhaps conservative guess given all this.",,,,,,,
Claude 3 Sonnet,3/4/2024,Training cost,,,,,,,,,,Unspecified unreleased,Unknown,,,,API access,Unreleased,,,,,"Based on leaks, Claude 3 Opus and Sonnet probably cost >$1M to train.",,,"Claude 3 models are trained on a proprietary mix of publicly available information on the Internet as of August 2023, as well as non-public data from third parties, data provided by data labeling services and paid contractors, and data we generate internally. We employ several data cleaning and filtering methods, including deduplication and classification. The Claude 3 suite of models have not been trained on any user prompt or output data submitted to us by users or customers, including free users, Claude Pro users, and API customers.",,"Like its predecessors, Claude 3 models employ various training methods, such as unsupervised learning and Constitutional AI [6]. These models were trained using hardware from Amazon Web Services (AWS) and Google Cloud Platform (GCP)","Per https://time.com/6980000/anthropic/
""Claude 3 cost somewhere between $30 million and $300 million to train""
This would seem to include all three versions.

Ballpark estimate, based on relative API costs:
sqrt($30M * $300M) * (3 / (0.25 + 3 + 15)) = $15.6M
(cost) * (Sonnet share of API cost)

Convert to 2020 dollars: $12.9M",,,
Claude 3 Opus,3/4/2024,"SOTA improvement,Training cost",,1.640001e+25,,,,,,,,Unspecified unreleased,Speculative,,,,API access,Unreleased,,,,Benchmarks,"Based on leaks, Claude 3 Opus and Sonnet probably cost >$1M to train.",,Training compute estimated from benchmark scores.,"Claude 3 models are trained on a proprietary mix of publicly available information on the Internet as of August 2023, as well as non-public data from third parties, data provided by data labeling services and paid contractors, and data we generate internally. We employ several data cleaning and filtering methods, including deduplication and classification. The Claude 3 suite of models have not been trained on any user prompt or output data submitted to us by users or customers, including free users, Claude Pro users, and API customers.",,"Like its predecessors, Claude 3 models employ various training methods, such as unsupervised learning and Constitutional AI [6]. These models were trained using hardware from Amazon Web Services (AWS) and Google Cloud Platform (GCP)","Per https://time.com/6980000/anthropic/
""Claude 3 cost somewhere between $30 million and $300 million to train""
This would seem to include all three versions.

Ballpark estimate, based on relative API costs:
sqrt($30M * $300M) * (15 / (0.25 + 3 + 15)) = $78.0M
(cost) * (Sonnet share of API cost)

Convert to 2020 dollars: $64.7M",,,
Aramco Metabrain AI,3/4/2024,Training cost,2.5E+11,1.05e+25,7E+12,,,,,,,,Likely,,,,Unreleased,,,,,Operation counting,,"""It has 250 billion parameters that are adjustable during training to generate outputs or make predictions.""",6*250B*7T=1.05e+25,"""The AI was trained using seven trillion data points, collecting more than 90 years of company history.""",,,,,,
Mistral Large,2/26/2024,Training cost,,1.1199999999999999e+25,,,2500,,,,NVIDIA H100 SXM5 80GB,,Likely,,,,API access,Unreleased,,TRUE,,Cost,"~$20M training cost: https://www.wsj.com/tech/ai/the-9-month-old-ai-startup-challenging-silicon-valleys-giants-ee2e4c48
https://x.com/EMostaque/status/1762152740938031484?s=20 ",,"https://www.wsj.com/tech/ai/the-9-month-old-ai-startup-challenging-silicon-valleys-giants-ee2e4c48

Mistral spent <20 million euro (meaning approximately 20 million?) to train Mistral Large:

https://x.com/EMostaque/status/1762152740938031484?s=20
""assuming this is on H100s with @Scaleway who are €1.9/hour => 10m H100 hours (c 30m A100 hrs), 3 months at 4k H100s :timer_clock:"" -Emad Mostaque

Assuming bf16 or fp16, H100 SXM performance is 989 TFLOPS
At 1.9 euro per H100-hour and 30% utilization, spending 20M euro produces 1.12*10^25 FLOP.
https://www.wolframalpha.com/input?i=20+million+%2F+%281.9%2Fhour%29+*+989+TFLOPS+*+0.30 ",,,Speculation by Emad Mostaque: 20M euro spent at Scaleway (1.9 euro per H100-hour) would be around 3 months on 4000 H100s.,"In February 2024, 20M EUR = 22M USD
Converting to 2020 USD, this is 18.5M
https://www.in2013dollars.com/us/inflation/2024?endYear=2020&amount=22000000",,,
MegaScale (Production),2/23/2024,SOTA improvement,5.3E+11,3.9e+24,,,504,12288,0.48,,NVIDIA A100,,Speculative,,,,Unreleased,Unreleased,,TRUE,10849658.51586543,Other,Improves SOTA in FLOP utilization for distributed LLM training by 1.34X.,"Production run is stated to have ""hundreds of billions of parameters"". Since the authors also do a number of experiments with a 530B model, I speculate they've used 530B for the production model.","Speculative. The model is stated to have trained for ""several weeks"". Assuming 530B parameters and ""several"" = 3, compute can be estimated from the 175B model's stated PFLOP/sec:
2166.3 aggregate PFlops/sec * 3 weeks * 7 days/week * 24 hours/day * 3600 seconds/hour = 3.9e+24.
As an upper bound, say 8e+24. ",,"Speculative. Authors note production system was trained on ""multi-trillions of tokens"". This could refer to training for multiple epochs on the same 300B tokens used to train the 175B and 530B models outlined in more detail in the paper. Alternatively, it could refer to a larger dataset of perhaps 3-9 trillion tokens.","Speculative. Authors state ""several weeks"". For analysis, I've assumed this means around 3 weeks.",,,,"Code for MegaScale (also called veScale) training system are released under Apache Licence: https://github.com/volcengine/vescale
The model itself is unreleased."
Sora Turbo,2/15/2024,Significant use,,,,,,,,,,Unspecified unreleased,Unknown,,,,Unreleased,Unreleased,,,,,,,,"Sora was trained on diverse datasets, including a mix of publicly available data, proprietary data accessed through partnerships, and custom datasets developed in-house. These consist of:

Select publicly available data, mostly collected from industry-standard machine learning datasets and web crawls.

Proprietary data from data partnerships. We form partnerships to access non-publicly available data. For example, we partnered with Shutterstock⁠ Pond5 on building and delivering AI-generated images. We also partner to commission and create datasets fit for our needs.

Human data: Feedback from AI trainers, red teamers, and employees. ",,,,,,
Sora,2/15/2024,SOTA improvement,,,,,,,,,,Unspecified unreleased,Unknown,,,,Unreleased,Unreleased,,,,,,,,"Sora was trained on diverse datasets, including a mix of publicly available data, proprietary data accessed through partnerships, and custom datasets developed in-house. These consist of:

Select publicly available data, mostly collected from industry-standard machine learning datasets and web crawls.

Proprietary data from data partnerships. We form partnerships to access non-publicly available data. For example, we partnered with Shutterstock⁠ Pond5 on building and delivering AI-generated images. We also partner to commission and create datasets fit for our needs.

Human data: Feedback from AI trainers, red teamers, and employees. ",,,,,,
Gemini 1.5 Pro,2/15/2024,Significant use,,1.580001e+25,,,,,,,Google TPU v4,Unspecified unreleased,Speculative,,,,API access,Unreleased,,,,Benchmarks,"Google DeepMind's current best public model, being used for their products.",MoE architecture,Training compute imputed from benchmark scores.,,,,,,,API access: https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models
Aya,2/12/2024,SOTA improvement,13000000000,,,,,128,,,Google TPU v4,,Speculative,mT5-XXL,1.22683392e+21,,Open weights (unrestricted),Unreleased,,,54253.90483162765,,"from abstract ""We introduce extensive new evaluation suites that broaden the state-of-art for multilingual eval across 99
language""",13B  - fine tune of mT5 - from last page - model card ,"13B parameters, batch size = 256, sequence length = 1024 (for both input and output), 30K updates
- aproximation 6ND = 6 * 13B * 2 * 1024 * 30K * 256= 1226833920000000000000 = 1.22683392e+21
""We finetune mT5 models using the Adafactor optimizer [Shazeer & Stern, 2018] with a learning rate of 3 × 10−4 and a batch size of 256. We find that using a smaller learning rate compared to 1 × 10−3 leads to a better downstream performance, which is potentially due to the diverse nature of our IFT mixture. Both input and target sequence length are set to 1024.""
""We train all the models for 30,000 update steps with data packing enabled.16 This results in a training budget of 25M samples. ""","""Expansion of Language Coverage We significantly expand the size of available training data to directly address the linguistic inequality of recent NLP development. "" from the paper
""Datasets: xP3x, Aya Dataset, Aya Collection, DataProvenance collection, ShareGPT-Command."" from https://huggingface.co/CohereForAI/aya-101and https://huggingface.co/CohereForAI/aya-101#data-sources","at least 835 GB + size of ShareGPT-command + size of DataProvenance collection
https://huggingface.co/CohereForAI/aya-101#data-sourcesxP3x  - 680GB - from 
https://huggingface.co/datasets/CohereForAI/xP3x

aya_dataset - 138MB - https://huggingface.co/datasets/CohereForAI/aya_dataset

aya collection - 155GB - https://huggingface.co/datasets/CohereForAI/aya_collection",,,"13B parameters, batch size = 256, sequence length = 1024 (for both input and output), 30K updates
- aproximation 6ND = 6 * 13B * 2 * 1024 * 30K * 256= 1226833920000000000000 = 1.22683392e+21
""We finetune mT5 models using the Adafactor optimizer [Shazeer & Stern, 2018] with a learning rate of 3 × 10−4 and a batch size of 256. We find that using a smaller learning rate compared to 1 × 10−3 leads to a better downstream performance, which is potentially due to the diverse nature of our IFT mixture. Both input and target sequence length are set to 1024.""
""We train all the models for 30,000 update steps with data packing enabled.16 This results in a training budget of 25M samples. """,,Apache 2.0
Qwen1.5-72B,2/4/2024,SOTA improvement,72000000000,1.3e+24,3E+12,,,,,,,Unspecified unreleased,Confident,,,,Open weights (restricted use),Unreleased,,,,Operation counting,"#1 in C-Eval (84.1, better than Qwen-72B. https://qwenlm.github.io/blog/qwen1.5/, https://cevalbenchmark.com/static/leaderboard.html)",72B,"3T training tokens: https://github.com/QwenLM/Qwen2/issues/97 

6 * 72 billion * 3 trillion = ~1.3e24","""We pretrained the models with a large amount of data, and we post-trained the models with both supervised finetuning and direct preference optimization.""",3 trillion tokens from this response https://github.com/QwenLM/Qwen2/issues/97,,,,,"restriction on >100m monthly users:

https://huggingface.co/Qwen/Qwen1.5-72B/blob/main/LICENSE"
Qwen-VL-Max,1/25/2024,SOTA improvement,7000000000,,,,,,,,,Unspecified unreleased,Confident,,,,API access,,,,,,"""Notably, Qwen-VL-Max outperforms both GPT-4V from OpenAI and Gemini from Google in tasks on Chinese question answering and Chinese text comprehension""","Not stated. Qwen-VL (less capable, presumably smaller version) is 9.6B

Upd: 7B parameters mentioned here
https://github.com/QwenLM/Qwen-VL#qwen-vl-plus",,,,,,,,https://help.aliyun.com/zh/dashscope/developer-reference/tongyi-qianwen-vl-plus-api
AlphaGeometry,1/17/2024,SOTA improvement,151000000,,,,,,,,Google TPU v3,,Confident,,,,Open weights (unrestricted),Open source,Open source,,,,"""On a test set of 30 latest olympiad-level problems, AlphaGeometry solves 25, outperforming the previous best method that only solves ten problems and approaching the performance of an average International Mathematical Olympiad (IMO) gold medallist.""","""Overall, the transformer has 151 million parameters, excluding embedding layers at its input and output heads.""","Training details. Don't think there's enough info for a FLOP estimate.

""Our customized tokenizer is trained with ‘word’ mode using
SentencePiece36 and has a vocabulary size of 757. We limit the maximum context length to 1,024 tokens and use T5-style relative position embedding37. Sequence packing38,39 is also used because more
than 90% of our sequences are under 200 in length. During training, a
dropout40 rate of 5% is applied pre-attention and post-dense. A 4 × 4 slice of TPUv3 (ref. 41) is used as its hardware accelerator. For pretraining, we train the transformer with a batch size of 16 per core
and a cosine learning-rate schedule that decays from 0.01 to 0.001
in 10,000,000 steps. For fine-tuning, we maintain the final learning rate of 0.001 for another 1,000,000 steps""",Synthetic dataset of geometry proofs,"100m examples of theorem-proofs

""By using existing symbolic engines on a diverse set of random theorem premises, we extracted 100 million synthetic theorems and their
proofs, many with more than 200 proof steps, four times longer than
the average proof length of olympiad theorems.""",,,,,"Apache 2.0: https://github.com/google-deepmind/alphageometry 

Data is synthetic so can be reproduced using open code"
Palmyra X 003,1/1/2024,SOTA improvement,72000000000,,,,,,,,,,,,,,API access,,,,,,"""Palmyra X 003 launched as the #3 model on Stanford’s HELM benchmark and has since claimed the top spot in translation accuracy.""",,,,,,,,,
Kimi Explorer,1/1/2024,SOTA improvement,,,,,,,,,,,Unknown,,,,,,,,,,"Highest score at SuperCLUE benchmark for ""AISearch"" in term of ""Basic Capabilities https://www.superclueai.com/",,,,,,,,,
CoRe,12/29/2023,SOTA improvement,12400000000,,,,,,,,NVIDIA A100 SXM4 40 GB,"GSM8K,ASDiv",Speculative,GPT-J-6B,,,,,,,,,"We evaluate our CoRe framework on several mathematical reasoning datasets and achieve decent improvement over state-of-the-art methods, up to 9.6% increase over best baselines.","""Since the default setting consists of two GPT-J (6B) and a DeBERTa-large (0.4B), we note our backbone as “GPT-J 12B”, which implies around 12.4 billion parameters in total. """,,"We consider several widely-used math word problem datasets: GSM8K (Cobbe et al., 2021), ASDivA (Miao et al., 2020), SingleOp (Roy et al., 2015), SinlgeEq (Koncel-Kedziorski et al., 2015) and MultiArith (Roy and Roth, 2015). (Details in Appendix A). Following the general setting as in (Kojima et al., 2022; Wei et al., 2022c), we employ accuracy as the evaluation metric for all datasets.",,,,,,
Gemini Nano-2,12/19/2023,Significant use,3250000000,,,,,,,,Google TPU v5e,Unspecified unreleased,Confident,,,,Unreleased,,,,,,"Significant use; deployed on Android phones such as the Pixel: https://store.google.com/intl/en/ideas/articles/pixel-feature-drop-december-2023/

""Despite their size, they show exceptionally strong performance on factuality,
i.e. retrieval-related tasks, and significant performance on reasoning, STEM, coding, multimodal and multilingual tasks""",3.25B,"More tokens than Chinchilla-optimal:

""The number of tokens used to train the largest models were determined following the approach in Hoffmann et al. (2022). The smaller models are trained for significantly more tokens to improve performance for a given inference budget, similar to the approach advocated in Touvron et al. (2023a)""

Chinchilla was 1.4T tokens for 70B params, so Chinchilla-optimal for 3.25B params would be ~1.4T/20 = 70B tokens.

So compute was significantly greater than 3.25B * 70B * 6, which is 1.4e21. 

Touvron et al. is the Llama 1 paper, in which a 6.7B model is trained for 1T tokens. Using the same ratio, a 3.25B model would be trained on ~500B tokens. 3.25 * 500B * 6 = 9.75e21. No guarantee that the exact ratio for Nano is close to Llama's, of course.","""Gemini models are trained on a dataset that is both multimodal and multilingual. Our pre-training dataset uses data from web documents, books, and code, and includes image, audio, and video data.""",,,,,,"May be API access in the future. There is an Android API but it ""is under a closed early access preview program at this time"": https://ai.google.dev/gemini-api/docs/get-started/android_aicore"
Gemini Nano-1,12/19/2023,Significant use,1800000000,,,,,,,,Google TPU v5e,Unspecified unreleased,Confident,,,,Unreleased,,,,,,"Significant use; deployed on Android phones such as the Pixel: https://store.google.com/intl/en/ideas/articles/pixel-feature-drop-december-2023/

""Despite their size, they show exceptionally strong performance on factuality,
i.e. retrieval-related tasks, and significant performance on reasoning, STEM, coding, multimodal and multilingual tasks""",1.8B,"More tokens than Chinchilla-optimal:

""The number of tokens used to train the largest models were determined following the approach in Hoffmann et al. (2022). The smaller models are trained for significantly more tokens to improve performance for a given inference budget, similar to the approach advocated in Touvron et al. (2023a)""","""Gemini models are trained on a dataset that is both multimodal and multilingual. Our pre-training
dataset uses data from web documents, books, and code, and includes image, audio, and video data.""",,,,,,"May be API access in the future. There is an Android API but it ""is under a closed early access preview program at this time"": https://ai.google.dev/gemini-api/docs/get-started/android_aicore"
FunSearch,12/14/2023,"SOTA improvement,Historical significance",15000000000,3.87e+23,0,,48,,,,,,Speculative,PaLM 2,0,,Open weights (unrestricted),Unreleased,,,,Hardware,Improved SOTA for the cap set problem. Can plausibly claim the first instance of a LLM system making a genuine and novel scientific contribution.,"From the section called ""Pretrained LLM"": ""We use Codey, an LLM built on top of the PaLM2 model family... Because FunSearch relies on sampling from an LLM extensively, an important performance-defining tradeoff is between the quality of the samples and the inference speed of the LLM. In practice, we have chosen to work with a fast-inference model (rather than slower-inference, higher-quality)""

Unclear which PaLM2 model was used (of Gecko, Otter, Bison, and Unicorn); above quote indicates it was perhaps Otter or Bison, but not Unicorn. Exact parameter counts are not publicly disclosed for any of these models. In comparisons where FunSearch uses StarCoder-15B, Codey is an improvement but not obviously of an entirely different model class.

I report the 15B parameters from StarCoder-15B, used as an open-source comparison","Appendix A.5: ""Finding the full-sized symmetric admissible set I(15, 10) required the generation and analysis of approximately two million programs... To reproduce admissible set experiments done above (generating 2 million samples) one would have to use 15 instances of StarCoder-15B running on A100 40 GB GPU each and 5 CPU servers (each running 32 evaluators in parallel) for two days. We estimate that when running on Google Cloud, the price of an experiment is around $800 – $1400, and the energy usage around 250 – 500 kWh; i.e., 0.5% of the energy used for training StarCoder""

15 GPUs * 7.80E+13 FLOP/GPU-sec * 2 days * 24 hours/day * 3600 sec/hour = 2.02e20 FLOP for the GPU servers

We should also add the compute used to train the PaLM2 variant used as the base LLM. Since we don't have any details about this model, I use the compute from StarCoder-15B (used as the open source comparison point): 3.87e+23 FLOP

Unclear how to evaluate the compute from the CPU servers implementing the evolutionary algorithm, but this is very likely dwarfed by the pre-training compute for the LLM.","""The experiments carried out in this paper do not require any data corpus other than the publicly available OR-Library bin packing benchmarks""","""The experiments carried out in this paper do not require any data corpus other than the publicly available OR-Library bin packing benchmarks""","Appendix A.5: ""To reproduce admissible set experiments done above (generating 2 million samples) one would have to use 15 instances of StarCoder-15B running on A100 40 GB GPU each and 5 CPU servers (each running 32 evaluators in parallel) for two days""","Appendix A.5: ""We estimate that when running on Google Cloud, the price of an experiment is around $800 – $1400, and the energy usage around 250 – 500 kWh; i.e., 0.5% of the energy used for training StarCoder"" (in reference to a replication done using StarCoder-15B)

Estimate (800+1400)/2 = $1100 at time of publication. CPI conversion to 2020 dollars: $929",No finetuning,,"Code to run FunSearch with an LLM of your choice is open source under Apache 2.0 (software) and CC-BY (all else). However, the actual LLM used in the main experiments is unknown and may or may not be one of the Codey models available via API access.

(in other words code is available for the search tool but not for the model): https://github.com/google-deepmind/funsearch "
CogAgent,12/14/2023,SOTA improvement,18000000000,6.707e+22,287000000,,,,,,,"COYO-700M,LAION-2B,Common Crawl,Unspecified unreleased",Likely,CogVLM-17B,,,Open weights (restricted use),Open source,Open source,,,Operation counting,See Table 1,,"States 12.6 TFLOP per 1120x1120 image forward pass. Trained 60k steps with 4608 batch size, and then 10k with 1024 batch size.
12.6 TFLOP * (60000*4608 + 10000*1024) = 3.76e21

Uses pretrained CogVLM as base (6.331e22 FLOP), along with EVA2-CLIP-L. EVA2-CLIP-L's FLOPs are potentially estimable, but based on details about EVA2-CLIP-g/14 (a larger model), they likely contribute negligibly to CogAgent.

Sum: 6.707e22","From section 2.3, pretraining data includes:
- Synthetic renderings with text from language pre-training dataset (80M image-text pairs)
- OCR from natural images from COYO [6] and LAION-2B (18M image-text pairs)
- Academic documents (9M image-text pairs)
- Visual grounding dataset with bounding boxes (see CogVLM) (40M image-text pairs)
- Common Crawl Screenshot 400K (400k images with  140M QA pairs)",,,,,,"Code is Apache License 2.0; model is under a more restrictive custom licence which still allows commercial usage but which limits uses undermining Chinese national security and unity.

finetune code (this model is a finetune): https://github.com/THUDM/CogVLM/blob/main/finetune_demo/finetune_cogagent_demo.py "
Mixtral 8x7B,12/11/2023,Significant use,46700000000,,,,,,,,,,Confident,,,,Open weights (unrestricted),Unreleased,,,,,"Frequently downloaded: https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1

Probably the best OS model by a big margin at time of release, e.g. #7 on Chatbot Arena, above Gemini Pro and Claude 2.1: https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard
","46.7B *sparse* params. 12.9B params used on average:

""Concretely, Mixtral has 46.7B total parameters but only uses 12.9B parameters per token. It, therefore, processes input and generates output at the same speed and for the same cost as a 12.9B model.""",,"""Mixtral is pretrained with multilingual data using a context size of 32k tokens""",,,,,,Apache 2.0
SeamlessM4T,12/8/2023,SOTA improvement,2300000000,,,,,,,,NVIDIA V100,,Confident,W2v-BERT,,,Open weights (unrestricted),Open source,Open source,,,,"""As an improved version of SeamlessM4T,
SeamlessM4T v2 delivers state-of-the-art semantic accuracy across different speech and text translation tasks
while supporting nearly 100 languages as input speech or text""",2.3B,,"Several datasets including unlabeled speech, ASR data, TTS data",~5M hours of audio data (figure 2),,,expanded from 1M hours data to 4.5M hours,,"looks like code is MIT licensed, model is CC 4.0?
https://github.com/facebookresearch/seamless_communication?tab=readme-ov-file
train code: https://github.com/facebookresearch/seamless_communication/blob/main/src/seamless_communication/cli/m4t/finetune/trainer.py"
Llama Guard,12/7/2023,SOTA improvement,7000000000,1.6e+23,4096000,1,,,,,NVIDIA A100 SXM4 80 GB,,Confident,Llama 2-7B,1.7e+17,,Open weights (restricted use),Unreleased,,,,Operation counting,"""Llama Guard, a Llama2-7b model that is instruction-tuned on our collected dataset, albeit low in volume, demonstrates strong performance on existing benchmarks such as the OpenAI Moderation Evaluation dataset and oxicChat, where its performance matches or exceeds that of currently available content moderation tools. """,7B,"1.7e17 finetune compute, plus Llama 2-13B pretrain compute (1.6e+23)","Dataset of prompt-response pairs of human-AI conversations

""We leverage the human preference data about harmlessness from Anthropic (Ganguli et al., 2022). From
this dataset, we pick the first human prompt and discard the corresponding response from the assistant, as
well as all the other turns to create an initial single-turn prompt dataset. Next, we use one of our internal
Llama checkpoints to generate a mix of cooperating and refusing responses for these prompts. We employ
our expert, in-house red team to label the prompt and response pairs for the corresponding category based
on the taxonomy defined in Section 2. The red-teamers annotate the dataset for 4 labels: prompt-category,
response-category, prompt-label (safe or unsafe), and response-label (safe or unsafe). During the annotation
process, we also do data cleaning, and discard examples with badly formatted inputs or outputs. The final
dataset comprises of 13,997 prompts and responses, with their respective annotations.""","14k prompt-response pairs. Based on training details it's trained on ~4M tokens, which is stated to be ~1 epoch:
2 * 4096 * 500 = 4,096,000
(batch size) * (sequence length) * (steps)",,,"""We train on a single machine with 8xA100 80GB GPUs using a batch size of 2, with sequence length of 4096, using model parallelism of 1 and a learning rate of 2 × 10−6. We train for 500 steps, which corresponds to ∼1 epoch over our training set.""

6 * 2*4096*500 * 7 billion = 1.7e17",,"Llama 2 license
https://github.com/facebookresearch/PurpleLlama/tree/main/Llama-Guard"
Gemini 1.0 Ultra,12/6/2023,"SOTA improvement,Training cost",,5.0000000001e+25,,,2400,57000,,29827341.919963885,Google TPU v4,Unspecified unreleased,Speculative,,,,API access,Unreleased,,TRUE,24175462.27839436,"Benchmarks,Hardware","""Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined.""",,"This number is an estimate based on limited evidence. In particular, we combine information about the performance of Gemini Ultra on various benchmarks compared to other models, and guesstimates about the hardware setup used for training to arrive at our estimate. Our reasoning and calculations are detailed in this Colab notebook.
https://colab.research.google.com/drive/1sfG91UfiYpEYnj_xB5YRy07T5dv-9O_c","""Gemini models are trained on a dataset that is both multimodal and multilingual. Our pretraining dataset uses data from web documents, books, and code, and includes image, audio, and video data... We find that data quality is critical to a highlyperforming model, and believe that many interesting questions remain around finding the optimal dataset distribution for pretraining.""",,"Dylan Patel, author of SemiAnalysis, speculates that the training duration of Gemini may have been 100 days.",,,,API access: https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models
Gemini 1.0 Pro,12/6/2023,Significant use,,1.830001e+24,,,,,,,Google TPU v4,Unspecified unreleased,Speculative,,,,API access,Unreleased,,,,Benchmarks,"Default/free model on gemini.google.com

From paper:
""Broadly, we find that the performance of Gemini Pro outperforms inference-optimized models such as GPT-3.5 and performs comparably with several of the most capable models available, and Gemini Ultra outperforms all current models. In this section, we examine some of these findings.""",,"Training compute estimated from benchmark scores.

Our reasoning and calculations for Gemini 1 Ultra are detailed in this Colab notebook.
https://colab.research.google.com/drive/1sfG91UfiYpEYnj_xB5YRy07T5dv-9O_c

","""Gemini models are trained on a dataset that is both multimodal and multilingual. Our pretraining dataset uses data from web documents, books, and code, and includes image, audio, and video data... We find that data quality is critical to a highlyperforming model, and believe that many interesting questions remain around finding the optimal
dataset distribution for pretraining.""",,,,,,API access: https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models
Mamba-24M (SC09),12/1/2023,SOTA improvement,23400000,,305280000,100,,,,,,SC09,Confident,,,,,,,,,,"""SC09 is a benchmark speech generation dataset (Donahue, McAuley, and Puckette 2019; Warden 2018), consisting
of 1-second clips sampled at 16000 Hz of the digits “zero” through “nine” with highly variable characteristics. We
largely follow the autoregressive training setup and generation protocol of Goel et al. (2022).
Table 4 shows automated metrics of the Mamba-UNet model compared to a variety of baselines from Goel et al.
(2022): WaveNet (Oord et al. 2016), SampleRNN (Mehri et al. 2017), WaveGAN (Donahue, McAuley, and Puckette
2019), DiffWave (Z. Kong et al. 2021), and SaShiMi. A small Mamba model outperforms the state-of-the-art
(and much larger) GAN- and diffusion- based models. A larger model parameter-matched to the baselines further
improves on fidelity metrics dramatically.""",Table 4,,"""SC09 is a benchmark speech generation dataset (Donahue, McAuley, and Puckette 2019; Warden 2018), consisting of 1-second clips sampled at 16000 Hz of the digits “zero” through “nine” with highly variable characteristics""","Section 4.4.2: ""We largely follow the autoregressive training setup and generation protocol of Goel et al. (2022)""
In which they model raw audio waveforms, such that each sample is a datapoint.

SC09 is 5.3 hours long. 5.3h * 3600 sec/h * 16k samples/sec = 305,280,000 samples

Appendix E.4.2: ""We used a learning rate of 0.002 and 200000 training steps at a batch size of 16... training went through 100 epochs""",,,,,
Qwen-72B,11/30/2023,SOTA improvement,72000000000,1.3e+24,3E+12,,,,,,,,Confident,,,4000000,Open weights (restricted use),Unreleased,,,,Operation counting,"SOTA on several Chinese benchmarks, with highest average rating overall for Chinese benchmarks:

https://opencompass.org.cn/leaderboard-llm",72B,"72 billion params, 3 trillion tokens
72b * 3T * 6 = 1.3e24","""It is pretrained on over 3 trillion tokens, including Chinese, English, multilingual texts, code, and mathematics, covering general and professional fields""",Assuming not trained for multiple epochs.,,,,"Table 1 https://arxiv.org/abs/2309.16609
(this is uncertain because this table only lists sizes up to 14B. 72B was released after the paper)","up to 100m active users:
https://github.com/QwenLM/Qwen/blob/main/Tongyi%20Qianwen%20LICENSE%20AGREEMENT"
PPLX-70B-Online,11/29/2023,Significant use,70000000000,,,,,,,,,,Likely,Llama 2-70B,,,API access,,,,,,"Probably significant use: ""Perplexity, which has just 41 employees and is based out of a shared working space in San Francisco, has 10 million monthly active users, an impressive number for a young start-up."" However, this includes everyone who uses Perplexity's app which also uses third party models like GPT-4.

https://www.nytimes.com/2024/02/01/technology/perplexity-search-ai-google.html

",70B,,"Fine-tuned on website excerpts:

""Website excerpts, which we call “snippets”, are provided to our pplx-online models to enable responses with the most up-to-date information.

Fine-tuning: our PPLX models have been fine-tuned to effectively use snippets to inform their responses. Using our in-house data contractors, we carefully curate high quality, diverse, and large training sets in order to achieve high performance on various axes like helpfulness, factuality, and freshness.""",,,,"""Fine-tuning: our PPLX models have been fine-tuned to effectively use snippets to inform their responses. Using our in-house data contractors, we carefully curate high quality, diverse, and large training sets in order to achieve high performance on various axes like helpfulness, factuality, and freshness. Our models are regularly fine-tuned to continually improve performance.""",,
GNoME for crystal discovery,11/29/2023,Historical significance,16240000,,69000,1000,,4,,,Google TPU v4,,Likely,,,,Unreleased,Unreleased,,,1696.636272390938,,Economic impacts from development of commercially and socially valuable protein designs and materials,"""The pretrained potential has 16.24 million parameters.""
This refers to the GNoME network, which is a ""Gaussian Network Model of Energy"" for predicting crystal potential energy of new crystals.","Pretraining involved 36.43M steps:
""The learning rate was decreased to a new value of 2 × 10−4 after approximately 23 million steps, to 5 × 10−5 after a further approximately 11 million steps and then trained for a final 2.43 million steps. Training was performed on four TPU v3 chips.""

""Inference on an A100 GPU on a 50-atom system takes approximately 14 ms""
3.12e14 FLOP/s * 0.014 s = 4.368e12 FLOP per 50 atom system

Batch size was 32. Multiply inference FLOPs by 3 to account for forward and backward passes during training.
32 * 4.368e12 * 36.43 million * 3 = 1.53e22

This seems implausible – on 4 TPUv3 chips this would take
(1.53e22 / (4 * 1.23e14)) / (3600 * 24) = 360 days","""Initial models are trained on a snapshot of the Materials Project from 2018 of approximately 69,000 materials""","""Initial models are trained on a snapshot of the Materials Project from 2018 of approximately 69,000 materials""",,,,,
Inflection-2,11/22/2023,"Significant use,Training cost",,1.001e+25,,,,5000,,12961959.001361668,NVIDIA H100 SXM5 80GB,Unspecified unreleased,Confident,,,,Hosted access (no API),Unreleased,,TRUE,7732579.928411527,"Hardware,Benchmarks","Inflection-2 either already powers Pi or soon will: https://inflection.ai/inflection-2

Inflection has claimed that Pi has >1m users: https://x.com/inflectionAI/status/1699100179390210091?s=20",,"""Inflection-2 was trained on 5,000 NVIDIA H100 GPUs in fp8 mixed precision for ~10²⁵ FLOPs""

(the second 1 is there because of airtable being wonky, it's not a real sig fig)",,,,,,,"via Pi, no API"
Claude 2.1,11/21/2023,Significant use,,,,,,,,,,Unspecified unreleased,Unknown,Claude 2,,,API access,Unreleased,,,,,,,,,,,,,,
Nemotron-3-8B,11/15/2023,SOTA improvement,8000000000,1.8e+23,3.8E+12,,456,1024,0.3473,214467.02013524104,NVIDIA A100,"Unspecified unreleased,Flan,P3 (Public Pool of Prompts)",Confident,,,,Open weights (restricted use),,,,904992.9349364166,"Operation counting,Hardware","""The Nemotron-3-8B-QA model offers state-of-the-art performance, achieving a zero-shot F1 score of 41.99% on the Natural Questions dataset. This metric measures how closely the generated answer resembles the truth in ‌QA. """,,"https://huggingface.co/nvidia/nemotron-3-8b-base-4k

""This model was trained on a dataset containing 3.8 Trillion tokens of text""

8 billion * 3.8 trillion * 6 = 1.8e23

Also, using the hardware method: ""1,024 A100s were used for 19 days to train the model.""

19*1024 * 312 trillion * 24 * 3600 * 0.3 = 1.57e23","""NVIDIA models are trained on a diverse set of public and proprietary datasets. This model was trained on a dataset containing 3.8 Trillion tokens of text. The dataset contains 53 different human languages (including English, German, Russian, Spanish, French, Japanese, Chinese, Italian, and Dutch) and 37 programming languages. The model also uses the training subsets of downstream academic benchmarks from sources like FLANv2, P3, and NaturalInstructions v2""",,19 days,,,,"can't use to train other models:

https://developer.download.nvidia.com/ai-foundation-models/nvidia-ai-foundation-models-license-10Nov2023.pdf"
Qwen-Audio-Chat,11/14/2023,SOTA improvement,8460000000,,,,,,,,,,Likely,,,,Open weights (restricted use),,,,,,"""A notable achievement of Qwen-Audio is its state-of-the-art performance on the test set of Aishell1, cochlscene, ClothoAQA, and VocalSound""","the model has two components - audio and language.
670M + 7.7B = 8.46B
""The audio encoder is composed of 640M parameters""

""Qwen-Audio incorporates a large language model as its foundational component.
The model is initialized using pre-trained weights derived from Qwen-7B (Bai et al., 2023a). Qwen-7B is a 32-layer Transformer decoder model with a hidden size of 4096, encompassing a total of 7.7B parameters.""",,multiple audio and language sources,not clear,,,,,"Qwen license:

https://github.com/QwenLM/Qwen-Audio/blob/main/LICENSE"
GraphCast,11/14/2023,SOTA improvement,,2.1e+22,,,,,,,,,Speculative,,,,Open weights (unrestricted),,,,,Hardware,"""Our state-of-the-art model delivers 10-day weather predictions at unprecedented accuracy in under one minute""",Not mentioned in paper.,"""Training GraphCast took roughly four weeks on 32 Cloud TPU v4 devices using batch parallelism.""

4.6: ""we use bfloat16 floating point precision""

2.1e22 = 2.75E+14 FLOP/s * 32 * 60* 60 * 24 * 7 * 4","According to the blog post, ""we trained GraphCast on four decades of weather reanalysis data, from the ECMWF’s ERA5 dataset. This trove is based on historical weather observations such as satellite images, radar, and weather stations using a traditional NWP to ‘fill in the blanks’ where the observations are incomplete, to reconstruct a rich record of global historical weather.""
https://deepmind.google/discover/blog/graphcast-ai-model-for-faster-and-more-accurate-global-weather-forecasting/",,,,,,"Apache 2 
https://github.com/google-deepmind/graphcast"
Volcano 13B,11/13/2023,SOTA improvement,13000000000,4.56e+22,,1,30,,,,NVIDIA A100 SXM4 80 GB,"LAION,SBU,ShareGPT4V,Unspecified unreleased",Likely,LLaVA 1.5,8.1e+19,128,Open weights (non-commercial),,,,,Hardware,"""Volcano effectively reduces multimodal hallucination and achieves state-of-the-art on MMHal-Bench, POPE, and GAVIE"" (hallucination benchmarks)",13B,"Base model is LLaVa-1.5 13B, which used 4.55e22 FLOP (mostly coming from Llama base)

""For this research, we used an NVIDIA A100-SXM4-80GB GPU and an AMD EPYC 7513 32-Core Processor running at 2.0778 GHz. Training
VOLCANO 7B required 8 GPUs and took a total of 15 hours, while training VOLCANO 13B took 30 hours.""
3.12e14 * 8 * 30 * 3600 * 0.3 = 8.1e19 finetune compute","trained on synthetic data: ""To train VOLCANO, we collect initial responses for visual questions from an open-source LMM and
generate feedback and revisions using a proprietary
LLM as shown in Figure 3 (Akyürek et al., 2023;
Madaan et al., 2023; Ye et al., 2023b; Wang et al.,
2023d; Kim et al., 2023).""

https://huggingface.co/datasets/kaist-ai/volcano-train","https://huggingface.co/datasets/kaist-ai/volcano-train

558k image-text pairs, rest of dataset is ~1M examples of text data; length per sequence is not clear",,,"""For this research, we used an NVIDIA A100- SXM4-80GB GPU and an AMD EPYC 7513 32-Core Processor running at 2.0778 GHz. Training VOLCANO 7B required 8 GPUs and took a total of 15 hours, while training VOLCANO 13B took 30 hours""

= 8 * 312 teraflops * 30 * 3600 * 0.4 utilization (assumed)
= 8.1e19
",,"dataset and weights are open, no license"
SPHINX (Llama 2 13B),11/13/2023,SOTA improvement,19900000000,3.04e+22,,,290,32,,239188.6875340231,NVIDIA A100 SXM4 40 GB,"LAION-400M,LAION-COCO,RefinedWeb",Likely,Llama 2-13B,4.00000000001e+21,,Open weights (restricted use),Open (restricted use),Open access (restricted use),,28281.566571321346,Hardware,"""as shown in Figure 2, SPHINX can achieve impressive fine-grained visual perception for high-resolution images, which exhibits state-of-the-art performance on extensive evaluation benchmarks, e.g., MMBench (Liu et al., 2023f), MME (Fu et al., 2023a), and POPE (Li et al., 2023e).""","SPHINX + Llama 2 13B
SPHINX component involves four vision encoders:
- CLIP - ViT
- CLIP - ConvNeXt V2 (89M to 659M params, depending on size)
- DinoV2 - ViT (22M to 1.14B params, depending on size)
- Q-former (188M params)
Also involves to projection networks

Huggingface Hub model files appear to be 39.8GB. Assuming models are stored in fp16 there are 2 bytes per parameter, so 39.8 / 2 = 19.9B parameters.","""The pre-training time is around 125 hours on 32 A100 GPUs with a 7B
language model and about twice the time with a 13B language model... The fine-tuning takes about 38 hours with 16 A100 GPUs with a 13B
language model.""

((125*2 * 32) + (38 * 16)) * 3.12e14 * 3600 * 0.3 = 2.9e21

Component vision encoders were initialized from pre-trained:
- CLIP ViT: 1.5e22 FLOPs for L/14@336
- ConvNeXt V2: 6.8e21 FLOPs for largest
- DinoV2: 7.42e+21 FLOPs for largest
- Q-former: 1.2e21 FLOPs for largest

(Based on full parameter count, SPHINX probably uses largest models)

Sum: 3.04e22 FLOPs","""We use two image captioning datasets LAION-400M (Schuhmann et al.,
2021) and LAION-COCO (Schuhmann et al., 2022) for multi-modal alignment. As we full-finetune the language model backbone for long steps, we also jointly train with a text-only dataset RefinedWeb (Penedo et al., 2023) to avoid harming its text reasoning capability due to catastrophic forgetting.""",""" For the joint training on both images and texts, we form each
batch with 640 image-text pairs from LAION-400M or LAION-COCO and 65, 536 text tokens from RefinedWeb""

As per Figure 7 there was 20000 training steps","""The pre-training time is around 125 hours on 32 A100 GPUs with a 7B
language model and about twice the time with a 13B language model.""
"" The fine-tuning takes about 38 hours with 16 A100 GPUs with a 13B
language model.""",,"32 A100 * 312 TFLOPS/A100 * 290 hours * 40% utilization ~= 4e21 FLOP
https://www.wolframalpha.com/input?i=250+hours+*+312+TFLOPS+*+32+*+0.4",,"https://github.com/Alpha-VLLM/LLaMA2-Accessory

looks like same as LLama license

finetune code: https://github.com/Alpha-VLLM/LLaMA2-Accessory/tree/main/SPHINX "
MultiBand Diffusion,11/8/2023,SOTA improvement,,2.6e+19,,,48,,,22.81032329809286,NVIDIA V100,"Common Voice,DNS,MTG-Jamendo,FSD50K,AudioSet",Confident,,,,Open weights (unrestricted),Open source,Open source,,,Hardware,"""At equal bit rate, the proposed approach outperforms state-of-the-art generative techniques in terms of perceptual quality""",,"""It takes around 2 days on 4 Nvidia V100 with 16 GB to train one of the 4 models.""

125 tflop/s for V100 SXM (not clear which they used, could be PCI given small number - still same OOM thus confident)
4 * 125 trillion * 2 * 24 * 3600 * 0.3 = 2.6e19","""We train on a diverse set of domains and data. We use speech from the train set of Common Voice 7.0 (9096 hours) [Ardila et al., 2019] together with the DNS challenge 4 (2425 hours) [Dubey et al., 2022]. For music, we use the MTG-Jamendo dataset (919h) [Bogdanov et al., 2019]. For the environmental
sound we use FSD50K (108 hours) [Fonseca et al., 2021] and AudioSet (4989 hours) [Gemmeke et al., 2017]. We used AudioSet only for the research that is described in the publication and for the benefit of replicability. For evaluation, we also use samples from an internal music dataset.""",9096+2425+919+108+4989=17537 hours,around 2 days,,,,"training, inference, and models (MIT)
https://github.com/facebookresearch/audiocraft/blob/main/docs/MBD.md "
OmniVec,11/7/2023,SOTA improvement,,,,2000,,,,,,"AudioSet,Something-Something v2 (SSv2),English Wikipedia,ImageNet-1k,SUN RGB-D,ModelNet40",Unknown,BERT-Large,,,,,,,,,"Table 13.

E.g. SOTA on ImageNet at 92.4 top-1 accuracy",,,"Many datasets across several modalities

""We use AudioSet (audio) [23], Something-Something v2 (SSv2)(video) [31], English Wikipedia (text), ImageNet1K (image) [17], SUN RGB-D (depth maps) [71], ModelNet40 (3D point cloud) [93] for pretraining the network.""",,,,"Appears to build on several models, like BERT and ViT (Table 1)",,
mPLUG-Owl2,11/7/2023,SOTA improvement,7120000000,,400672000,1,,,,,,"Conceptual Captions (CC3M),Conceptual Captions 12M (CC12M),COCO,LAION,COYO-700M",Speculative,Llama 2-7B,1.7000000001e+19,,Open weights (unrestricted),,,,,,"""Extensive experiments illustrate the effectiveness and generalization abilities of mPLUG-Owl2, which achieves state-of-the-art performance on 8 classic vision-language benchmarks using a single generic model.""","""As depicted in Figure 2, our model, referred to as mPLUGOwl2, is composed of three main components: a fundamental vision encoder, a visual abstractor, and a language decoder. Specifically, we utilize ViT-L/14 as the
vision encoder and LLaMA-2-7B [58] as the language decoder""
ViT-L/14 has 123M parameters and Llama 2 7B has 7B parameters.","ViT-L/14 and Llama 2-7b compute, plus 1.7e19 joint pretrain FLOP (6 * 400M * 7.1B) and 4e16 joint finetune FLOP. Everything is a negligible fraction except the Llama 2 compute.","""mPLUG-Owl2 is first pre-trained on image-text pairs and fine-tunes on mono-modal and multi-modal instruction data. For pre-training data, we randomly pick about 400 million image-text pairs from five public datasets: Conceptual Captions (CC3M/CC12M) [9], COCO [35], Laion-en [49], COYO [7], DataComp [18]. For instruction data, we collect 5 types of datasets including 1) image captioning (i.e., TextCaps [53], COCO [35]); 2) image question answering (i.e., VQAv2 [21], OKVQA [43], OCR-VQA [44], GQA [24], and A-OKVQA [50]); 3) region-aware QA (i.e., Ref-COCO [69], VisualGenome [26]); 4) multi-modal instruct data (i.e., LLaVA-instruct-150K [38]); 5) text-only instruct data (i.e., ShareGPT-80K [1], SlimOrca [34]). Details can be found in the Appendix.""
According to the appendix, the instruction-tuning dataset was 1.23MB total across text, dialog, captions, and visual question-answering. This can't be much more than 1.5M updates per epoch, and the paper says ""For the instruction tuning stage, we train the whole model for 1 epoch with a learning rate of 2e-5 and batch size 256"".","400 million image-text pairs at pre-training + 672k image-text pairs at instruction tuning (table 14) = 400672000 images

+ 558k text instructions (table 14) ",,,https://www.wolframalpha.com/input?i=6+*+400+million+*+7.12+billion,,"Apache 2
https://github.com/X-PLUG/mPLUG-Owl/tree/main/mPLUG-Owl2"
GPT-4 Turbo,11/6/2023,SOTA improvement,,2.2e+25,,,,,,,,Unspecified unreleased,Unknown,,,,API access,Unreleased,,,,Benchmarks,"""More capable"" than GPT-4 according to OpenAI, with larger context window",Not known. Maybe smaller/sparser than GPT-4.,Estimated using benchmark imputation,,,,,,,
CogVLM-17B,11/6/2023,SOTA improvement,17000000000,6.331e+22,1518534581,,,,,,,"VQAv2,LAION-2B,COYO-700M,OKVQA,TextVQA,OCR-VQA,ScienceQA,LLaVA-Instruct-150k,LRV-Instruction,LLaVAR,Flickr30K Entities,RefCOCO,Visual7W,VisualGenome,COCO,TextCaps",Confident,Vicuna-7B v0,2e+22,,Open weights (restricted use),Unreleased,Open source,,,Reported,"""CogVLM-17B
achieves state-of-the-art performance on 17 clas-
sic cross-modal benchmarks, including 1) im-
age captioning datasets: NoCaps, Flicker30k, 2)
VQA datasets: OKVQA, TextVQA, OCRVQA,
ScienceQA, 3) LVLM benchmarks: MM-
Vet, MMBench, SEED-Bench, LLaVABench,
POPE, MMMU, MathVista, 4) visual grounding
datasets: RefCOCO, RefCOCO+, RefCOCOg,
Visual7W. Codes and checkpoints are available at
https://github.com/THUDM/CogVLM""","CogVLM-17B has 10 billion vision parameters and 7 billion language parameters. However, ""the total number of trainable parameters is 6.5B"".

""CogVLM model comprises four fundamental components: a vision transformer (ViT) encoder, an MLP adapter, a pretrained large language model (GPT), and a visual expert module.""

ViT: EVA2-CLIP-E, last layer removed (5B params with last layer, non-trainable)
MLP adapter: 2 layers, parameter count unavailable
GPT: Vicuna1.5-7B (7B params)
Visual expert module: parameter count unclear","from table 8 on page 17

230.1 FLOPS*days 
so 
10**15*24*3600*230.1= 1.988e22

Since this training uses pretrained weights from EVA02-CLIP-E and Vicuna1.5-7B, we report the full number of FLOPs baked into the model.

EVA02-CLIP-g/14 is stated to have taken 25 days to train 12B samples using 64 A100-40GB GPUs, implying: 
25 days * 24 hr/day * 3600 sec/hr * 64 GPU * 7.80E+13 FLOP/GPU-sec * 30% efficiency = 3.23e21

EVA02-CLIP-E doesn't give a training time; it saw 1/4 as many samples as the g/14 model but has 4.27x more parameters; as a rough estimate, assume it took the same number of FLOPs to train.

Vicuna1.5-7B's training compute is ~entirely embedded in the base Llama-7b weights, which took 4.02e+22 FLOPs

Total: 1.988e22 + 3.23e21 + 4.02e22 = 6.331E22","Pretraining uses LAION-2B, COYO-700M, plus a newly created visual grounding dataset of 40M images.
Generalist models CogVLM-Chat and CogVLM-Grounding are additionally finetuned on VQAv2, OKVQA, TextVQA, OCRVQA, ScienceQA, LLaVA-Instruct, LRV-Instruction, LLaVAR, Flickr30K Entities, RefCOCO, Visual7W, and VisualGenome.
Additional tests finetune on the training sets from COCO and TextCaps.","After filtering, about 1.5B image-text pairs are left for pretraining in stage one. Stage two of pretraining adds a visual grounding dataset of 40M images with generated noun bounding boxes. These are filtered from LAION-115M so that 75% of images contain at least two bounding boxes.

Two different kinds of finetuning are done, each using a number of datasets:
- CogVLM-Chat: VQAv2 (11059040), OKVQA (70275), TextVQA (453360), OCRVQA (1002146), ScienceQA (21208), LLaVAInstruct (150000), LRV-Instruction (300000), LLaVAR (1633000)
- CogVLM-Grounding: Flickr30K Entities (520000), RefCOCO (142209), Visual7W (889388), VisualGenome (1700000)

Additional experiments finetune using the training sets from COCO (413915 in train) and TextCaps (109765 in train)

In sum, pretraining and finetuning appear to contain 1,500,000,000 and 18,534,581 datapoints, respectively.",,,Trained from Vicuna1.5-7B weights,"8192 in pretraining stage 1, 1024 in stage 2","code is Apache, model more restrictive, commercial allowed, subject to PRC laws and interests

code isn't training code"
LLaVA 1.5,11/5/2023,SOTA improvement,13000000000,7.807e+22,1200000,1,24,8,,,NVIDIA A100,Unspecified unreleased,Confident,Vicuna-13B v0,7.008768e+19,,Open weights (restricted use),,,,7070.929307644795,Hardware,"from abstract: ""we establish stronger baselines that achieve state-of-the-art across 11 benchmark""","from abstract ""Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in ~1 day on a single 8-A100 node. ""","""Due to the increased image input resolution to 336^2, the training of LLaVA-1.5 is ∼2× as long as LLaVA: ∼6 hours of pretraining and ∼20 hours of visual instruction tuning using 8× A100s.""
26 * 3600 * 8 * 3.12e14 * 0.3 = 7.0e19

Fine-tuned from Vicuna-13B (itself finetuned from LLaMa-13B), which used 7.8e22 FLOPs

7.0e19 + 7.8e22",from https://huggingface.co/liuhaotian/llava-v1.5-13b#training-dataset,1.2M text-image pairs from https://huggingface.co/liuhaotian/llava-v1.5-13b#training-dataset,"from abstract ""Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in ~1 day on a single 8-A100 node. """,,"""Due to the increased image input resolution to 336^2, the training of LLaVA-1.5 is ∼2× as long as LLaVA: ∼6 hours of pretraining and ∼20 hours of visual instruction tuning using 8× A100s.""

26 * 3600 * 8 * 3.12e14 * 0.3 = 7.0e19",,Llama 2 license
Grok-1,11/4/2023,Training cost,3.14E+11,2.90000000001e+24,6.2E+12,,,,,,,Unspecified unreleased,Likely,,,,Open weights (unrestricted),Unreleased,,TRUE,,Benchmarks,"""On these benchmarks, Grok-1 displayed strong results, surpassing all other models in its compute class, including ChatGPT-3.5 and Inflection-1""","""314B parameter Mixture-of-Experts model with 25% of the weights active on a given token"". So effectively 78B parameters

Mixture of 8 experts: https://github.com/xai-org/grok-1","""On these benchmarks, Grok-1 displayed strong results, surpassing all other models in its compute class, including ChatGPT-3.5 and Inflection-1. It is only surpassed by models that were trained with a significantly larger amount of training data and compute resources like GPT-4""

Per table, Grok-1 is surpassed by Palm 2, Claude 2, GPT-4, so it required less compute than these three models. Palm 2 was trained on 7e24 FLOP.

GPT-3.5 is ~2.6e24. Inflection-1's compute is not public/known by us but Inflection says Inflection-1 compute was <= Palm-540B's (which was ~2.5e24). 

For optimal training, our current working hypothesis is that you still need something like Chinchilla scaling on the total number of parameters in the model, even for MoE models, so optimal dataset size would be 20*310B tokens. With 25%*314B params active per forward pass, this would be around 3e24 FLOP.
https://www.wolframalpha.com/input?i=20*310+billion+*+6+*+25%25+*+314+billion","""Base model trained on a large amount of text data, not fine-tuned for any particular task.""

""The training data used for the release version of Grok-1 comes from both the Internet up to Q3 2023 and the data provided by our AI Tutors.""","(Speculative confidence, see compute notes)",,,,,apache 2.0
RT-Trajectory,11/3/2023,SOTA improvement,,,,,,,,,,RT-1,Unknown,,,,,,,,,,"from blog https://deepmind.google/discover/blog/shaping-the-future-of-advanced-robotics/

""When tested on 41 tasks unseen in the training data, an arm controlled by RT-Trajectory more than doubled the performance of existing state-of-the-art RT models: it achieved a task success rate of 63%, compared with 29% for RT-2.""",seems to be based on the RT-1 architecture (35M parameters) with some modifications (section 3.3),"Given the architecture seems to use 35M parameters, it seems unlikely this is above 1e23 FLOP.","""We use the RT-1 (Brohan et al., 2023b) demonstration dataset for training""

also trained with retroactively-generated trajectories created by humans, by code written by GPT-4, and image generation models",,,,,,
BLUUMI,11/3/2023,SOTA improvement,1.76E+11,,38000000000,8,,,,,AMD Radeon Instinct MI250X,"Parsebank,mC4,Common Crawl,Wikipedia",Likely,BLOOM-176B,,4194304,Open weights (unrestricted),,,,,,"SOTA for Finnish: ""Our best monolingual model outperforms this result by over 10% points and the BLUUMI model by over 20% points, representing a substantial advance in the state of the art in the capability of generative models trained for Finnish.""",176 billion,,Finnish data from several sources,"38B tokens
""In total, the final pretraining dataset (including oversampling) consists of 38 billion tokens when processed with our Finnish tokenizer.""",,,"They ""continued pretraining"" of BLOOM on Finnish data. Don't think they specify the number of tokens they trained BLOOM/BLUUMI on; for their smaller models it was 300b.",Table 5.,"The BigScience RAIL License
https://turkunlp.org/gpt3-finnish
https://huggingface.co/TurkuNLP/bloom-finnish-176b"
Yi-34B,11/2/2023,Significant use,34000000000,6.1e+23,3.1E+12,,,128,,,NVIDIA A100,Unspecified unreleased,Confident,,,,Open weights (restricted use),Unreleased,,,113138.0969599182,Operation counting,"2nd most popular model on HuggingFace: https://decrypt.co/206195/new-open-source-ai-model-from-china-boasts-twice-the-capacity-of-chatgpt

also maybe the best open-source model, does better than Llama 2-70B on several benchmarks",34b,"""The dataset we use contains Chinese & English only. We used approximately 3T tokens"" sounds like this means it was trained on 3T tokens, not necessarily that the dataset contains 3T tokens?

If so, 34b * 3T * 6 = 6.1e23","Chinese and English dataset

""For pretraining, we construct 3.1 trillion tokens of English and Chinese corpora using a cascaded data deduplication and quality filtering pipeline. For finetuning, we polish a small scale (less than 10K) instruction dataset over multiple iterations such that every single instance has been verified directly by our machine learning engineers. For vision-language, we combine the chat language model with a vision transformer encoder and train the model to align visual representations to the semantic space of the language model.""","""language models pretrained from scratch on 3.1T highly-engineered large amount of
data, and finetuned on a small but meticulously polished alignment data.""",,,,,"apply for commercial license:
no training code
https://github.com/01-ai/Yi/blob/main/MODEL_LICENSE_AGREEMENT.txt

the model https://huggingface.co/01-ai/Yi-34B-Chat Apache 2.0
""If you create derivative works based on this model, please include the following attribution in your derivative works: ...."""
Cohere Embed,11/2/2023,SOTA improvement,,,,,,,,,,Unspecified unreleased,Unknown,,,,API access,Unreleased,,,,,"""We are releasing new English and multilingual Embed versions with either 1024 or 384 dimensions. All models can be accessed via our APIs. As of October 2023, these models achieve state-of-the-art performance among 90+ models on the Massive Text Embedding Benchmark (MTEB) and state-of-the-art performance for zero-shot dense retrieval on BEIR.""",,"https://docs.cohere.com/docs/environmental-impact

Embed v2 (older version) produced 6689.76 kg CO2 to train. Using the calculator Cohere links (https://mlco2.github.io/impact/) that's the equivalent of 80,000 TPUv3-hours in the ""us-west1"" region. That's 3.5e22 FLOP without considering utilization. However, I have no idea which region Cohere's GPUs are in (looks like CO2/energy can vary a lot by region), and they probably used a more recent GPU.",,"""First, they have been trained on questions and answers from a large web crawl. When we presented our multilingual-v2.0 model last year, we had a collection of over 1.4 billion question-and-answer pairs from 100+ languages on basically every topic on the internet.""

""Hence, the second stage involved measuring content quality. We used over 3 million search queries from search engines and retrieved the top-10 most similar documents for each query.""",,,,,
Skywork-13B,10/30/2023,SOTA improvement,13000000000,2.5e+23,3.18E+12,1,940,512,0.565,,NVIDIA A800,SkyPile,Confident,,,16000000,Open weights (restricted use),Open (restricted use),Open access (restricted use),,,Operation counting,"""We show that our model not only excels on popular benchmarks, but also achieves state of the art performance in Chinese language modeling on diverse domains""",13B,"""Our Skywork-13B is trained on a cluster of 64 NVIDIA-HGX-A800 nodes, a total of 512 A800-80G SXM GPUs... The training process of Skywork-13B spanned a total of 39 days.""

They note that ""we achieved a token throughput of 1873 per GPU per second and a model flops utilization (MFU) of 56.5%... "". 

""MFU"" was coined in the Palm paper (https://arxiv.org/pdf/2204.02311.pdf) and only counts operations used to train the model, not all operations observed on the hardware. MFU is lower than traditionally measured utilization.

Using the 56.5% number, and a peak tensor performance of 623.8 TFLOPS for the A800, this suggests 512 * 623.8 TFLOPS * 39 days * 86400 seconds/day * 0.565 = 6.08e23 FLOP.

Based on C=6ND, with 13B parameters and 3.2T tokens, we have C=6*(13B)*(3.2T)=2.5e23 FLOP.

Since the reported MFU is quite high, and would imply a higher compute usage than 6ND, it seems they may have trained on mixed precision and with the GPUs not always operating in the 623.8 TFLOPS mode.","""In order to train Skywork-13B, we build SkyPile, a vast, high quality corpus comprising more than 6 trillion tokens. A segment of the corpus, comprising over 150 billion tokens of web text, has been open sourced to facilitate research and training on Chinese LLMs""","The full SkyPile dataset is 6 trillion tokens, roughly half English and half Chinese: (https://huggingface.co/Skywork/Skywork-13B-base).

The model is trained for the equivalent of 0.53 epochs on the full dataset, or 3.18 trillion unique tokens. This is around 2.78 trillion words, based on an average of 1 word/token for the Chinese portion and 0.75 word/token on the English portion.",39 days,,,Table 3,"commercial but restrictive license: https://github.com/SkyworkAI/Skywork/blob/main/LICENSE

part of the training data is open, but only 2.5%: ""In order to train Skywork-13B, we build SkyPile, a vast, high quality corpus comprising more than 6 trillion tokens. A segment of the corpus, comprising over 150 billion tokens of web text, has been open sourced to facilitate research and training on Chinese LLMs""

training code: https://github.com/SkyworkAI/Skywork/blob/main/train/train.py "
ChatGLM3-6B,10/27/2023,SOTA improvement,6000000000,5.04e+22,1.4E+12,,,,,,,Unspecified unreleased,Likely,,,,Open weights (restricted use),Unreleased,,,,Operation counting,"Aiming at GPT-4V, ChatGLM3 has implemented iterative upgrades of several new functions this time, including:

CogVLM with multi-modal understanding capabilities, looks at image semantics, and achieved SOTA on more than 10 international standard image and text evaluation data sets;",6B from https://arxiv.org/abs/2406.12793,"Highly speculative.
Assume 1 epoch on 1.4T tokens.
6 FLOP/token/param * 1.4T tokens * 6B params=50.4 * 10 ^(12+9) = 5.04*10^(22)",ChatGLM2 corpus pretraining plus human preference alignment training,"""ChatGLM-6B was pre-trained on approximately one trillion tokens of Chinese and English corpus""
""By further realizing more diverse training datasets, more sufficient training steps, and more optimized training strategies, ChatGLM3-6B topped 42 benchmarks across semantics, mathematics, reasoning, code, and knowledge.""
The ChatGLM website states that the latest ChatGLM service is based on (and upgraded from) ChatGLM2, which was trained on 1.4T tokens. Assume that ChatGLM3 is trained on at least the same number of tokens.
Sources:
https://chatglm.cn/
https://github.com/THUDM/ChatGLM2-6B/blob/main/README_EN.md
https://www.zhipuai.cn/en/news/76

here (https://github.com/Kwai-Kolors/Kolors/blob/master/imgs/Kolors_paper.pdf) they confirm the dataset size ""Consequently, in Kolors, we utilize the open-source ChatGLM3-6B-Base as text encoder, which has been pre-trained with over 1.4 trillion bilingual tokens, resulting in a robust capability for Chinese language understanding.""",,,,,weights available with restricted license: https://huggingface.co/THUDM/chatglm-6b/blob/main/MODEL_LICENSE 
DiT-XL/2 + CADS,10/26/2023,SOTA improvement,675000000,,,,,,,,,ImageNet,Likely,DiT-XL/2,,,,,,,,,"""Further, using an existing pretrained diffusion model, CADS achieves a new state-of-the-art FID of 1.70 and 2.31 for class-conditional ImageNet generation at 256×256 and 512×512 respectively""",original parameter count for DiT-XL/2,,,,,,,,
CODEFUSION (Python),10/26/2023,SOTA improvement,75000000,7.92e+18,4390400,,11,4,,8.542235671062665,NVIDIA V100,,Confident,,,,,,,,2651.850782728711,Hardware,"See Table 1, SOTA in Python code generation",Table 1,"V100 performance: 125 teraFLOPS according to https://www.nvidia.com/en-us/data-center/v100/

11 hours * 4 GPUs * 125 teraFLOPS/GPU * 0.40 utilization = 7.92e18 FLOP",,"Section A3, Table 5: for python, 56k samples with an average length of 78.4 tokens","""The system used to run the experiments uses an Intel Core i7 processor (base at 1.8 GHz) along with 4 V100 GPU units, a 64-bit operating system, and 56 GB RAM. CODEFUSION took 8 hours to pre-train and 3 hours to fine-tune on average for each dataset.""",,,,
DALL·E 3,10/19/2023,SOTA improvement,,,,,,,,,,Unspecified unreleased,Unknown,,,,API access,Unreleased,,,,,,,,,,,,,,https://platform.openai.com/docs/models/dall-e
ERNIE 4.0,10/17/2023,Significant use,,,,,,,,,,,Unknown,,,,,,,,,,"Likely SOTA for Mandarin? But very little info available.

Lots of users (https://www.cnn.com/2023/12/15/tech/gpt4-china-baidu-ernie-ai-comparison-intl-hnk/index.html):

""Baidu says ERNIE has racked up 70 million users. That’s compared with 150 million users for ChatGPT, according to an estimate from Similarweb, a digital data and analytics company.""","""similar architecture with 3.5 version""  -interpreter dub at 01:25:08 https://www.youtube.com/watch?v=wYozcsavRuM",may be mentioned here https://www.youtube.com/watch?v=wYozcsavRuM,may be mentioned here https://www.youtube.com/watch?v=wYozcsavRuM,,,,,,
RT-2-X,10/13/2023,SOTA improvement,55000000000,,,,,,,,,Open X-Embodiment,Confident,RT-2,,,Unreleased,Unreleased,,,,,"""Emergent skills evaluation. To investigate the transfer
of knowledge across robots, we conduct experiments with
the Google Robot, assessing the performance on tasks like
the ones shown in Fig. 5. These tasks involve objects and
skills that are not present in the RT-2 dataset but occur in the
Bridge dataset [95] for a different robot (the WidowX robot).
Results are shown in Table II, Emergent Skills Evaluation
column. Comparing rows (1) and (2), we find that RT-2-X
outperforms RT-2 by ∼ 3×, suggesting that incorporating
data from other robots into the training improves the range
of tasks that can be performed even by a robot that already
has large amounts of data available. Our results suggest that
co-training with data from other platforms imbues the RT-2-
X controller with additional skills for the platform that are
not present in that platform’s original dataset.""",55B,,"""The Open X-Embodiment Dataset contains 1M+ real robot
trajectories spanning 22 robot embodiments, from single
robot arms to bi-manual robots and quadrupeds. The dataset
was constructed by pooling 60 existing robot datasets from
34 robotic research labs around the world and converting
them into a consistent data format for easy download and
usage. We use the RLDS data format [119], which saves data
in serialized tfrecord files and accommodates the various
action spaces and input modalities of different robot setups,
such as differing numbers of RGB cameras, depth cameras
and point clouds. It also supports efficient, parallelized data
loading in all major deep learning frameworks. For more
details about the data storage format and a breakdown of all
60 datasets, see robotics-transformer-x.github.io.""

""Note that the robotics data mixture used in our experiments includes 9 embodiments which is fewer than the entire Open X-Embodiment dataset
(22) – the practical reason for this difference is that we have
continued to extend the dataset over time, and at the time
of the experiments, the dataset above represented all of the
data""",,,,"""RT-2-X is trained via
co-fine-tuning (similarly to the original RT-2 [9]), with an approximately one to one split of the original VLM data
and the robotics data mixture. N""

RT-2 is in turn a fine-tune of Pali-X 55B",,
Ferret (13B),10/11/2023,SOTA improvement,13000000000,,,3,120,8,,,NVIDIA A100,GRIT,Confident,Vicuna-13B v0,4.04e+19,,Open weights (non-commercial),Open (non-commercial),Open source,,7072.612714018849,,"claimed SOTA on a new benchmark ""To evaluate this new capability, we introduce Ferret-Bench, covering three new types of tasks: Referring Description, Referring Reasoning, and Grounding in Conversation. We benchmark existing MLLMs and observe that Ferret can outperform the best of them by 20.4% on average.""",13B,"Fine-tuned from Vicuna-13B, which we don't have an estimate for. Finetuning cost is ~4e19.

""Training Details. We initialize the image encoder with CLIP-ViT-L/14@336p, the LLM with Vicuna, and the projection layer with LLaVA’s first-stage weights, leaving the visual sampler randomly initialized. After the initialization, Ferret is trained on the aforementioned GRIT data for three epochs, optimized by Loshchilov & Hutter (2017) with a learning rate of 2e − 5 and a batch size of 128. The training takes ∼5/2.5 days on 8 A100 GPU for a Ferret-13B/7B.""

5 * 24 * 3600 * 0.3 utilization (assumption) * 312 TFLOP/s = 4.04e19","""In order to make the refer-and-ground capability in Ferret open-vocabulary, instruction-following, and robust, we collect GRIT, a Ground-and-Refer Instruction-Tuning dataset with 1.1M samples. GRIT contains multiple levels of spatial knowledge, covering objects, relationships, region descriptions, and complex reasoning. It includes both text-in location-out (grounding) and location-in textout (referring) data, as well as data that mixes location and text in both input and output""",,"""The training takes ∼5/2.5 days on 8 A100 GPU for a Ferret-13B/7B.""",,"""The training takes ~5 days on 8 A100 GPU for a Ferret-13B""

5 * 24 * 3600 * 0.3 utilization (assumption) * 312 TFLOP/s = 4.04e19",,"https://github.com/apple/ml-ferret?tab=License-1-ov-file#readme
confusingly, the license page in the repo is permissive and MIT-like, but the README says ""The data, and code is intended and licensed for research use only. They are also restricted to uses that follow the license agreement of LLaMA, Vicuna and GPT-4. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should not be used outside of research purposes."" 
train script: https://github.com/apple/ml-ferret/blob/main/experiments/ferret_13b_train.sh "
FinGPT-13B,10/7/2023,SOTA improvement,13000000000,1.6e+23,,,17.25,1,,,NVIDIA GeForce RTX 3090,,Likely,Llama 2-13B,6.532488e+17,,Open weights (unrestricted),Open source,Open source,,773.5965245485567,Hardware,SOTA for financial sentiment analysis,"Finetunes using LoRA, so only trains 3.67 million parameters",From Llama 2-13B,Financial sentiment data (for fine-tuning): https://huggingface.co/datasets/FinGPT/fingpt-sentiment-train,,https://github.com/AI4Finance-Foundation/FinGPT?tab=readme-ov-file,"Finetuning cost for FinGPT v3.3 given as $17.25 at github repo; paper notes cost to train a financial model using their methods are ""typically"" between $100 - $300","fine-tuned Llama 2 13B

RTX 3090 for 17 hours, at a cost of $17

35.5 trillion flops * 17 * 3600 * 0.3 = 6.532488e+17",,"MIT license (though probably subject to Llama 2 license too)
https://github.com/AI4Finance-Foundation/FinGPT/blob/master/LICENSE

train code: https://github.com/AI4Finance-Foundation/FinGPT/blob/master/fingpt/FinGPT_Benchmark/train.sh "
CTM (CIFAR-10),10/1/2023,SOTA improvement,,,60000,1.7,,4,,,NVIDIA V100,CIFAR-10,Unknown,,,128,,,,,2652.482789257276,,"""CTM... achieves new state-of-the-art FIDs for single-step diffusion model sampling on CIFAR-10 (FID 1.73)""",,"Almost certainly <1e23 FLOP due to the small scale experiments.

""We use 4×V100 (16G) GPUs for CIFAR-10 experiment""
100K training iterations
",,100K training iterations / 60K images in the training dataset = 1.7 epochs,,,,,
Amazon Titan,9/28/2023,Training cost,2E+11,4.7999999999999996e+24,4E+12,,1152,13760,0.2696,,NVIDIA A100,,Likely,,,,API access,Unreleased,,TRUE,12166402.812291188,"Hardware,Operation counting",,"200B dense model
https://importai.substack.com/p/import-ai-365-wmd-benchmark-amazon","trained using NVIDIA NeMo: https://blogs.nvidia.com/blog/nemo-amazon-titan/

13,760 NVIDIA A100 chips (using 1,720 P4d nodes). It took 48 days to train.
from https://importai.substack.com/p/import-ai-365-wmd-benchmark-amazon

counting operations: 6*200000000000*4000000000000=4.8e+24

gpu usage: 312000000000000(FLOP/s)*0.3*13760*1152*3600=5.3413281792e+24",,"4T tokens of data, based on comments from amazon engineer James Hamilton at a 2024 talk: https://perspectives.mvdirona.com/2024/01/cidr-2024/
Also cited here:
https://lifearchitect.ai/titan/",,,,,
Show-1,9/27/2023,SOTA improvement,,,,,,,,,NVIDIA A100,WebVid-10M,Unknown,,,,Open weights (non-commercial),Unreleased,Open access (non-commercial),,,,"""Our approach achieves state-of-the-art performance on standard benchmarks including UCF-101 and MSR-VTT.""",,,"""WebVid-10M is a large-scale dataset of short videos with textual descriptions sourced from stock footage sites. The videos are diverse and rich in their content. 10.7M video-caption pairs. 52K total video hours.""","WebVid-10M
10.7M video-caption pairs. 52K total video hours.",,,,,"https://github.com/showlab/Show-1 don't see training code
Attribution-NonCommercial 4.0 International
"
GPT-4V,9/25/2023,Significant use,,,,,,,,,,Unspecified unreleased,Unknown,GPT-4,,,API access,Unreleased,,,,,Incorporated into ChatGPT,,,"""The pre-trained model was first trained to predict the next word in a document, using a large dataset of text and image data from the Internet as well as licensed sources of data. It was then fine-tuned with additional data, using an algorithm called reinforcement learning from human feedback (RLHF),[8, 9] to produce outputs that are preferred by human trainers.""",,,,,,
AlphaMissense,9/22/2023,SOTA improvement,93000000,,9000000,4,,,,,,"MGnify,UniRef90",Likely,AlphaFold 2,,,Unreleased,Open source,Open source,,,,"""By combining structural context and evolutionary conservation, our model achieves state-of-the-art results across a wide range of genetic and experimental benchmarks, all without explicitly training on such data."" [Abstract]","""The model architecture is similar to that of AlphaFold (21), with minor modifications""
Reference is to the AlphaFold 2 paper; that model had 93 million parameters","From supplementary materials: ""We independently trained three AlphaFold models and fine-tuned them independently on variants. We followed the training procedure described in (21), (only the “Initial training” stage) ... AF training is carried out for about 7e6 steps on single-chain structures ... Fine-tuning is carried out @until auROC of the evaluation set converges (about 350k samples, each training sample contains maximum 50 variants)""

Table S4 gives details. Total samples seen across the three pretraining models are (7.8M + 7.5M + 5.85M) = 21.15M

Each sequence is cropped to 256 elements long, which suggests 5.4B tokens seen in training.","Supplemental materials section on training data lists sources:
75% of pre-training structures are self-distillation data sampled from MGnify and UniRef90.

Fine-tuning data on benign variants come from gnomAD v2.1.1 (1.25M variants), the Great Ape project (95k variants), and FigShare (2k variants).
Fine-tuning data for pathogenic variants are sampled from the missense proteome map to create a dataset with balanced positive and negative labels.
Suggests a total of 2.7M variants, each 256 long.","7800000 samples - size of training dataset (see Table S4 in supplementary materials)
+1,345,605 variants for fine-tuning (but less could be used)  see Table S1

around 9000000 samples is quite confident estimation",,,,,"Apache for code. weights not released
https://github.com/google-deepmind/alphamissense"
Robot Parkour,9/12/2023,SOTA improvement,500000,,,,,,,,NVIDIA GeForce RTX 3090,,Confident,,,,,,,,,,,"Parkour policy details on page 8, table 11.","The paper provides some details on the training time and hardware used:

Each specialized skill policy (climbing, leaping, etc) was pre-trained with soft dynamics constraints for 12 hours using 1 Nvidia RTX 3090 GPU.
The skills were then fine-tuned with hard dynamics constraints for 6 hours each.
The final parkour policy distillation process used 4 computers with 1 RTX 3090 GPU each, training for an unspecified amount of time.
So the total training time was at least 12 + 6 x 5 = 42 hours for the initial skills, plus an additional unknown time for the distillation.

The hardware used was high-end Nvidia RTX 3090 GPUs, which at the time of paper writing would have been top of the line GPUs. Multiple GPUs were used in parallel during the distillation stage.","Isaac Gym simulated proprioceptive data, images, and actions",,,,,,
Falcon-180B,9/6/2023,Training cost,1.8E+11,3.76e+24,2.625E+12,1,4320,4096,0.1892,10340911.710964862,NVIDIA A100 SXM4 40 GB,RefinedWeb,Confident,,,4194304,Open weights (restricted use),Unreleased,,TRUE,3622388.5616717767,"Reported,Operation counting","""It's currently at the top of the Hugging Face Leaderboard for pre-trained Open Large Language Models and is available for both research and commercial use.""

""This model performs exceptionally well in various tasks like reasoning, coding, proficiency, and knowledge tests, even beating competitors like Meta's LLaMA 2.""","""Falcon 180B is a super-powerful language model with 180 billion parameters""","43,500 petaflop-days per Table 1 of the paper

43500 * 1e15 * 24 * 3600 = 3.76e24


C = 6ND = 6 FLOP/token/parameter * 3.5 trillion tokens * 180 billion parameters = 3.78*10^24 FLOP","""The Falcon series is made of three causal decoder-only models trained on up to 4,096 A100. We assembled a pretraining dataset of 3,500 billion tokens, predominantly sourced from our work on RefinedWeb (Penedo et al., 2023)–a massive filtered and deduplicated web dataset""

Training dataset composition is described in Table 3. Falcon was trained for 1 epoch.",3.5 trillion tokens * (~3 words per 4 tokens) ~= 2.625 trillion words,"Stanford CRFM foundation model ecosystem graph data page https://crfm.stanford.edu/ecosystem-graphs/index.html?asset=Falcon-180B says 9 months, which is the maximum possible amount of time: training began sometime in 2023, and it was released in September. 

However, 6 months is more realistic. That is the length of the gap between Falcon 40B and Falcon 180B. Additionally, the amount of compute is specified in the paper, so there is only one degree of freedom in the uncertain values of training duration and hardware utilization rate. At six months, the utilization is unusually low, so the training was probably not longer than that.","From Hugging Face:
""Falcon-180B was trained on up to 4,096 A100 40GB GPUs, using a 3D parallelism strategy (TP=8, PP=8, DP=64) combined with ZeRO.""
""Falcon-180B was trained on AWS SageMaker, on up to 4,096 A100 40GB GPUs in P4d instances.""
https://huggingface.co/tiiuae/falcon-180B

Utilization must have been at least 12.5%, and they probably did not use the whole 4096 GPU cluster for 9 months, so it was probably higher. Lower bound estimate:
https://www.wolframalpha.com/input?i=%286+FLOP+*+3.5+trillion+*+180+billion%29+%2F+%284096*312+teraFLOPS+*+9+months%29",,"from paper (https://arxiv.org/pdf/2311.16867.pdf):

Batch size 2048 (presumably sequences) per Table 16. Warmed up using smaller batches for first 100B tokens.

""All Falcon models are pretrained with a 2,048 sequence length""

2048*2048 = 4194304","""Falcon 180b can be commercially used but under very restrictive conditions, excluding any ""hosting use""."" https://huggingface.co/blog/falcon-180b"
Swift,8/30/2023,SOTA improvement,56804,5.337e+16,,,0.833,1,,,NVIDIA GeForce RTX 3090,,Likely,,,,Unreleased,,,,773.8775402191181,Hardware,"""Our work marks the first time, to our knowledge, that an autonomous mobile robot achieved world-champion-level performance in a real-world competitive sport.""","The control network is an MLP with input dimension 31, two hidden layers of size 128, and an output of dimension 4.
(31+1)*128+(128+1)*128+(128+1)*4 = 21124

Gate detector is a 6 layer U-net with 
8*(3^3*3+1) + 16*(3^2*8+1) + 16*(3^2*16+1) + 16*(5^2*16+1) + 16*(7^2*16+1) + 16*(7^2*16+1) = 35680

35680 + 21124 = 56804","Policies are trained for a total of 1 × 108 environment interactions, which takes 50 min on a workstation (i9 12900K, RTX 3090, 32 GB RAM DDR5). Fine-tuning is performed for 2 × 107 environment interactions.

35.58 TFLOPS * 50 min * 60 s/min * 0.50 utilization = 5.337*10^16 FLOP",,,"50 minutes (training details, page 8)",,,,
Jais,8/29/2023,SOTA improvement,13000000000,3.08e+22,3E+11,,600,,,,,"Abu El-Khair,Aranews,ArabicText 2022,C4 Arabic,Arabic Wikipedia,ArabicNews 2020,Maktabah,United Nations Parallel Corpus,The Pile,Books3,arXiv,PubMed Central,WebText2,English Wikipedia,FreeLaw,PubMed Abstracts,DeepMind Mathematics,Project Gutenberg,BookCorpus2,EuroParl,PhilPapers,YouTube Subtitles,NIH Grant Abstracts,Enron Emails,GitHub",Confident,,,3932160,Open weights (unrestricted),,,,,Operation counting,SOTA at Arabic language tasks.,"""With 13 billion parameters, they demonstrate better knowledge and reasoning capabilities in Arabic""",C = 6ND = 6 * 13 billion params * 395 billion tokens = 3.081e+22 FLOP,"It was pretrained on 395 billion tokens, including 116 billion Arabic tokens, 232 billion English tokens, and 46 billion tokens of code.
The Arabic data consists of 72 billion tokens, which was augmented by 18 billion tokens of translated English text and then upsampled 1.6 times to reach 116 billion tokens.
The English data is sampled from the Pile dataset and consists of 232 billion tokens.
The code data consists of 46 billion tokens sampled from GitHub.",395B tokens ~= 300B words,2023 June 25 to July 18 = 25 days = 600 hours,,,"""After packing, we used a global batch size of 1,920 sequences of 2,048 tokens each. """,apache 2.0
PeptideBERT,8/28/2023,SOTA improvement,,4.9e+16,21700000000.99997,30,4.067,1,,,NVIDIA GeForce GTX 1080 Ti,,Confident,ProtBERT-UniRef,4.980528e+16,,Open weights (unrestricted),Open source,Open source,,552.7802604940294,Hardware,"""Our model has achieved state of the art (SOTA) for predicting Hemolysis, which is a task for determining peptide’s potential to induce red blood cell lysis.""",,"""Compute for fine-tuning ProtBERT: 1 NVidia GeForce GTX 1080Ti, 30 epochs, batch size 32, model trained for individual tasks with training time ranging from 58-116 minutes, assuming 
from Table 1 we have 244 minutes
11.34e12 FLOPs and 0.3 utilization rate FLOP = 244 min * 60 sec/min * 11.34e12 FLOP/sec *0.3 = 4.9e16 FLOP,",,"Pretraining:
217,000,000 sequences × 100 residues = 2.17 × 10¹⁰ tokens

Fine-tuning sequences:
9,316 + 29,892 + 17,185 = 56,393 sequences
56,393 × 100 residues = 5.64 × 10⁶ tokens

Total tokens:
2.17 × 10¹⁰ + 5.64 × 10⁶ ≈ 2.17 × 10¹⁰",244 minues from Table 1,,"""Compute for fine-tuning ProtBERT: 1 NVidia GeForce GTX 1080Ti, 30 epochs, batch size 32, model trained for individual tasks with training time ranging from 58-116 minutes, assuming 
from Table 1 we have 244 minutes
11.34e12 FLOPs and 0.3 utilization rate FLOP = 244 min * 60 sec/min * 11.34e12 FLOP/sec *0.3 = 4.9e16 FLOP,",,"MIT (models, training, inference): https://github.com/ChakradharG/PeptideBERT "
Qwen-VL,8/24/2023,SOTA improvement,9600000000,,1400000000,1,,,,,,,Likely,Qwen-7B,,,Open weights (restricted use),Unreleased,,,,,"""As the results shown, our Qwen-VL and Qwen-VL-Chat both achieve obviously better results compared to previous
generalist models in terms of both two tasks. Specifically, on zero-shot image caption task, Qwen-VL achieves
state-of-the-art performance (i.e., 85.8 CIDEr score) on the Flickr30K karpathy-test split, even outperforms
previous generalist models with much more parameters (e.g., Flamingo-80B with 80B parameters).""",9.6B total - Table 1,"Qwen-7B and ViT as base models, trained on 1.5B image-text pairs","""Our pre-training dataset is composed of several publicly accessible sources and some in-house data.
We made an effort to clean the dataset of certain patterns. As summarized in Table 2, the original dataset
contains a total of 5 billion image-text pairs, and after cleaning, 1.4 billion data remain, with 77.3% English
(text) data and 22.7% Chinese (text) data.""",1.4B text-image pairs,,,"50k steps, 30k batch size (table 8)",,
GGNN,8/5/2023,SOTA improvement,,7.56e+21,,,,2,,,NVIDIA A100 SXM4 80 GB,,Confident,ESM2-650M,,,,,,,1769.2870682237342,Other,"""In this work, we integrate the knowledge learned by well-trained protein language models into several state-of-the-art geometric networks and evaluate a variety of protein representation learning benchmarks, including protein-protein interface prediction, model quality assessment, protein-protein rigid-body docking, and binding affinity prediction. Our findings show an overall improvement of 20% over baselines.""","ESM-2 650M is used as the main PLM, they run ablations with versions up to 3B. Unclear how many parameters are are in the geometric graph neural network module.","ESM-2 650M is very likely the majority of FLOPs, since they only used 2 A100s (ESM-2 650M used 512 V100s for 8 days). As such I'm reporting the compute from ESM-2 650M here only.",,,,,,,
RT-2,7/28/2023,SOTA improvement,55000000000,,,,,,,,,RT-1,Confident,PaLI-X,,,,,,,,,"""We compare our method to multiple state-of-the-art baselines that challenge different aspects of our method. All of the baselines use the exact same robotic data... Here, on average, both instantiations of RT-2 perform similarly, resulting in ∼2x improvement over the next two baselines, RT-1 and MOO, and ∼6x better than the other baselines""","""We train two specific instantiations of RT-2 that leverage pre-trained VLMs: (1) RT-2-PaLI-X is built from 5B and 55B PaLI-X (Chen et al., 2023a), and (2) RT-2-PaLM-E is built from 12B PaLM-E (Driess et al., 2023).""

55B and 12B have similar overall performance","""""For RT-2-PaLI-X-55B, we use learning rate 1e-3 and batch size 2048 and co-fine-tune the model for 80K gradient steps""
Sequence length not stated","""The vision-language datasets are based on the dataset mixtures from Chen et al. (2023b) and Driess et al. (2023). The bulk of this data consists of the WebLI dataset, which is around 10B image-text pairs across 109 languages, filtered to the top 10% scoring cross-modal similarity examples to give
1B training examples""
""The robotics dataset is based on the dataset from Brohan et al. (2022).""

Chen et al and Driess et al are the original Pali-X and Palm-E papers.  image-text web data

Brohan et al is the RT-1 paper",,,,"""For RT-2-PaLI-X-55B, we use learning rate 1e-3 and batch size 2048 and co-fine-tune the model for 80K gradient steps""

",,
AudioLM,7/26/2023,SOTA improvement,1500000000,3.9e+18,820800000,,,,,,Google TPU v4,LibriLight,Speculative,,,,,,,,,Operation counting,"Compared to other systems without text supervision, AudioLM achieves the highest
sWUGGY scores across both splits. Similarly, it also attains the
highest score in the sBLIMP metric, improving by 8% relative
over the previous state-of-the-art (CPC-BERT [59]).","""We use identical decoder-only Transformers in
all stages, with 12 layers, 16 attention heads, embedding
dimension of 1024, feed-forward layer dimension of 4096
and dropout of 0.1, together with T5-style relative positional
embeddings [38], resulting in a model parameter size of
0.3B per stage.""

Three stages (figure 2), and 300M per stage. Plus 600M parameters for w2v-BERT XL, so 1.5B total","""We train each stage on 16 TPUv4s with batch size of 256 for 1M steps.""

That's for the 900M-param transformers

If there's 256 passes in each batch, then using 6ND that's 900m * 256m * 6 = 1.3e18. sanity check: 16 tpu4s is 4.4e15 FLOP/s. 1.3e18 FLOP / 4.4e15 FLOP/s is 295 seconds. adjusting for utilization it would be ~1000 seconds or 15 minutes? probably too short, so 1.3e18 seems too low.

upd there are 3 stages -> 1.3e18*3 = 3.9e+18 (Speculative due to reasoning above)",,"60k hours of English speech
13680*60000 = 820800000 words

https://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.sxcem9l5k3ce",,,,,
Llama 2-70B,7/18/2023,"Historical significance,Significant use,Highly cited,Training cost",70000000000,8.1e+23,1.5E+12,1,1728,1000,0.419197502,1099604.9936612176,NVIDIA A100 SXM4 80 GB,Llama 2 dataset,Confident,,,4000000,Open weights (restricted use),Unreleased,,,884796.5963891526,"Hardware,Operation counting",Model has been open-sourced and frequently downloaded. The paper claims that Llama 2 is the current best open-source chat model as of its release date.,"Llama has been released in 7B, 13B, 34B, and 70B variants.","""Pretraining utilized a cumulative 3.3M GPU hours of computation on hardware of type A100-80GB"" of which 1720320 GPU hours were used to train the 70B model.

311.84 BF16 TFLOP/s * 1720320 hours * 0.40 utilization = 7.725e+23 FLOP.

Alternatively: the model was trained for 1 epoch on 2 trillion tokens and has 70B parameters. C = 6ND = 6*70B*2T = 8.4e+23 FLOP.","2 trillion tokens of publicly available text, with no text from Meta's products.
""Our training corpus includes a new mix of data from publicly available sources, which does not include data from Meta’s products or services. We made an effort to remove data from certain sites known to contain a high volume of personal information about private individuals. We trained on 2 trillion tokens of data as this provides a good performance–cost trade-off, up-sampling the most factual sources in an effort to increase knowledge and dampen hallucinations.""",2 trillion tokens ~= 1.5 trillion words,"Model was trained from January 2023 to July 2023, which is six months. However, the training run duration did not take up this whole period. According to a Meta employee interviewed by Epoch, Llama 2 34B and 70B were trained on different clusters, with overlapping training periods. Based on an estimate of 1000 GPUs, it would have taken 72 days.","A100 cost in 2023: $1.10/hour
Training time: 1720320 A100 GPU-hours
Inflation adjustment: $1.000 2020 = $1.145 2023",,,"Llama 2 license. can't use outputs to train models.

https://github.com/meta-llama/llama/blob/main/LICENSE"
Llama 2-7B,7/18/2023,"Historical significance,Significant use,Highly cited",70000000000,8.4e+22,1.5E+12,1,,,,114259.38527188863,NVIDIA A100 SXM4 80 GB,Llama 2 dataset,Confident,,,4000000,Open weights (restricted use),Unreleased,,,,"Hardware,Operation counting",Model has been open-sourced and frequently downloaded. The paper claims that Llama 2 is the current best open-source chat model as of its release date.,"Llama has been released in 7B, 13B, and 70B variants.","Trained on 2 trillion tokens per Table 1. 

C = 6ND = 6*7B*2T = 8.4e+22 FLOP.

Also, 7B model was trained on 184320 GPU-hours

312 trillion * 184320 * 3600 * 0.3 = 6.21e22","2 trillion tokens of publicly available text, with no text from Meta's products.
""Our training corpus includes a new mix of data from publicly available sources, which does not include data from Meta’s products or services. We made an effort to remove data from certain sites known to contain a high volume of personal information about private individuals. We trained on 2 trillion tokens of data as this
provides a good performance–cost trade-off, up-sampling the most factual sources in an effort to increase knowledge and dampen hallucinations.""",2 trillion tokens ~= 1.5T words,,"A100 cost in 2023: $1.10/hour
Training time: 184320 A100 GPU-hours
Inflation adjustment: $1.000 2020 = $1.145 2023

184320 * 1.10 / 1.145 = $177,075",,,"Llama 2 license. can't use outputs to train models.

https://github.com/meta-llama/llama/blob/main/LICENSE"
Claude 2,7/11/2023,Historical significance,,3.866e+24,,,,,,,,Unspecified unreleased,Speculative,,,,API access,Unreleased,,TRUE,,"Benchmarks,Hardware",,,https://colab.research.google.com/drive/1MdPuhS4Emaf23VXYZ-ooExDW-5GXZkw0#scrollTo=Ds0Q5X8aMnOY,"From model card: ""Claude models are trained on a proprietary mix of publicly available information from the Internet, datasets
that we license from third party businesses, and data that our users affirmatively share or that crowd workers provide. Some of the human feedback data used to finetune Claude was made public [12] alongside our RLHF [2] and red-teaming [4] research.
Claude 2’s training data cuts off in early 2023, and roughly 10 percent of the data included was non-English.""",,,,,,
xTrimoPGLM -100B,7/6/2023,"SOTA improvement,Training cost",1E+11,6.2e+23,,,3912,768,,1818526.294987458,NVIDIA A100 SXM4 40 GB,UniRef50,Confident,,,2097152,Unreleased,Unreleased,,,679602.2897189565,"Reported,Operation counting,Hardware","""Our extensive experiments reveal that xTrimoPGLM significantly outperforms other advanced baselines in diverse protein understanding tasks (13 out of 15 tasks across four categories)""","Abstract: ""training xTrimoPGLM at an unprecedented scale of 100 billion
parameters and 1 trillion training tokens""","""xTrimoPGLM-100B is trained on a cluster of 96 DGX-A100 GPU (8×40G) servers in FP16 precision from January 18 to June 30, 2023. During this time, xTrimoPGLM-100B has consumed 1 trillion tokens from the dataset consisting of Uniref90 and ColAbFoldDB. As of the current date, xTrimoPGLM-100B continues its pre-training process to pass through as many tokens as possible""

6 * 100 billion params * 1T tokens = 6e23

8*96 * 312 trillion * 163 days * 24 * 3600 * 0.3 ~= 1e24

directly given in the paper (Table 9, or Table 4 in new version): 6.2E+23 ",,~24M protein sequences,163 days,,,"""We employ batches of 2,048 sequences, each 1,024 tokens in length""",
InternLM,7/6/2023,SOTA improvement,1E+11,6.000001e+23,1000000000,,,,,,NVIDIA A100 SXM4 80 GB,,Confident,,,,,,,,,Operation counting,"(from Google-translated page) ""In addition to using academic datasets to evaluate InternLM, we also use human examinations to assess its capabilities. InternLM can achieve good scores on examination benchmarks such as MMLU, AGIEval, C-Eval, and GAOKAO-bench that cover different languages and subjects, scoring higher than ChatGPT on multiple benchmarks""",Pre-training a bilingual 100B Foundation model on data with over a trillion tokens,6 * 100b * 1t = 6e23,,"""Pre-training a bilingual 100B Foundation model on data with over a trillion tokens""",Training performance for the open-source InternLM-7B: https://github.com/InternLM/InternLM/blob/main/doc/en/train_performance.md,,,,
Pangu-Weather,7/5/2023,SOTA improvement,256000000,3.98e+22,,100,1536,192,,51279.01751905432,NVIDIA V100,ERA5,Confident,,,,Open weights (non-commercial),Unreleased,Open access (non-commercial),,127426.65686311126,Hardware,"""In meteorology, the Pangu Meteorology Model (or Pangu-Weather) is the first AI model to have surpassed state-of-the-art numerical weather prediction (NWP) methods in terms of accuracy. The prediction speed is also several orders of magnitude faster. In the past, predicting the trajectory of a typhoon over 10 days took 4 to 5 hours of simulation on a high-performance cluster of 3,000 servers. Now, the Pangu model can do it in 10 seconds on a single GPU of a single server, and with more accurate results.""

https://www.huaweicloud.com/intl/en-us/news/20230707180809498.html","4*64 million = 256M params

""We trained four deep networks with lead times (the time difference
between input and output) at 1 h, 3 h, 6 h and 24 h, respectively... 

This modification increases the number of bias parameters by a factor of 527, with each 3D deep network containing approximately 64 million parameters.""","""Each of the four deep networks was trained for 100 epochs, and
each of them takes approximately 16 days on a cluster of 192 NVIDIA
Tesla-V100 GPUs.""

192 * 4 * 16 * 24 * 3600 * 125 teraflops * 0.3 utilization = 3.98e22","""We used a single point in time for both input and output. The time resolution
of the ERA5 data is 1 h; in the training subset (1979–2017), there were
as many as 341,880 time points, the amount of training data in one
epoch""","""We used a single point in time for both input and output. The time resolution
of the ERA5 data is 1 h; in the training subset (1979–2017), there were
as many as 341,880 time points, the amount of training data in one
epoch... We fed all included weather variables, including 13 layers of upper-air
variables and the surface variables""

341,880 is the number of hours in ~40 years. But there's lots of data for each hour.","4*16 = 64 days
""Each of the four deep networks was trained for 100 epochs, andeach of them takes approximately 16 days on a cluster of 192 NVIDIA Tesla-V100 GPUs.""
",,"Possibly based on Pangu 3? Pangu-Weather is mentioned in the Pangu 3 announcement. But the architecture description doesn't seem to resemble Pangu 3. So it seems like Pangu-Weather is one of the higher-level models that can be attached to Pangu 3. 

https://www.huaweicloud.com/intl/en-us/news/20230707180809498.html 
",,"Models and code here: https://github.com/198808xc/Pangu-Weather 

Commercial use forbidden"
Stable Diffusion XL (SDXL),7/4/2023,Significant use,3400000000,,,,,,,,,Unspecified unreleased,Speculative,,,,,,,,,,Looks like this is now the main/flagship Stable Diffusion model,"""...result in a model size of 2.6B parameters in the UNet, see Tab. 1. The text encoders have a total size of 817M parameters.""",,,,,,,,
HyenaDNA,6/27/2023,SOTA improvement,6600000,1.811e+21,2945000000,679.1171477,672,8,,5000,NVIDIA A100,Human Reference Genome (GRCh38/hg38),Confident,,,64000000,,,,,7079.804571805214,Hardware,"""On fine-tuned benchmarks from the Nucleotide Transformer, HyenaDNA reaches state-of-the-art (SotA) on 12 of 18 datasets using a model with orders of magnitude less parameters and pretraining data.1 On the GenomicBenchmarks, HyenaDNA surpasses SotA on 7 of 8 datasets on average by +10 accuracy points, and by as much as +20 accuracy points on enhancer identification.""","Table A.1 shows details, largest experiment is on far right.","8 Nvidia A100 (80GB) GPUs, ~4 weeks
(4 * 7 * 24 * 3600) seconds * (8 * 3.12e14) FLOP/sec * 0.3 (utilization) = 1.811e21","""For pretraining, we use a single human reference genome [...] For the test set, we use chromosomes 14 and X, exclusively"" 
Human genome is ~3.2B nucleotide pairs, 14 and X are ~101M and 154M respectively.","Human genome is ~3.2B nucleotide pairs, 14 and X are ~101M and 154M respectively. Largest run sees 2T tokens, which implies ~679 epochs.","""For example, the largest model with context length 1M was trained on 2T tokens over 4 weeks.""",,,"Table A.1 indicates largest model saw sequence length of 1M, and that batch sizes range from 64-1024. In section 3.2: ""Our sequence length schedule starts at L1 = 64, then doubles the window at each stage while keeping the global batch size constant."" I assume smallest batch size was used for largest sequence length.",
ERNIE 3.5,6/27/2023,SOTA improvement,,,,,,,,,,,Unknown,,,,,,,,,,SOTA scores on AGIEval and MMLU. See article in China Science Daily: https://mp.weixin.qq.com/s/QVdkmofRSTgjQ7UOFX7s1g,,,,,,,,,
RoboCat,6/20/2023,SOTA improvement,1180000000,,,,,,,,,,Speculative,,,,,,,,,,,"""Most of the experimental results are based on models with a 1.18B-parameter decoder-only transformer (Vaswani et al., 2017) with 24 layers, an embedding size of 2048, and a post-attention feedforward hidden size of 8196."" page 8",,"""We use a diverse and large number of datasets for training RoboCat. These include data from agent experience, human demonstrations and self-generated data, on both simulated and real-world robot environments. See Section 3.4 for details on our datasets.""",,,,,,
MusicGen,6/8/2023,SOTA improvement,3359000000,,,,,,,,,ShutterStock and Pond5 music data collections,Likely,,,,,,,,,,"""We conduct extensive empirical evaluation, considering both automatic and human studies, showing the proposed approach is superior to the evaluated baselines on a standard text-to-music benchmark""","""We train autoregressive transformer models at different sizes: 300M, 1.5B, 3.3B parameters""

Uses EnCodec 32kHz (HF version has 59M params) for audio tokenization.","We train the 300M, 1.5B and 3.3B parameter models, using respectively 32, 64 and 96 GPUs, with mixed precision.

Unclear how many epochs used so FLOP calculation is not feasible.","""We use 20K hours of licensed music to train MUSICGEN. Specifically, we rely on an internal dataset of 10K high-quality music tracks, and on the ShutterStock and Pond5 music data collections with respectively 25K and 365K instrument-only music tracks. All datasets consist of full-length music sampled at 32 kHz with metadata composed of a textual description and additional information such as the genre, BPM, and tags.""","""We train on 30-second audio crops sampled at random from the full track... We use 20K hours of licensed music""

20000 hours * 60 min/hour * 2 inputs/min = 2400000 input sequences

EnCodec is run at 32kHz but after convolutions has a frame rate of 50 Hz, suggesting 2400000 * 30s * 50/s = 3,600,000,000 audio tokens.

Not confident enough in this calculation to add to database.",,,,,
LTM-1,6/6/2023,SOTA improvement,,,,,,,,,,,Unknown,,,,,,,,,,Very long context window - 5M tokens,,"Must be below 1e23 FLOP, as it's trained with a single A100.",,,,,,,
PaLI-X,5/29/2023,SOTA improvement,55000000000,,1400000000,,,,,,,WebLI,Likely,UL2,,,,,,,,,"""PaLI-X advances the state-of-the-art on most vision-and-language benchmarks considered (25+ of them).""",55B (table 1),,"""The main pretraining data for our model is based on WebLI [5], consisting of roughly one billion images with alt-texts from the web and OCR annotations (using the GCP Vision API), covering over 100 languages. In addition to WebLI ⟨image, text⟩ pairs, we introduce here Episodic WebLI data, where each episode corresponds to a set of such pairs. We aim to have each episode contain loosely related images (i.e., they are clustered according to their URL field), so as to encourage attention among examples in an “episode”. We find this new dataset (with 75M episodes and around 400M images in total) important for developing the few-shot capabilities of the model.""","1 billion images with alt texts in WebLI, 400m images in Episodic WebLI data",,,,,
Goat-7B,5/23/2023,SOTA improvement,7000000000,,,1,,,,,NVIDIA A10 PCIe,,Speculative,LLaMA-7B,2.02e+18,,Open weights (non-commercial),Open (non-commercial),Open source,,,,"""We introduce Goat, a fine-tuned LLaMA model that significantly outperforms GPT-4 on a range of arithmetic tasks. Fine-tuned on a synthetically generated dataset, Goat achieves state-ofthe-art performance on BIG-bench arithmetic sub-task.""",7B,2.78e+22 for base LLaMA-7B,"Model was fine-tuned from LLaMA-7B.

Fine-tuning dataset is a synthetic math dataset:

""We generate the dataset synthetically using a Python script. The dataset consists of around 1 million question-answer pairs. The answer contains
the proposed CoT as well as the final numerical output. The numbers are randomly generated, hence
ensuring a very low probability of instances being
duplicated, although small numbers may be sampled multiple times. We sample from log space to
ensure the numbers are equally likely to be sampled
from different orders of magnitude, which is similar to the sampling method used by Lee and Kim
(2023). The details of the dataset are presented in
Appendix F.""",Fine-tune dataset had 1 million question-answer pairs. likely ~10 tokens per pair?,,,"""Goat-7B can be easily fine-tuned using LoRA on a 24GB VRAM GPU... In particular, the fine-tuning process for a specific arithmetic sub-task, such as 8-digit addition using 100K instances, takes only approximately 1.5 hours on an A10 GPU to achieve near-perfect accuracy""

Info isn't very complete - no timeframe specified for the VRAM GPU, I'm not sure how many tokens are in the fine-tune dataset and they use LoRA. Maybe it's 15 A10-hours total (1M total instances)? But safe to assume it's a small fraction of Llama's pretrain compute.

125 trillion (A10 FLOPs) * 15 * 3600 * 0.3 = 2.02e18",,"no license noted. perhaps LLaMA 1 license by default (non-comm). repo with finetune (i.e. training since this is a Llama finetune) code
https://github.com/liutiedong/goat"
CodeT5+,5/20/2023,SOTA improvement,16000000000,,,10.8,,,,,NVIDIA A100,,,,,,Open weights (unrestricted),,,,,,"""We extensively evaluate CodeT5+ on over 20 code-related benchmarks in different settings, including zero-shot, finetuning, and instruction-tuning. We observe state-of-the-art (SoTA) model performance on various code-related tasks, such as code generation and completion, math programming, and text-to-code retrieval tasks""","""We implemented a family of CodeT5+ models, with model sizes ranging from 220M to 16B""",,"""We enlarge the pretraining dataset of CodeSearchNet [Husain et al., 2019] with the recently released GitHub Code dataset""","""We use the CodeT5 tokenizer to tokenize the multilingual dataset, resulting in 51.5B tokens""",,,,,https://github.com/salesforce/CodeT5/blob/main/LICENSE.txt
ONE-PEACE,5/18/2023,SOTA improvement,4000000000,1.8e+20,1600000000,4.7,,,,,,"LAION-2B,LAION-Audio-630K",Speculative,,,,Open weights (unrestricted),Open source,Open source,,,Operation counting,""" ONEPEACE achieves leading results in both uni-modal and multi-modal tasks, including image classification (89.8%
accuracy on ImageNet w/o privately labeled data), semantic segmentation (63.0% mIoU on ADE20K), audio-text
retrieval (outperforming previous SOTAs on AudioCaps and Clotho by a large margin), audio classification (91.8%
zero-shot accuracy on ESC-50, 69.7% accuracy on FSD50K, 59.6% accuracy on VGGSound w/o visual information),
audio question answering (86.2% accuracy on AVQA w/o visual information), image-text retrieval (84.1% I2T R@1
on MSCOCO and 97.6% I2T R@1 on Flickr30K w/o intermediate finetuning and ranking), and visual grounding
(89.26%/83.23%/89.27% scores on RefCOCO/+/g test sets).""","""we propose ONE-PEACE, a model with 4B parameters""","4 billion params * 7.5 billion data * 6 = 1.8e20.

see training dataset size notes. this estimate required some more assumptions than usual.","""For image-text pairs, we use LAION-2B... For audio-text pairs, we mainly use the environmental sound datasets processed by [76].""

looks like there's additional fine-tuning data as well","""After these steps, we retain about 1.5 billion image-text pairs""
...
""We also perform simple cleaning on the data, which involves removing samples with text lengths less than 3 or greater than
512, as well as texts containing non-English or emoji characters. Ultimately, we obtain about 2.4 million audio-text pairs, with a total duration of around 8,000 hours""

8000 hours = 480,000 minutes = ~109,440,000 words at 228 wpm

https://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.3pbt0hfgv7pq

Trained on 10 epochs for audio. For text, they train on ""200K steps with a batch size of 32768"" = 6,533,600,000
Adding together, they train on ~ 7.5b data points on a dataset of 1.6b, for ~4.7 epochs on average.",,,,,"Apache 2.0, includes train code
https://github.com/OFA-Sys/ONE-PEACE/tree/main/one_peace "
CoEdiT-xxl,5/17/2023,SOTA improvement,11000000000,,3000000,5,,,,,NVIDIA A100,,Likely,Flan-T5 11B,1e+18,,Open weights (non-commercial),Open (non-commercial),Open access (non-commercial),,,,"""We achieve state-of-the-art performance on multiple text editing tasks: grammatical error correction, text simplification, sentence fusion, iterative text editing, and three stylistic editing
tasks (formality style transfer, neutralization,
and paraphrasing).""",11B,finetuned from Flan-T5,"82k pairs of editing examples:

""we fine-tune a pre-trained
sequence-to-sequence model on a parallel corpus
of instruction-based 82K input-output pairs. The
inputs and outputs are sourced from publicly available corpora for different text editing tasks""

""Our dataset creation is based on the ITERATER+
dataset proposed by Kim et al. (2022) who combined datasets from various text editing tasks (See
Table 1). Their work, in turn, is based on Du et al (2022b), who categorized each edit into MEANINGCHANGED or NON-MEANING-CHANGED.""","82k pairs of sentences. Roughly 20 words per sentence based on examples but mean length could be higher due to outliers.
40*82k = ~3,000,000",,,"""We fine-tune different versions of pre-trained FLANT5 (Chung et al., 2022a) models on the COEDIT dataset. Specifically, we use FLANT5-L (770M parameters), FLANT5-XL (3B parameters), FLANT5-XXL (11B parameters) models.""

""Each model is trained for 5 epochs with early stopping. All models were fine-tuned on A100 GPUs using Deepspeed""

6 * 5 epochs * 3 million words (rough estimate) * 11 billion = 9.9e17 ~= 1e18

",,"cc-by-nc (non commercial) for weights: https://huggingface.co/grammarly/coedit-large
training code/data here with unclear licenses: https://github.com/vipulraheja/coedit"
Med-PaLM 2,5/16/2023,SOTA improvement,3.4E+11,,,,,,,,,MultiMedQA,Likely,PaLM 2,,,Unreleased,Unreleased,,,,,https://paperswithcode.com/sota/question-answering-on-medqa-usmle ,from PaLM 2,,"We applied instruction finetuning to the base LLM following the protocol used by Chung et al. [21]. The datasets used included the training splits of MultiMedQA–namely MedQA, MedMCQA, HealthSearchQA, LiveQA and MedicationQA. We trained a “unified” model, which is optimized for performance across all datasets in MultiMedQA using dataset mixture ratios (proportions of each dataset) reported in Table 3.","Dataset Count Mixture ratio
MedQA 10,178 37.5%
MedMCQA 182,822 37.5%
LiveQA 10 3.9%
MedicationQA 9 3.5%
HealthSearchQA 45 17.6%

MedMCQA (https://proceedings.mlr.press/v174/pal22a/pal22a.pdf, Table 2) has on average 12.77+ 2.69+67.52 = 82.98 tokens per datapoint",,,,,
InstructBLIP,5/11/2023,SOTA improvement,13000000000,1.94e+20,,,36,16,,,NVIDIA A100 SXM4 40 GB,"COCO,Web CapFilt,NoCaps,Flickr30K Entities,TextCaps,VQAv2,VizWiz,GQA,OKVQA,ScienceQA,OCR-VQA,TextVQA,LLaVA-Instruct-150k",Confident,Vicuna-13B v0,1.9408896e+20,,Open weights (non-commercial),,,,14166.043366089072,Hardware,from abstract - SOTA on ScienceQA,13B form 2.6,"""All models are trained utilizing 16 Nvidia A100 (40G) GPUs and are completed within 1.5 days.""
16 * 3.12e14 * 1.5 * 24 * 3600 * 0.3 = 1.94e20","COCO Caption, Web CapFilt, NoCaps, Flickr30K, TextCaps, VQAv2, VizWiz, GQA, Visual Spatial Reasoning, IconQA, OKVQA, A-OKVQA, ScienceQA, Visual Dialog, OCR-VQA, TextVQA, HatefulMemes, LLaVA-Instruct-150K, MSVD-QA, MSRVTT-QA, iVQA","""All models are instruction-tuned with a maximum of 60K steps""

""We employ a batch size of 192, 128, and 64 for the 3B, 7B, and 11/13B models, respectively. ""","""All models are trained utilizing 16 Nvidia A100 (40G) GPUs and are completed within 1.5 days.""",,"flops = (16) * (312 * 10**12) * (1.5* 24 * 3600) * (0.3) = 1.9e20
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)

""All models are trained utilizing 16 Nvidia A100 (40G) GPUs and are completed within 1.5 days.""",,"LlaMA/Vicuna license, non-comm:
https://github.com/salesforce/LAVIS/tree/main/projects/instructblip"
PaLM 2,5/10/2023,"SOTA improvement,Training cost",3.4E+11,7.34e+24,2.7E+12,,,,,4865570.064,Google TPU v4,,Likely,,,,API access,Unreleased,,TRUE,,Operation counting,,"Model Architecture: ""PaLM-2 is a new state-of-the-art language model. We have small, medium, and large variants that use stacked layers based on the Transformer architecture, with varying parameters depending on model size. Further details of model size and architecture are withheld from external publication.""
However, the parameter count was leaked to CNBC: https://www.cnbc.com/2023/05/16/googles-palm-2-uses-nearly-five-times-more-text-data-than-predecessor.html","Compute Requirements ""Not reported.""
Paper suggests heuristic of  C=6ND. Based on 340B parameters and 3.6T tokens, training compute would be around 7.3*10^24 FLOP.","""The PaLM 2 pre-training corpus is composed of a diverse set of sources: web documents, books, code, mathematics, and conversational data. The pre-training corpus is significantly larger than the corpus used to train PaLM (Chowdhery et al., 2022). PaLM 2 is trained on a dataset that includes a higher percentage of non-English data than previous large language models, which is beneficial for multilingual tasks"" (page 9)","""The pre-training corpus is significantly larger than the corpus used to train PaLM"" so greater than 6e+11. According to the leaked documents viewed by CNBC, the corpus was 3.6 trillion tokens or around 2.7*10^12 words.

https://www.cnbc.com/2023/05/16/googles-palm-2-uses-nearly-five-times-more-text-data-than-predecessor.html",,PaLM 2 was trained on TPU v4 according to the model card (pages 91-92),,,
StarCoder,5/9/2023,SOTA improvement,15500000000,8.46e+22,1E+12,1,625.5,512,0.2272361,212217.65075330864,NVIDIA A100 SXM4 80 GB,The Stack,Confident,,,4000000,Open weights (restricted use),Unreleased,,,453322.1740125758,"Reported,Hardware","""We perform the most comprehensive evaluation of Code LLMs to date and show that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model. Furthermore, StarCoder outperforms every model that is fine-tuned on Python""","""We trained a 15.5B parameter model""","FLOP reported here, 8.46e22
https://huggingface.co/bigcode/starcoder


""We trained our model on a GPU cluster with 512 A100 80 GB GPUs... Based on the total number of GPU hours that training took (320,256) and an average power usage of 280W per GPU... The fine-tuned model adds 3.5% of training time""

320256 * 312 tFLOP/s * 3600 * 1.035 * 0.3 (utilization assumption) = 1.12e23","""StarCoderBase is trained on 1 trillion tokens sourced from The Stack (Kocetkov et al., 2022), a large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process""","""StarCoderBase is trained on 1 trillion tokens sourced from The Stack""","625.5 hours = 320256 /512
512 GPUs from ""We trained our model on a GPU cluster with 512 A100 80 GB GPUs ""

320256 GPU hours from ""Based on the total number of GPU hours that training took (320,256)""
citations from sections 5.6 and 5.7",,,"""The model was trained for 250k iterations, with a batch size of 4M tokens, for a total of one trillion tokens.""","some restrictions

https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement

data is The Stack, which has multiple licenses
https://huggingface.co/datasets/bigcode/the-stack-dedup "
ImageBind,5/9/2023,SOTA improvement,932000000,,,64,,,,,"NVIDIA V100,NVIDIA A100","SUN RGB-D,LLVIP,Ego4D,AudioSet",Likely,ViT-Huge/14,,,Open weights (non-commercial),Open (non-commercial),Open access (non-commercial),,,,"""we set a new state-of-the-art on emergent zero-shot recognition tasks across modalities, outperforming specialist supervised models""",used ViT-Huge 630M as an image/video encoder and OpenCLIP-302m as text encoder,,""" For the naturally available paired data, we use
the (video, audio) pairs from the Audioset dataset [19], (image, depth) pairs from the SUN RGB-D dataset [69], (image, thermal) pairs from the LLVIP dataset [32] and (video,
IMU) pairs from the Ego4D dataset [23].""",,,,,,"Creative commons non-commercial
models and code in this repo: https://github.com/facebookresearch/ImageBind/blob/main/README.md 
train code: https://github.com/facebookresearch/ImageBind/blob/main/imagebind/models/imagebind_model.py "
Agile Soccer Robot,4/26/2023,SOTA improvement,,,,,240,,,,,,Unknown,,,,Unreleased,,,,,,"Likely the best bipedal soccer AI, since it's DeepMind, and related work section just discusses results involving specific soccer skills and quadruped robots:

""Whether bipedal or quadrupedal, navigation represents only a fraction of animal and human capabilities. Motivated by this observation, there is a growing interest in whole body control, i.e. tasks in which the whole body is used in flexible ways to interact with the environment. Examples include climbing (Rudin et al., 2022a), getting-up from the ground (Ma et al., 2023), catching objects (Ma et al., 2023), and mobile manipulation with legs (Cheng et al., 2023). Recently, reinforcement learning has been applied to learn simple soccer skills, including goalkeeping (Huang et al., 2022), ball manipulation on diverse terrains (Bohez et al., 2022; Ji et al., 2023), and shooting (Ji et al.,
2022). These works focus on a narrower set of skills than the 1v1 soccer game, and the quadrupedal platform is inherently more stable and therefore presents an easier learning challenge.""",,,self-play training in simulation,""". The get-up teacher learns to get up relatively quickly and trained in total for approximately 2.4 · 10^8 environment steps,
equivalent to approximately 70 days of simulation time, or 14 hours of wall-clock time. The soccer
teacher was trained for 2 · 10^9 environment steps, which took 158 hours of training, equivalent to
approximately 580 days of simulated match""","14+158+68 hours:
""Training the get-up and soccer teachers took 14 and 158 hours (6.5 days), respectively, and distillation and self-play
took 68 hours (see Appendix B for details)""",,,,
LLaVA,4/17/2023,SOTA improvement,13000000000,7.8049e+22,,,10,8,,42.46267260692187,NVIDIA A100,Conceptual Captions (CC3M),Confident,Vicuna-13B v0,4.9e+19,,Open weights (unrestricted),Open source,Open source,,7084.6712076106505,Hardware,"When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%.",13B,"8 * 3.12e14 * (18 * 3600) * 0.3 = 4.9e19
num gpus * peak flops * time *assumed utilization rate 
""We train all models with 8× A100s. Pretraining on CC-595K completes within 4 hours. Finetuning on Instruct-158K completes within 10 hours. Finetuning on ScienceQA completes within 4 hours."" so 18 hours of time in total.

However, they use Vicuna as their LLM backbone, which used 7.8e22 FLOPs in training. Total FLOPs are then 4.9e19 + 7.8e22 = 7.8049e22","""We pre-train our model on the filtered CC-595K subset for 1 epoch with a learning rate of 2e-3 and a batch size of 128, and fine-tune on the proposed LLaVA-Instruct-158K dataset ""","595K + 158K = 753K image text pairs
""This results in around 595K image-text pairs""
""We collect 158K unique language-image instruction-following samples in total, including 58K in conversations, 23K in detailed description, and 77k in complex reasoning, respectively. ""","""We train all models with 8× A100s. Pretraining on CC-595K completes within 4 hours. Finetuning on Instruct-158K completes within 10 hours. Finetuning on ScienceQA completes within 4 hours.""",,"8 * 3.12e14 * (18 * 3600) * 0.3 = 4.9e19
num gpus * peak flops * time *assumed utilization rate 
""We train all models with 8× A100s. Pretraining on CC-595K completes within 4 hours. Finetuning on Instruct-158K completes within 10 hours. Finetuning on ScienceQA completes within 4 hours."" so 18 hours of time in total.",,"apache 2.0

train repo: https://github.com/haotian-liu/LLaVA?tab=readme-ov-file#train 

model: https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md "
DINOv2,4/14/2023,SOTA improvement,1140000000,7.41851136e+21,142000000,,,,,10203.60518105836,NVIDIA A100 SXM4 40 GB,,Confident,,,,Open weights (unrestricted),Open source,Open source,,,Hardware,"""Our family of models drastically improves over
the previous state of the art in self-supervised learning and reaches performance comparable with weakly-
supervised features.""
",1.14B from https://huggingface.co/facebook/dinov2-giant,"table 14

22016 * 3600 * 312 * 10 ** 12 * 3/10 = 7.41851136e+21
gpu hours in seconds * flops of A100 * assumed utilization  rate",new dataset  - named LVD142M Table 15,new dataset  - named LVD142M Table 15,,,,,"apache 2.0

training code and weights here: https://github.com/facebookresearch/dinov2 "
Incoder-6.7B,4/9/2023,SOTA improvement,6700000000,3.00001e+21,,1,576,,,3129.0771365574788,NVIDIA V100,,Confident,,,,Open weights (non-commercial),Unreleased,Open access (non-commercial),,,Reported,"""Zero-shot infilling with bidirectional context substantially outperforms approaches based on left-to-right-only models, and on several tasks
obtains performance comparable to state-of-the-art models fine-tuned on the tasks""",6.7B,"per table 5, required 3 zettaflop (3e21) to train.

also, ""INCODER-6.7B was trained on 248 V100 GPUs for 24 days""

hardware method: 125 trillion * 248 * 24 * 24 * 3600 * 0.3 = 2e22. suggests their utilization was quite low, or 24 days was just calendar time.
","Code from GitHub and StackOverflow

""To train our models, we collect a corpus of (1) public code with permissive, non-copyleft, opensource licenses from GitHub and GitLab and (2) StackOverflow questions, answers, and comments.
Our primary focus in this paper is on the Python language, but we also include code files from
28 total languages and StackOverflow content from all available languages.""","216 GB: ""Our final pre-training corpus contains a total of 159 GB of code, 52 GB of it
in Python, and a total of 57 GB of content from StackOverflow""",24,,,,"CC-BY-NC 4.0 (non commercial)

data is open: ""To train our models, we collect a corpus of (1) public code with permissive, non-copyleft, opensource licenses from GitHub and GitLab and (2) StackOverflow questions, answers, and comments.""

inference code, not training code in this repo: https://github.com/dpfried/incoder/blob/main/README.md "
Segment Anything Model,4/5/2023,Highly cited,636000000,7.8e+21,1100000000,2,68,256,,15888.411228475235,NVIDIA A100,Segment Anything 1B,Confident,ViT-Huge/14,7.8e+21,,Open weights (unrestricted),Unreleased,,,226735.9259263604,Hardware,,"From Facebook website: https://segment-anything.com/
""How big is the model? The image encoder has 632M parameters.
The prompt encoder and mask decoder have 4M parameters.""","""SAM was trained on 256 A100 GPUS for 68 hours. We acknowledge the environmental impact and cost of training
large scale models. The environmental impact of training the released SAM model is approximately 6963 kWh""

68*256 A100-hours = 
17408 hours * 3600 * 312 trillion * 0.4 (utilization assumption for image models)
= 7.82e21

max A100 power is 400W. 6,963,000 watt-hours / 400 watts = 17407.5 hours (so they probably just calculated backwards from power rating, and this doesn't give any info on utilization)","""Dataset (§5). Our final dataset, SA-1B, includes more than
1B masks from 11M licensed and privacy-preserving images (see Fig. 2). SA-1B, collected fully automatically using the final stage of our data engine, has 400× more masks
than any existing segmentation dataset [66, 44, 117, 60],
and as we verify extensively, the masks are of high quality
and diversity. Beyond its use in training SAM to be robust
and general, we hope SA-1B becomes a valuable resource
for research aiming to build new foundation models.""","""SA-1B contains 11M diverse, high-resolution, licensed, and privacy protecting images and 1.1B high-quality segmentation masks.""
segmentation mask is a map that identifies segments in an image","""SAM was trained on 256 A100 GPUS for 68 hours""",,see Training Compute notes,,"Apache 2.0 license
don't see pretrain code in the repo, could be wrong

https://github.com/facebookresearch/segment-anything"
BloombergGPT,3/30/2023,SOTA improvement,50558868480,2.36e+23,5.32E+11,0.8,1270,512,0.327,369586.1352802876,NVIDIA A100,,Confident,,,4200000,Unreleased,Unreleased,,,453498.3266248031,"Reported,Hardware","""We validate BloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite of internal benchmarks that most accurately reflect our intended usage. Our mixed dataset training leads to a model that outperforms existing models on financial tasks by significant margins without sacrificing performance on general LLM benchmarks.""",,"2.36e23 per Table 4

(using our usual hardware method, 512 A100s over 53 days would be 512 * 312 teraFLOP/s * 53 * 24 * 3600 * 0.3 = 2.19e23)","""To train BloombergGPT, we construct “FinPile”, a comprehensive dataset consisting of a range of English financial documents including news, filings, press releases, web-scraped financial documents, and social media drawn from the Bloomberg archives. These documents have been acquired through our business process over the past two decades. We augment FinPile with public data widely used to train LLMs. The result is a training corpus that is roughly half domain-specific text and half general-purpose text.""","708.9 billion tokens. At 0.75 English words per token, that's 532B words","""~53 days""",,,"""in the first 7,200 steps, we use a batch size of 1,024 (2.1M tokens), then switch to a batch size of 2,048 (4.2M tokens) for the remainder of training.""",
VideoMAE V2,3/29/2023,SOTA improvement,1000000000,9.7e+21,,1200,336,64,,18339.96928068276,NVIDIA A100 SXM4 80 GB,,Confident,ViT-G/14,9.7e+21,,Open weights (unrestricted),Open source,Open source,,56687.84260872892,Hardware,"""Finally, we successfully train a video ViT model with a
billion parameters, which achieves a new state-of-the-art
performance on the datasets of Kinetics (90.0% on K400
and 89.9% on K600) and Something-Something (68.7% on
V1 and 77.0% on V2).""",1B,"finetuned on ViT-g (smaller than ViT-G with 1B params)

""It takes more than two weeks to pre-train a ViT-g model with VideoMAE
on 64 A100 GPUs""

64 * 312 trillion * 2 * 7 * 24 * 3600 * 0.4 (utilization assumption) = 9.7e21","""To well support the billion-level ViT model pretraining, we build two large-scale video datasets for our proposed progressive training. For self-supervised pre-training of VideoMAE V2, we build a million-level unlabeled video
dataset by collecting clips from multiple resources such
as Movie, Youtube, Instagram, General Webs, and manual recordings from scripts, and the dataset is termed as
UnlabeledHybrid""","1.35 million video clips. Not sure about average length (34 seconds, but that's only reported for Instagram portion).

""In total, there are around 1.35M clips in our mixed dataset and
this is the largest dataset ever used for video masked autoencoding.",2 weeks,,"finetuned on ViT-g (smaller than ViT-G with 1B params)

""It takes more than two weeks to pre-train a ViT-g model with VideoMAE
on 64 A100 GPUs""

64 * 312 trillion * 2 * 7 * 24 * 3600 * 0.4 (utilization assumption) = 9.7e21
",,"MIT
https://github.com/OpenGVLab/VideoMAEv2/blob/master/LICENSE"
Firefly,3/21/2023,Significant use,,,,,,,,,,Adobe Stock,Unknown,,,,,,,,,,"Integrated into Photoshop. Users generate >200m images within a few months of release:

https://venturebeat.com/ai/adobe-stock-creators-arent-happy-with-firefly-the-companys-commercially-safe-gen-ai-tool/

As of October 2024, users have generated 13B images since March 2023. Paid users get 100 generations per month (and can continue at a slower rate after that). Assuming an average of 100 monthly generations per user, that's around 6.7M monthly average users across 19.5 months.",,,"""The current Firefly generative AI model is trained on a dataset of licensed content, such as Adobe Stock, and public domain content where copyright has expired.""

https://www.adobe.com/products/firefly.html",,,,,,
PanGu-Σ,3/20/2023,SOTA improvement,1.085E+12,4.67e+23,2.4675E+11,1.836,2400,512,,,Huawei Ascend 910,,Confident,,,524288,Unreleased,Unreleased,,,351495.4313105972,Hardware,"""Our experimental findings show that PanGu-{\Sigma} provides state-of-the-art performance in zero-shot learning of various Chinese NLP downstream tasks.""","""In this work, we present PanGu-Σ , a large language model with sparse architecture containing 1.085 trillion parameters.""","It has sparse architecture, so we can't use C=6ND.
""We develop PanGu-Σ model under the framework of MindSpore and train it on a cluster with only 512 Ascend 910 AI Accelerators with 329 billion tokens over 100 days.""
100 days * 512 processors * 320 teraFLOPS/processor * 33% utilization = 4.67e+23 FLOP
https://www.wolframalpha.com/input?i=100+days+*+512+*+320+terahertz+*+0.33","""329B tokens in more than 40 natural and programming languages""",329B tokens ~= 247B words,"We develop PanGu-Σ model under the framework of MindSpore 5
and train it on a cluster with only 512 Ascend 910 AI Accelerators [28] with 329 billion tokens over 100 days.",,,"""We train PanGu-Σ with global batch size of 512 with sequence length of 1024 for each sample""",
Gen-2,3/20/2023,SOTA improvement,,,,,,,,,,,Unknown,,,,,,,,,,"Website claims SOTA improvement over Stable Diffusion and Text2Live, paper forthcoming",,,,,,,,,
LEP-AD,3/15/2023,SOTA improvement,3007381000,,1244420,,,,,,,,Confident,ESM2-3B,,,Unreleased,Open (non-commercial),Open access (non-commercial),,,,"""We report new best-in-class state-of-the-art results compared
to competing methods such as SimBoost, DeepCPI, Attention-DTA, GraphDTA,
and more using multiple datasets, including Davis, KIBA, DTC, Metz, ToxCast,
and STITCH. Finally, we find that a pre-trained model with embedding of proteins
(the LED-AD) outperforms a model using an explicit alpha-fold 3D representation of proteins (e.g., LEP-AD supervised by Alphafold)""","Uses ESM-2 3B. Table 2 gives details on the non-ESM layers. The GCN appears to have about 3.31M parameters and the linear layers should have 771k and 3.3M, respectively. So total is ~3.007B",No indication of the training used here. ESM-2 3B used 3e22.,Table 1,"Largest dataset appears to be STITCH, at 1244420 drug-target pairs.",,,,,https://github.com/adaga06/LEP-AD unclear license
GPT-4,3/15/2023,"Highly cited,SOTA improvement,Training cost",,2.1e+25,4.9E+12,2,2280,25000,0.34,40586592.57781653,NVIDIA A100 SXM4 40 GB,Unspecified unreleased,Speculative,,,,API access,Unreleased,,TRUE,22146708.683295924,Hardware,"See the paper, p.1: ""On a suite of traditional NLP benchmarks, GPT-4 outperforms both previous large language models and most state-of-the-art systems (which often have benchmark-specific training or hand-engineering).""",,"90% CI: 8.2E+24 to 4.4E+25

NOTE: this is a rough estimate based on public information, much less information than most other systems in the database.

Calculation and confidence intervals here: https://colab.research.google.com/drive/1O99z9b1I5O66bT78r9ScslE_nOj5irN9?usp=sharing",,"Speculative. Reported secondhand by online sources such as Semianalysis, but not verified by OpenAI. If total number of tokens seen was 13T, text was repeated for 2 epochs, and text was the majority of tokens, then dataset size roughly is 13T*0.75/2 = 4.9T words.

Note this examines only the text dataset, since GPT-4 was first and foremost a language model. However, the vision component had its own vision dataset, which we believe accounted for a much smaller part of the compute budget.",(Speculative) SemiAnalysis conjectures that GPT-4 training took 90-100 days with utilization of 32-36%.,,,,
Falcon-40B,3/15/2023,Historical significance,40000000000,2.4e+23,1E+12,,1440,384,0.3864,319783.1572,NVIDIA A100,RefinedWeb,Confident,,,2359296,Open weights (unrestricted),Unreleased,,,340173.4453754254,"Operation counting,Reported",,Model comes in 7B and 40B variants.,"C = 6ND = 6 * 40B * 1000B = 2.4e+23 FLOP (assuming one epoch)

Table 1 from https://arxiv.org/pdf/2311.16867 Falcon paper

2,800 petaflop-days * 1e15 * 24 * 3600 = 2.4192e+23 FLOPs","Falcon-40B was trained on 1,000B tokens of RefinedWeb, a high-quality filtered and deduplicated web dataset which we enhanced with curated corpora. Significant components from our curated copora were inspired by The Pile (Gao et al., 2020).",1000B tokens ~= 750B words,"""Falcon-40B was trained on AWS SageMaker, on 384 A100 40GB GPUs in P4d instances.""
""Training started in December 2022 and took two months.""",,,"Batch size 1152 (presumably sequences) per Table 16. Warmed up using smaller batches for first 100B tokens.

""All Falcon models are pretrained with a 2,048 sequence length""

https://arxiv.org/pdf/2311.16867.pdf
",apache 2.0
Claude,3/14/2023,"Historical significance,SOTA improvement",,,,,,,,,,Unspecified unreleased,Unknown,,,,,,,,,,,,,,,,,,,
PaLM-E,3/6/2023,SOTA improvement,5.62E+11,,,,,,,,,,Likely,PaLM (540B),,,,,,,,,"""Our largest model, PaLM-E-562B with 562B parameters, in addition to being trained on robotics tasks, is a visual-language generalist
with state-of-the-art performance on OK-VQA, and retains
generalist language capabilities with increasing scale.""",562B,"Based on Palm-540B and ViT-22B and then trained on robotics data.

","""Our three robot environments (Fig. 1) include a Task and Motion Planning (TAMP) domain where a robot has to manipulate (grasp and stack) objects, a table-top pushing environment, and a mobile manipulation domain. In each domain, PaLM-E is trained on expert data from that domain. In many cases, this is a sparse amount of data per task. The TAMP tasks involve large combinatorics over possible plans, and many decision sequences are infeasible. PaLM-E has to generate plans that consist of multiple steps, with complicated decision boundaries. The multi-object tabletop pushing environment is taken from the publicly available Language-Table dataset (Lynch et al., 2022) and is challenging since it includes several objects, large cardinality of language, and complex pushing dynamics""",,,,"Based on Palm-540B and ViT 22B. No compute details given.

""We scale PaLM-E up to 562B parameters, integrating the 540B PaLM (Chowdhery et al., 2022) LLM and the 22B Vision Transformer (ViT) (Dehghani et al., 2023) into, to our knowledge, the largest vision-language model currently reported.""",,
AudioGen,3/5/2023,SOTA improvement,1000000000,9.5e+21,2.304E+11,,168,,,9429.740911,NVIDIA A100,"AudioSet,AudioCaps",Likely,,,,Open weights (non-commercial),Open source,Open source,,,Hardware,"""We propose a state-of-the-art auto-regressive audio generation model conditioned on textual descriptions or audio prompts, as evaluated with objective and subjective (human
listeners) scores.""","""We trained two sets of ALMs, one with 285M parameters (base) and the other with 1B parameters (large).""","""the large model was trained on 128 A100 GPUs for 200k steps (∼1 week)""
A100s are 312 teraflop/s
128 * 312 trillion * 7 * 24 * 3600 * 0.3 (utilization assumption) = 7.2e21

Text encoding uses T5-Large, which used 2.3e21 FLOP in pre-training per Flan paper: https://arxiv.org/abs/2210.11416 ","""We use a set of several datasets: AudioSet (Gemmeke et al., 2017), BBC sound effects,
AudioCaps (Kim et al., 2019), Clotho v2 (Drossos et al., 2020), VGG-Sound (Chen et al., 2020),
FSD50K (Fonseca et al., 2021), Free To Use Sounds 2
, Sonniss Game Effects 3
, WeSoundEffects 4
,
Paramount Motion - Odeon Cinematic Sound Effects 5
. All audio files were sampled at 16kHz.
For textual descriptions we use two types of annotations. The first one is multi-label annotations,
available for the datasets: AudioSet, VGG-Sound, FSD50K, Sinniss Game Effects, WeSoundEffects, Paramount Motion - Odeon Cinematic Sound Effects.""","""Overall we are left with ∼4k hours for training data.""
mix of speech and other sounds

Training the audio autoencoder uses reconstruction loss on sequence of raw audio samples. Audio files are in 16kHz, so
16k * 4k * 3600 = 230.4B samples

Audio language modelling operates on tokens; ""each second of audio is represented by 500 tokens"". 
4k * 3600 * 500 = 7.2B tokens",1 week,,,,"MIT license, but non-commercial for weights: https://github.com/facebookresearch/audiocraft/blob/main/LICENSE_weights

training info: https://github.com/facebookresearch/audiocraft/blob/main/docs/AUDIOGEN.md "
DiT-XL/2,3/2/2023,SOTA improvement,675000000,6e+20,,,,,,111048.19613085664,Google TPU v3,ImageNet,Confident,Stable Diffusion (LDM-KL-8-G),,,,,,,,"Hardware,Other","""our largest DiT-XL/2 models outperform all prior diffusion models on the classconditional ImageNet 512×512 and 256×256 benchmarks,
achieving a state-of-the-art FID of 2.27 on the latter.""",675M,"~6e20, based on eyeballing Figure 9. It's between 1e11 and 1e12 gigaflop (1 gigaflop = 1e9 flop), and about 80% of the way towards 1e12 on a log scale. 10^0.8 is about 6. 

3M iterations with a batch size of 256.

""Compute. We implement all models in JAX [1] and train
them using TPU-v3 pods. DiT-XL/2, our most computeintensive model, trains at roughly 5.7 iterations/second on a
TPU v3-256 pod with a global batch size of 256""
256*123000000000000 FLOPs/s * 800000 training steps / 5.7 iterations/second * 0.3 = 1.3258105e+21",,didn't state which ImageNet set,,,,,
LLaMA-65B,2/24/2023,"Historical significance,Highly cited",65200000000,5.5e+23,1.34E+12,1.09,500,2048,0.4746,576476.4930991895,NVIDIA A100,"CCNet,GitHub,Wikipedia,books,arXiv,Stack Exchange",Confident,,,4000000,Open weights (non-commercial),Unreleased,,,1814594.7887512865,Operation counting,Widely-used foundation model that has been adapted for others such as Alpaca.,"Model card, table 1: https://github.com/facebookresearch/llama/blob/53011c3d7946dadb8274a4c5c7586ab54edf792d/MODEL_CARD.md","1.4e12 tokens * 6.52e10 parameters * 6 FLOP/token/parameter = 5.5e23 FLOP

Compared to 2048 A100 GPUs each with 311.84 TFLOPS maximum performance for 21 days, this implies 47% utilization.
https://www.wolframalpha.com/input?i=5.5*10%5E23+FLOP+%2F+%282048+*+311.84+teraFLOPS+*+21+days%29","The model was trained using the following source of data: CCNet [67%], C4 [15%], GitHub [4.5%], Wikipedia [4.5%], Books [4.5%], ArXiv [2.5%], Stack Exchange[2%]. The Wikipedia and Books domains include data in the following languages: bg, ca, cs, da, de, en, es, fr, hr, hu, it, nl, pl, pt, ro, ru, sl, sr, sv, uk. See the paper for more details about the training set and corresponding preprocessing.","Table 1 indicates that 1.4T tokens involved sampling sub-datasets at more or less than one epoch. Correcting for this:

(1.1 epoch * 3.3TB) + (1.06 epoch * 0.783TB) + ... = 1.4T tokens
5.24 epoch-TBs = 1.4T tokens
5.24 epoch-TB * 1000 GB/TB * 200M token/GB = 1.4T tokens
1.05T epoch*token = 1.4T tokens
1 epoch = 1.34T tokens
","""When training a 65B-parameter model, our code processes around 380 tokens/sec/GPU on 2048 A100 GPU with 80GB of RAM. This means that training over our dataset containing 1.4T tokens takes approximately 21 days.""","1023384 processor-hours on A100 GPUs. May 2023 cost rate is $1.36/GPU-hour on Azure ML cloud. https://azure.microsoft.com/en-us/pricing/details/machine-learning/ 
According to https://www.bls.gov/data/inflation_calculator.htm, $1.18 in May 2023 = $1.00 in January 2020.
$1391674 / 1.18 = $1179385 in 2020 USD.",,,"""we are releasing our model under a noncommercial license focused on research use cases"" https://ai.meta.com/blog/large-language-model-llama-meta-ai/"
BASIC-L + Lion,2/13/2023,SOTA improvement,3070000000,,,,,,,,,,Confident,,,,,,,,,,"""On vision-language contrastive learning, we achieve 88.3% zero-shot and 91.1% fine-tuning accuracy on ImageNet, surpassing the previous best results by 2% and 0.1%, respectively""",parameter count of original BASIC-L,"This model is BASIC-L retrained with a different optimizer, Lion. Lion seems more compute-efficient, so we should expect compute to be less than BASIC-L.",,,,,,,
ViT-22B,2/10/2023,SOTA improvement,21743000000,1.93248e+23,4000000000,2.9,347.4,1024,0.549,285555.57016183576,Google TPU v4,JFT-4B,Confident,,,,Unreleased,Unreleased,,,435562.3547730654,Hardware,"""The largest
ViT-22B sets the new SOTA on the challenging ObjectNet test set""","21.743B, Table 1","""ViT-22B was trained using 256 visual tokens per image, where each token represents a 14 × 14 patch extracted from 224 × 224 sized images. ViT-22B is trained for 177k steps with batch size of 65k: approximately 3 epochs""

""ViT-22B was trained on 1024 TPU V4 chips for 177K steps""

""Using these techniques, ViT-22B processes 1.15k tokens per second per core during training (forward and backward pass) on TPUv4 [...] ViT-22B’s model flops utilization (MFU) is 54.9%""

256 * 177k * 65k = 2.945T tokens

So training time is 2.945T tokens / (1.15k * 2 * 1024) tokens/s = 1.25M seconds = 347.4 hours

So 1024 TPUv4 chips for 1.25M seconds at 54.9% MFU:
1024 * 2.75e14 * 1.25M * 0.549 = 1.93248e23","""Dataset. ViT-22B is trained on a version of JFT (Sun et al., 2017), extended to around 4B images (Zhai et al.,
2022a). These images have been semi-automatically annotated with a class-hierarchy of 30k labels""","""Dataset. ViT-22B is trained on a version of JFT (Sun et al., 2017), extended to around 4B images (Zhai et al.,
2022a). These images have been semi-automatically annotated with a class-hierarchy of 30k labels""","""Using these techniques, ViT-22B processes 1.15k tokens per second per core during training (forward and backward pass)""
From model card we know they trained with 1024 TPUv4 chips, and there are 2 cores per chip. Total number of tokens was 177K steps * 65k images/step * 256 tokens/image = 2.945T tokens

So training time is 2.945T tokens / (1.15k * 2 * 1024) tokens/s = 1.25M seconds = 347.4 hours",,,,don't see it here: https://github.com/google-research/vision_transformer?tab=readme-ov-file#available-vit-models 
ProteinDT,2/9/2023,SOTA improvement,,,197000000.9999998,,,,,,,UniProtKB,Unknown,SciBERT,,,Unreleased,,,,,,"""Compared to six state-of-the-art protein sequence representation methods, ProteinDT can obtain consistently superior performance on four of six benchmark tasks.""",,,They extract a subset of 441K protein-text pairs,"Total amino acids: 197,000,000 residues

Final calculation: 1.97 × 10⁸ datapoints

Value = 197,000,000 = 1.97e8",,,,,
Gen-1,2/6/2023,SOTA improvement,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
Flan T5-XXL + BLIP-2,1/30/2023,Highly cited,12100000000,,,,200,,,99690.24664120316,NVIDIA A100 SXM4 40 GB,"COCO,LAION-400M",Confident,Flan-T5 11B,1.2e+21,,Open weights (unrestricted),Open source,Open source,,,,,"12.1B, per Table 2. 

only 108M trainable params (i.e. params trained during the BLIP process)","fine-tuned from Flan-T5 XXL (11B) and ViT-g

fine-tuning compute:

""using a single 16-A100(40G) machine, our largest model with
ViT-g and FlanT5-XXL requires less than 6 days for the first
stage and less than 3 days for the second stage.""

16 * 9 days * 24 * 3600 * 312 teraflops * 0.3 ~= 1.2e21","""We use the same pre-training dataset as BLIP with 129M images in total, including COCO (Lin
et al., 2014), Visual Genome (Krishna et al., 2017), CC3M (Sharma et al., 2018), CC12M (Changpinyo et al.,
2021), SBU (Ordonez et al., 2011), and 115M images from the LAION400M dataset""","""We use the same pre-training dataset as BLIP with 129M images in total""","""less than 6 days for the first
stage and less than 3 days for the second stage""
9*24 is 216, rounding down a bit is 200 hours",,"ViT-g is the other base model.

""using a single 16-A100(40G) machine, our largest model with
ViT-g and FlanT5-XXL requires less than 6 days for the first
stage and less than 3 days for the second stage.""

16 * 9 days * 24 * 3600 * 312 teraflops * 0.3 ~= 1.2e21",,"BSD license (commercial)
https://github.com/salesforce/LAVIS/tree/main/projects/blip2"
BLIP-2 (Q-Former),1/30/2023,SOTA improvement,1480000000,1.20000000001e+21,,,200,16,,1960.8225382725807,NVIDIA A100 SXM4 40 GB,"COCO,LAION-400M,Conceptual Captions (CC3M),Conceptual Captions 12M (CC12M),VisualGenome,SBU",Confident,,,,Open weights (unrestricted),Open source,Open source,,14179.988778270785,Hardware,"""BLIP-2 achieves state-of-the-art performance on various vision-language tasks""","Q-Former has 188M params. The BLIP-2 system overall has ""54x fewer trainable parameters"" than Flamingo80B.",https://www.wolframalpha.com/input?i=312+teraFLOPS+*+16+*+200+hours+*+0.33,"""We use the same pre-training dataset as
BLIP with 129M images in total, including COCO (Lin
et al., 2014), Visual Genome (Krishna et al., 2017),
CC3M (Sharma et al., 2018), CC12M (Changpinyo et al.,
2021), SBU (Ordonez et al., 2011), and 115M images from
the LAION400M dataset (Schuhmann et al., 2021).""",,"""For example, using
a single 16-A100(40G) machine, our largest model with
ViT-g and FlanT5-XXL requires less than 6 days for the first
stage and less than 3 days for the second stage.""
9 days = 216 hours",,,,"https://github.com/salesforce/LAVIS/tree/main/projects/blip2 includes training and inference code

models here: https://huggingface.co/models?other=blip-2 "
DDPM-IP (CelebA),1/27/2023,SOTA improvement,295000000,3.5e+20,203000,681,120,,,390.4861317667304,NVIDIA V100,CelebA,Likely,,,,,,,,,Hardware,"""For instance, on CelebA 64×64, we achieve a new state-of-theart FID score of 1.27, while saving 37.5% of the training time""","295M for CelebA model, per Table 9","""We use Pytorch 1.8 (Paszke et al., 2019) and trained all the models on different NVIDIA Tesla V100s (16G memory). In
more detail, we use 2 GPUs to train the models on CIFAR10 for 2 days, and 4 GPUs to train the models on ImageNet 32×32
for 34 days. For LSUN tower 64×64, CelebA 64×64 and FFHQ 128×128, we used 16 GPUs to train the models for 3 days,
5 days and 4 days, respectively""

5*16 V100-days for CelebA.

5 * 16 * 24 * 3600 * 125 teraflops * 0.4 ~= 3.5e20",,,5 days,,,,
MusicLM,1/26/2023,SOTA improvement,860000000,,,,,,,,,Free Music Archive,Confident,W2v-BERT,,,,,,,,,"""We demonstrate that our method outperforms baselines on MusicCaps, a hand-curated, high-quality
dataset of 5.5k music-text pairs prepared by musicians.""","""We use decoder-only Transformers for modeling the semantic stage and the acoustic stages of AudioLM. The models
share the same architecture, composed of 24 layers, 16 attention heads, an embedding dimension of 1024, feed-forward
layers of dimensionality 4096, dropout of 0.1, and relative
positional embeddings (Raffel et al., 2020), resulting in
430M parameters per stage.""

""stage"" seems to mean semantic + acoustic, so 860M total",,"""We train SoundStream and w2v-BERT on the Free Music
Archive (FMA) dataset (Defferrard et al., 2017), whereas
the tokenizers and the autoregressive models for the semantic and acoustic modeling stages are trained on a dataset containing five million audio clips, amounting to 280k hours of
music at 24 kHz. Each of the stages is trained with multiple passes over the training data""",>280k hours,,,also MuLan and SoundStream,,
Ankh_large,1/16/2023,SOTA improvement,1900000000,6.5e+21,14000000000,68,,64,,4802.398249072418,Google TPU v4,UniRef50,Confident,,,524288,Open weights (non-commercial),,,,27229.31456276312,"Operation counting,Third-party estimation","""On average, Ankh improved the PLM SOTA performance by 4.8%""","Figure 1 indicates 1.15B parameters, but both the huggingface model and a replication (https://huggingface.co/ElnaggarLab/ankh-large and https://www.biorxiv.org/content/10.1101/2023.07.05.547496v1.full.pdf) indicate 1.9B parameters.
Notebook for counting params: https://colab.research.google.com/drive/1EGI5_vDl4pOBUukJexMHQR16BFKJe4a5?usp=sharing","Table 9 from here: https://www.biorxiv.org/content/10.1101/2023.07.05.547496v1.full.pdf

Can also be manually estimated based on the details in Table 11 and 4.6.1 Exp 4. 14B residues * 68 epochs = 952B tokens seen in forward passes. However, only 20% of tokens are masked as individual targets; other tokens in consecutive spans are collapsed into single-token targets to reduce computations. For masking rate of 20%, the average sequence will have 36% as many targets as input tokens under this strategy. This is the relevant number of backward passes:
(2 * 952B * 19B) + (4 * 952B * 0.36 * 19B) = 6.22e22

36% figure verified here: https://colab.research.google.com/drive/1ETsmp_KRMK8kIRA5kdfcO9QiPK28cBQ6?usp=sharing ","""We build upon the same results by pre-training our baseline on UniRef50.""","Pretrained over UniRef50; 45M proteins and 14B amino acids, per Table 2

952B tokens from Table 9 at:
https://www.biorxiv.org/content/10.1101/2023.07.05.547496v1
(This is total tokens over multiple epochs)",,,,Table 11,"cc non-commercial:
https://github.com/agemagician/Ankh/blob/main/LICENSE.md"
Nucleotide Transformer,1/15/2023,SOTA improvement,2500000000,8.08e+21,3E+11,1,672,128,,51064.70621394041,NVIDIA A100,"Human Reference Genome (GRCh38/hg38),1000 Genomes Project",Likely,,9.35e+17,,,,,,113456.5902448678,"Operation counting,Hardware","""We show that the representations alone match or outperform specialized
methods on 11 of 18 prediction tasks, and up to 15 after fine-tuning.""","""We built four distinct foundation language models of different sizes, ranging from 500M up to 2.5B parameters""","""Training the largest parameter model required a total of 128 GPUs across 16 compute nodes for 28 days""

In repo, they default to jnp.float32, but recommend fp16 or bp16 for activations in the docstring. JAX defaults to TF32 so they should be utilizing tensor cores.

Assuming 1.56e14 FLOP/s for 32-bit calculations and 0.3 utilization rate

Estimate: 1.56e14 FLOP/s * 128 GPUs * 28 days * 24 h/day * 3600 s/h * 0.3 utilization rate = 1.45e22

Or, with 6ND:
""the model processed a total of 300B tokens during training""
6 * 300B * 2.5B = 4.5e21

Geometric mean: sqrt(1.45e22 * 4.5e21) = 8.08e21","""pre-trained them on three different datasets encompassing the Human
reference genome, a collection of 3,200 diverse human genomes, and 850 genomes from several species""
","Largest dataset is the 1000 Genome dataset, with 3202 genomes for a total of 20.5 trillion nucleotides. However, for each dataset training was run until the model saw 300B tokens.","""Training the largest parameter model required a total of 128 GPUs across 16 compute nodes for 28 days""",,"""All fine-tuning runs were performed on a single node with eight A100 GPUs. [...] On average, a fine-tuning run lasted 20 minutes for the 500M parameter models, and 50 minutes for the 2.5B parameter models.""

Estimate: 78e12 FLOP/s * 8 GPUs * 50 min * 60 seconds * 0.5 utilization rate = 9.36e17",,
VALL-E,1/5/2023,SOTA improvement,353000000,1.01e+19,820800000,,,,,11.40575196486363,NVIDIA V100,LibriLight,Speculative,,,,Unreleased,,,,,Operation counting,"""VALL-E significantly outperforms
the state-of-the-art zero-shot TTS system [Casanova et al., 2022b] in terms of speech naturalness and
speaker similarity, with +0.12 comparative mean option score (CMOS) and +0.93 similarity mean
option score (SMOS) improvement on LibriSpeech""","""Both the AR model and the NAR model have the same transformer architecture with 12
layers, 16 attention heads, an embedding dimension of 1024, a feed-forward layer dimension of 4096, and a dropout of 0.1""

Ben's script says that's 353M parameters, using n_block 12, d_model 1024, d_ff 4096, encoder only False

https://github.com/bencottier/ml-parameter-count/blob/main/parameter_count.py","""The models are trained using 16 NVIDIA TESLA V100 32GB GPUs with a batch size of 6k acoustic
tokens per GPU for 800k steps""

353M * 800k * 6k * 6 = 1.01e19

16 V100s is 2080 teraFLOP or 2e15 FLOP so 1e19 would take 1.5 hours at 100% utilization or ~5 hours at 30%. Is that plausible?","""60K hours of English speech with over 7000 unique speakers.""","60k hours
~13,680 words/hour * 60,000 = 820800000 words
https://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.3pbt0hfgv7pq",,,,,
Hybrid H3-2.7B,12/28/2022,SOTA improvement,2700000000,8.49e+20,4E+11,509.02,,8,,,NVIDIA A100 SXM4 80 GB,The Pile,,,,,Open weights (unrestricted),Unreleased,Open source,,7092.290277,,Results table shows SOTA performance for some benchmarks,2.7B,6 FLOP/token/parameter * 400000000000 training tokens * 2700000000 parameters = 6.48e+21 FLOP,,"""We train hybrid models at sizes 125M, 355M, 1.3B, and 2.7B on the Pile [21] for 400B tokens""","""All models were trained on either a single 16xA100-40GB node or a cluster of 8xA100-80GB nodes.""",,,,"apache 2.0
repo (weights and inference only): https://github.com/HazyResearch/H3/blob/main/README.md "
CaLM,12/19/2022,SOTA improvement,86000000,2.9e+19,2304000000,14,960,4,,,NVIDIA Quadro RTX 4000,European Nucleotide Archive (ENA),Likely,,,,,,,,1418.583589472499,"Hardware,Operation counting","""We show that large language models trained on codons, instead of amino acid sequences, provide high-quality representations that outperform comparable state-of-the-art models across a variety of tasks. In some tasks, like species recognition, prediction of protein and transcript abundance, or melting point estimation, we show that a language model trained on codons outperforms every other published protein language model, including some that contain over 50 times more parameters"" [Abstract]","""We trained a large language model with 86M parameters""","""4 NVIDIA Quadro RTX4000 GPUs for 40 days""

Calculation assuming FP32, utilization 30%:
= (40 * 24 * 3600) s * 7.1e12 FLOP/s * 0.3 * 4 GPU = 2.999808e+19

alternative calculation:
""Gradients were accumulated to an effective batch size of 1,000 examples, or approximately 256,000 tokens. ""
""(66,000 gradient steps, 14 full epochs)""

256000*66000*14*86000000*6=1.220567e+20","""The training set was constructed from the European Nucleotide Archive [39], with significant preprocessing to limit redundancy and save computational cost.""","""a dataset of 9M non-redundant and diverse cDNA sequences identified from whole-genome sequencing""

""Gradients were accumulated to an effective batch size of 1,000 examples, or approximately 256,000 tokens. ""

9000000*256000/1000=2304000000 tokens","""The model reported in this work was trained on 4 NVIDIA Quadro
RTX4000 GPUs for 40 days (66,000 gradient steps, 14 full epochs)""",,,,
RT-1,12/13/2022,SOTA improvement,35000000,,,,,,,,,RT-1,Confident,,,,Open weights (unrestricted),Open source,Open source,,,,"""Across each category, we find that RT-1 outperforms the prior
models significantly. On seen tasks, RT-1 is able to perform 97% of the more than 200 instructions successfully, which is 25% more than BC-Z and 32% more than Gato. On unseen tasks, RT-1
shows it is capable of generalizing to novel instructions, performing 76% of the never-before-seen
instructions, 24% more than the next best baseline""","""we also limit the size of the model compared to
the original publication, which was 1.2B parameters (resulting in on robot inference time of 1.9s),
to be of similar size to RT-1 (37M parameters for Gato vs. 35M for RT-1""

16M params for image tokenizer, 19M for the transformer",,"""We utilize a dataset that we gathered over the course of 17 months with a fleet of 13 robots, containing
∼130k episodes and over 700 tasks""

Episode is an example of robot following instructions",,,,,,"weights and code here, apache 2.0:
https://github.com/google-research/robotics_transformer

data here, also apache: 
https://github.com/google-deepmind/open_x_embodiment"
TranceptEve,12/10/2022,SOTA improvement,,,,,,,,,,ProteinGym,Unknown,Tranception,,,,,,,,,"""Besides its broader application scope, it achieves state-of- the-art performance for mutation effects prediction, both in terms of correlation with experimental assays and with clinical annotations from ClinVar.""",,,,,,,,,
DeepNash,12/1/2022,SOTA improvement,,,,,,,,,,,Unknown,,,,,,,,,,"DeepNash beat existing state-of-the-art AI methods in Stratego and achieved a year-to-date (2022) and all-time top-three ranking on the Gravon games platform, competing with human expert players.",,"""The final agent was trained using 768 MXU’s (matrix multiplication unit) for Learners and
256 MXU’s for Actors (using 256 TPU’s in total).""
Some more details in Table S1 (in supplementary materials)",,"768 * 7.21M trajectories? (Table S1)

768 * 7.21M = 5,537,280,000

https://www.science.org/doi/suppl/10.1126/science.add4679/suppl_file/science.add4679_sm.pdf",,,,,
GPT-3.5 Turbo,11/30/2022,"Historical significance,Significant use",20000000000,,,,,,,,,Unspecified unreleased,Speculative,,,,API access,Unreleased,,,,,"https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/

was the default free model in ChatGPT, so likely one of the most popular models in existence",20B parameters according to Table 1 in Microsoft's CODEFUSION paper: https://arxiv.org/pdf/2310.17680.pdf,,,,,,,,available on API: https://platform.openai.com/docs/models/gpt-3-5-turbo 
GPT-3.5,11/28/2022,"Historical significance,Significant use,SOTA improvement,Training cost",,2.578e+24,,,,,,4625550.747400068,NVIDIA A100 SXM4 40 GB,,Speculative,,,,API access,Unreleased,,TRUE,,"Comparison with other models,Benchmarks",,"Parameter count may be 175B based on OpenAI's statements that text-davinci-003 is in the GPT-3.5 series of models. It was also stated to be 175B in the Microsoft CODEFUSION paper, but the paper was reportedly retracted because the authors did not know the parameter count.",https://colab.research.google.com/drive/1QSxa8YCWjEBQU7mrXLhw6TP1VX5oqgdW#scrollTo=Gt6Z6oZ26clI,,,,,,,
DiT-XL/2 + Discriminator Guidance,11/28/2022,SOTA improvement,,,,10,,,,,NVIDIA A100,,Unknown,DiT-XL/2,,,,,,,,,"""Using our algorithm, we achive state-of-the-art results on ImageNet 256x256 with FID 1.83 and recall 0.64, similar to the validation data's FID (1.68) and recall (0.66)""",,"This is a finetune of DiT-XL/2, so its compute won't be much higher.",,,,,,,
Discriminator Guidance,11/28/2022,SOTA improvement,,2.1570000001e+20,,10,481,,,337.8811363173893,NVIDIA A100 PCIe,,Confident,,,,Open weights (non-commercial),Open (non-commercial),Open access (non-commercial),,,Hardware,"""Using our algorithm, we achive state-of-the-art results on ImageNet 256x256 with FID 1.83 and recall 0.64, similar to the validation data's FID (1.68) and recall (0.66).""
https://paperswithcode.com/paper/refining-generative-process-with",,481 hours * 312 TFLOPS (A100) * 40% utilization,,,Table 6,,,,"https://github.com/alsdudrla10/DG
Attribution-NonCommercial-ShareAlike 4.0 International

checkpoints and train code here: https://github.com/alsdudrla10/DG/blob/main/README.md "
ALM 1.0,11/28/2022,SOTA improvement,335000000,,,,,,,,,ArabicText 2022,Speculative,,,,,,,,,,SOTA results on Arabic-language benchmark ALUE.,335M parameters: https://github.com/FlagAI-Open/FlagAI/blob/master/examples/ALM/README.md,,"""ALM-1.0 uses the largest open-source Arabic text dataset ArabicText 2022. You can check ArabicText 2022 for more information.""",,,,,,
CICERO,11/22/2022,SOTA improvement,,,,,,,,,,WebDiplomacy,Unknown,,,,Open weights (non-commercial),Open source,Open source,,,,"""We introduce Cicero, the first AI agent to achieve human-level performance in Diplomacy""","""We took R2C2 (22) as our base model – a 2.7B parameter Transformer-based (23) encoder-decoder model pre-trained on text from the Internet using a BART de-noising objective (24).""",,,"""We obtained a dataset of 125,261 games of Diplomacy played online at webDiplomacy.net. Of these, 40,408 games contained dialogue, with a total of 12,901,662 messages exchanged between players. Player accounts were de-identified and automated redaction of personally identifiable information (PII) was performed by webDiplomacy. We refer to this dataset hereafter as WebDiplomacy .""",,,,,"creative commons (non comm) for model weights, MIT for code
https://github.com/facebookresearch/diplomacy_cicero?tab=readme-ov-file#license-for-model-weights"
AR-LDM,11/20/2022,SOTA improvement,1500000000,5.1e+20,,50,194,,,745.8360575556148,NVIDIA A100,,Confident,Stable Diffusion (LDM-KL-8-G),,,Unreleased,Open (non-commercial),Open access (non-commercial),,,Hardware,"The first latent diffusion model for coherent visual story synthesizing.
""Quantitative results show that AR-LDM achieves SoTA FID scores on PororoSV, FlintstonesSV, and the newly introduced challenging dataset VIST containing natural images""",Table 1,8 NVIDIA A100 GPUs for 8 days,,"PororoSV, FlintstonesSV and VIST. All storytelling datasets, sizes would be possible to look up.",8 NVIDIA A100 GPUs for 8 days,,,,"no weights, no license. training and inference code here:
https://github.com/xichenpan/ARLDM"
Fusion in Encoder,11/18/2022,SOTA improvement,330000000,1.3e+20,,,48,,,233.0630322095263,NVIDIA A100 SXM4 80 GB,TriviaQA,Likely,,,,,,,,,Hardware,"""Using our proposed method, we outperform the current state-of-the-art method by 2.5 Exact Match score on the Natural Question dataset while using only 25% of parameters and 35% of the latency during inference, and 4.4 Exact Match on WebQuestions dataset""",330M,"""The experiments were run on 8x80GB Nvidia A100s with 800GB RAM and 4x32-core CPUs, and each experiment took around 1 day for NQ and 2 days for TriviaQA with large models. Inference was run on the same system, and took 2 minutes.""

2 days * 24 * 3600 * 8 * 312 teraflop/s * 0.3 utilization = 1.3e20",,79k per table 11 (probably number of question-answer pairs),2 days,,,,
Galactica,11/16/2022,SOTA improvement,1.2E+11,3.24e+23,1.06E+11,4,,128,,591076.8943544837,NVIDIA A100 SXM4 80 GB,Galactica Corpus,Likely,,,2000000,Open weights (non-commercial),Unreleased,,,113523.59989094557,Operation counting,"""We outperform existing models on a range of scientific tasks. On technical knowledge probes such as LaTeX equations, Galactica outperforms the latest GPT-3 by 68.2% versus 49.0%. Galactica also performs well on reasoning, outperforming Chinchilla on mathematical MMLU by 41.3% to 35.7%, and PaLM 540B on MATH""","""The largest 120B model we train runs on a single NVIDIA A100 node""","Authors state the model is trained on 450b tokens. Using 6 FLOP/token/parameter, this is 6*120b*450b = 3.24e23","""Our corpus consists of 106 billion tokens from papers, reference material, encyclopedias and other scientific sources. We combine natural language sources, such as papers and textbooks, and natural sequences, such as protein sequences and chemical formulae. We process LATEX where we can capture it, and also include academic code to capture computational science""","""Total dataset size = 106 billion tokens""",,,,"Table 1: batch size 2M, warmup 1.1B (out of 450B tokens)","cc-by-nc (non-commercial): https://huggingface.co/facebook/galactica-120b 

repo but no training code: https://github.com/paperswithcode/galai/blob/main/README.md "
EVA-01,11/14/2022,SOTA improvement,1011000000,1.501e+22,29600000,150,348,128,,29374.46909439904,NVIDIA A100 SXM4 40 GB,"ImageNet21k,COCO,Conceptual Captions 12M (CC12M),Conceptual Captions (CC3M)",Confident,,,,Open weights (unrestricted),Open source,Open source,,113525.84154878548,Hardware,"from abstract 'Via this pretext task, we can efficiently scale up EVA to one billion parameters, and sets new records on a broad range of representative vision downstream tasks, such as image recognition, video action recognition, object detection, instance segmentation and semantic segmentation without heavy supervised training.'",1011M from table 3,"flops = (128) * (3.12e14) * (14.5 * 24 * 3600) * (0.3) = 1.501e22
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)

from Table 3, time and num gpus, GPU model is on page 4 (A100), precision is fp16 and likely utilizes tensor cores","from table 3 : ImageNet-21K, CC12M, CC3M, Object365, COCO, ADE",from table 3: 29.6M images,from Table 3 14.5 days = 348 hours,,,,MIT: https://github.com/baaivision/EVA pretrain code: https://github.com/baaivision/EVA/tree/master/EVA-01/eva 
AltCLIP_M9,11/12/2022,SOTA improvement,,,,10,,,,,,"Conceptual Captions (CC3M),LAION-400M,TSL2019,OPUS,WuDao Corpora,LAION-2B",Unknown,CLIP (ViT L/14@336px),,,Open weights (unrestricted),Open source,Open source,,,,"""We set new state-of-the-art performances on a bunch of tasks including ImageNet-CN, Flicker30kCN, COCO-CN and XTD""",,,,,,,,,https://github.com/FlagAI-Open/FlagAI/blob/master/examples/AltCLIP/altclip_ft_bmtrain.py Apache-2.0 license
InternImage,11/10/2022,SOTA improvement,1080000000,2.408e+21,427000000,30,,,,,,"LAION-400M,Conceptual Captions 12M (CC12M),ImageNet-1k",Confident,,,,Open weights (unrestricted),,,,,Operation counting,"""InternImage-H achieved a new record 65.4 mAP on COCO test-dev and 62.9 mIoU on ADE20K, outperforming current leading CNNs and ViTs""","1.08B, table 1","InternImage-H is pre-trained on a 427 million joint dataset of public Laion-400M [61], YFCC-15M [62], and CC12M [63] for 30 epochs, and then fine-tuned the model on ImageNet-1K for 20 epochs. ImageNet-1K has 1,281,167 images.

Table 2 says InternImage-H uses 188 GFLOP per forward pass at 224 resolution, and 1478 GFLOP at 640

Table 7 indicates training InternImage-H was done at a scale of ""224/640"" so presumably there was pretraining at 224x224 resolution and then some fine-tuning at 640x640. It's not clear how much training was done at each resolution, but typically this is a small fraction of total training (e.g. Noisy Student finds it sufficient to train for 350 epochs at smaller resolution, and then fine-tune at the higher resolution for 1.5 epochs). We'll ignore the additional FLOPs from high resolution training.

Total training FLOPs:
188e9 FLOP/image * (427M images * 30 epochs) + (1.281M images * 20 epochs) * 3 (additional FLOPs for backward pass) = 2.408e21","""To further explore the capability of our model and match the large-scale private data used in previous methods [16, 20, 59], we adopt M3I
Pre-training [60], a unified pre-training approach available
for both unlabeled and weakly-labeled data, to pre-train
InternImage-H on a 427 million joint dataset of public
Laion-400M [61], YFCC-15M [62], and CC12M [63] for
30 epochs, and then we fine-tune the model on ImageNet1K for 20 epochs.""",,,,,,"https://github.com/OpenGVLab/InternImage
MIT license
"
mT0-13B,11/3/2022,SOTA improvement,13000000000,,20000000000,,,,,,,xP3,Confident,mT5-XXL,1.01e+21,,Open weights (unrestricted),Unreleased,,,,,"""Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results.""

Table 1",13B,"fine-tuned from mT5

1.37e22 fine-tune compute","""In addition, we introduce xP3, a composite of supervised datasets in 46 languages with English and machine-translated prompts. 

https://huggingface.co/datasets/bigscience/xP3","per https://huggingface.co/datasets/bigscience/xP3, 94,941,936 KB or 94GB 

if approx 200M words per GB, that's ~20B words (rougher estimate because it's multilingual)

https://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.ieihc08p8dn0",,,"""We finetune the models for an additional 13 billion tokens with loss only being computed on target tokens...
For finetuning mT5, we follow the same procedure as described above for BLOOM, except that inputs are fed into the encoder and thus are not space-separated from targets.""

13B * 13B * 6 = 1.01e21",,apache 2.0
Mogrifier RLSTM (WT2),11/3/2022,SOTA improvement,35000000,1.4e+17,,250,,,,,,WikiText-2,Confident,,,,Unreleased,Unreleased,,,,Operation counting,"""On top of these improvements, the RLSTM
outperformed the LSTM by a small margin, and we established a new state of the art on both datasets""",Table 1,6ND = 6*35000000*2666667*250 = 1.4000002e+17,,,,,,,
BLOOMZ-176B,11/3/2022,SOTA improvement,1.76E+11,,20000000000,,,,,,,xP3,Likely,BLOOM-176B,1.3728e+22,,Open weights (unrestricted),Open source,Open source,,,,"""Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results.""

Table 1",176B,"fine-tuned from BLOOM-176B

1.37e22 fine-tune compute","""In addition, we introduce xP3, a composite of supervised datasets in 46 languages with English and machine-translated prompts. 

https://huggingface.co/datasets/bigscience/xP3","per https://huggingface.co/datasets/bigscience/xP3, 94,941,936 KB or 94GB 

if approx 200M words per GB, that's ~20B words (rougher estimate because it's multilingual)

https://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.ieihc08p8dn0",,,"""We use publicly available pretrained BLOOM models ranging from 560 million to 176 billion parameters. BLOOM models are large decoder-only language models pretrained for around 350 billion tokens with an architecture similar to GPT-3
(Brown et al., 2020). We finetune the models for an additional 13 billion tokens with loss only being
computed on target tokens.""

13B * 176B * 6",,"Apache 2.0
train/eval code: https://github.com/bigscience-workshop/xmtf?tab=readme-ov-file#train-models 
weights: https://huggingface.co/bigscience/bloomz "
eDiff-I,11/2/2022,SOTA improvement,9100000000,5.46e+19,1000000000,,,,,,NVIDIA A100,Unspecified unreleased,Likely,,,,API access,,,,,Operation counting,"SOTA zero-shot FID on COCO 2014, Table 1

May be significantly used, via Nvidia Picasso: https://www.nvidia.com/en-us/gpu-cloud/picasso/","9.1B for config D, Table 1","6ND = 6*9100000000*1000000000=5.46e+19 (likely, might change because of several epochs / dataset division)

""The base model was trained using 256 NVIDIA A100 GPUs, while the two super-resolution models were trained with 128 NVIDIA A100 GPUs each"" 
no info on duration","""We use a collection of public and proprietary datasets to train our model. To ensure high-quality training data, we apply heavy filtering using a pretrained CLIP model to measure the image-text alignment score as well as an aesthetic scorer to rank the image quality""","""The final dataset to train our model contains about one billion text-image pairs""",,,,,
EnCodec,10/24/2022,SOTA improvement,,,,300,,,,,NVIDIA A100,"DNS,Common Voice,AudioSet,FSD50K,Jamendo",Unknown,,,,Open weights (non-commercial),Open source,Open source,,,,""" Finally, our best model, EnCodec, reaches state-of-the-art scores for speech and for
music at 1.5, 3, 6, 12 kbps at 24 kHz, and at 6, 12, and 24 kbps for 48 kHz with stereo channels.""",,"""We train all models for 300 epochs, with one epoch being 2,000 updates with the Adam optimizer with a batch size of 64 examples of 1 second each, a learning rate of 3 · 10−4 , β1 = 0.5, and β2 = 0.9. All the models are traind using 8 A100 GPUs""","""We train EnCodec on 24 kHz monophonic across diverse domains, namely: speech, noisy speech, music and
general audio while we train the fullband stereo EnCodec on only 48 kHz music. For speech, we use the clean speech segments from DNS Challenge 4 (Dubey et al., 2022) and the Common Voice dataset (Ardila et al., 2019).
For general audio, we use on AudioSet (Gemmeke et al., 2017) together with FSD50K (Fonseca et al., 2021).
For music, we rely on the Jamendo dataset (Bogdanov et al., 2019) for training and evaluation and we further
evaluate our models on music using a proprietary music dataset.""","~17k hours total, per Table A.1",,,,,"MIT for repo in general, non commercial weights. Dataset is in repo.
https://github.com/facebookresearch/audiocraft/blob/main/docs/ENCODEC.md"
U-PaLM (540B),10/20/2022,SOTA improvement,5.4E+11,2.53e+24,,,120,512,,,Google TPU v4,,Confident,PaLM (540B),4e+21,,Unreleased,Unreleased,,TRUE,218023.49948616367,Comparison with other models,"""We show that U-PaLM 540B outperforms PaLM 540B on 21 out of 26 tasks. Given that PaLM is
the SOTA language model on these tasks, this makes U-PaLM the new state-of-the-art on these tasks.""

performance improvement equivalent to 2x training efficiency: ""Impressively, at 540B scale, we show an approximately 2x computational savings rate where U-PaLM achieves the same performance as the final PaLM 540B model at around half its computational budget """,,"""The total number of extra tokens we train on for the 540B
model is approximately 1.3 Billion which constitutes 0.16% extra computation... Training an U-PaLM 540B model only consumes 512 TPUv4 chips and finishes in about 5 days which is considered to be lightweight.""

original PaLM was 2.527e+24. adding 0.16% is ~2.53e24","""To keep things consistent, we train this model with the same data mixture as PaLM and do not rely on additional sources of data (labeled or unlabeled).""",,5 days,,"""The total number of extra tokens we train on for the 540B
model is approximately 1.3 Billion which constitutes 0.16% extra computation... Training an U-PaLM 540B model only consumes 512 TPUv4 chips and finishes in about 5 days which is considered to be lightweight.""

PaLM was 2.5e24
0.16% of that is 4e21",,
LMSI-Palm,10/20/2022,SOTA improvement,5.4E+11,,,,,,,,,GSM8K,Confident,PaLM (540B),,,Unreleased,,,,,,"""We show that our approach improves the general reasoning ability of a 540B-parameter LLM (74.4%->82.1% on GSM8K, 78.2%->83.0% on DROP, 90.0%->94.4% on OpenBookQA, and 63.4%->67.9% on ANLI-A3) and achieves state-of-the-art-level performance, without any ground truth label.""",540B,"(fine-tuned from Palm-540B, which was 2.52e24)",Trained on chain-of-thought PaLM output from several datasets of questions that require reasoning. See section 4,,,,"""To reduce the training burden, we sample 5k examples from the non-football and football partition of the DROP dataset, and sample 5k examples from ANLI-A2 and ANLI-A3. For each dataset, we fine-tune the model for 10k steps with a learning rate of 5e−5
and a batch size of 32."" Not sure about sequence length",,
Flan-T5 11B,10/20/2022,Highly cited,11000000000,3.3e+22,1E+11,,,,,98374.29476250126,Google TPU v4,,Confident,T5-11B,7.6e+19,,Open weights (unrestricted),Unreleased,,,,Reported,,11B,"Table 2: 0.2% greater than T5 xxl, which used 3.3e22 FLOP","Various instruction examples for many tasks:

""Our final set of finetuning tasks is sourced from a combination of tasks from FLAN, T0, Natural Instructions, along with some dialog, program synthesis, and chain-of-thought reasoning tasks, as described in Figure 2. We provide specific pointers and citations in Table 24. All data sources are publicly
available. We also remove all MMLU tasks from Natural Instructions to preserve its role as a broad benchmark of 57 held-out tasks for evaluation. In total, there are 1,836 tasks."" ","""For T5 models without instruction finetuning, we use LM-adapted models, which were produced by training T5 on 100B additional tokens from C4 on a standard language modeling objective""",,,"7.6e19, per Table 2",,"apache 2.0 license

https://github.com/google-research/t5x/blob/main/docs/models.md#flan-t5-checkpoints"
Flan-PaLM 540B,10/20/2022,"Highly cited,SOTA improvement",5.4E+11,2.4999999999999997e+24,,,37,512,0.3,,Google TPU v4,Flan,Confident,PaLM (540B),5.6e+21,,Unreleased,Unreleased,,TRUE,218023.49948616367,"Reported,Hardware",">1k cites

""Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU.""",540B,"0.2% greater than Palm 540B, which used 2.5e24","Various instruction examples for many tasks:

""Our final set of finetuning tasks is sourced from a combination of tasks from FLAN, T0, Natural Instructions, along with some dialog, program synthesis, and chain-of-thought reasoning tasks, as described in Figure 2. We provide specific pointers and citations in Table 24. All data sources are publicly
available. We also remove all MMLU tasks from Natural Instructions to preserve its role as a broad benchmark of 57 held-out tasks for evaluation. In total, there are 1,836 tasks."" ",,"""we only use 0.2% of the pre-training compute to instruction-finetune Flan-PaLM 540B (approximately 512 v4 TPU chips for 37 hours)""",,"5.6e21 per Table 2

""we only use 0.2% of the pre-training compute to instruction-finetune Flan-PaLM 540B (approximately 512 v4 TPU chips for 37 hours)""

512 * 37 * 3600 * 275 teraflops * 0.3 = 5.6e21 (so 30% utilization was correct)",,
GenSLM,10/11/2022,SOTA improvement,25000000000,1.42e+21,56000000001.00008,,,,,,,"SARS-CoV-2 genome dataset,BV-BRC",Confident,,,,,,,,,Reported,"""Together, these capabilities go beyond state-of-the-art techniques
for global-scale whole genome surveillance of pandemic-causing
viruses and address a critical infrastructure need for the global
public health organization"" - SOTA improvement on very specific task",See Table 3,"See Table 3
Overall ZettaFlops 1.42","SARS-CoV-2 genome datasets from multiple sources:

""we used >1.5 million high-quality BV-BRC SARSCoV-2 complete genome sequences""

""We also utilized a dataset collected by the Houston Methodist Hospital System - one of the largest single-institution collections of SARS-CoV-2 genome sequences in the United States. [...]  Sequences with >256 ambiguous characters were discarded, leaving 16,545 total sequences""

Prokaryotic gene sequence dataset from BV-BRC:
""To allow for better generalization and avoid overfitting of the models to the SARS-CoV-2 data, we used >110 million unique prokaryotic gene sequences from BV-BRC""","110,000,000 sequences * 512 tokens/sequence = 56,320,000,000 tokens (5.6e10)",,,,,
Diplodocus,10/11/2022,SOTA improvement,,,,,,,,,,,Unknown,,,,Open weights (non-commercial),Open source,Open source,,,,"SOTA Improvement in no-press Diplomacy
""In a 200-game no-press Diplomacy tournament involving 62 human participants spanning skill levels from beginner to expert, two Diplodocus agents both achieved a higher average score than all other participants who played more than two games, and ranked first and third according to an Elo ratings model. """,may be estimated from https://github.com/facebookresearch/diplomacy_cicero?tab=readme-ov-file,,"""we train the architecture described in Appendix F on a dataset of roughly 46000 online Diplomacy games provided by webdiplomacy.net.""
then self-play training","""we train the architecture described in Appendix F on a dataset of roughly 46000 online Diplomacy games provided by webdiplomacy.net.""
then self-play training",,,,,"creative commons (non comm) for model weights, MIT for code
https://github.com/facebookresearch/diplomacy_cicero?tab=readme-ov-file#license-for-model-weights"
Phenaki,10/5/2022,SOTA improvement,1800000000,,,,,,,,,"LAION-400M,Unspecified unreleased",,,,,,,,,,,"""To the best of our knowledge, this is the first time a paper studies generating videos from time variable prompts""","Unless specified otherwise, we train a 1.8B parameter Phenaki model on a corpus of ∼15M textvideo pairs at 8 FPS mixed with ∼50M text-images plus ∼400M pairs of LAION-400M [41] (more
details in Appendix B.3). The model used in the visualisations in this paper was trained for 1 million
steps at a batch size of 512, which took less than 5 days. In this setup 80% of the training data came
from the video dataset and each image dataset contributed 10%.",,"Unless specified otherwise, we train a 1.8B parameter Phenaki model on a corpus of ∼15M textvideo pairs at 8 FPS mixed with ∼50M text-images plus ∼400M pairs of LAION-400M [41] (more details in Appendix B.3). The model used in the visualisations in this paper was trained for 1 million steps at a batch size of 512, which took less than 5 days. In this setup 80% of the training data came from the video dataset and each image dataset contributed 10%.","Unless specified otherwise, we train a 1.8B parameter Phenaki model on a corpus of ∼15M textvideo pairs at 8 FPS mixed with ∼50M text-images plus ∼400M pairs of LAION-400M [41] (more
details in Appendix B.3). The model used in the visualisations in this paper was trained for 1 million
steps at a batch size of 512, which took less than 5 days. In this setup 80% of the training data came
from the video dataset and each image dataset contributed 10%.",,,,,
DiffDock,10/4/2022,SOTA improvement,20240000,7.2e+19,17000,850,432,,,,NVIDIA RTX A6000,PDB (Protein Data Bank),Likely,,,,Open weights (unrestricted),,,,,Hardware,"""DiffDock obtains a 38% top-1 success rate (RMSD<2A) on PDBBind, significantly outperforming the previous state-of-the-art of traditional docking (23%) and deep learning (20%) methods""","""For determining the hyperparameters of DIFFDOCK’s score model, we trained
smaller models (3.97 million parameters) that fit into 48GB of GPU RAM before scaling it up to the final model (20.24 million parameters) that was trained on four 48GB GPUs""

There's a separate 4.77M ""confidence model"" that helps make predictions along with the score model","""We trained our final score model on four 48GB RTX A6000 GPUs for 850 epochs (around 18 days).""

4 * 38.7 teraflops * 18 days * 24 * 3600 * 0.3 = 7.2e19

https://www.techpowerup.com/gpu-specs/rtx-a6000.c3686","""We evaluate our method on the complexes from PDBBind [Liu et al., 2017], a large collection of protein-ligand structures collected from PDB [Berman et al., 2003], which was used with time-based splits to benchmark many previous works""
","""We employ the time-split of PDBBind proposed by Stark et al. [2022] with 17k complexes from 2018 or earlier for training/validation and 363 test structures from 2019 with no ligand overlap with the training complexes""",18 days,,,,
Make-A-Video,9/29/2022,SOTA improvement,,,,,,,,,,"LAION,WebVid-10M,HD-VILA-100M",Unknown,,,,,,,,,,,,,,,,,,,
Whisper,9/21/2022,SOTA improvement,1550000000,4.2072663e+21,9302400000,3,,,,,,Unspecified unreleased,Likely,,,256,Open weights (unrestricted),Unreleased,,,,Hardware,,Table 1,See figure 9,,"""When scaled to 680,000 hours of multilingual and multitask
supervision, the resulting models generalize well
to standard benchmarks and are often competitive
with prior fully supervised results but in a zeroshot transfer setting without the need for any finetuning.""


13,680 words/h * 680,000h = 9,302,400,000 words",,,,Table 17,"MIT for weights:
https://github.com/openai/whisper

the repo looks like just inference code to me. also, this paper says it's just inference code and they reproduced their version of Whisper through other means: https://arxiv.org/pdf/2309.13876 "
PaLI,9/14/2022,SOTA improvement,16900000000,1.69e+23,1600000000,1,240,1024,,50878.10777366616,Google TPU v4,WebLI,Likely,,,,Unreleased,Unreleased,,,436202.7314577782,"Operation counting,Hardware","""PaLI achieves state-of-the-art in multiple vision and language tasks
(such as captioning, visual question-answering, scene-text understanding)""","3.9b Image Encoder, 
14b Multimodal Encoder-Decoder","Pre-training the ViT component involved 1.1 million steps (they train over 1M steps but run the last 100k twice and then average the two resulting models). Batch size is 16384 and the inputs are 224x224. Table 8 indicates a forward pass with ViT-e/14 on a 224 image takes 1980 GFLOPs, so total training compute for the ViT-e/14 model is:
1980e9 * 16384 * 1.1 million * 3 (account for backward passes) = 1.07e23

In the ""overal model"" section, they then say: ""The largest model, PaLI-17B, is pretrained using 1,024 GCP-TPUv4 chips for 7 days"". It is then trained for another 3 days on 512 chips at higher resolution. 

I assume the stated TPUv4 training does not include the ViT pretraining, since it amounts to fewer FLOPs than we estimate above for the ViT.

275 teraFLOP/s * ((1024 * 7) + (512 * 3)) * 24 * 3600 * 0.3 (utilization assumption) = 6.2e22

Total: 1.07e23 + 6.2e22 = 1.69e23","""we introduce WebLI, a multilingual imagelanguage dataset built from images and texts available on the public web... Due to the abundance of multilingual content on the internet, the collection process for the WebLI dataset can be scaled to cover 10 billion images and 12 billion alt-texts. In addition to annotation with web text, we use publicly available automatic service to extract OCR annotations on all images, resulting in 29 billion image-OCR pairs. To balance quality and retain scale, we filter the dataset to the highest quality subset retaining only the top 10% scoring of the original WebLI image-text pairs (about 1B examples), which we use to train PaLI""","""During training, the model passes over 1.6B images, one epoch over the entire pretraining dataset""",10,,,,
BEIT-3,8/22/2022,SOTA improvement,1900000000,7e+19,,,,,,,,"ImageNet21k,COCO,English Wikipedia,BookCorpus (BooksCorpus, Toronto Book Corpus)",Likely,,,,Unreleased,,,,,Operation counting,"from abstract: 'In this work, we introduce a general-purpose multimodal foundation model BEiT-3, which achieves state-of-the-art transfer performance on both vision and vision-language tasks.'",1.9B from Table 2,"from Table 11, 1M training steps with batch size 6144. 
From Table 2 we have that model have 1.9B parameters.
Model is VIT",from Table 3,"from Table 3
21M pairs image text,
14M images,160GB documents",,,,,
BlenderBot 3,8/10/2022,SOTA improvement,1.75E+11,4.3e+23,1300000000,,,128,,,NVIDIA A100 SXM4 40 GB,BlenderBot 3 Data,Likely,OPT-175B,1.5e+21,262144,Open weights (non-commercial),Open source,Open source,TRUE,113634.05202226162,Operation counting,"""Human evaluations show its superiority to existing open-domain dialogue agents, including its predecessors""",,(taken from OPT-175 base),"Fine-tuned from OPT-175B.

""The fine-tuning data for BB3 comprises roughly 4 million source/target examples spread across the various training modules. This corresponds to around 1.13B training tokens. When fine-tuning the OPT-based BB3 models, we additionally included 600k examples ( 170m tokens) of pre-training data to help with training stability. Table 16 and Table 17 enumerate the breakdown by module.""
",,,,"""The 30B and 175B parameter BlenderBot 3 models were each trained for one epoch of the training data
on 64 (30B) or 128 (175B) x 40gb A100 GPUs; we found that the model (especially the 175B version)
overfit significantly when seeing the training data more than once. The 175B model was trained with
a batch size of 2^18 and the 30B model was trained with a batch size of 2^19, resulting in roughly 5600
updates and 2800 updates respectively.""

175b params * 5600 * 2^18 * 6 = 1.5e21
","Note that this is batch size for fine-tuning. Blenderbot is based on OPT-175B which had batch size 2M.

""The 175B model was trained with a batch size of 2^18""
2^18 = 262144","weights have a non-commercial license, must go through request form: https://docs.google.com/forms/d/e/1FAIpQLSfRzw8xVzxaxgRyuodTZtkcYADAjzYjN5gcxx6DMa4XaGwwhQ/viewform

meanwhile training code is here. repo is MIT-licensed https://github.com/facebookresearch/ParlAI/blob/main/parlai/scripts/train_model.py "
GLM-130B,8/4/2022,SOTA improvement,1.3E+11,3.5490054945e+23,4E+11,1,1440,768,0.325,820296.6313095269,NVIDIA A100 SXM4 40 GB,"The Pile,WuDao Corpora",Confident,,,8650752,Open weights (non-commercial),Unreleased,,,681845.1304541394,"Operation counting,Hardware","""GLM-130B achieves an accuracy of 80.2% on zero-shot LAMBADA (En), while 76.2% for GPT-3 175B and 77.9% for the SOTA offered by PaLM 540B.""",Dense model,"""96 NVIDIA A100 (40G * 8) servers for 2 months""

312 TFLOPS/GPU * 96 servers * 8 GPU/server * 2 months * 32.5% utilization = 4.037e23

utilization rate - citation from the paper: ""we report hardware FLOPs utilization (HFU) of 43.3% and model FLOPs utilization (MFU) of 32.5% due to re-materialization.""

Aligns pretty well with 6ND:
6 * 400B * 130B = 3.12E23

Geometric mean: sqrt(4.037e23 * 3.12e23) = 3.549e23","""The pre-training data includes 1.2T Pile (train split) (Gao et al., 2020) English, 1.0T Chinese WudaoCorpora (Yuan et al., 2021), and 250G Chinese corpora (including online forums, encyclopedia, and
QA) we crawl from the web, which form a balanced composition of English and Chinese contents""","400B ""We completed the 400B-token training and evaluation of GLM-130B in July, and subsequently released the model and pre-training details in August 2022. ""  from https://arxiv.org/pdf/2406.12793

""As of July 3rd, 2022, GLM-130B has been trained on over 400 billion text tokens (200B each for Chinese and English)""","""During the 60-day access to the cluster, we manage to train GLM-130B for 400 billion tokens""
60 days * 24 = 1,440 hours",,,,non commercial license. looks like inference but not training code: https://github.com/THUDM/GLM-130B/blob/main/MODEL_LICENSE
AlexaTM 20B,8/2/2022,SOTA improvement,19750000000,2.04374016e+23,1.319E+12,,2880,128,0.4935,267943.21130997164,NVIDIA A100,"mC4,Wikipedia",Confident,,,2000000,API access,,,,113643.12380745166,Hardware,The Abstract reports SOTA improvement on multiple benchmarks.,See Table 1 on p.3 of the paper,"Training throughput is reported as 154 TFLOP/s - see p.5 of the paper.
""We relied on an internal and optimized version of DeepSpeed that we have since open-sourced (Chiu & Zheng, 2022) to obtain training throughput of up to 154 TFLOPS/GPU on 16 AWS p4d.24xlarge compute instances.""

Accelerator compute days are reported as 15,360 days - see Table 17 on p.18 of the paper.",,"See Table 2 on p.3 of the paper.

119B Wikipedia tokens + 1.2T mC4 tokens = 1319000000000 tokens","See p.5 of the paper: ""We trained AlexaTM 20B for 120 days on 128 A100 GPUs...""",,,"""We trained AlexaTM 20B for 120 days on 128 A100 GPUs for the total of 500k updates with the accumulated batch size of 2 million tokens""",https://aws.amazon.com/about-aws/whats-new/2022/11/alexatm-20b-model-available-sagemaker-jumpstart/?nc1=h_ls
OmegaPLM,7/22/2022,Historical significance,670000000,1.03514112e+22,8960000000.999996,,,,,52400.32118528479,NVIDIA A100 SXM4 80 GB,UniRef50,Confident,,,2097152,,,,,,Hardware,"""Here, we introduce OmegaFold, the first computational method to successfully predict high-resolution protein structure from a single primary sequence alone. Using a new combination of a protein language model that allows us to make predictions from single sequences and a geometry-inspired transformer model trained on protein structures, OmegaFold outperforms RoseTTAFold and achieves similar prediction accuracy to AlphaFold2 on recently released structures""","""Our model contains 66 layers with around 670 million parameters without sharing parameters, which doubles the layer count of ESM-1b but roughly retains the parameter count.""","""OmegaPLM is implemented in PyTorch (44) and trained for 2,560 GPU Nvidia A100 80G days."" 
""Default precision format in Nvidia A100 GPUs is set to TensorFloat-32 for matrix operations.""

Assume 0.3 utilization for language model

Estimate: (2560 * 24 * 3600) s * 156e12 FLOP/s * 0.3 * = 1.04e22","""After pretraining on sequences in UniRef50 (dated at 2021/04)""","Number of sequences: 35 x 10^6
Sequence length: 512
Total data points: 35 x 10^6 x 512 = 1.792 x 10^10 tokens

First stage crop size: 256
First stage data points: 35 x 10^6 x 256 = 8.96 x 10^9 tokens

Additional structural data: ~110,000 sequences = 7.68 x 10^7 tokens

Final estimate: 8.96 x 10^9 tokens","2,560 GPU Nvidia A100 80G days",,,"""[...] each batch contains 4,096 sequences and each sequence is padded or cropped to 512 residues"" 4096 * 512 = 2097152",
ESM2-15B,7/21/2022,SOTA improvement,15000000000,7.35000000001e+22,12000000000,72,1440,512,,163467.82019979745,NVIDIA V100,UniRef50,Confident,,,,Open weights (unrestricted),Unreleased,Open source,,340970.24160896294,"Hardware,Third-party estimation","""The resulting ESM-2 model family significantly outperforms previously state-of-the-art ESM-1b (a ∼650 million parameter model) at a comparable number of parameters, and on structure prediction benchmarks it also outperforms other recent protein language models""","""we train models up to 15B parameters""","from xTrimoPGLM paper Table 9 (https://www.biorxiv.org/content/10.1101/2023.07.05.547496v1): 5.1e22 FLOP

from Arb Research (https://arbresearch.com/files/gen_bio.pdf): ""ESM-2-15B: 270000 updates x 3.2M batch size x 15 B “connections” x 6. : 7.8e22 FLOP

from the paper's Supplementary Materials: 
""We trained each model over 512 NVIDIA V100 GPUs. ESM2 700M took 8 days to train. The 3B parameter LM took 30 days. The 15B model took 60 days.""
60 days x 512 V100s x an imputed 30% utilization"": 1e23 FLOP

Geometric mean: 7.35e22","""UniRef50, September 2021 version, is used for the training of ESM models""","Section A.1.1:
""This allowed ESM-2 models to train on over 60M protein sequences.""

Average protein sequence is 200 tokens, per https://epoch.ai/blog/biological-sequence-models-in-the-context-of-the-ai-directives#fn:4 
60M * 200 = 12B tokens

Epochs: 15B model used 270k steps at 3.2M token batch size
270k * 3.2M / 12B = 72",,,,,"MIT weights, CC BY 4.0 data
https://github.com/facebookresearch/esm?tab=readme-ov-file#available-esmssd

may just be inference code in the repo^"
BLOOM-176B,7/11/2022,"Historical significance,Highly cited",1.76247E+11,3.65664e+23,3.79E+11,1,2808,384,0.5,901068.6599742996,NVIDIA A100 SXM4 80 GB,BigScience ROOTS Corpus,Confident,,,4194304,Open weights (restricted use),Unreleased,,,341004.3433715435,Hardware,"Was the largest open-source model at the time. 1000+ researchers, many from important orgs such as Microsoft and NVIDIA.

https://huggingface.co/bigscience/bloom","See ""Technical Specifications"" on Hugging Face:
https://huggingface.co/bigscience/bloom","https://bigscience.huggingface.co/blog/bloom Blog post says 117 days.

384 A100 GPUs * 314 TFLOPS throughput per GPU * 117 days * 0.3 (utilization assumption) = 3.65664e23
https://www.wolframalpha.com/input?i=384+*+314+TFLOPS+*+117+days+*+0.3","In total, 1.6 terabytes of pre-processed text was converted into 350 billion unique tokens as BLOOM's training datasets.
arXiv:2210.15424

""BLOOM was trained on the ROOTS corpus (Lauren¸con et al., 2022), a composite collection
of 498 Hugging Face datasets (Lhoest et al., 2021) amounting to 1.61 terabytes of text that
span 46 natural languages and 13 programming languages. A high-level overview of this
dataset can be seen in Figure 3, while a detailed itemized list of every language along with
its linguistic genus, family and macroarea is presented in Table 1""","Table 3.5 https://arxiv.org/pdf/2211.05100

366B (pretrain) + 13B (finetune) = 379B  tokens total ",117 days * 24 hours/day,,,Table 3. 2048*2048,responsible use restrictions: https://bigscience.huggingface.co/blog/the-bigscience-rail-license
NLLB,7/6/2022,SOTA improvement,54500000000,1.751113728e+22,3.6E+11,,,,,50667.25034038439,NVIDIA A100 SXM4 80 GB,,,,,1000000,Open weights (unrestricted),Open source,Open source,,,Hardware,"""Our model achieves an improvement of 44% BLEU relative to the previous state-of-the-art""","Section 8.2.4: ""The model has a total of 54.5B parameters
and FLOPs similar to that of a 3.3B dense model""","Section 8.8:
"" To train NLLB-200, a cumulative
of 51968 GPU hours of computation was performed on hardware of type A100-SXM-80GB""
See also Table 48

Section 8.2.4 states they use FP16

NVIDIA Datasheet states 312TFLOPS for FP16
https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-nvidia-us-2188504-web.pdf

Assuming 0.3 utilization:

312e12*3600*51968*0.3

Also:
""Our final model is a Transformer
encoder-decoder model in which we replace the Feed Forward Network (FFN) layer in
every 4th Transformer block with a Sparsely Gated Mixture of Experts layer containing 128
experts. We use model dimension 2048, FFN dimension 8192, 16 attention heads, 24 encoder
layers and 24 decoder layers. We use Pre-LayerNorm (Xiong et al., 2020) as described in
Section 6.1.1. We share the embedding weights of the encoder input embedding, decoder
input embedding and decoder output embedding layers. We use an overall dropout of 0.3,
attention dropout 0.1 and EOM with peom=0.2. The model has a total of 54.5B parameters
and FLOPs similar to that of a 3.3B dense model.""",,"[WORDS]

Section 8.2.2: ""As we prepare to train on the final 202 language dataset comprising of over 18B sentence
pairs and 2440 language directions""

18B sentences * 20 words/sentence",,,,"""We train the model for 300k steps using the 4 phase curriculum
described in Section 8.2.3. We use an effective batch size of 1M tokens per update.""","MIT

train code: https://github.com/facebookresearch/fairseq/blob/nllb/examples/nllb/modeling/README.md "
CodeT5-large,7/5/2022,SOTA improvement,770000000,2.72e+21,10500000000,150,504,,,4478.145684414144,NVIDIA A100,GitHub,Likely,,,,Open weights (unrestricted),,,,,Hardware,"""Our method not only achieves new SOTA results on the challenging APPS benchmark, but also shows strong zero-shot transfer capability with new SOTA results on the simpler MBPP benchmark.""","""We pretrain a CodeT5-large model (770M) from scratch following T5-large’s architecture""","""We perform our experiments on a kubernetes with 16 A100-40G GPUs on Google Cloud Platform and the total pretraining duration is around 21 days""

16 * 312tFLOP/s * 21 * 24 * 3600 * 0.3 (utilization assumption) = 2.72e21","""We enlarge the Python pretraining dataset using the recently released
large-scale Github Code dataset5. We have compiled public, non-personal information from GitHub consisting of permissively licensed Python code (e.g. “mit”, “apache-2”, “bsd-3-clause”, “bsd-2- 126clause”, “cc0-1.0”, “unlicense”, “isc”). The resulting Python dataset (GCPY) has 10.5B tokens and is 10x larger than the CodeSearchNet (CSN) corpus [Husain et al., 2019] used in the original CodeT5 [Wang et al., 2021]""",10.5b tokens,21 days,,,,https://github.com/salesforce/CodeT5?tab=BSD-3-Clause-1-ov-file#readme
Minerva (540B),6/29/2022,SOTA improvement,5.4035E+11,2.7415e+24,26000000000,,696,1024,,,Google TPU v4,arXiv,,PaLM (540B),2.1429e+23,,Unreleased,Unreleased,,TRUE,436538.00637838274,Hardware,,"""To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model (PaLM).""

Our approach is to start with the PaLM pretrained decoder-only transformer language models Chowdhery et al. (2022), and further train (finetune) them on our mathematical dataset using an autoregressive objective.
Table 2 contains the main model and training hyperparameters.

See Table 2","Minerva was fine-tuned from PaLM using the same hardware. Assume the same model FLOPs utilization rate for pre-training and fine-tuning.

PaLM pretraining time: 6144 TPU for 1200 hours + 3072 TPU for 336 hours = @8404992 TPU-hours
Minerva finetuning time: 1024 TPU for 696 hours = 712704 TPU-hours
So fine-tuning added 8.5% more compute.

Minerva total compute = PaLM pretraining compute * (712704+8404992)/(8404992) = 2.7415*10^24 FLOP
https://www.wolframalpha.com/input?i=%28712704%2B8404992%29%2F%288404992%29+*+2.5272*10%5E24

","PaLM, finetuned on arxiv","""Our models were trained on a dataset of 38.5B tokens"" + PaLM

upd 38.5B tokens - sie of the dataset, the model saw 26B tokens in 399k steps (see Table 2)",,,,,
ProGen2-xlarge,6/27/2022,SOTA improvement,6400000000,1.35e+22,3.5E+11,,,,,11850.178410269727,Google TPU v3,"UniRef90,BFD30",Confident,,,,Open weights (unrestricted),Unreleased,Open source,,,"Hardware,Third-party estimation","""ProGen2 models show state-of-the-art performance in capturing the distribution of observed evolutionary sequences, generating novel viable sequences, and pre- dicting protein fitness without additional finetuning.""","""We introduce a suite of protein language models, named ProGen2, that are scaled up to 6.4B parameters""","Estimate 1:
""350,000 steps x 1m batch size x 6.4 B “connections” x 6"" - Arb Research (https://arbresearch.com/files/gen_bio.pdf)
Steps and batches from Table 1. 
FLOP estimate: 1.3e22

Table 9 from here: https://www.biorxiv.org/content/10.1101/2023.07.05.547496v1.full.pdf
FLOP estimate: 1.4e22

Geometric mean = 1.35e22 FLOP","""The standard PROGEN2 models are pretrained on a mixture of Uniref90 (Suzek et al., 2015) and BFD30 (Steinegger & Söding, 2018) databases""",350B from Table 9 https://www.biorxiv.org/content/10.1101/2023.07.05.547496v1,,,,,"BSD license (permissive)
https://github.com/salesforce/progen?tab=BSD-3-Clause-1-ov-file#readme"
Parti,6/22/2022,SOTA improvement,20000000000,3.962895376192635e+23,4800000000,,,,,344852.94872144435,Google TPU v4,"LAION-400M,FIT400M,JFT-4B",,,,,Unreleased,Unreleased,,TRUE,,Operation counting,"""Second, we achieve consistent quality improvements by scaling the encoder-decoder Transformer model up to 20B parameters, with a new state-of-the-art zero-shot FID score of 7.23 and finetuned FID score of 3.22 on MS-COCO""","Abstract: ""we achieve consistent quality improvements
by scaling the encoder-decoder Transformer model up to 20B parameters""","Calculated from architecture. Does not take into account the encoding and decoding of text and images, only the transformer stack.

Table 1 shows for the 20B model
16 encoder layers
64 decoder layers
Dmodel = 4096
Dhidden = 16384
Num heads = 64

Just below table 1:
""We use a maximum length of text tokens of 128, and the length of image tokens are fixed to 1024""

I take the length of the sequence to be 100 for the encoder stack and 1024 for the decoder stack.

Section 3, Training: ""a total
of 450,000 steps and final ratio of 0.025. We use a global batch size of 8192 during training.""",,,,,,,"""For these reasons, we have decided not to release our Parti models, code, or data for public use without further safeguards in place""
https://sites.research.google/parti/"
CoCa,6/14/2022,SOTA improvement,2100000000,7.3e+22,4800000000,6.875,120,2048,,78043.37569,Google TPU v4,"JFT-3B,ALIGN",Confident,,,,Unreleased,Unreleased,,,873207.3344090481,Hardware,"""Notably on ImageNet classification, CoCa obtains 86.3% zero-shot top-1 accuracy, 90.6% with a frozen encoder and learned classification head, and new state-of-the-art 91.0% top-1 accuracy on ImageNet with a finetuned encoder.""","""Our largest CoCa model (""CoCa"" in short) follows the ViT-giant setup in [21] with 1B-parameters in the image encoder and 2.1B-parameters altogether with the text decoder""","""Pretraining CoCa takes about 5 days on 2,048 CloudTPUv4 chips""

275 teraFLOP/s * 2048 * 5 * 24 * 3600 * 0.3 (assumed utilization) = 7.3e22","""CoCa is pretrained from scratch in a single stage on both webscale alt-text data and annotated images by treating all labels simply as texts. We use the JFT-3B dataset [21] with label names as the paired texts, and the ALIGN dataset [13] with noisy alt-texts.""","JFT is 3 billion captioned images, ALIGN is 1.8 billion captioned images

""we use a batch size of 65,536 image-text pairs, where half of each batch comes from JFT and ALIGN, respectively. All models are trained on the
combined contrastive and captioning objectives in Eq.(4) for 500k steps, roughly corresponding to 5 epochs on JFT and 10 epochs on ALIGN.""

(5*3b+10*1.8b)/4.8b=6.875 epochs on average",5 days,,,"65,536 image-text pairs",
MetaLM,6/13/2022,SOTA improvement,,,,,,,,,,The Pile,Unknown,,,,,,,,,,"Abstract: ""Experimental results across various language-only and vision-language benchmarks show that our model outperforms or is competitive with specialized models on finetuning, zero-shot generalization, and few-shot learning.""",,,,,,,,,
DITTO,6/6/2022,SOTA improvement,750000000,1.1e+19,,7.16,,,,,,WikiText-103,,,,,Unreleased,Open source,Open source,,,,"Achieves SOTA on CNN/DailyMail by fine-tuning and improving on BART-large, which is SOTA",,,,,,,,,"open code
https://github.com/Jxu-Thu/DITTO

training: https://github.com/Jxu-Thu/DITTO/blob/main/train.py "
Diffusion-GAN,6/5/2022,SOTA improvement,,,,,,,,,NVIDIA V100,"CIFAR-10,LSUN Bedroom,AFHQ,LSUN Church,STL-10,FFHQ",Unknown,,,,,,,,,,"""We demonstrate the advantages of Diffusion-GAN over strong GAN
baselines on various datasets, showing that it can produce more realistic images with higher stability and data efficiency than state-of-the-art GANs.""",,"Must be <1e23 FLOP, all experiments were done with 4 or 8 V100s.","They experimented with the following datasets: ""CIFAR-10 (Krizhevsky, 2009), STL-10 (Coates et al., 2011), LSUN-Bedroom (Yu et al., 2015), LSUN-Church
(Yu et al., 2015), AFHQ(Cat/Dog/Wild) (Choi et al., 2020), and FFHQ (Karras et al., 2019)""",,,,,,
CogVideo,5/29/2022,Historical significance,9400000000,,5400000,,,,,,,Unspecified unreleased,Speculative,CogView2,3.0456e+19,,Open weights (unrestricted),Open source,Open source,,,Operation counting,The world's largest and first opensource large-scale pre-trained text-to-video model.,,,,"""trained on 5.4 million text-video pairs""",,,"6ND = 6*9400000000*5400000=3.0456e+17

(number of epochs is unknown)",,"https://github.com/THUDM/CogVideo
Apache 2
train code: https://github.com/THUDM/CogVideo/blob/CogVideo/pretrain_cogvideo.py "
Tranception,5/27/2022,SOTA improvement,700000000,7.24e+21,75000000001.00037,,336,64,,15247.43608848737,NVIDIA A100,UniRef100,Confident,,,,Open weights (unrestricted),,,,56859.714898306775,Hardware,"""We introduce Tranception, a novel transformer architecture leveraging autoregressive predictions and retrieval of homologous sequences at inference to achieve state-of-the-art fitness prediction performance. Given its markedly higher performance on multiple mutants, robustness to shallow alignments and ability to score indels, our approach offers significant gain of scope over existing approaches.""","""Our largest transformer model, Tranception L, has 700M parameters and is trained on UniRef100 (Suzek et al., 2014)""","Trained using 64 A100 GPUs for two weeks.
64 * 312 teraFLOP/s * 14 days * 24 hours/day * 3600 seconds/hour * 0.3 utilization (assumption)
= 7.24e21","""We therefore train our final model (700M parameters) on UniRef100""","Total tokens = Number of Sequences × Average Sequence Length
249,000,000 × 300 = 74,700,000,000 ≈ 7.5 × 10¹⁰ tokens",2 weeks,,,,MIT
Imagen,5/23/2022,"Significant use,SOTA improvement,Highly cited",7762000000,1.46e+22,860000000,,96,256,,7915.823806150154,Google TPU v4,"LAION-400M,Unspecified unreleased",Likely,,,,API access,Unreleased,,,109175.04391063086,Hardware,,"2B 64x64 generation model, 600M 64->256 super-resolution model, 400M 256->1024 super-resolution model
Uses encodings from a frozen T5-XXL, which should be included in total parameter count. Loading the model directly, there are 4,762,310,656 parameters in the encoder.
2B + 4.762B + 600M + 400M = 7.762 billion","256 TPU v4 chips for 64x64, for 4 days
128 TPU v4 chips for 64->256, for 2 days
128 TPU v4 chips for 256->1024, for 2 days

256 TPUs * 275 teraFLOPS/TPU * 4 days + 2 * (128 TPUs * 275 teraFLOPS/TPU * 2 days) * 40% utilization = 1.46e+22 FLOP",,"""We train on a combination of internal datasets, with ≈ 460M
image-text pairs, and the publicly available Laion dataset [61], with ≈ 400M image-text pairs.""",4 days,,,,
SimCSE,5/18/2022,"Highly cited,SOTA improvement",,,,3,,,,,,,Unknown,RoBERTa Large,,,,,,,,,,,,"""Training details. We start from pre-trained checkpoints of BERT (Devlin et al., 2019) (uncased) or RoBERTa (Liu et al., 2019) (cased) and take the [CLS] representation as the sentence embedding (see §6.3 for comparison between different pooling methods). We train unsupervised SimCSE on 106 randomly sampled sentences from English Wikipedia, and train supervised SimCSE on the combination of MNLI and SNLI datasets (314k). More training details can be found in Appendix A""",,,,,,
Gato,5/12/2022,SOTA improvement,1180000000,4.02e+21,5.24288E+11,,96,256,,3523.0649800752253,Google TPU v3,,,,,,Unreleased,Unreleased,,,255907.33702054748,"Hardware,Operation counting","SOTA at Meta-World MT50 tasks (96.6%) page 14, section 5.5","""This section focuses on in-simulation evaluation.
Figure 10 compares the full 1.18B parameter Gato"" p.10","256 (16x16x) TPUv3 chips x 123e12 FLOPS/chip x 4 days x 86400 seconds/day * 0.4 utilization = 4.35e21 FLOPs

Similar value by 6NC:
6 * 524288000000 * 1.18B = 3.71e21

Using geometric mean:
sqrt(4.35e21 * 3.71e21) = 4.02e21","Table 1 lists the datasets used. Note that actual training appears to have used fewer than the 1.5T control tokens + unspecified number of vision/language tokens: ""Training of the model is performed on a 16x16 TPU v3 slice for 1M steps with batch size 512 and token sequence length L = 1024, which takes about 4 days.""
1M * 512 * 1024 = 524,288,000,000 tokens in training",,4 days,,,,
UL2,5/10/2022,SOTA improvement,20000000000,1.2e+23,1E+12,,744,512,0.29925187,126785.76203549476,Google TPU v4,C4,Confident,,,65536,Open weights (unrestricted),,,,218378.65933256023,"Hardware,Operation counting","""by scaling our model up to 20B parameters, we achieve SOTA
performance on 50 well-established supervised NLP tasks""",Taken from Directory of LLMs,"Trained on 1T tokens
20B * 1T * 6 = 1.2e23 

Second source: Section 5.1 says model was trained on 512 TPUv4 chips, and took slightly over 1 month
512 * 2.75e14 * 31 * 24 * 3600 * 0.3 = 1.13e23",'The model is trained on a total of 1 trillion tokens on C4 (2 million steps).',1T tokens,"around 31 days from 'Pre-training took approximately slight more than one month for about 1 trillion
tokens.' from section 5.1
so around 31*24 = 744
",,,"""We pre-train all models for 500K steps with a batch size of 128 and a sequence length of 512 inputs and 512 targets using the C4 corpus. The total approximate tokens seen during pre-training is approximately 32 billion tokens.""

500k*128*512 ~= 32B
128*512=65,536",Apache 2.0
DeBERTaV3large + KEAR,5/4/2022,SOTA improvement,418000000,,,10,,,,,,,Confident,DeBERTaV3large,,,,,,,,,"""The proposed system, Knowledgeable External Attention for commonsense Reasoning (KEAR), reaches human parity on the open CommonsenseQA research benchmark with an accuracy of 89.4\% in comparison to the human accuracy of 88.9\%.""

SOTA per https://paperswithcode.com/sota/common-sense-reasoning-on-commonsenseqa","DeBERTaV3-large had 418M params, per Table 2",this is a fine-tuned version of DeBERTaV3-large,"""We present details of the 17 datasets that we use for training data retrieval in Table 1. All the datasets are multiple-choice or classification datasets related to commonsense reasoning, and we include dataset details in the appendix.""",,,,,,
OPT-175B,5/2/2022,"Significant use,Highly cited",1.75E+11,4.3e+23,1.8E+11,1.6667,793.5,1024,0.47115,731667.6068059877,NVIDIA A100 SXM4 80 GB,"The Pile,BookCorpus (BooksCorpus, Toronto Book Corpus),CC-Stories,Pushshift Reddit",Confident,,,2000000,Open weights (non-commercial),Open source,Open source,TRUE,909984.4296839886,Reported,https://ai.meta.com/blog/opt-175b-large-language-model-applications/,"""In line with Meta AI’s commitment to open science, we are sharing Open Pretrained Transformer (OPT-175B), a language model with 175 billion parameters trained on publicly available data sets""","https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/chronicles/final_update.md

""As of yesterday, at 12:46pm PST on January 6, our 175B model finally completed its training run on 300B tokens. This required ~4.30E+23 FLOPs of compute""","""The pre-training corpus contains a concatenation
of datasets used in RoBERTa (Liu et al., 2019b),
the Pile (Gao et al., 2021a), and PushShift.io Reddit (Baumgartner et al., 2020; Roller et al., 2021)""
...
""RoBERTa We included the BookCorpus (Zhu et al., 2015) and Stories (Trinh and Le, 2018) subsets of the RoBERTa corpus and utilized an updated version of CCNews, containing news stories crawled through September 28, 2021. This CCNews v2 corpus was preprocessed the same way as the original RoBERTa CCNews (Liu et al., 2019b).

The Pile We included a subset of the Pile (Gao et al., 2021a), including: CommonCrawl, DM Mathematics, Project Gutenberg, HackerNews, OpenSubtitles, OpenWebText2, USPTO and Wikipedia. Other subsets of the Pile were eliminated
...

PushShift.io Reddit We included a subset of the Pushshift.io corpus produced by Baumgartner et al. (2020) and previously used by Roller et al. (2021). To convert the conversational trees
into language-model-accessible documents, we extracted the longest chain of comments in each thread and discarded all other paths in the tree. This reduced the corpus by about 66%.","""The training data contains 180B tokens corresponding to 800 GB of data""

1 token ~ 0.75 words","4.3*10^23 FLOP / (147 TFLOPS) = 813000 A100-hours
https://www.wolframalpha.com/input?i=4.3*10%5E23+FLOP+%2F+%28147+TFLOPS%29

""As of yesterday, at 12:46pm PST on January 6, our 175B model finally completed its training run on 300B tokens. This required ~4.30E+23 FLOPs of compute, or roughly ~33 days of continuous training on 1024 80GB A100s (assuming no hardware issues, no numerical instabilities, etc.).""",,,Table 1,"non-commercial for weights:
https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/MODEL_LICENSE.md

training code (MIT) https://github.com/facebookresearch/metaseq/blob/main/docs/training.md "
Flamingo,4/29/2022,"Highly cited,SOTA improvement",80000000000,2.18972000000001e+23,,,360,1536,,183423.16330597224,Google TPU v4,"MultiModal MassiveWeb,LTIP,VTP,ALIGN",Confident,Chinchilla,,,Unreleased,Unreleased,,,655208.6062150282,Hardware,"""For tasks lying anywhere on this spectrum, a single Flamingo model can achieve a new state of the art with few-shot learning, simply by prompting the model with task-specific examples. On numerous benchmarks, Flamingo outperforms models fine-tuned on thousands of times more task-specific data.""","""We obtain three models, Flamingo-3B, Flamingo-9B and Flamingo-80B""

"" The Flamingo-80B model builds on top of the frozen Chinchilla 70B language model [42]. Starting from the very first layer and before every seventh transformer blocks, we add a GATED XATTN-DENSE layer attending to the visual inputs; this accounts for 10B additional learned parameters. For simplicity, we refer to this model as simply Flamingo throughout the paper""","1536 TPU v4 chips for 15 days. Assuming 40% utilization:
C = 1536 TPU * 275*10^12 FLOP/s/TPU * 15 day * 86400 s/day * 0.40 = 2.2*10^23 FLOP

""All training and evaluation was performed on TPUv4 instances. The largest model containing 80 billion parameters is trained on QUSV chips for 15 days and sharded across 16 devices.""

""All trained parameters and optimizer accumulators are stored and updated in float32; all activations and gradients are computed in bfloat16 after downcasting of parameters from float32 to bfloat16""",,"Flamingo was trained on a mixture of web-scraped datasets:
43M pages of text with interleaved images (MultiModal MassiveWeb dataset)
312M image-text pairs (LTIP dataset)
27M video-text pairs (VTP dataset)
1.8B image-alt text pairs (ALIGN dataset)

Training dataset size is at least 2.1 billion.",1536 TPU v4 chips for 15 days,,,,
Sparse all-MLP,4/14/2022,SOTA improvement,9410000000,6.0770304e+19,1E+11,,112,,,,,"RoBERTa dataset,CC100",,,,,Unreleased,,,,,Hardware,"Abstract:
""Our model also outperforms
the RoBERTa-Large model on several English tasks of the GLUE benchmark by 0.3% on average while handling 99 more languages.""","Table 2: ""In Section 4.4, we run our large model (9.41B parameters)""","112 hours on 32 V100 GPUs
assumed 0.33 util rate
32*112*60*60*0.3*1.57E+13
",,100B tokens (Table 2) so 75B words.,,,,,
Stable Diffusion (LDM-KL-8-G),4/13/2022,"Significant use,Highly cited",1450000000,5e+22,400000000,,585.9375,256,,111248.21698072633,NVIDIA A100,LAION-400M,,,,,Open weights (restricted use),,,,227539.7263970043,Hardware,,See Table 2,"""I get 5e22 FLOP. 150k hours on A100 [1] gives 150*10^3 hours * 3600 seconds/hour * 3.12E+14 peak performance of A100 * 0.33 utilisation = 5e22  FLOP""

[1] https://twitter.com/EMostaque/status/1563870674111832066","Depends on the specific task; see sec 4

""we train a 1.45B parameter
KL-regularized LDM conditioned on language prompts on
LAION-400M""",,"total chip-hours divided by number of GPUs
150k/256",,,,OpenRAIL license
BERT-RBP,4/7/2022,SOTA improvement,110000000,1.4e+20,78000001.00000003,3,,,,,,RBPSuite,Confident,DNABERT,2.2e+16,,Open weights (non-commercial),Open (non-commercial),Open access (non-commercial),,,Hardware,"""Our model outperformed state-of-the-art prediction models using the eCLIP-seq data of 154 RBPs"" [Abstract] - SOTA improvement on a very specific task","Base model is BERT base (110M parameters), pre-trained on human reference genome (DNABert: https://academic.oup.com/bioinformatics/article/37/15/2112/6128680)","See DNABert entry:

""Since the pre-training of DNABERT model is resource-intensive (about 25 days on 8 NVIDIA 2080Ti GPUs)""

Assuming FP16 and 30% utilization

Calculation = (25 * 24 *3600) s * 2.7e13 FLOP/s per GPU * 8 GPUs * 0.3 utilization = 1.4e20 FLOP","See DNABert entry: ""We generated training data from human genome [...]"" [2.2.2 Pre-training]

""An eCLIP-seq dataset previously generated from the ENCODE3 database by Pan et al. (2020) was used. The original dataset consisted of 154 RBP sets with up to 60 000 positive RNA sequences that bind to the corresponding RBP and the same number of negative sequences."" [2.2 Data preparation]","RBPs: 154
Sequences per RBP: 15,000
Total sequences = 154 × 15,000 = 2,310,000
Tokens per sequence = 34
Total tokens = 2,310,000 × 34 = 78,540,000
Final estimate: 7.8 × 10⁷ tokens",,,"""The models were trained on four NVIDIA Tesla V100 GPUs (128
GB memory). The training of one RBP model using 19 200 samples
took <10 min.""

Calculation assuming FP16 and 30% utlization and NVIDIA Tesla V100 SMX2 model: 
10 min * 60 sec/min * 3.1e13 FLOP/s * 4 GPU * 0.3 utilization = 2.2e16",,"No clear license: https://github.com/kkyamada/bert-rbp

train script: https://github.com/kkyamada/bert-rbp/blob/master/examples/run_finetune.py 

data also doesn't have a clear license: http://www.csbio.sjtu.edu.cn/bioinf/RBPsuite/dataset_new.html"
DALL·E 2,4/6/2022,"Highly cited,SOTA improvement",3500000000,,650000000,,,,,,,"CLIP,DALL-E",Confident,,,,,,,,,,,"""Our decoder architecture is the 3.5 billion parameter GLIDE model""","Decoder architecture is similar to Imagen (1.46E+22), but trained on 1.6e9 datapoints (Table 3) rather than Imagen's 5.1e9 datapoints.

DALL-E 2 uses two models as priors. I estimate the prior model's FLOP as 6*N*D = 6 * 1e9 * 4096 * 1e6 = 2.5e19 FLOP. However, this seems low compared to CLIP.

So it may be possible to estimate DALL-E 2's compute by analogy to Imagen, but there is a lot of uncertainty and more research would be needed.",,"""When training the encoder, we sample from the CLIP [39] and DALL-E [40] datasets (approximately 650M images in total) with equal probability""",,,,,
PaLM (540B),4/4/2022,"Highly cited,SOTA improvement,Training cost",5.4035E+11,2.5272e+24,5.85E+11,1,1536,6144,0.462,2945949.763287097,Google TPU v4,"Wikipedia,GLaM dataset,LaMBDA dataset,GitHub",Confident,,,4000000,Unreleased,Unreleased,,TRUE,2621496.0548983514,Hardware,"Demonstrates continued benefits of scaling, as well as discontinuous improvements in performance","""To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model (PaLM).""","See Table 20.

6144 TPUv4 for 1200 hours + 3072 TPUv4 for 336 hours.
Equivalent to 6144 TPUv4 for 1368 hours.

46.2% model FLOPs utilization

""The 540B-parameter PaLM model sustained a remarkable 57.8% of the peak hardware floating point performance over 50 days while training on TPU v4 supercomputers. "" https://cloud.google.com/blog/topics/systems/tpu-v4-enables-performance-energy-and-co2e-efficiency-gains",,"""The PaLM pretraining dataset consists of a high-quality corpus of 780 billion tokens that represent a wide range of natural language use cases.""

1 token ~ 0.75 words","6144 TPUv4 for 1200 hours + 3072 TPUv4 for 336 hours.

Equivalent to 6144 TPUv4 for 1368 hours.","Training compute and utilization rate exclude rematerialization FLOP, but cost should account for rematerialization.",,"""For the largest model, we use batch size 512 (1M tokens) until step 50k, then double it to 1024 (2M tokens) until step 115k, and finally double again it to 2048 (4M tokens) until training is complete at step 255k""",
Chinchilla,3/29/2022,SOTA improvement,70000000000,5.76e+23,1.05E+12,1,,,,,"Google TPU v4,Google TPU v3","MassiveWeb,C4",Confident,,,3000000,Unreleased,Unreleased,,TRUE,,Reported,"Proposes new scaling law, with good empirical results","""We test this hypothesis by training a predicted compute-optimal model, \chinchilla, that uses the same compute budget as \gopher but with 70B parameters and 4× more more data. \chinchilla uniformly and significantly outperforms \Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks.""","""Both Chinchilla and Gopher have been trained for the same number of FLOPs but differ in the size of the model and the number of training tokens.""

We see the number of flops in table 3","MassiveWeb, Books, C4, News, Github, Wikipedia (Table A1)","Table 1 shows Chinchilla was training on 1.4 trillion tokens

1 token ~ 0.75 words",,,,"Table 1. ""1.5M → 3M""",
"Segatron-XL large, M=384 + HCP",3/21/2022,SOTA improvement,257000000,2.65e+19,,167.02,,,,,,,,,,,Unreleased,Open (non-commercial),Open access (non-commercial),,,,"""Empirically, this curriculum learning strategy consistently improves perplexity over various large, highly-performant state-of-the-art Transformer-based models on two datasets, WikiText-103 and ARXIV""",,,,,,,,,"code, no license
https://github.com/richardbaihe/robustlm"
ViT-G (model soup),3/10/2022,SOTA improvement,1843000000,3.4e+21,,8,,,,,,,Confident,,,,Open weights (non-commercial),Unreleased,Open access (non-commercial),,,Operation counting,"""When fine-tuning large pre-trained models such as CLIP, ALIGN, and a ViT-G pre-trained on JFT, our soup recipe provides significant improvements over the best model in a hyperparameter sweep on ImageNet. The resulting ViT-G model, which attains 90.94% top-1 accuracy on ImageNet, achieved a new state of the art.""",This is from the original ViT-G paper,"This is a fine-tuned version of ViT-G, which required 3.4e21 to train per PCD/Akronomicon.

Fine-tuning compute is likely minor in comparision:
""Models are fine-tuned at a batch size of 512 for either 10,000 or 20,000 steps (approximately 4 or 8 epochs)... all models are fine-tuned at 518 × 518 resolution""
At 20k steps, we have (518^2) * 512 * 20k = 2.75e12 pixels seen in fine-tuning, compared to (224^2) * 32768 * 5M = 8.22e15 in pre-training.",,,,,,,"no license
code here, may just be inference code: https://github.com/mlfoundations/model-soups "
MegaSyn,3/7/2022,Historical significance,,,,,,,,,,ChEMBL,Unknown,,,,Unreleased,,,,,,"Notable example of an AI model having a potential dual use for bio/chemical weapons:

""To narrow the universe of molecules we chose to drive the generative model towards compounds like the nerve agent VX, one of the most toxic chemical warfare agents developed during the 20th century—a few salt-sized grains of VX, (6–10 mg)5, is sufficient to kill a person. Nerve agents such as Novichoks have also been in the headlines recently6.

In less than 6 hours after starting on our in-house server, our model generated forty thousand molecules that scored within our desired threshold. In the process, the AI designed not only VX, but many other known chemical warfare agents that we identified through visual confirmation with structures in public chemistry databases. Many new molecules were also designed that looked equally plausible. These new molecules were predicted to be more toxic based on the predicted LD50 in comparison to publicly known chemical warfare agents (Figure 1). This was unexpected as the datasets we used for training the AI did not include these nerve agents. The virtual molecules even occupied a region of molecular property space that was entirely separate to the many thousands of molecules in the organism-specific LD50 model, which is mainly made up of pesticides, environmental toxins, and drugs (Figure 1). By inverting the use of our machine learning models, we had transformed our innocuous generative model from a helpful tool of medicine to a generator of likely deadly molecules.""","model details here: https://chemrxiv.org/engage/chemrxiv/article-details/61551803d1fc335b7cf8fd45

""The variational autoencoder utilizes an encoder-decoder architecture to map chemical space into a latent vector 34. The encoder is composed of 3 LSTM layers of 512 units each followed by a linear layer of 64 units (the latent space).
Our decoder is comprised of 3 LSTM layers of 512 units each with dropout of 0.2 between
all layers""",,"https://chemrxiv.org/engage/chemrxiv/article-details/61551803d1fc335b7cf8fd45

""The initial model is trained on ChEMBL 28’s ~2 million compounds""",,,,,,
Statement Curriculum Learning,3/2/2022,SOTA improvement,774000000,,2.75E+11,,,,,,,"Common Crawl,WebMath",,,,,,,,,,,"""by applying this expert iteration to a manually curated set
of problem statements, we achieve state-of-the-art on the miniF2F benchmark, automatically solving
multiple challenging problems drawn from high school olympiads.""",,Probably below 1e23 FLOP given the small model size.,"300 billion tokens from Common Crawl
72 billion tokens (220 GB) of code from WebMath
25000 theorems from mathlib
327 math problems from competitions and textbooks

The model was also trained on its own self-generated proofs","Table on p12 gives WebMath dataset size in GB of code. Uncompressed code probably has a similar number of tokens per gigabyte as natural language text, on the order of 3e8 tokens per GB.",,,,,
DeepNet,3/1/2022,SOTA improvement,3200000000,,12000000000,,,,,,,"CCMatrix,OPUS",,,,,,,,,,,"""Remarkably, on a multilingual benchmark with 7,482 translation directions, our 200-layer model with 3.2B parameters significantly outperforms the 48-layer state-of-the-art model with 12B parameters by 5 BLEU points""","""Remarkably, on a multilingual benchmark with 7,482 translation directions, our 200-layer model with 3.2B parameters significantly outperforms the 48-layer state-of-the-art model with 12B parameters by 5 BLEU points, which indicates a promising scaling direction""

EDIT 05/05/2022: The 12B model was presented in an earlier paper. This paper presents a 3.2B model","They show results on par with the original Transformer, so probably less than 2.3e19 FLOP.",,""" The final data consists of 102 languages, 1932 directions, and
12B sentence pairs.""",,,,,
PolyCoder,2/26/2022,SOTA improvement,2700000000,1.1e+21,,,1000,,,,NVIDIA Quadro RTX 8000,,Likely,,,,,,,,,Hardware,"""In the C programming language, PolyCoder outperforms
all models including Codex""",2.7B for largest model,"""We use GPT-NeoX toolkit 11 to
train the model efficiently in parallel with 8 Nvidia RTX 8000 GPUs on a single machine. The wall
time used to train the largest 2.7B model is about 6 weeks""

8 * 130 TFLOP/s * 6 * 7 * 24 * 3600 * 0.3 (utilization) ~= 1.1e21","Code scraped from GitHub. ""249GB of code across 12 programming languages on a single machine.""","249GB

They trained on 39B tokens per Table 3, but I'm not sure how many epochs that is. May be <1. ",6 weeks,,,,
ST-MoE,2/17/2022,SOTA improvement,2.69E+11,2.9e+23,1.5E+12,0.84,,,,,,C4,Likely,,,1000000,Unreleased,Open source,Open source,,,Operation counting,"""ST-MoE-32B improves the current state-of-the-art on the test server submissions for both ARC Easy (92.7 → 94.8) and ARC Challenge (81.4 → 86.5).""",269B. it's called ST-MoE-32B because it's equivalent to a 32B dense model.,"The paper claims ""scaling a sparse model to 269B parameters, with a computational cost comparable to a 32B dense encoder-decoder"". If this is true for training cost, then 6*32e9*1.5e12 = 2.9e23","""The pre-training dataset used to train our Sparse 32B model is a mix of C4 (Raffel et al., 2019) and the dataset introduced in GLaM (Du et al., 2021).""","""We pre-train for 1.5T tokens on a mixture of English-only C4 dataset (Raffel et al., 2019) and the dataset from GLaM (Du et al., 2021) summarized in Appendix E""
",,,,""" We use 1M tokens per batch""","Apache License 2.0
Code for our models is available at https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/transformer/moe.py"
Midjourney V1,2/15/2022,"Significant use,Historical significance",,,,,,,,,,Unspecified unreleased,Unknown,,,,Hosted access (no API),Unreleased,,,,,"Significant historical usage and controversy as one of the first generative AI art models. For example:
https://www.nytimes.com/2022/09/02/technology/ai-artificial-intelligence-artists.html",,,,,,,,,
ProteinBERT,2/10/2022,SOTA improvement,16000000,6.5e+19,32000000001,6.4,672,1,,,NVIDIA Quadro RTX 5000,UniRef90,Confident,,,26008,,,,,511.39675520829087,Hardware,"""ProteinBERT obtains near state-of-the-art performance, and sometimes exceeds it, on multiple benchmarks covering diverse protein properties (including protein structure, post-translational modifications and biophysical attributes)""","""Altogether, it includes ∼16M trainable parameters, making it substantially smaller than other protein language models""","""Pretraining speed on a single GPU (Nvidia Quadro RTX 5000) was 280 protein records per second. We trained the model for 28 days over ∼670M records""

28 * 24 * 3600 * 89 TFLOP/s * 0.3 (assumed utilization) = 6.5e19
https://www.wolframalpha.com/input?i=28+days+*+89+TFLOP%2Fs+*+0.3",,"Number of proteins: 106,000,000
Average protein length: 300 amino acids

Total unique tokens = 106,000,000 × 300 = 31,800,000,000 ≈ 3.2e10 tokens",28 days,,,"Supplementary materials: ""During pretraining we used batch sizes of 128, 64 or 32 for episodes of 128, 512 or 1,024 tokens, respectively"" Since they seem to be used in equal parts, taking geometric mean: ((128*128)*(64*512)*(32*1024))**(1/3) = 26,008",
LaMDA,2/10/2022,Historical significance,1.37E+11,3.55e+23,1.56E+12,,1385,1024,0.565,229949.98625999544,Google TPU v3,Infiniset,Confident,,,256000,Unreleased,Unreleased,,TRUE,1024572.2817390452,Hardware,,"""LaMDA is a family of Transformer-based neural language models specialized for dialog, which have up to 137B parameters""","""The total FLOPS is 56.5% * 123 TFLOPS/s * 1024 chips * 57.7 days
= 3.55E+23""
From https://arxiv.org/pdf/2201.08239.pdf p.18
","LaMDA's underlying dataset is called 'Infiniset', and besides the dialogue also involves common crawl, wikipedia, a mixture of english and non-english web documents, and data from programming-related sites (so LaMDA models can also dabble in code).","""and are pre-trained on 1.56T words of public dialog data and web text""",57.7 days * 24,,,"""All models were trained with 256K tokens per batch""",
GPT-NeoX-20B,2/9/2022,Historical significance,20000000000,9.31627008e+22,3.41173E+11,1.4,2160,96,,184272.8073600264,NVIDIA A100 SXM4 40 GB,The Pile,,,,3150000,Open weights (unrestricted),Open source,Open source,,85381.89156398654,Hardware,,,Trained for 3 months on 96 A100s (according to correspondence with author). Let's say 0.4 utilization rate.,,"""In aggregate, the Pile consists of over 825GiB of raw text data""

Figure 4","see other notes
",,,"""we opt to use the same batch size as OpenAI’s 175B model–approximately 3.15M tokens, or 1538 contexts of 2048 tokens each, and train for a total of 150,000 steps""",Apache 2.0. training code: https://github.com/EleutherAI/gpt-neox 
RETRO-7B,2/7/2022,SOTA improvement,7500000000,1.68e+22,4.1943E+11,,,,,,,WikiText-103,,,,,Unreleased,Unreleased,,,,Operation counting,"""Our largest model obtains state-of-the-art results on a range of downstream evaluation
datasets including Wikitext103""","""Retro provides a constant gain for models ranging from 150M to 7B parameters, and Retro can be improved at evaluation time by increasing the database size and the number of retrieved neighbours. """,C=6ND = 6 * 7e9 * 400e9 = 1.7e22 ,,"""we train for 419,430,400,000 training tokens"" ~= 315B words.",,,,,
AlphaCode,2/2/2022,SOTA improvement,41100000000,1.63944e+23,,,147.2,3750,,,Google TPU v4,"CodeContests,Unspecified unreleased",,,,4718592,Unreleased,Unreleased,,,1601024.4557014774,Hardware,,41.1B. Table 3,"Figure 7 (a) shows a maximum training compute budget of approx 23000 TPU-days per model.
23000 days * 24 h/day * 3600 sec/h * 2.75e14 FLOP/s * 0.3 utilization = 1.64e23 FLOP","Looks like evaluation data is released but not pretraining data:

""We use large transformer language models to generate code, pre-training them
on selected GitHub code and fine-tuning on our curated set of competitive programming problems...
A core part of developing our system was ensuring that submissions are rigorously evaluated and
that evaluation problems are truly unseen during training, so difficult problems cannot be solved
by copying from the training set. Towards this goal, we release a new training and evaluation
competitive programming dataset, CodeContests""",Appendix part A has answers for pretraining.,"Figure 7 (a) shows that the models were trained for around 23000 TPU-days. We know they trained on TPUv4s, and in appendix D.1 they say they have 3750 TPUv4 and TPUv4i. Assuming they trained only on the 3750 TPUv4s, that suggests 23000 / 3750 = 6.13 days, or 147.2 hours.",,,"2304 token sequences, 2048 batch size. 2304 * 2048 = 4718592

trained on 967B tokens and 205k steps. 967B/205k = 4717073, so seems they didn't do warmup",
InstructGPT 175B,1/27/2022,"Historical significance,Highly cited",1.75E+11,3.19181e+23,3.74E+11,,,,,,,,Confident,GPT-3 175B (davinci),5.181e+21,,,,,,,Reported,,"""We train three model sizes (1.3B, 6B, and 175B parameters)""","""training our 175B PPO-ptx model requires 60 petaflops/s-days, compared to 3,640 petaflops/s-days for GPT-3 (Brown et al., 2020)""

60/3640 = +1.65% to base model compute

base model was reported 3.14e+23 FLOP

3.14e+23 * 1.0165 = 319181000000000000000000",,"Table 6 - describes **number of prompts**

26584 + 6623 = 33207

This is added to GPT-3 dataset size.",,,,,
InstructGPT 6B,1/27/2022,"Historical significance,Highly cited",6000000000,,,,,,,,,,Confident,GPT-3 6.7B,,,,,,,,,,"""We train three model sizes (1.3B, 6B, and 175B parameters)""",,,,,,,,
InstructGPT 1.3B,1/27/2022,"Historical significance,Highly cited",1300000000,,,,,,,,,,Confident,GPT-3 XL,,,,,,,,,,"""We train three model sizes (1.3B, 6B, and 175B parameters)""",,,,,,,,
OntoProtein,1/23/2022,SOTA improvement,420000000,,160000001.00000018,,,,,,,ProteinKG25,,ProtBERT-BFD,,,,,,,,,Experimental results show that OntoProtein can surpass state-of-the-art methods with pre-trained protein language models in TAPE benchmark and yield better performance compared with baselines in protein-protein interaction and protein function prediction1.,"""For the protein encoder, we use the pre-trained ProtBert from Elnaggar et al. (2020).""",,"""the ProteinKG25 dataset used for pre-training contains about 612,483 entities and 4,990,097 triples, aligned with GO annotations and including protein sequences.""","Data Summary:
- Protein tokens: 488,000 × 300 = 1.464 × 10^8
- GO term tokens: 612,483 × 20 = 1.224966 × 10^7
- Knowledge graph triples: 4,990,097
Total: (1.464 × 10^8 + 1.224966 × 10^7 + 4.99 × 10^6) ≈ 1.626 × 10^8 data points",,,,,
AbLang (heavy sequences),1/22/2022,SOTA improvement,355000000,,2290000001.0000024,20,,,,,,Observed Antibody Space (OAS) database,Confident,,,,,,,,,,"""AbLang restores residues more accurately and faster than a current state-of-the-art protein language model ESM-1b, emphasizing the benefits and potential of an antibody specific language model"" - SOTA improvement for a very specific task","""The hyperparameters were selected to be similar to those used
in the RoBERTa paper (Liu et al., 2019).""

Liu et al., 2019 link: https://arxiv.org/pdf/1907.11692.pdf
""We begin by training RoBERTa following the BERTLARGE architecture (L = 24, H = 1024, A = 16, 355M parameters)""",,,"Heavy Chain: 14,126,724 sequences × 160 residues = 2,260,275,840 datapoints
Light Chain: 187,068 sequences × 160 residues = 29,930,880 datapoints
Total: 2,260,275,840 + 29,930,880 = 2,290,206,720 datapoints (2.29B)",,,,,
data2vec (vision),1/20/2022,SOTA improvement,705134592,,1281167,800,,,,,,ImageNet-1k,,,,,,,,,,,"""Experiments on the major benchmarks of speech recognition, image classification, and natural lan guage understanding demonstrate a new state of the art or competitive performance to predominant approaches""","Section 4: ""We experiment with two model sizes: data2vec Base and
data2vec Large, containing either L = 12 or L = 24 Trans-
former blocks with H = 768 or H = 1024 hidden dimen-
sion (with 4 × H feed-forward inner-dimension)""
",,,"Section 5.1: 
""we pretrain data2vec on the images of the ImageNet-1K training
set""",,,,,
data2vec (speech),1/20/2022,SOTA improvement,705134592,,13132800,,,,,,,LibriSpeech,,,,,,,,,,,"""Experiments on the major benchmarks of speech recognition, image classification, and natural lan guage understanding demonstrate a new state of the art or competitive performance to predominant approaches""","Section 4: ""We experiment with two model sizes: data2vec Base and
data2vec Large, containing either L = 12 or L = 24 Trans-
former blocks with H = 768 or H = 1024 hidden dimen-
sion (with 4 × H feed-forward inner-dimension)""
",,,"Section 5.2:
""we pre-train data2vec on the 960
hours of speech audio data from Librispeech (LS-960)""

13,680 words per hour",,,,,
data2vec (language),1/20/2022,SOTA improvement,705134592,,3300000000,,,,,,,"BookCorpus (BooksCorpus, Toronto Book Corpus),English Wikipedia",,,,,Open weights (unrestricted),Open source,Open source,,,,"""Experiments on the major benchmarks of speech recognition, image classification, and natural lan guage understanding demonstrate a new state of the art or competitive performance to predominant approaches""","Section 4: ""We experiment with two model sizes: data2vec Base and
data2vec Large, containing either L = 12 or L = 24 Trans-
former blocks with H = 768 or H = 1024 hidden dimen-
sion (with 4 × H feed-forward inner-dimension)""
",,,"Section 5.3: ""we adopt the same training setup as BERT (Devlin et al., 2019)
by pre-training on the Books Corpus (Zhu et al., 2015) and English Wikipedia data over 1M updates and a batch size of 256 sequences.""",,,,,"MIT License
Models and code are available at www.github.com/pytorch/fairseq/tree/master/examples/data2vec"
Detic,1/7/2022,SOTA improvement,88000000,2.34399744e+19,16900000,,24,32,,191.44581825616132,NVIDIA V100,"ImageNet21k,Conceptual Captions (CC3M),LVIS",Speculative,,,,Open weights (unrestricted),Open source,Open source,,21352.648670572937,Hardware,"""On open-vocabulary COCO, our method outperforms the previous state-of-the-art OVR-CNN [ 72 ] by 5 point with the same detector and data""","from https://github.com/microsoft/Swin-Transformer Swin-B have 88M, 
from page 8 :  'Training our ResNet50 model takes ∼ 22 hours on 8 V100 GPUs. The large 21K Swin-B model trains in ∼ 24 hours on 32 GPUs.'","28.26e12* 32 * 24*3600*0.3 =2.34e19 = peak flops * num gpus * num seconds * assumed utilization rate
for Swin-B model from page 8 :  'Training our ResNet50 model takes ∼ 22 hours on 8 V100 GPUs. The large 21K Swin-B model trains in ∼ 24 hours on 32 GPUs.'","table above section 5.1
""We evaluate Detic on the large-vocabulary object detection dataset LVIS [18 ]""
""Image-supervised data. We use two sources of image-supervised data: ImageNet-21K [10] and Conceptual Captions ""","14M + 1.5M + 1.2M + 100K + 100K = 16900000.0
table above section 5.1","from page 8 :  'Training our ResNet50 model takes ∼ 22 hours on 8 V100
GPUs. The large 21K Swin-B model trains in ∼ 24 hours on 32 GPUs.'",,,,"Apache, models and code: https://github.com/facebookresearch/Detic"
ERNIE-ViLG,12/31/2021,SOTA improvement,10000000000,,145000000,,,,,,,,,,,,,,,,,,"""we train a 10-billion parameter ERNIE-ViLG model on a large-scale dataset of 145 million (Chinese) image-text pairs which achieves state-of-the-art performance for both text-to-image and image-to-text tasks""","""To explore the landscape of large-scale pre-training for bidirectional text-image generation, we pre-train a 10-billion parameter model on a large-scale dataset of 145 million high-quality Chinese image-text pairs.""",,,"To explore the landscape of large-scale pre-training for bidirectional text-image generation, we pre-train a 10-billion parameter model on a large-scale dataset of 145 million high-quality Chinese image-text pairs.",,,,,
ERNIE 3.0 Titan,12/23/2021,SOTA improvement,2.6E+11,1.0421e+24,6.68E+11,,,1920,,,"NVIDIA Tesla V100 DGXS 32 GB,Huawei Ascend 910",ERNIE 3.0 Corpus,Confident,,,1048576,Hosted access (no API),Unreleased,,TRUE,2106.480893731142,Operation counting,"""Empirical results show that the ERNIE 3.0 Titan outperforms the state-of-the-art models on 68 NLP datasets.""","""[We] developed... distributed training technology, including fine-grained parallelism, heterogeneous hardware-aware training, and fault tolerance mechanism to train the 260B model on both Nvidia V100 GPU and Ascend 910 NPU clusters.""
See also:
https://twitter.com/BaiduResearch/status/1468633977242243078?t=6q4zuLNdTSc4GUBe9OM5Aw&s=19","The paper suggests that ERNIE 3.0 Titan uses more compute than GPT-3. This is consistent with the 6ND approximation.

C = 6ND = 6 (FLOP/param/token) * (260B params) * (668B tokens) = 1.0421*10^24 FLOP",,"""To ensure the success of the pre-training of ERNIE 3.0 Titan, we utilize the ERNIE 3.0 Corpus [ 2 ], a large-scale, wide-variety, and high-quality Chinese text corpora amounting to 4TB""

Assuming 167M words/tokens per GB",,,,"""The maximum sequence length of context and
the memory length of language generation is 512 and 128, respectively""

In table 1, they use a global batch size of 512 when data parallelism is ""1"" and 2048 when DP is ""4"". Not sure I fully understand this part but I guess they'd use parallelism as much as possible given how they talk about it.

2048 * 512 = 1048576.","The Ernie 3.0 Titan model was used in Ernie bot. Today, ERNIE has been widely deployed across finance, healthcare, insurance, equity, Internet, logistics, and other fields.

http://research.baidu.com/Blog/index-view?id=165"
XGLM-7.5B,12/20/2021,SOTA improvement,7500000000,2.25e+22,1740000000,1,504,256,,104152.22590136188,NVIDIA A100,"Subset of CC100-XL,CC100-XL,Common Crawl",Confident,,,,Open weights (non-commercial),Unreleased,,,227803.45886835255,"Operation counting,Hardware","""Our largest model (XGLM7.5B) sets a new state of the art performance for few-shot learning in more than 20 representative languages (including medium- and low-resource languages) for the tasks of commonsense reasoning, natural language inference and machine translation.""","""Our largest model with 7.5 billion parameters sets new state of the art""","""The XGLM 7.5B model was trained on 256 A100 GPUs for about 3 weeks, at a speed of 311.6k words per second""

256 * 312 teraFLOP/s * 21 * 24 * 3600 * 0.3 utilization assumption ~= 4.3e22

also, it was trained for 500B tokens. Using Compute = 6ND, we have
6 * 500B * 7.5B = 2.25e22

311k tokens per second * 7.5B params * 6 is 1.35e16 FLOP/s. divide that by 312 teraFLOP/s, which is A100 peak compute, gets 43, suggesting low utilization (17%) of the 256-GPU cluster, or somewhat higher if there's more than one token per word. So I'll use the 6ND number.","*they built a closed dataset based on open Common Crawl ""We extend the pipeline used for mining the CC100 corpus (Conneau et al., 2020; Wenzek et al., 2020) to generate CC100-XL, a significantly larger multilingual dataset covering 68 Common Crawl (CC) snapshots (from Summer 2013 to March/April 2020) and 134 languages.","Training Data. Our models are trained on a static multilingual corpus extracted from CommonCrawl, with English text comprising 32.6% of the total number of tokens corresponding to 163B tokens.
163B / 0.326 = 500B total

Note that this dataset is sampled from the much larger CC100-XL, outlined in Appendix F and here: https://huggingface.co/facebook/xglm-7.5B#training-data-statistics

The huggingface link sums to 1.64T tokens, while the Data Card in the appendix claims 1.9T tokens.","appendix A : ""The XGLM 7.5B model was trained on 256 A100 GPUs for about 3 weeks, at a speed of 311.6k words per second""",,,,"MIT license
https://github.com/facebookresearch/fairseq/tree/main/examples/xglm

https://github.com/facebookresearch/fairseq/blob/main/examples/xglm/model_card.md#primary-intended-use"
LDM-1.45B,12/20/2021,Highly cited,1450000000,,400000000,0.66,,,,,NVIDIA A100,LAION-400M,Confident,,,,Open weights (unrestricted),Open source,Open source,,,,,1.45B,,,400M image-text pairs,,,,,MIT: https://github.com/CompVis/latent-diffusion/blob/main/LICENSE
GLIDE,12/20/2021,Highly cited,3500000000,4.7e+22,250000000,,,,,,,DALL-E,Speculative,,,,,,,,,Comparison with other models,,"""Samples from a 3.5 billion parameter text-conditional diffusion model using classifier-free guidance are favored by human evaluators to those from DALL-E, even when the latter uses expensive CLIP reranking""","""Note that GLIDE was
trained with roughly the same training compute as DALL-E
but with a much smaller model (3.5 billion vs. 12 billion
parameters)""",,"Section 4:
""We train our model on the same dataset as DALL-E (Ramesh et al., 2021)""

This paper used 250M image-text pairs
https://arxiv.org/pdf/2102.12092.pdf",,,,,
Contriever,12/16/2021,SOTA improvement,110000000,1.57e+20,,,,,,,,"Wikipedia,CCNet",Likely,BERT-Large,,,Open weights (non-commercial),Open (non-commercial),Open access (non-commercial),,,Operation counting,"""We observe that when
used as pre-training, contrastive learning leads to strong performance: contriever obtains the best results
among dense bi-encoder methods for the nDCG@10, and is state-of-the-art for the recall@100 (improving the
average recall@100 from 65.0 to 67.1). This strong recall@100 performance can be further exploited by using
a cross-encoder2
to re-rank the retrieved documents: this leads to the state-of-the-art on 8 datasets of the
BEIR benchmark for the nDCG@10, as well as on average""","Based on BERT base, which had 110m params.

""We initialize the network with the publicly available BERT base uncased model.""","Pre-training:
""We use the random cropping data augmentation, with documents of 256 tokens... batch size of 2,048 and 500,000 steps""
256 * 2048 * 500k * 100M * 6 = 1.57e20

Fine-tuning looks unlikely to move final sum much beyond this.","""Documents are simply random piece of text sampled from a mix between Wikipedia and CCNet data """,,,,actually BERT base,,non-commercial for weights/code: https://github.com/facebookresearch/contriever/blob/main/LICENSE
LongT5,12/15/2021,SOTA improvement,3000000000,,2E+11,3.2,,128,,,Google TPU v3,C4,Confident,,,,Open weights (unrestricted),Open source,Open source,,128145.99696745229,,"from abstract: ""We are able to achieve state-of-the-art results on several summarization tasks and outperform the original T5 models on question answering tasks.""",3B from section 4.1,"architecture is sparse so we cannot use 6ND method,
from 3.1.1 ""we simply replace the encoder
self-attention operation in T5 with a sparse sliding-
window local attention operation following the im-
plementation in ETC ""
at the end of section 3.1.2 there is information about 
complexity O(l(r + l/k)) of local attention
from 4.1.1 ""We pre-train LongT5 models for 1M steps on
4096 input sequence length and 910 output se-
quence length.
batch size is 128 (from 4.1 configurations section)
so with l = 4096, k = 16, r = 127, 
so l(r+l/k) = 1568768, but we are not sure about constant.

if normal attention have complexity O(l^2), and l^2 = 16777216
16777216/1568768 = 10.7
We can try to estimate that LongT5 would have 10 times less compute that normal architecture.","from 4.1.1 ""The same as T5.1.1, we pre-train LongT5 only on the C4 dataset
(Raffel et al., 2019b), and we do not apply dropout during pre-training.""","size of C4, from https://huggingface.co/datasets/c4 , C4 dataset is a collection of about 750GB of English-language text
200M word/GB * 4/3 token/word * 750GB = 200000000000 tokens

Actual tokens seen:
1M steps * (4096 input len + 910 output len) * 128 batch size = 641B tokens, so around 3.2 epochs.",,,,,"Apache 2.0: https://github.com/google-research/longt5
train code: https://github.com/google-research/longt5/blob/master/longt5/tasks.py "
GLaM,12/13/2021,SOTA improvement,1.2E+12,3.6363112434e+23,6E+11,,1366,1024,0.286963696,541437.4162400038,Google TPU v4,"Wikipedia,GLaM dataset",Confident,,,1000000,Unreleased,Unreleased,,TRUE,437413.9513787687,"Operation counting,Hardware","""As shown in Table 5, GLaM (64B/64E) is better than the dense model and outperforms the previous finetuned state-of-the-art (SOTA) on this dataset in the open-domain setting""",1.2 trillion parameters,"The network activates 96.6 billion parameters per token and trained for 600B tokens.

6 * 600B * 96.6B = 3.478e23

Digitizing figure 4 (d) indicates 139.67 TPU-years of training. 
2.75e14 * 139.67 * 365.25 * 24 * 3600 * 0.3 = 3.636e23

Since these are close, we will use the 6NC estimate and derive hardware utilization from the training time information.

Later they say they measured 326W power usage per chip, which could maybe be used to estimate utilization.","""To train our model, we build a high-quality dataset of 1.6 trillion tokens that are representative of a wide range of natural language use cases. Web pages constitute the vast quantity of data in our unlabeled dataset. However, their quality ranges from professional writing to low-quality comment and forum pages.""","The dataset is made of 1.6 trillion tokens, but later in the paper they say they only train the largest model for 600b tokens. 600b / 0.75 words/token = 800b words.

""The complete GLaM training using 600B tokens consumes only 456 MWh and emits 40.2 net tCO2e.""","Note that they give several energy estimates. Use the complete training figures for 600B tokens, not the GPT-3 comparison values with 280B tokens.

""326W measured system power per TPU-v4 chip""
""The complete GLaM training using 600B tokens consumes only
456 MWh""
1024 TPU v4 chips
(456 MWh) / (326W/chip * 1024 chips) = 1366 hours",,,"""We use a maximum sequence
length of 1024 tokens, and pack each input example to have
up to 1 million tokens per batch.""",
Gopher (280B),12/8/2021,SOTA improvement,2.8E+11,6.31e+23,3E+11,1,920,4096,0.378,616611.1391817601,Google TPU v3,MassiveTex,Confident,,,6000000,Unreleased,Unreleased,,TRUE,4100965.606974364,Reported,"""These models are evaluated on 152 diverse tasks, achieving state-of-the-art performance across the majority""","Language modelling provides a step towards intelligent communication systems by harnessing large repositories of written human knowledge to better predict and understand the world. In this paper, we present an analysis of Transformer-based language model performance across a wide range of model scales -- from models with tens of millions of parameters up to a 280 billion parameter model called Gopher. These models are evaluated on 152 diverse tasks, achieving state-of-the-art performance across the majority. Gains from scale are largest in areas such as reading comprehension, fact-checking, and the identification of toxic language, but logical and mathematical reasoning see less benefit. We provide a holistic analysis of the training dataset and model's behaviour, covering the intersection of model scale with bias and toxicity. Finally we discuss the application of language models to AI safety and the mitigation of downstream harms.","Table A26
6.31E+08 Train PFLOPs",,"""We train all models for 300 billion tokens with a 2048 token context window, using the Adam (Kingma and Ba, 2014) optimiser.""

1 token ~ 0.75 words","""We trained Gopher for 920 hours in November and December 2020 in Google’s Georgia datacentre. The PUE of the datacenter at this time was 1.08; the net tCO2e per MWh in October 2020 was 0.33. Using an estimate of 283W drawn per chip, this leads to a total of 380 net tCO2e""",,,"Table 1. ""Furthermore, we increase Gopher’s batch size from three to six million tokens per batch during training""",
Student of Games,12/6/2021,SOTA improvement,,3.667927300468287e+22,,,,,,,,,Speculative,,,,Unreleased,Unreleased,,,,,"""Player of Games reaches strong performance in chess and Go, beats the strongest openly available agent in heads-up no-limit Texas hold'em poker (Slumbot), and defeats the state-of-the-art agent in Scotland Yard""",,"""We trained a version of AlphaZero using its original settings in chess and Go, e.g. , using 800 MCTS simulations during training, with 3500 concurrent actors each on a single TPUv4, for a total of 800k training steps. SOG was trained using a similar amount of TPU resources.""",,,,,,,
NÜWA,11/24/2021,SOTA improvement,870000000,4.8384e+21,,,,,,,,"Conceptual Captions (CC3M),Moments in Time,VATEX",,,,,Unreleased,Unreleased,,,,Hardware,"""NÜWA achieves state-of-the-art results on text-to-image generation, text-to-video generation, video prediction, etc""",Section 4.1,"From AI Tracker:
""Compute cost: End of Sec 4.1: ""We pre-train on 64 A100 GPUs for two weeks"". Info sheet from NVIDIA (https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet.pdf) gives single precision TensorFloat 32 performance of 156 TFLOPs/s. So we get 64 x 14 x 156 = 140,000 TFLOPs/s x days.""

Multiply by seconds/day and 30% utilization",,"we first pre-train N  ̈UWA on three
datasets: Conceptual Captions [22] for text-to-image (T2I)
generation, which includes 2.9M text-image pairs, Mo-
ments in Time [26] for video prediction (V2V), which in-
cludes 727K videos, and VATEX dataset [43] for text-to-
video (T2V) generation, which includes 241K text-video
pairs.",,,,,https://github.com/microsoft/NUWA
Florence,11/22/2021,"Historical significance,SOTA improvement",893000000,4.831e+22,900000000,,240,512,,106950.61569328008,NVIDIA A100 SXM4 40 GB,FLD-900M,Confident,,,,Unreleased,Unreleased,,,455737.5358105461,Hardware,,"""Our Florence pretrained model has in total 893M parameters, including the language transformer with 256M parameters and the CoSwin-H transformer with 637M parameters.""","""The model takes 10 days to train on 512 NVIDIA A100 GPUs with 40GB memory per GPU.""
512 * 312 teraFLOPS * 10 days * 35% utilization = 4.831e22 FLOP","900 million image-text pairs curated from internet images and descriptions

""We leverage large quantities of image-text data available
publicly on the internet. Specifically, we construct a 900
million image-text-pair dataset, called FLD-900M (FLD
stands for FLorenceDataset), using a programmatic data
curation pipeline that processes around 3 billion Internet
images and their raw descriptions in parallel.  <..>The final form of the FLD-900M dataset consists of 900M images with 900M free-form texts (ranging from one word, phase to sentences), 9.7M unique queries, and 7.5B tokens in total.
",,10 days on 512 A100 40GB,,,,
BASIC-L,11/19/2021,SOTA improvement,3070000000,4.12e+22,6700000000,,,,,1684.770712126102,Google TPU v4,"JFT,ALIGN",Likely,,,,Unreleased,Unreleased,,,,Hardware,"SOTA on ImageNet for a model that was not trained on ImageNet images:
""We present a combined scaling method – named BASIC – that achieves 85.7% top-1 accuracy on the ImageNet ILSVRC-2012 validation set without learning from any labeled ImageNet example. This accuracy
surpasses best-published similar models – CLIP and ALIGN – by 9.3%""",2.4B image model + 670M text model,"6.9k + 1k + 0.8k = 8.7k TPUv4 core-days for BASIC-L, per Table 8

Two cores per chip, and 275 teraflop/s per chip 
(https://cloud.google.com/tpu/docs/system-architecture-tpu-vm#tpu_v4)

275 teraflops * 8700/2 * 24 * 3600 * 0.4 (assumed utilization) = 8.3e22","For pretraining (Section 8), we use the JFT dataset. This dataset has been
used in previous publications (Zhai et al., 2021; Dosovitskiy et al., 2021; Kolesnikov et al., 2020), but it has been constantly expanded. The JFT version used in our experiments has 5B images, each of which can be associated to one or multiple labels out of 29K possible classes.


""Starting from the ALIGN dataset, which contains 1.7B weakly-aligned image-text pairs (Jia et al., 2021), we collect 5B more image-text pairs, hence expanding the dataset size by roughly 4 times. We acquire
these 5B image-text pairs from the JFT dataset""",6.7B image-text pairs,,,,"65536, but these are image-text pairs not tokens
""For the batch size, we use 65536 contrastive
learning examples per minibatch""",
Swin Transformer V2 (SwinV2-G),11/18/2021,"SOTA improvement,Highly cited",3000000000,1.1e+21,,,,,,2326.6636503781665,NVIDIA A100 SXM4 40 GB,ImageNet21k,Confident,,,,Open weights (unrestricted),Open source,Open source,,,Hardware,"""It set new performance records on 4 representative vision tasks, including ImageNet-V2
image classification, COCO object detection, ADE20K semantic segmentation, and Kinetics-400 video action classification.""",,"trained on ""<0.5k"" TPUv3 core-days per Table 2 (not trained on TPUs, this is a comparison with other papers)

A core is 123/2 teraflops

500 core-days
= 500 * 123/2 trillion * 24 * 3600 * 0.4 utilization
~= 1.1e21","""We conduct experiments on ImageNet-1K image classification (V1 and V2) [18, 55], COCO object detection [44], and ADE20K semantic segmentation [85]. For the 3B model experiments, we also report the accuracy on Kinetics-400 video action recognition [37].""

• Image classification. ImageNet-1K V1 and V2 val are
used [18,55] for evaluation. ImageNet-22K [18] which
has 14M images and 22K categories is optionally employed for pre-training. For the pre-training our largest
model SwinV2-G, a privately collected ImageNet22K-ext dataset with 70 million images is used.",,,,,,"MIT license

https://github.com/microsoft/Swin-Transformer

training code: https://github.com/microsoft/Swin-Transformer/blob/main/get_started.md "
ViT-G/14 (LiT),11/15/2021,SOTA improvement,3005000000,,4000000000,4.5,,,,,Google TPU v3,"Conceptual Captions 12M (CC12M),YFCC-100M,Unspecified unreleased",Confident,ViT-G/14,,,,,,,,,"""For example, it achieves 82.5% accuracy on the challenging ObjectNet test set [1], outperforming the previous state-of-the-art
method [46] by 10.2%.""",Table 7,"They start with the ViT-G/14 image model and train their own text model. ViT-G/14 is 3.4e21. 

They also say ""We use 128 TPU cores by default for the above experiments, and 256 TPU cores for our best run with 18 billion seen image-text pairs"" which may be relevant.","CC12M, YFCC100m, and their novel dataset:
""Our dataset. We collect 4 billion image and alt-text
pairs following the same process as ALIGN [31], with the
same image-based filtering but simpler text-based filtering.
Appendix L shows that reducing text filtering does not harm
performance. To avoid misleading evaluation results, we
remove from our dataset near-duplicate images of all splits
from all datasets we evaluate on. We do not consider the
creation of our dataset a main contribution of this paper; we
just simplify the data collection process in ALIGN [31] to
demonstrate the efficacy of our methods at scale.""","Largest dataset is ""4 billion image and alt-text pairs"". This is rounded down slightly; the other datasets are much smaller.",,,,,
Masked Autoencoders ViT-H,11/11/2021,"Highly cited,SOTA improvement",632000000,4.6e+20,1281167,1600,69,,,,,ImageNet-1k,Speculative,ViT-Huge/14,,,Open weights (non-commercial),Open (non-commercial),Open access (non-commercial),,,"Hardware,Operation counting","""By fine-tuning with a 448 size, we achieve 87.8% accuracy, using only IN1K data. The previous best accuracy, among all methods using only IN1K data, is 87.1% (512 size)... We improve over the state-of-the-art by a nontrivial margin in the highly competitive benchmark of IN1K (no external data). Our result is based on vanilla ViT, and we expect advanced networks will perform better.""

See Table 3","Three models:
ViT-B (86M), ViT-L (304M), ViT-H (632M)","128 TPU-v3 cores trained for 1600 epochs. Times are given for 800 epochs in Table 2; largest model (ViT-H) took 34.5 hrs for 800.
128 TPU-v3 cores * 0.5 chips/core * 34.5 hours * 2 * 1.23E+14 FLOP/sec / chip * 3600 sec/hour  * 40% utilization = 7.84e20 FLOP

Note that the operations counting method disagrees:
2 × 632000000 connections × 3 × 1281167 training examples × 1600 of epochs  = 7.8e18 FLOP

Manual calculation with `calflops` package roughly agrees with hardware-time calculation: 
286.21 GFLOPS/observation * 1281167 observations * 1600 epochs = 5.86e20 FLOP

See reproduction here: https://colab.research.google.com/drive/1KCsmrfPzT9BgGO_YQthnz4oP3QRqbw5o?usp=sharing

Weighting three estimates equally:
(7.84e20 + 7.8e18 + 5.86e20)/3 = 4.6e20",,,"Table 2 gives wall times for training ViT-L and ViT-H to 800 epochs; later it is stated that the systems are each trained for 1600 epochs.
(34.5 hours / 800 epochs) * 1600 epochs = 69 hours","$0.62 / 32 cores of TPU-v3 * (128 cores / 32 cores) = $2.65/hour
CPI conversion to 2020: $2.25
$2.25/hour * 69 hours = $155.25","UNCERTAIN
128 TPU-v3 cores trained for 1600 epochs. Times are given for 800 epochs in Table 2; largest model (ViT-H) took 34.5 hrs for 800.
128 TPU-v3 cores * 0.5 chips/core * 34.5 hours * 2 * 1.23E+14 FLOP/sec / chip * 3600 sec/hour  * 40% utilization = 7.84e20 FLOP

Note that the operations counting method disagrees:
2 × 632000000 connections × 3 × 1281167 training examples × 1600 of epochs  = 7.8e18 FLOP
",,"Code at https://github.com/facebookresearch/mae

This project is under the CC-BY-NC 4.0 license

training code: https://github.com/facebookresearch/mae/blob/main/PRETRAIN.md "
Projected GAN,11/1/2021,SOTA improvement,,1.05e+19,3000000,,,,,,"NVIDIA V100,NVIDIA Quadro RTX 6000",,Confident,,,,,,,,,Hardware,"""It is further compatible with resolutions of up to one Megapixel and advances the state-of-the-art Fréchet Inception Distance (FID) on twenty-two benchmark datasets""",Possibly calculable from Appendix Table 8,"""With this setting, each experiment takes roughly 100-200 GPU hours on a NVIDIA V100,
for more details we refer to the appendix.""

""We conduct our experiments on an internal cluster with several nodes, each with up to 8 Quadro RTX
6000 or NVIDIA V100 using PyTorch 1.7.1 and CUDA 11.0.""

In appendix table 7, takes 10.1 seconds per 1k images on 8 Quadro RTX 6000s. Longest training run for Projected GAN appears to be in Figure 4 (left), at 14M images, though this is overtrained and the largest checkpoint used for evaluations was 10M.
10M images * 10.1 s/1000 images * 8 * 3.26e13 FLOP/s * 0.4 = 1.05e19",They experiment with 22 image datasets. Largest appears to be LSUN-Bedroom at 3M images.,They experiment with 22 image datasets. Largest appears to be LSUN-Bedroom at 3M images.,,,,,
CodeT5-base,11/1/2021,SOTA improvement,220000000,1.56e+21,,150,288,,,3114.8690946174474,NVIDIA A100,"CodeSearchNet,BigQuery",Likely,,,,Open weights (unrestricted),Open source,Open source,,,Hardware,"""Extensive experiments show that CodeT5 yields state-of-the-art results on the fourteen sub-tasks in CodeXGLUE.""","""We build CodeT5 based on Huggingface’s T5 (Raffel et al., 2020) PyTorch implementation and employ two sizes of CodeT5-small (60M) and CodeT5-base (220M)""","""We pre-train the model with the denoising objective for 100 epochs and bimodal dual training for further 50 epochs on a cluster of 16 NVIDIA A100 GPUs with 40G memory. The total training time for CodeT5-small and CodeT5- base is 5 and 12 days, respectively""

16 * 312 teraFLOP/s * 12 * 24 * 3600 * 0.3 (utilization assumption) = 1.56e21","""We follow Feng et al. (2020) to employ CodeSearchNet (Husain et al., 2019) to pre-train CodeT5, which consists of six PLs with both unimodal and bimodal data. Apart from that, we additionally collect two datasets of C/CSharp from
BigQuery1 to ensure that all downstream tasks have overlapped PLs with the pre-training data. In total, we employ around 8.35 million instances for pretraining""","""In total, we employ around 8.35 million instances for pretraining"" 
Instances meaning code snippets/examples, not tokens.","""The total training time for CodeT5-small and CodeT5- base is 5 and 12 days, respectively""",,,,"BSD-3-Clause license

https://github.com/salesforce/CodeT5"
S4,10/31/2021,SOTA improvement,249000000.00000003,6e+20,,509.02,,,,,,WikiText-103,,,,,Open weights (unrestricted),Open source,Open source,,,,"""S4 achieves strong empirical results across a diverse range of established benchmarks, including... SoTA on every task from the Long Range Arena benchmark""",,,,,,,,,"Apache 2 0
repo with training, inference, checkpoints:
https://github.com/state-spaces/s4"
EfficientZero,10/30/2021,SOTA improvement,,,,,,,,,,,Unknown,,,,,,,,,,"""Our method is 176% and 163% better
than the previous SoTA performance, in mean and median human normalized score respectively""",,"""Our implementation is computationally friendly. To train an Atari agent for 100k steps, it only needs 4 GPUs to train 7 hours.""",,,,,,,
Eve,10/27/2021,SOTA improvement,15010300,,,,,,,,,UniRef100,Likely,,,,Unreleased,Open source,Open source,,,,"""Our model EVE (evolutionary model of variant effect) not only outperforms computational approaches that rely on labelled data but also performs on par with, if not better than, predictions from high-throughput experiments, which are increasingly used as evidence for variant classification"" [Abstract] - SOTA improvement for very specific task","""The Bayesian VAE architecture in EVE is comprised of a symmetric 3-layer encoder & decoder architecture (with 2,000-1,000-300 and 300-1,000-2,000 units respectively) and a latent space of dimension 50 [...] We use a single set of parameters for the encoder (ϕp) and learn a fully-factorized gaussian distribution over the weights of the decoder (θp)""
They train a new VAE for each protein, and it doesn't seem like they trim the input sequence length, so the largest model will be the one trained for the largest input protein. Supplementary materials 1 gives statistics for each protein; the longest is 5202, which would indicate a network of size 15,010,300",,"""a Bayesian VAE was trained on a multiple sequence alignment retrieved by searching approximately 250 million protein sequences in UniRef""",,,,,,"The model code is available at https://github.com/OATML-Markslab/EVE
MIT license"
base LM+GNN+kNN,10/17/2021,SOTA improvement,274000000,7.3e+18,,,,,,,,WikiText-103,,,,,Open weights (unrestricted),Open source,Open source,,,,,,,,,,,,,"MIT License, training/eval code and weights
https://github.com/ShannonAI/GNN-LM"
T0-XXL,10/15/2021,Highly cited,11000000000,9.1819e+20,,,27,256,,11671.840843653788,Google TPU v3,P3 (Public Pool of Prompts),Confident,T5-11B,,,Open weights (unrestricted),Open source,Open source,,256452.45740223827,Hardware,"""we compare T0 to the zero-shot performance of the largest language models available as of writing, i.e., various GPT-3 models up to 175B parameters...
We find that T0 matches or exceeds the performance
of all GPT-3 models on 9 out of 11 held-out datasets""","""Unless specified otherwise, we use the XXL version which
has 11B parameters.""","From Table 1 and section B.1, a single run uses 27 hours of a 512 core slice of a TPU-v3 pod. 
512 * 0.5 * 1.23e14 * 3600 * 27 * 0.3 = 9.18e20
(cores) * (chip/core) * (FLOP/chip-sec) * (sec/hour) * (hours) * (utilization assumption)",,"Multitask - 12 tasks, 62 datasets. See fig 2 for details. 

This is going to be a nightmare to figure out! TODO figure out the sizes of each of these 62 datasets!

All datasets from here: https://arxiv.org/pdf/2109.02846.pdf

From B.2: ""across all of our training runs (including preliminary test experiments not described in this paper) we trained for 250 billion tokens""","For main model, 27 hours (Table 1)

Total time taken to train for all experiments was 270 hours ""These training runs corresponded to about 270 total hours of training on a v3-512 Cloud TPU device.""",,,,"Apache-2.0 license
https://github.com/bigscience-workshop/t-zero

training scripts: https://github.com/bigscience-workshop/t-zero/blob/master/training/README.md "
Yuan 1.0,10/12/2021,SOTA improvement,2.4573E+11,3.5380000000001e+23,1E+12,,,2128,0.45,,,"Common Crawl,Wikipedia,Sogue News",Confident,,,6881280,API access,Unreleased,,TRUE,,Reported,"""The zero-shot average scores of both LM and PLM are superior to the SOTA one. On Csldcp, Tnews and Iflytek tasks, we surpass the zero-shot SOTA by a large margin""","Table 2: Parameters of Yuan models.
""Parameters (billion)""","Table 9: 4095 petaFLOPS-days which equals 3.538*10^23 FLOP

https://www.wolframalpha.com/input?i=4095+petaFLOPS+*+1+day
","""A Chinese corpus with 5TB high-quality text is built, which is sufficient to train Yuan 245B model without sampling the dataset twice.""

In order to obtain the high-quality dataset, we develop a Massive Data Filtering System (MDFS) built on Spark to clean and filter the raw data, and train a Bert-based model to select high quality samples. MDFS is consisted of three parts, data collection, coarse filtering and fine filtering (Fig. 5). The raw data is collected from Common Crawl, Sogou News, SogouT, Encyclopedia, and Books (Table 3). To process these raw data, we run MDFS system on a high performance cluster with 36 nodes.","""Yuan 1.0 was trained on a new Chinese dataset of 5TB high-quality text that was built on 850TB raw data from Internet.""

1 GB ~ 167M words in English or 333M words in Chinese. For a mixed dataset of mostly Chinese, 5TB may be equivalent to around 1T words.

Table 2: 180B training tokens",,,,"Table 2. Batch size 3360, sequence length 2048. 3360*2048 = 6881280",https://github.com/Shawn-IEITSystems/Yuan-1.0
Megatron-Turing NLG 530B,10/11/2021,"SOTA improvement,Training cost",5.3E+11,1.17e+24,2.7E+11,,770,4480,0.302,3704291.3087597536,NVIDIA A100 SXM4 80 GB,"Common Crawl,The Pile,CC-Stories,Realnews",,,,3932160,Unreleased,Unreleased,,TRUE,3989424.741941752,Third-party estimation,"The 105-layer, transformer-based MT-NLG improved upon the prior state-of-the-art models in zero-, one-, and few-shot settings",,"https://www.lesswrong.com/posts/bGuMrzhJdENCo8BxX/nvidia-and-microsoft-releases-530b-parameter-transformer?commentId=HSJSNspKp94tFcSCx

source: https://lair.lighton.ai/akronomicon/
9938 PF-days * 3600 * 24 * 10^15  = 8.586432e+23"," In addition to Common Crawl data, we leveraged a number of other previously generated datasets. From The Pile, we selected Books3, OpenWebText2, Stack Exchange, PubMed Abstracts,
Wikipedia, Gutenberg (PG-19), BookCorpus2, NIH ExPorter, and Pile-CC datasets. We also included the
CC-Stories and RealNews datasets used to train Megatron","""Our training dataset consists of 339 billion tokens and we
trained MT-NLG on 270 billions tokens by blending the 15 training datasets as described above. We also set aside 2% of our data for validation.""

1 token ~ 0.75 words","Total compute was 1.17*10^24 FLOP.
They don't directly report the utilization and training speed when using the full Selene supercomputer with 560 DGX * 8 A100/DGX = 4480 GPUs. See section 2.3 Hardware Setup.

At 280 DGX, the utilization is 126/312 = 40% and a batch takes 60 seconds; at 350, it is 39% for 50 seconds; at 420, it is 36% for 44 seconds.

The overall utilization was 30.2% and the full cluster has 560 DGX. Dividing the total compute by the total performance of 4480 A100 at 30.2% utilization gives 770 hours.",,,"""The sequence length is 2048 and the global batch size is 1920. We used 8-way tensor and 35-way pipeline parallelism. The learning rate is 5.0e −5 . We used one billion tokens for linear learning rate warmup. We used cosine decay for the learning rate targeting to reach 10% of its value over 340 billion tokens. Over the first 12 billion tokens, we started at a batch size of 32 and gradually increased the batch size in increments of 32, until we reach the final batch size of 1920"" 

Final batch size is 1920 * 2048 = 3932160",
AlphaFold-Multimer,10/4/2021,"Highly cited,SOTA improvement",,4.35e+21,147328,68,384,64,,7966.233013,Google TPU v3,PDB (Protein Data Bank),Confident,AlphaFold 2,,,Open weights (unrestricted),Unreleased,Open source,,64120.37846954732,Hardware,"""On a benchmark dataset of 17 heterodimer proteins without templates (introduced in [2]) we achieve at least medium accuracy (DockQ [3] ≥ 0.49) on 14 targets and high accuracy (DockQ ≥ 0.8) on 6 targets, compared to 9 targets of at least medium accuracy and 4 of high accuracy for the previous state of the art system (an AlphaFold-based system from [2])""

""For heteromeric interfaces we successfully predict the interface (DockQ ≥ 0.23) in 67% of cases, and produce high accuracy predictions (DockQ ≥ 0.8) in 23% of cases, an improvement of +25 and +11 percentage points over the flexible linker modification of AlphaFold [4] respectively""

""For homomeric interfaces we successfully predict the interface in 69% of cases, and produce high accuracy predictions in 34% of cases, an improvement of +5 percentage points in both instances""","""Multiple changes to the AlphaFold system were made to adapt it to training on protein complexes, which are detailed below. Summarizing briefly, we [...] make various small adjustments to the structure losses and the model architecture."" [2. Methods]

Hence, this will have approximately the same amount of parameters as AlphaFold2","Section: 2.5. Training Regimen
""We train the model to convergence (approximately 10M samples, for 2 weeks) across 128 TPUv3 cores [...]. Then we [...] run two separate fine-tuning stages (one further day of training each)""

Assuming: FP16 and utilization 0.4

Calculation: (14+2) days * 24 hours/day * 60 min/hour * 60 sec/min * (128 TPU cores/2 cores per chip) * 1.23e14 FLOP/s per chip * 0.4 utilization = 4.35e21 FLOPs","""The training dataset comprised structures from the Protein Data Bank (PDB) [13] with a maximum release date of 2018-04-30"" [2.5. Training Regimen]","See: https://www.rcsb.org/stats/growth/growth-released-structures for 2018

""We train the model to convergence (approximately 10M samples, for 2 weeks) across 128 TPUv3 cores with a batch size of 1 per TPU core. Then we halve the learning rate and double the number of sequences fed into the MSA stack before running two separate fine-tuning stages (one further day of training each)""

10000000/147328 ~ 68 epochs","Section: 2.5. Training Regimen
""We train the model to convergence (approximately 10M samples, for 2 weeks) across 128 TPUv3 cores [...]. Then we [...] run two separate fine-tuning stages (one further day of training each)""",,,,"While the AlphaFold code is licensed under the Apache 2.0 License, the AlphaFold parameters and CASP15 prediction data are made available under the terms of the CC BY 4.0 license

https://github.com/google-deepmind/alphafold"
TrOCR,9/21/2021,SOTA improvement,558000000,,721200000,,,32,,,NVIDIA V100,,Confident,,,,Open weights (unrestricted),Open source,Open source,,21376.325079294176,,"from conclusion ""Experiment results show that TrOCR achieves state-of-the-art results on printed, handwritten and scene text recognition with just a simple encoder-decoder model, without any post-processing steps""",558M table 5,May be computed from github and datasets details. Uses pretrained BEiT and DeiT models.,"""To build a large-scale high-quality dataset, we sample two million document pages from the publicly available PDF files on the Internet.""
From the Experiment section: ""In total, the first-stage pre-training dataset contains 684M textlines."" ""The handwritten dataset for the second-stage pre-training consists of 17.9M textlines"" ""In total, the printed dataset consists of 3.3M textlines.""
and from MJSynth, SynthText datasets there is ""about 16M text images.""","The input data to the model are images.
684M + 17.9M + 3.3M + 16M",,,,,MIT: https://github.com/microsoft/unilm/tree/master/trocr
PLATO-XL,9/20/2021,SOTA improvement,11000000000,9.9e+21,1.5E+11,,,256,,,NVIDIA V100,,Confident,,,,Open weights (unrestricted),,,,171012.36549936663,Operation counting,,,"""In PLATO-XL, each model was trained for a total of 150B tokens, with
a batch size of 2M tokens.""

150B * 11B * 6 = 9.9e21",,"""In PLATO-XL, each model was trained for a total of 150B tokens, with
a batch size of 2M tokens.""
811M English and 1.2B Chinese (context, response) samples. So if the average response is at least 75 tokens, the 150B tokens seen in training don't include repeat tokens. This seems plausible.",,,,,"apache 2.0
https://github.com/PaddlePaddle/Knover/blob/develop/LICENSE"
HyperCLOVA 204B,9/10/2021,SOTA improvement,2.04E+11,,5.6E+11,,,,,,NVIDIA A100,Unspecified unreleased,Speculative,,,,,Unreleased,,,,,"""HyperCLOVA with our training configuration shows state-of-the-art in-context zero-shot and few-shot learning performances on various downstream tasks in Korean""",https://www.navercorp.com/navercorp_/ir/announce/2023/NAVER_CEO%20letter%20to%20shareholders_Aug%202023_Eng.pdf,"Estimations for 82B model (marked as lower bound estimations)

""For experiments in Section 4, the model trained with 150B is used for fair comparison, because not all models are finished training at the same iteration. However, experiments in Section 5.2 use the model trained with 300B tokens, as HyperCLOVA Studio provided the 39B and 82B models trained with 300B tokens.""

82e9 connections * 2 FLOP/connection * 300e9 tokens * 3 backward pass = 1.476e23 FLOP

Calculation using GPU time corroborates this:
- ""Our model is based on megatron-LM (Shoeybi et al., 2019) and trained on the NVIDIA Superpod, which includes 128 strongly clustered DGX servers with 1,024 A100 GPUs.""
- ""It takes 13.4 days to train a model with 82B parameters with 150B tokens."" Assume 300B tokens takes twice as long, 26.8 days.
- Assume the default of 30% utilization rate for large language models.

1024 A100 GPUs * 312e12 FLOP/second * 0.3 utilization * 26.8 days * 24 * 60 * 60 seconds/day = 2.219e+23 FLOP",,"https://twitter.com/arankomatsuzaki/status/1397583304610783238
https://venturebeat.com/ai/naver-trained-a-gpt-3-like-korean-language-model/",,,,,
PermuteFormer,9/6/2021,SOTA improvement,149697024,2.775e+18,103000000,30,,8,,,NVIDIA V100,WikiText-103,Speculative,,,,Unreleased,Open source,Open source,,5344.909219153732,Operation counting,"""Results show that
PermuteFormer uniformly improves the performance of Performer, accelerates convergence, and
achieves state-of-the-art on some tasks.""","Parameterization appears to be similar to a vanilla transformer. 6 layers, hidden dimension of 512, feed forward dimension of 1024, 8 attention heads. This would imply 20,447,232 parameters without embedding weights, and 512*vocab_size embedding weights (assuming tied embedding and unembedding projections)

They appear to use word-level tokenization: ""We evaluate unidirectional PermuteFormer on WikiText-103 (Merity et al., 2017). It is a language modeling dataset with about 103 million tokens,"" and I confirmed that word-level tokenization results in about 102M words across train-test-validation.

If this is the case, there are 267,735 unique words, so the embedding layer alone would be 137,080,320 parameters, for a total of 149,697,024.","6 * (30 * 103M) * 149,697,024 = 2.775e18

This seems a bit small relative to their statement: ""It takes about 10 days on 8 V100 GPUs to get all the figures in this paper"" which suggests about 2.7e20 FLOPs at 30% MFU.

Table 2 indicates that Performer and PermuteFormer take 0.23x to 0.58x as long to train as a Transformer model. Figure 2 appears to be the most compute intensive figure, and would take about 4 * (2.775e18) + 1 * (2.775e18 / 0.365) = 1.9e19 FLOPs. ",,WikiText-103 is about 103M tokens,Running all code needed to produce plots took about 10 days on 8 V100s,,,,no specific license. training code: https://github.com/cpcp1998/PermuteFormer/tree/master/language_model 
MEB,9/4/2021,Significant use,1.35E+11,,,,,,,,,,,,,,,,,,,,"""MEB is running in production for 100 percent of Bing searches, in all regions and languages.""",See paper title,,,"""MEB uses three years of search logs from Bing as training data."" TODO convert",,,,,
FLAN 137B,9/3/2021,"Highly cited,SOTA improvement",1.37E+11,2.047e+24,2.49E+12,1,,,,230526.76439336664,Google TPU v3,"Wikipedia,Unspecified unreleased",Confident,LaMDA,,,Unreleased,Unreleased,,TRUE,,Operation counting,"Abstract: 
""FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 datasets that we evaluate.""","Abstract:
""We take a 137B parameter pretrained language model and instruction tune it on
over 60 NLP datasets verbalized via natural language instruction templates. We
evaluate this instruction-tuned model, which we call FLAN, on unseen task types.""

Many models seem to be using the same 137B base transformer model?","From section 2.4: Pretraining was done over 2.49T tokens.
6 * 2.49T * 137B = 2.047e24 
Also, ""instruction tuning takes around 60 hours on a TPUv3 with 128 cores."" 128 TPUv3 cores = 64 TPUv3 chips. Environmental considerations section claims this took less than 2% of total time
1.23e14 * 64 * 60 * 3600 * 0.3 = 5.10e20","Abstract: ""We take a 137B parameter pretrained language model and instruction tune it on over 60 NLP datasets""","""Model architecture and pretraining. In our experiments, we use LaMDA-PT, a dense left-to-right, decoder-only transformer language model of 137B parameters (Thoppilan et al., 2022). This model is pretrained on a collection of web documents (including those with computer code), dialog data, and Wikipedia, tokenized into 2.49T BPE tokens with a 32k vocabulary using the  SentencePiece library (Kudo & Richardson, 2018). Around 10% of the pretraining data was non-English. Note that LaMDA-PT only has language model pretraining (c.f. LaMDA, which was finetuned for dialog).""",,,"""In our experiments, we use LaMDA-PT, a dense left-to-right,
decoder-only transformer language model of 137B parameters (Thoppilan et al., 2022) [...] Note that LaMDA-PT only has language model pretraining (c.f. LaMDA, which was finetuned for dialog)."" In our entry for LaMDA we only measured pre-training compute, so we just specify LaMDA as the base model of FLAN 137B.",,
XLMR-XXL,8/17/2021,SOTA improvement,10700000000,3.366e+22,1.67E+11,3.139449102,,,,,,CC100,Confident,,,1048576,Open weights (unrestricted),Unreleased,Open source,,,Operation counting,"Abstract:
""Our model also outperforms
the RoBERTa-Large model on several English tasks of the GLUE benchmark by 0.3% on average while handling 99 more languages.""","Section 2.1:
"" ...XLM-RXXL (L= 48, H = 4096, A = 32, 10.7B params)""","Trained for 500k steps at a batch size of 2048 with sequence length of 512 = 524,288,000,000 tokens seen.
6 * 10700000000 * 524,288,000,000 = 3.366e22",,"""We pretrain the models on the CC100 dataset, which corresponds to 167B tokens in 100 languages.""",,,,Batches of 2048 with sequence length of 512,https://github.com/facebookresearch/fairseq/tree/main/examples/xlmr
DNABERT,8/15/2021,SOTA improvement,110000000,1.07e+20,3000000000,4.04,600,,,,NVIDIA GeForce RTX 2080 Ti,Human Reference Genome (GRCh38/hg38),Confident,,,,Open weights (unrestricted),Open source,Open source,,,"Hardware,Operation counting","""We show that the single pre-trained transformers model can simultaneously achieve state-of-the-art performance on prediction of promoters, splice sites and transcription factor binding sites, after easy fine-tuning using small task-specific labeled data."" [Abstract] - SOTA improvement on very specific task","""We used the same model architecture as the BERT base, which consists of 12 Transformer layers with 768 hidden units and 12 attention heads in each layer, and the same parameter setting across all the four DNABERT models during pre-training""

Known to have 110 million parameters as reported in: https://arxiv.org/pdf/1810.04805v2.pdf
""We primarily report results on two model sizes: BERTBASE (L=12, H=768, A=12, Total Parameters=110M) [...]""","""Since the pre-training of DNABERT model is resource-intensive (about 25 days on 8 NVIDIA 2080Ti GPUs)""

Assuming FP16 and 30% utilization

Calculation = (25 * 24 *3600) s * 2.7e13 FLOP/s per GPU * 8 GPUs * 0.3 utilization = 1.4e20 FLOP

Alternatively:
""DNABERT takes a sequence with a max length of 512 as input... We pre-trained DNABERT for 120k steps with a batch size of 2000""
6 * 512 * 2000 * 120k * 110M = 8.11e19

Geometric mean: 1.07e20","""We generated training data from human genome [...]"" [2.2.2 Pre-training]. ","The human genome is around 3 billion base pairs (https://useast.ensembl.org/Homo_sapiens/Info/Annotation).
The authors use both non-overlapping sampling and random sampling from a human genome, though the source is unspecified.","""Since the pre-training of DNABERT model is resource-intensive (about 25 days on 8 NVIDIA 2080Ti GPUs)""",,,,"Apache 2.0, code and weights: https://github.com/jerryji1993/DNABERT"
Zidong Taichu,8/11/2021,Historical significance,3200000000,8.016e+20,41750000000,,,,,,,,Confident,,,,,,,,,Operation counting,"The world’s first image, language, and audio trimodal pre-trained model.",共32亿参数 translated as A total of 3.2 billion parameters ,4.175e10 * 3.2e9 * 6 = 8.016e20 FLOP,,"主要采用CLUE与WMT中收集的中文数据，同时我们加入了额外收集的对话数据以及翻译平行语料中的中文部分，总共约250G的中文语料，领域覆盖广泛。

From context, seems to mean 250GB

250GB * 167M tokens/GB = 4.175e+10 tokens

https://gitee.com/zidongtaichu/multi-modal-models/tree/master/text#%E6%95%B0%E6%8D%AE%E9%9B%86",,,,,
Jurassic-1-Jumbo,8/11/2021,Training cost,1.78E+11,3.7e+23,2.25E+11,,,,,,NVIDIA A100,,,,,3200000,API access,Unreleased,,TRUE,,Third-party estimation,"""Training such a large model, on over 800 GPUs over many months""

Lower-bound cost estimate:
800 GPUs * $1/GPU-hour * 4 months = $2.3M
True cost was probably substantially higher. ""many months"" implies more than 4, and the GPUs were probably more expensive than $1/hour.","""Jurassic-1 models come in two sizes, where the Jumbo version, at 178B parameters, is the largest and most sophisticated language model ever released for general use by developers.""",see here https://docs.google.com/document/d/1B8x6XYcmB1u6Tmq3VcbAtj5bzhDaj2TcIPyK6Wpupx4/edit,,"""Our model was trained with the conventional self-supervised auto-regressive training objective on 300B tokens drawn from publicly available resources""

1 token ~ 0.75 words",,,,"""Namely, we used a base learning rate of 1.2 × 10−4 and 0.6 × 10−4 , and a batch size of 2M and 3.2M tokens, for J1-Large and J1-Jumbo, respectively. We also used a linear warm-up over roughly the first 375 million tokens, and gradually increased the batch size from 32K tokens up to its target value for the first few billion tokens.""",
W2v-BERT,8/7/2021,SOTA improvement,1000000000,,,,,,,,,LibriLight,Confident,,,,,,,,,,"""Our experiments show that w2v-BERT achieves competitive results
compared to current state-of-the-art pre-trained models on the LibriSpeech benchmarks when using the Libri-Light 60k corpus as the
unsupervised data. In particular, when compared to published models such as conformer-based wav2vec 2.0 and HuBERT, our model
shows 5% to 10% relative WER reduction on the test-clean and
test-other subsets""",1B for XXL model,,"""We use the Libri-Light unlab-60k subset [34], which contains
about 60,000 hours of unannotated speech audio, for pre-training
w2v-BERT models. For our main results, we use the LibriSpeech
960hr subset [35] as the supervised data, and use the 100hr subset
for ablation studies""",,,,,,
YOLOX-X,8/6/2021,"Highly cited,SOTA improvement",99100000,6.34275e+20,2500000,300,,8,,,NVIDIA V100,COCO 2017,Likely,,,128,Open weights (unrestricted),Open source,Open source,,5346.624864155351,Operation counting,Table 6,"99.1M, table 3","""We train the models for a total of 300 epochs with 5 epochs warmup on COCO train2017 [17]. We use stochastic gradient descent (SGD) for training ... The batch size is 128 by default to typical 8-GPU devices ... input size is evenly drawn from 448 to 832 with 32 strides""

Training is done on 300 epochs of the 2.5 million image-label pairs in COCO train2017.

Table 3 indicates 281.9 GFLOP per forward pass on a 640x640 image. The mean image width/height is 640, though using this to estimate training FLOPs is probably a slight underestimate as FLOPs will scale roughly linearly in the number of pixels (which scale at width^2).

Ignoring this slight issue: 281.9e9 * 2.5M * 300 * 3 = 6.34e20","""We train the models for a total of 300 epochs with 5 epochs warmup on COCO train2017""","2.5 million image-label pairs, per Coco paper https://arxiv.org/abs/1405.0312",,,,,"Apache 2.0 for code/weights
https://github.com/Megvii-BaseDetection/YOLOX/blob/main/LICENSE"
6-Act Tether,8/3/2021,SOTA improvement,5000000,,,,,,,,,Matterport,Confident,,,,,,,,,,"""Our agents achieve 24.5% success and 8.1% SPL, a 37% and 8% relative improvement over prior state-of-the-art, respectively, on the Habitat ObjectNav Challenge""","""Agent parameter counts were all 5 − 6 million parameters, excluding parameters in auxiliary modules""","""In our experiments, we train each of our agents for 8 GPU-weeks (192 GPU-hours)"". No GPU specified.","""We experiment on the Matterport dataset (MP3D [4]), which has 90 scenes and 40 labeled semantic object categories.""",,,,,,
SEER,7/29/2021,SOTA improvement,1300000000,1.8e+22,1000000000,,195.5,512,,34114.252463690456,NVIDIA V100,Instagram,,,,,Open weights (non-commercial),Open (non-commercial),Open access (non-commercial),,342212.3909614999,Hardware,"SOTA for self-supervised models on ImageNet, which seems fair to consider a different benchmark than ImageNet for supervised models.

""Our final SElf-supERvised (SEER) model,
a RegNetY with 1.3B parameters trained on 1B random
images with 512 GPUs achieves 84.2% top-1 accuracy,
surpassing the best self-supervised pretrained model by 1%""","From abstract:
"" Our final SElf-supERvised (SEER) model, a RegNetY with 1.3B parameters...""","Numbers from section 3.2, they specifically mention using mixed precision training.
6125 ms / batch * 114890 batches = 8.14 days (they round to 8 in the text)

512 GPUs * 8.14 days * 24h/day * 3600s/h * 125 TFLOP/s * 0.4 (assumed utilization) = 1.800e22","Section 3.3:
""For our billion scale pretraining, we consider a dataloader that directly samples random, public, and non-EU images from Instagram""

Note the dataset is not static - it is refreshed every 90 days","""Overall, we train
on 1B images for a total of 122K iterations.""",6125 ms / batch * 114890 batches = 195.5 hours,,,,"https://github.com/facebookresearch/vissl/tree/main/projects/SEER

We share instructions on how to train SEER model on GPUs using PyTorch. First, Install VISSL and follow the data setup instructions to easily setup your data input with VISSL.

https://github.com/facebookresearch/vissl/blob/main/projects/SEER/MODEL_LICENSE.md"
HuBERT,7/27/2021,"Highly cited,SOTA improvement",1000000000,5.54e+21,820800000,,,,,,,"LibriSpeech,LibriLight",Speculative,,,,Open weights (unrestricted),Open source,Open source,,,Hardware,"Abstract: 
"" the
HuBERT model either matches or improves upon the state-ofthe-art wav2vec 2.0 performance on the Librispeech (960h) and
Libri-light (60,000h) benchmarks with 10min, 1h, 10h, 100h, and
960h fine-tuning subsets.""","From abstract:
""Using a 1B parameter model, HuBERT shows up to 19% and 13% relative WER reduction on the more challenging dev-other and test-other evaluation subsets""","GPU NOT SPECIFIED - for the sake of argument I assume something on the order of 1 TFLOP/s

Numbers from Section IV part C
0.1 * (960h * 32GPUs + 60000h * 256 GPUs) * 3600s/h * 1 TFLOP/s/GPU",,"""When the HuBERT model is pre-trained on either the standard Librispeech 960h [24] or the Libri-Light 60k hours [25], it either matches or improves upon the state-of-theart wav2vec 2.0 [6] performance on all fine-tuning subsets of 10mins, 1h, 10h, 100h, and 960h.""

1h ~ 13,680 words
13,680 * 60,000 = 820800000",,,,,"https://github.com/facebookresearch/fairseq/tree/main/examples/hubert

fairseq(-py) is MIT-licensed. The license applies to the pre-trained models as well.
includes training code"
GOAT,7/27/2021,SOTA improvement,3472816,2.412e+22,3.9E+11,,,,,84799.78517435073,Google TPU v3,XLand,Speculative,,,64,Unreleased,Unreleased,,,,Hardware,likely qualitatively SOTA,estimate described here: https://docs.google.com/document/d/1S9xZyCeITDOs-P1W_-liNW0WgVN-OLsSudVrPXMaLqw/edit?usp=sharing,"[Final calculation]
(8 TPUs) * (1.23e14 FLOP/TPU-s) * (0.1 utilization) / (50k steps/s) = 1.968e9 FLOP/step

(32 agents) * (383B steps/agent) * (1.968e9 FLOP/step) = 2.412e22 FLOPs

==========================
NOTES BELOW

6.1: Each agent is trained using 8 TPUv3s and consumes approximately 50,000 agent steps (observations) per second.
Multiple agents interacting probably mean a fairly low utilization rate, so let’s assume 0.10
8 * 1.23e14 * 0.1 / 50k = 1.968e9 FLOPs per step

The paper doesn’t say exactly how many agents they train in each population. The original PBT paper uses 32 agents for one task (in general it uses between 10 and 80), so as a guesstimate let’s go with that.

Figure 16: They train over 5 generations. Summing the number of steps, it looks like there were roughly 383B steps
32 * 383B * 1.968e9 = 2.412e22

Final estimate:
2.412e22

I do a confidence interval analysis here and find a 90% CI of 6.9e21 to 1.3e23, so we can call this estimate ""likely"" (within 1 OOM): https://colab.research.google.com/drive/1wGSTQxBExY6Fa0-d7msVumf5-KnsWLe6?usp=sharing",,Figure 16 shows steps per generation and agent. In total there are 1.5e10 + 4.0e10 + 2.5e10 + 1.1e11 + 2e11 = 3.9e11 steps per agent.,"see other notes
",,,,
Codex,7/7/2021,"Significant use,Highly cited",12000000000,7.344e+22,31800000000,,,,,,,,Likely,GPT-3 13B,,,API access,Unreleased,,TRUE,,,,"""With just a single sample, a 12B parameter Codex solves 28.8% of these problems, and a 300M parameter Codex solves 13.2% of these problems""","""The original training of GPT-3-12B consumed hundreds of petaflop/sdays of compute, while fine-tuning it to create Codex-12B
consumed a similar amount of compute.""
1 PFLOP/s-day = 8.64e19 FLOPs.
""Hundreds"" is likely between 200 and 900, geometric mean = 425.
2 * 425 * 8.64e19 = 7.344e22
",,"""Our training dataset was collected in May 2020 from 54 million public software repositories hosted on GitHub, containing 179 GB of unique Python files under 1 MB. We filtered out files which were likely auto-generated, had average line
length greater than 100, had maximum line length greater
than 1000, or contained a small percentage of alphanumeric
characters. After filtering, our final dataset totaled 159 GB.""

1 GB ~ 200M words",,,,,"Codex was available via the OpenAI API from announcement: https://openai.com/index/openai-codex/

It is still available via the Research Access Program. https://x.com/OfficialLoganK/status/1638336152800206858"
ERNIE 3.0,7/5/2021,SOTA improvement,10000000000,2.25e+22,6.68E+11,,,384,,39104.26710360624,NVIDIA V100,,,,,,Open weights (unrestricted),Open source,Open source,,256723.31059309733,Operation counting,"""ERNIE 3.0 achieved new state-of-the-art results across 54 Chinese NLP tasks""","""We trained the model with 10 billion parameters on a 4TB corpus consisting of plain texts and a large-scale knowledge graph.""","Section 3.3.3: 
""""The model is trained for
a total of 375 billion tokens""

Total compute approximated as 6*N*D",,"""To ensure the success of the pre-training of ERNIE 3.0, we construct a large-scale, wide-variety and high-quality Chinese text corpora amounting to 4TB storage size in 11 different categories.""

1 GB ~ 167M chinese words",,,,,"apache 2.0 
train code: https://github.com/PaddlePaddle/PaddleNLP/tree/develop/legacy/model_zoo/ernie-3.0#%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83 "
Adaptive Input Transformer + RD,6/28/2021,SOTA improvement,247000000.00000003,8.2e+19,,,,,,,,WMT14,,,,,Unreleased,Open source,Open source,,,,"""In particular, it yields substantial
improvements when applied to fine-tune large-scale pre-trained models, e.g., ViT, RoBERTa-large, and BART, and achieves state-of-the-art (SOTA) performances with the vanilla Transformer model """,,,,,,,,,"train and inference code for translation models, MIT license. no weights

https://github.com/dropreg/R-Drop/blob/main/fairseq_src/examples/translation_rdrop/README.md "
EfficientNetV2-XL,6/23/2021,"Highly cited,SOTA improvement",208000000,9.56e+19,14180000,30,45,16,,104.34013401561587,Google TPU v3,"ImageNet21k,ILSVRC 2012 subset of ImageNet",Confident,,,4096,Open weights (unrestricted),Open source,Open source,,16047.21161558559,Hardware,"""EfficientNetV2 achieves 87.3% top-1 accuracy on ImageNet ILSVRC2012, outperforming the recent ViT by 2.0% accuracy while 
 training 5x-11x faster using the same computing resources.""","208M for XL version (Table 7, page 7)","Table 7, page 7: 45 hours on 32 TPUv3 cores.

""Each v3 TPU chip contains two TensorCores.""
TPU performance per chip = 123e12 FLOP/s
32 cores = 16 chips

123e12 FLOP/s per chip * (32 cores / 2 cores per chip) * 45 hours * 3600 seconds/hour * 0.30 utilization = 9.56e19 FLOP

https://www.wolframalpha.com/input?i=123+terahertz+*+16+*+45+hours+*+0.3","""Setup: ImageNet21k (Russakovsky et al., 2015) contains
about 13M training images with 21,841 classes. The original
ImageNet21k doesn’t have train/eval split, so we reserve randomly picked 100,000 images as validation set and use the
remaining as training set. We largely reuse the same training
settings as ImageNet ILSVRC2012 with a few changes: (1)
we change the training epochs to 60 or 30 to reduce training
time, and use cosine learning rate decay that can adapt to
different steps without extra tuning; (2) since each image
has multiple labels, we normalize the labels to have sum
of 1 before computing softmax loss. After pretrained on
ImageNet21k, each model is finetuned on ILSVRC2012 for
15 epochs using cosine learning rate decay.""

Based on Table 7, where XL is trained on 5x as many TPU-hours as S, despite having 10x as many parameters, XL was probably trained on 30 epochs and S on 60 epochs.

","""ImageNet21k (Russakovsky et al., 2015) contains about 13M training images with 21,841 classes. The original ImageNet21k doesn’t have train/eval split, so we reserve randomly picked 100,000 images as validation set and use the remaining as training set...
After pretrained on ImageNet21k, each model is finetuned on ILSVRC2012 for 15 epochs using cosine learning rate decay.""

12.9M + 1.28M ~= 14,180,000",Table 7,,,"""Each model is trained for 350 epochs with total batch size 4096""","code and weights: https://github.com/google/automl/tree/master/efficientnetv2

Apache-2.0 license"
Denoising Diffusion Probabilistic Models (LSUN Bedroom),6/11/2021,"Highly cited,SOTA improvement",256000000,3.8e+20,3033042,,,,,436.3084845,Google TPU v3,LSUN Bedroom,,,,,Open weights (unrestricted),Open source,Open source,,,Hardware,"Novel approach to image synthesis that yields SOTA results on datasets like CIFAR-10

Abstract: 
""On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. ""","Appendix B: 
"" Our CIFAR10 model has 35.7 million parameters, and our LSUN and
CelebA-HQ models have 114 million parameters. We also trained a larger variant of the LSUN Bedroom model with approximately 256 million parameters by increasing filter count.""","Numbers in Appendix B
10.6h for the CIFAR model (batch size 128, 21 step/s)
2.2 step/s for the LSUN model, 1.15M steps so 702.8 hours

This is for TPUv3-8's, which seems to mean 8 cores (standard chip is 125 teraflop/s for 2 cores)
https://cloud.google.com/tpu/docs/regions-zones

1.25E14 FLOP/s * (8 cores / 2 cores/chip) * 702.8h * 3600s/h * 0.3 = 3.8e20",,"""We trained on CelebA-HQ for 0.5M steps, LSUN Bedroom for 2.4M steps, LSUN Cat for 1.8M steps, and LSUN Church for 1.2M steps.""

""The CelebA-HQ dataset is a high-quality version of CelebA that consists of 30,000 images at 1024×1024 resolution.""
https://paperswithcode.com/dataset/celeba-hq

LSUN bedroom has 3,033,042 examples. LSUN cat has 1,657,266 examples. LSUN church has 126,227 examples.
https://www.tensorflow.org/datasets/catalog/lsun
",,,,,"https://github.com/hojonathanho/diffusion

everything is openly avaiable but no license or terms of use information

train code: https://github.com/hojonathanho/diffusion/blob/master/scripts/run_lsun.py "
ALIGN,6/11/2021,"Highly cited,SOTA improvement",820000000,2.598670000001e+22,1600000000,,347.3,512,,32852.91660437908,Google TPU v3,"Conceptual Captions (CC3M),FIT400M",Confident,,,16384,Unreleased,Unreleased,,,513575.0111,Hardware,"""The aligned visual and language representations... set new state-of-the-art results on Flickr30K and MSCOCO image-text retrieval benchmarks""","From author communication

480M (image tower) + 340 M (text tower)","From author communication
14.82K TPUv3 core-days
Precision: bfloat16

Estimation
TPUv3 at float16: 123 TFLOPS/chip

123*10^12 TFLOPS/chip * (1 chip / 2 cores) * 14820 TPU core-days * 86400 s/day * 33% utilization = 2.599*10^22 FLOP
https://www.wolframalpha.com/input?i=14820+days+*+123+teraFLOPS+%2F+2+*+0.33",,"Dataset contains 1.8B image-text pairs, then some duplicates are removed.",14820 TPU core-hours / 1024 TPU cores = 347.3 hours,,,,
DeBERTa,6/10/2021,"Highly cited,SOTA improvement",1500000000,2.588e+22,15600000000,49.2,720,256,,6682.228999,NVIDIA V100,"Wikipedia,CC-Stories,OPENWEBTEXT,BookCorpus (BooksCorpus, Toronto Book Corpus)",,,,,Open weights (unrestricted),Open source,Open source,,171193.4561466547,Hardware,"""DeBERTa significantly outperforms all existing PLMs of similar size on MNLI and creates a new state of the art""","""...we scale up DeBERTa by training a larger version that consists of 48 Transform layers with 1.5 billion parameters""

Other versions are smaller and use a smaller pre-training dataset. These are distinguished in the paper (e.g. DeBERTa1.5B is the version of DeBERTa with 1.5 billion parameters).","Table 8: 16 DGX-2 nodes (x16 V100s each) for 30 days
16 * 16 * 1.3e14 * 30 * 24 * 3600 * 0.3 = 2.588e22","We pre-train our large models following the setting of BERT (Devlin et al., 2019), except that we use the BPE vocabulary of Radford et al. (2019); Liu et al. (2019c). For training data, we use Wikipedia (English Wikipedia dump3; 12GB), BookCorpus (Zhu et al., 2015) (6GB), OPENWEBTEXT (public Reddit content (Gokaslan & Cohen, 2019); 38GB), and STORIES (a subset of CommonCrawl (Trinh & Le, 2018); 31GB). The total data size after data deduplication (Shoeybi et al., 2019) is about 78G",""" DeBERTa is pretrained on 78G training data""

1GB ~ 200M words",30 days (Table 8),,,,"MIT

https://github.com/microsoft/DeBERTa

pretrain code: https://github.com/microsoft/DeBERTa/tree/master/experiments/language_model "
EMDR,6/9/2021,SOTA improvement,440000000,1.91e+21,1.716E+11,4.05,355,,,2773.3124307816734,NVIDIA A100,"Wikipedia,NQ (Natural Questions),TriviaQA",Confident,,,,Open weights (unrestricted),Open source,Open source,,,"Hardware,Operation counting","""Experiments on three benchmark datasets demonstrate that our proposed method outperforms all existing approaches of comparable size by 2-3% absolute exact match points, achieving new state-of-the-art results.""",Table 2,"""We run all of our experiments on a machine with 96 CPUs, 1.3TB physical memory, and 16 A100 GPUs. We use PyTorch (Paszke et al., 2019) to implement our proposed model. With this hardware setup, our experiments on NQ and TriviaQA took approximately 25 hours to complete, while experiments on WebQ took roughly 8 hours to complete. Before supervised training, we also perform a one-time unsupervised MSS pre-training for 82,000 steps that took roughly 1 week.""

1 week + 25 hours * 16 A100s
= ~193 * 16 A100-hours
= 193 * 16 * 3600 * 312 trillion * 0.3 = 1.04e21

Additionally, the model uses BERT, ICT, and T5 models. These required:
- BERT: 6 * 110M parameters * (1M * 256 * 256) inputs = 4.33e19 FLOP
- ICT: 6 * 220M parameters * (100k * 4096 * 256) inputs = 1.38e20 FLOP
- T5: 6 * 220M parameters * (1M * 2048 * 256) inputs = 6.92e20 FLOP

Total: 1.04e21 + 4.33e19 + 1.38e20 + 6.92e20 = 1.91e21","Pre-train different components on Wikipedia, BookCorpus, C4, and OpenWebText (Table 6), then training on the QA datasets","At the time of publication there were about 4B words (5.3B tokens) on English Wikipedia: https://en.wikipedia.org/wiki/Wikipedia:Size_of_Wikipedia#Yearly_statistics
BookCorpus has about 1B words (1.3B tokens), C4 has about 156B tokens, and OpenWebText has about 9B tokens.

From Table 6, it looks like all datasets were trained on for over one epoch.

BERT: 1M steps, batches of 256, sequence length 256 = 65.5B tokens vs 6.6B in Wikipedia + BookCorpus
ICT: 100k steps, batches of 4096, sequence length 256 = 104.9B tokens vs 5.3B in Wikipedia
T5: 1M steps, batches of 2048, sequence length 256 = 524.3B tokens vs 170.3B tokens in C4 + Wikipedia + OpenWebText

Total tokens: 171.6 billion

Some tokens were probably seen more times than others, but overall this corresponds to 4.05 epochs on the pre-training data. ","""We run all of our experiments on a machine with 96 CPUs, 1.3TB physical memory, and 16 A100 GPUs [...] our experiments on NQ and TriviaQA took approximately 25 hours to complete, while experiments on WebQ took roughly 8 hours to complete. Before supervised training, we also perform a one-time unsupervised MSS pre-training for 82,000 steps that took roughly 1 week""

Additionally, they pre-trained BERT, ICT, and T5 models, which took a combined 8.733e20 FLOPs. On 16 A100s at 0.3 utilization, that would have taken approximately 162 hours.

So total time for the largest experiment (NQ or TriviaQA) is around:
25 + 168 + 162 = 355",,,,"https://github.com/DevSinghSachan/emdr2

training scripts: https://github.com/DevSinghSachan/emdr2/tree/main/examples "
CoAtNet,6/9/2021,SOTA improvement,2440000000,4.27e+22,3000000000,1.83,,,,1887.1635925610951,Google TPU v3,JFT-3B,Confident,,,,Unreleased,Unreleased,,,,Hardware,"""Notably, when we further scale up CoAtNet with JFT-3B, it achieves 90.88% top-1 accuracy on ImageNet, establishing a new state-of-the-art result.""",,"20.1K TPU-v3 core-days

TPUs have two cores per chip, and a chip is 123 teraflop/s 
https://cloud.google.com/tpu/docs/system-architecture-tpu-vm#tpu_v3

123 teraflop/s * 20100/2 * 24 * 3600 * 0.4 (utilization assumption for non-language models) = 4.27e22","When only ImageNet-1K is used for training, CoAtNet achieves 86.0% top-1 accuracy, matching the prior art NFNet [20] under similar computation resource and training conditions. Further, when pre-trained on ImageNet-21K with about 10M images, CoAtNet reaches 88.56% top-1 accuracy when finetuned on ImageNet-1K, matching the ViT-Huge pre-trained on JFT-300M, a 23× larger dataset. Finally, when JFT-3B is used for pre-training, CoAtNet exhibits better efficiency compared to ViT, and pushes the ImageNet-1K top-1 accuracy to 90.88% while using 1.5x less computation of the prior art set by ViT-G/14 [26].","Used JFT-3B (3 billion images), but not stated for how many epochs. 

Based on GPU time, training took 4.27e+22 FLOPs. Table 5 indicates 2.586e12 FLOPs per image. Since training is roughly 3x the FLOP cost of inference, implies inference on full training set took 1.42e22 FLOP
Then # images trained over is around 1.42e22 / 2.586e12 = 5,491,105,955

So probably ~1.83 epochs on 3B images",,,,,
ViT-G/14,6/8/2021,SOTA improvement,1843000000,5.85e+22,3000000000,54.6,,2048,,3847.8613922131217,Google TPU v3,"JFT-3B,ImageNet",Confident,,,32768,Unreleased,Open source,Open source,,2054364.3396154805,"Hardware,Operation counting","""we successfully train a ViT model with two billion parameters, which attains a new state-of-the-art on ImageNet of 90.45% top-1 accuracy""",Table 2 of paper,"Digitizing Figure 9 indicates training used 27,200 TPUv3 core-days. TPUv3 is 123 teraflop/s per chip, 2 cores per chip.

1.23e14 * (1/2) * 27,200 * 24 * 3600 * 0.4 = 5.78e22

Alternatively, Table 2 indicates 965.3e9 FLOPs per forward pass on a 224^2 image. Table 4 indicates 5 million steps at a (normalized) batch size of 4096, and total flops including backward pass would be 3x the FLOPs from forward passes alone, so we get:
4096 * 5e6 * 965.3e9 * 3 = 5.93e22

(Note that actual batch size appears to have been 32,768)

Geometric mean: sqrt(5.78e22*5.93e22) = 5.85e22

However note that this leaderboard claims ViT-G/14 took 34 PF-days, or 2.94e21 FLOPs: https://web.archive.org/web/20211218185755/https://lair.lighton.ai/akronomicon/","We trained a large Vision Transformer, ViT-G/14, which
contains nearly two billion parameters. Section 3.6 details
the architecture’s shape. We evaluate the ViT-G/14 model on
a range of downstream tasks, and compare it to recent stateof-the-art results. We fine-tune on ImaegNet","""For this study, we use the proprietary JFT-3B dataset, a larger version of the JFT-300M dataset used in many previous works on large-scale computer vision models [31, 18, 11]. This dataset consists of nearly 3 billion images, annotated with a class-hierarchy of around 30k labels via a semi-automatic
pipeline""

Epochs: 5M steps (Table 11) * 32768 (batch size) / 3B = 54.6 epochs",,,,,"About the weights: https://twitter.com/giffmana/status/1402507421029916672

About the code: Apache 2.0
https://github.com/google-research/big_vision/blob/main/big_vision/configs/proj/scaling_laws/train_vit_g.py"
ByT5-XXL,5/28/2021,SOTA improvement,12900000000,8.1e+22,,,,,,92453.37635669748,Google TPU v3,mC4,Likely,,,1048576,Open weights (unrestricted),Open source,Open source,TRUE,,Operation counting,"""On the most realistic in-language setting, where some gold training data is available in all languages, ByT5 surpasses the previous state-of-art mT5 on all tasks and model sizes""","12.9B, from Table 1","""Like mT5, we set our sequence length to 1024 (bytes rather than tokens), and train for 1 million steps over batches of 2^20 tokens.""

12.9 billion * 1 million * 2^20 * 6 = ~8.1e22",,,"Table 9 indicates pretraining completes 25 sequences per second on a TPUv3-64 device. 

""we set our sequence length to 1024 (bytes rather than tokens), and train for 1 million steps over batches of 2^20 tokens."" 

So 1024 sequences per step * 1M steps = 1.024 billion sequences
1.024 B / 25 = 40.96M seconds = 11378 hours or 474 days. This seems implausible, so probably they just used a bigger TPU slice for the full training, but this is not indicated.",,,"""Like mT5, we set our sequence length to 1024 (bytes rather than tokens), and train for 1 million steps over batches of 2^20 tokens""","Apache:
https://github.com/google-research/byt5/blob/master/LICENSE

see mC4 notes for data accessibility

training script: https://github.com/google-research/byt5/blob/master/README.md "
Transformer local-attention (NesT-B),5/26/2021,Highly cited,90100000,2.40576e+19,1280000,,,,,,,ImageNet-1k,,,,,Open weights (unrestricted),Open source,Open source,,,Operation counting,,"Table A2, NesT-B is the largest size.","17.9 GFLOPS per forward pass
300 epochs
1.28M training examples
3.5 f_to_b pass ratio
(From Imagenet paper-data, Besiroglu et al., forthcoming) ",,,,,,,"Apache-2.0 license, includes train code and evaluation
https://github.com/google-research/nested-transformer"
CogView,5/26/2021,SOTA improvement,4000000000,2.68e+22,50000000000,,,512,,60071.706664791694,NVIDIA Tesla V100 DGXS 16 GB,WuDao Corpora,Likely,,,,Open weights (unrestricted),Open source,Open source,,285367.11236577464,Third-party estimation,"""CogView achieves the state-of-the-art FID on the blurred MS COCO dataset, outperforming previous GAN-based models and a recent similar work DALL-E""","""We propose CogView, a 4-billion-parameter Transformer with VQ-VAE tokenizer to advance this problem.""","source: https://lair.lighton.ai/akronomicon/
archived: https://github.com/lightonai/akronomicon/tree/main/akrodb","""We collected about 30 million text-image pairs from multiple channels, and built a 2.5TB new dataset (after tokenization, the size becomes about 250GB).""","""We collected about 30 million text-image pairs from multiple channels, and built a 2.5TB new dataset (after tokenization, the size becomes about 250GB).""

250GB * (1 word / 5 bytes) = 50 billion words or 67 billion tokens

So 30M text-image pairs and 50 billion words",,,,,"Apache 2 license

https://github.com/THUDM/CogView

train script: https://github.com/THUDM/CogView/blob/main/scripts/pretrain_single_node.sh "
MedBERT,5/20/2021,SOTA improvement,17000000,9.47e+18,,50.5,168,1,,62.48645493610712,NVIDIA Tesla V100 DGXS 32 GB,Cerner Health Facts,Likely,,,,Unreleased,Open source,Open source,,557.3925941869019,Hardware,"""This work is the first demonstration of significantly boosted
performance over state-of-the-art methods on multiple
clinical tasks with phenotyped cohorts.""","17M from ""This is possibly due to the fact that the untrained Med-BERT is an over-parameterized model (around 17 million parameters) with a huge
number of configurations, so it might overfit to the training data""","flops = (1) * (3.13e13) * (24*7 * 3600) * (0.5) = 9.47e18
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)
I assume higher utilization rate, because only 1 GPU is used.
Citation from the text:
""We used a single Nvidia Tesla V100GPU of 32 GB graphics memory capacity, and we trained the model for a week for more than 45 million steps, for which each step consists of 32 patients (batch size)."" - page 11

Note that public code appears not to make use of the tensor core speed up, thus I use 3.13e13 FLOP/sec","page 3 data source
""We extracted our cohorts from two databases: Cerner Health
Facts® (version 2017) (Cerner) and Truven Health MarketScan®
(Truven)""
""Our pretraining cohort for Med-BERT is consisting of 28 million
patients extracted from Cerner""""","data about 28M patients
""Our pretraining cohort for Med-BERT is consisting of 28 million
patients extracted from Cerner""","""We used a single Nvidia Tesla V100GPU of 32 GB graphics memory capacity, and we trained the model for a week for more than 45 million steps, for which each step consists of 32 patients (batch size)."" - page 11",,,,"Apache 2
https://github.com/ZhiGroup/Med-BERT
training guide: https://github.com/ZhiGroup/Med-BERT/tree/master/Pretraining%20Code 

""Initially we really hoped to share our models but unfortunately, the pre-trained models are no longer sharable. According to SBMI Data Service Office: ""Under the terms of our contracts with data vendors, we are not permitted to share any of the data utilized in our publications, as well as large models derived from those data."""
ADM,5/11/2021,"Highly cited,SOTA improvement",559000000,6.2e+21,1281167,381,,,,11274.484326547095,NVIDIA V100,"LSUN,ILSVRC 2012 subset of ImageNet",Confident,,,,Open weights (non-commercial),Open source,Open source,,,Hardware,"""We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models""","Largest model is denoted ImageNet 512, has 559M parameters","Largest run with their architecture improvements is the ImageNet 512 variant. Table 7 suggests utilization is around 30% for largest models (though we only see 256 x 256 and 128 -> 512)

Table 10: ImageNet 512 variant took 1914 V100-days of training
125e12 FLOP/sec * 1914 days * 24 h/day * 3600 sec/h * 0.3 = 6.2e21","""To evaluate our improved model architecture on unconditional image generation, we train separate diffusion models on three LSUN [71] classes: bedroom, horse, cat""","Biggest models are trained on ImageNet 512x512. ImageNet ILSVRC has 1,281,167 images in the training set, but it is possible some were filtered due to size.

Note that a smaller model was trained on LSUN {bedroom, horse, cat}, which forms a larger dataset:
3,033,042 + 2,000,340 + 1,657,266 = 6,690,648 images

Epochs ≈ (1,940,000 * 256) / 1,300,000 ≈ 381 epochs",,,,,"These models are intended to be used for research purposes only. In particular, they can be used as a baseline for generative modeling research, or as a starting point to build off of for such research.
These models are not intended to be commercially deployed. Additionally, they are not intended to be used to create propaganda or offensive imagery.

repo is here with training code, MIT License
https://github.com/openai/guided-diffusion"
ProtT5-XXL-BFD,5/4/2021,SOTA improvement,11000000000,3.7e+22,,5,,512,,43025.05718973151,Google TPU v3,BFD (Big Fantastic Dataset),Confident,,,,Open weights (unrestricted),Unreleased,,,513779.0240754994,Operation counting,"""For the per-residue predictions the transfer of the most informative embeddings (ProtT5) for the first time outperformed the state-of-the-art without using evolutionary information thereby bypassing expensive database searches.""",Table 2,"FLOP = 11B*2*(920k*512*4096) +  11B*4*(920k*512*4096), 920k steps using seq length 512 batch size 4096, ","First, T5-XL and T5-XXL were trained on BFD for 1.2M and 920k steps respectively (ProtT5-XL-BFD, ProtT5-XXL-BFD). In a second step, ProtT5-XL-BFD and ProtT5-XXL-BFD were fine-tuned on
UniRef50 for 991k and 343k steps respectively (ProtT5-XLU50, ProtT5-XXL-U50).","Table 1: 2122M proteins, 393B amino acids, 572 GB",,,,,"Licensed under the Academic Free License version 3.0

The ProtTrans project is a open source project supported by various partner companies and research institutions. We are committed to share all our pre-trained models and knowledge. 
https://github.com/agemagician/ProtTrans"
ProtT5-XXL,5/4/2021,SOTA improvement,11000000000,7.37e+22,4.07E+11,,,512,,85701.26256441118,Google TPU v3,"BFD (Big Fantastic Dataset),UniRef50",Confident,,,,Open weights (unrestricted),Unreleased,,TRUE,513779.0240754994,"Third-party estimation,Operation counting","""For the per-residue predictions the transfer of the most informative embeddings (ProtT5) for the first time outperformed the state-of-the-art
without using evolutionary information""","source: https://lair.lighton.ai/akronomicon/

archived: https://github.com/lightonai/akronomicon/tree/main/akrodb","7.37e22 from:
source: https://lair.lighton.ai/akronomicon/
archived: https://github.com/lightonai/akronomicon/blob/main/akrodb/Technical%20University%20of%20Munich/ProtT5-XXL.json

3.7E+22 from Table 9 https://www.biorxiv.org/content/10.1101/2023.07.05.547496v1

Manual calculation: forward passes on 512 * (4096*920k + 2048*343k) = 2.3T tokens
Backward passes on 15% of those, 2.3T * 0.15 = 343B tokens.
Total FLOPs: (2 * 11B * 2.3T) + (4 * 11B * 343B) = 6.57e22","First, T5-XL and T5-XXL were trained on BFD for 1.2M and 920k steps respectively (ProtT5-XL-BFD, ProtT5-XXL-BFD). In a second step, ProtT5-XL-BFD and ProtT5-XXL-BFD were fine-tuned on
UniRef50 for 991k and 343k steps respectively (ProtT5-XLU50, ProtT5-XXL-U50).
Table 1 and 2 give enough info to calculate epochs:
BFD: 512 (seq len) * 4096 (global batch) * 920k (steps) / 393B = 4.9 epochs
UniRef50: 512 (seq len) * 2048 (global batch) * 343k (steps) / 14B = 25.7 epochs","Table 2. ProtT5-XXL uses BFD100 and UniRef50, which sum to 407 billion amino acids.",,,,,"Licensed under the Academic Free License version 3.0

The ProtTrans project is a open source project supported by various partner companies and research institutions. We are committed to share all our pre-trained models and knowledge. 
https://github.com/agemagician/ProtTrans"
ProtBERT-BFD,5/4/2021,SOTA improvement,420000000,3.9e+22,8.9E+12,,,1024,,45350.73595674404,Google TPU v3,BFD (Big Fantastic Dataset),Confident,,,,Open weights (unrestricted),Unreleased,,,1027558.0481509988,Operation counting,"""For the per-residue predictions the transfer of the most informative embeddings (ProtT5) for the first time outperformed the state-of-the-art without using evolutionary information thereby bypassing expensive database searches.""",Table 2,"FLOP = 420M * 6 * (800k*512*32k + 200k*2048*6k) 
1M steps total split into two phases, (1) 800k steps, seq length 512 (batch size 32k) and (2) 200k steps, seq length 2048 (batch size 6k)
single TPU Pod V3-1024 (64 nodes and 1024 TPUs) info from paper and https://huggingface.co/Rostlab/prot_bert_bfd","ProtBert: BERT2 was trained using both UniRef100
and BFD-100 datasets (referred to as ProtBert and ProtBertBFD, respectively; Table 2)","""ProtBERT-BFD (420M parameters) saw around 27B proteins during pre-training"" 

Table 1: BFD has 2122M proteins, 393B amino acids, 572 GB
Suggests average amino acid length of 185

Implies 27B * 185 = 5T amino acids seen in training

However, Table 2 suggests number of tokens (amino acids) seen in training was:
(512*32768*800k) + (2048*6144*200k) = 15.9T amino acids in training

Geometric mean = 8.9T","figure 3 shows 19 hours per epoch, though this was on a different GPU setup than the one used for training.",,,,"Licensed under the Academic Free License version 3.0

The ProtTrans project is a open source project supported by various partner companies and research institutions. We are committed to share all our pre-trained models and knowledge. 
https://github.com/agemagician/ProtTrans"
ViT + DINO,4/29/2021,Highly cited,85000000,2.1e+20,,300,,,,380.2849049080349,NVIDIA V100,ImageNet,Confident,,,,Open weights (unrestricted),Open source,Open source,,,Hardware,,"85M, table 1","""Overall, training DINO with Vision Transformers
achieves 76.1 top-1 accuracy using two 8-GPU servers for 3
days""

GPU is V100

16 * 125 teraflops * 3 days * 0.4 utilization
= 2.1e20

However, this isn't the best result in the paper (which is 80.1% with ViT-B/8). 76.1% is the result from ViT-B/16 per Table 2, which may be 5x cheaper than ViT-B/8 based on Table 1?

upd:
 ""Table 8: Time and memory requirements. We show total running
time and peak memory per GPU (“mem.”) when running ViT-S/16
DINO models on two 8-GPU machines.""

2*8*125 teraflops*72.6h*3600*0.4=2.09088e+20","""We pretrain the models on the ImageNet dataset [60] without labels""",,,,,,"models and code including training: https://github.com/facebookresearch/dino

Apache-2.0 license
"
PLUG,4/19/2021,SOTA improvement,27000000000,3.5997696e+22,,,840,128,,108672.69136370042,NVIDIA A100,,,,,,Hosted access (no API),Unreleased,,,114191.06714895004,Hardware,Was a SOTA in CLUE 1.0 https://www.cluebenchmarks.com/classification10.html,,128 Nvidia A100 for 35 days,,,35 days,,,,https://nlp.aliyun.com/portal#/BigText_chinese
M6-T,3/5/2021,SOTA improvement,1.0027E+12,5.5e+21,1.9E+12,,,480,,13156.86123531961,NVIDIA Tesla V100 DGXS 32 GB,M6-Corpus,Likely,,,,Unreleased,Unreleased,,,267761.9673807506,Third-party estimation,"Improves on hardware SOTA for similar problems

Abstract: 
""We push the model
scale to over 1 trillion parameters and implement it on solely 480 NVIDIA V100-32GB GPUs, in comparison with the recent SOTAs [11; 6] on 2048 TPU cores.""",Table 5. Note model is sparse MoE with 960 experts; not all parameters are activated on the forward pass.,Estimate taken from https://www.governance.ai/research-paper/recent-trends-chinas-llm-landscape,M6-Corpus is a Chinese language multimodal dataset with 60.5B images and 111.8B tokens of text,60.5B images and 111.8B tokens of text,,,,,
Generative BST,3/5/2021,SOTA improvement,9431810048,1.449e+22,56800000000,1.76056338,,,,,,,Confident,,,,Open weights (unrestricted),,,,,Operation counting,"Abstract:
""Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.""",The largest model is a transformer with 9.4B parameters (Table 2),"""Both our 2.7B and 9.4B parameter models were trained with batches of approximately 500k label BPE tokens per batch [...] The 9.4B parameter model was trained [...] for a total of 200k SGD steps.""

Also note that the full dataset contains 56.8B label BPE tokens and 88.8B context tokens, so for each batch of 500k label tokens, there are likely 500k * 88.8B / 56.8B = 780k context tokens.

6 * 9.4318B * 200k * (500k + 780k) = 1.449e22","Section 6. Pre-training is done on Pushshift.io Reddit: ""Our final dataset contains 1.50B comments totaling 56.8B label BPE tokens and 88.8B context tokens.""

They finetune on a few smaller datasets: ConvAI2 (140k utterances), Empathetic Dialogues (50k utterances), Wizard of Wikipedia (194k utterances), Blended Skill Talk (76k utterances, generated by training language models on the previous three datasets and selecting the best outputs from each)","Section 6. Pre-training is done on Pushshift.io Reddit: ""Our final dataset contains 1.50B comments totaling 56.8B label BPE tokens and 88.8B context tokens.""
None of the fine-tuning datasets put a significant dent in the total dataset size.

Epochs: they do 200k steps, where each batch has 500k label tokens = 100B label tokens seen. 56.8B label tokens in pre-training dataset, so 1.76 epochs",,,,,
Meta Pseudo Labels,3/1/2021,SOTA improvement,480000000,4.79e+22,130000000,,264,1024,,53844.28059190435,Google TPU v3,"ImageNet,JFT-300M",,,,,Unreleased,Open source,Open source,TRUE,1028249.308,Hardware,,"Table 4
 480M","From communication with author:

22671 TPU days on specific hardware.

Which hardware did you use and in which configuration?
2048 cores of TPU v3.

Precision: Mixed. bfloat16 for activations, float32 for weights and optimizer slots.

2048 TPUv3 cores means 1024 TPUv3 chips, and the spec is 123e12 FLOP/second per chip with bfloat16 precision (Source: https://cloud.google.com/tpu/docs/system-architecture-tpu-vm)

So the compute estimate is:
1024 chips * 123e12 FLOP/second * 0.4 utilization * 11 days * 24 * 60 * 60 = 4.788191232e+22 FLOP",,"Section 4
Datasets. For this experiment, we use the entire ImageNet
training set as labeled data, and use the JFT dataset as unlabeled data. The JFT dataset has 300 million images, and
then is filtered down to 130 million images by Noisy Student
using confidence thresholds and up-sampling [77]. We use
the same 130 million images as Noisy Student","11 days from section 4:
""We train the model for 1 million steps in total,
which takes about 11 days for EfficientNet-L2 and 10 days
for EfficientNet-B6-Wide. ""

""Specifically, our training process runs on a cluster of 2,048
TPUv3 cores. ""
",,,,"Apache-2.0 license
https://github.com/google-research/google-research/blob/master/meta_pseudo_labels/README.md"
SRU++ Large,2/24/2021,SOTA improvement,234000000,1.1e+19,,36,,,,,,enwik8,,,,,Open weights (unrestricted),Open source,Open source,,,,"""our model achieves a state-of-the-art result on the ENWIK8 dataset using 1.6 days of training on an 8-GPU machine. """,,,,,,,,,"MIT license repo. says models available as package: https://github.com/asappresearch/sru

training: https://github.com/asappresearch/sru/blob/master/language_model/train_lm.py "
Rational DQN Average,2/18/2021,SOTA improvement,1683456,,,,,,,,,,,,,,,,,,,,,See figure 7,,,,,,,,
MSA Transformer,2/13/2021,SOTA improvement,100000000,5.49e+21,9.2976E+12,,,32,,13256.937301517895,NVIDIA Tesla V100 DGXS 32 GB,"UniRef50,UniRef30 (FKA UniClust30)",Likely,,,,Open weights (unrestricted),Unreleased,Open source,,17854.564630974684,Operation counting,"""The performance of the model surpasses current state-of-the-art unsupervised structure learning methods by a wide margin, with far greater parameter efficiency than prior state-of-the-art protein language models""","""We train an MSA Transformer model with 100M parameters..."" ","Based on: https://docs.google.com/spreadsheets/d/1enan21dFx03TkwufHgOwTVNBtuYlqNY9uurjIK6YS-8/edit#gid=0

Number of steps 4.5e5, batch size (tokens) 6.1e7, parameters 1e8

Calculation = 4e8 FLOP/bp * 4.5e5 bp + 2e8 FLOP/fp * 2.75e13 fp

Batch size: 512
Seq length: 100 * 1192 tokens
All models are trained on 32 V100 GPUs for 100k updates. The four models with best contact precision are then further trained to 150k updates. Finally, the best model at 150k updates is trained to 450k updates.

450k * 512 * 100 * 1192 * 100M * 6 = 1.65e22","""Models are trained on a dataset of 26 million MSAs. An MSA is generated for each UniRef50 sequence by searching UniClost30 with HHblits.""","""We train an MSA Transformer model with 100M parameters on a large dataset (4.3 TB) of 26 million MSAs, with an average of 1192 sequences per MSA.""
Average sequence is ~300 amino acids/tokens long.
26 million * 1192 * 300 = 9.3T tokens",,,,,"MIT: https://github.com/facebookresearch/esm

looks like no training code"
top-down frozen classifier,2/9/2021,SOTA improvement,,,,,,,,,,WSJ,Unknown,,,,Unreleased,Unreleased,,,,,"""Table 2 demonstrates that, to the best of our knowledge, top-down training results in state-of-the art character error rates for LSTM-based endto-end models on WSJ""",,,,,,,,,
DeiT-B,1/15/2021,Highly cited,86000000,7.884e+19,1280000,300,53,,,,NVIDIA V100,ImageNet,Confident,,,,Open weights (unrestricted),Open source,Open source,,,Hardware,,(DeiT-B),"2*86000000 parameters*3*1280000 training examples*300 epochs=1.98144e+17 FLOPs

compute [FLOP] = training time [s] × # of GPUs/TPUs × peak FLOP/s × utilization rate

(53h+20h)*3600*8*125000000000000 peak FLOP/s*0.3=7.884e+19

",,,"A typical training of 300 epochs takes 37 hours with 2 nodes or 53 hours on a single node for the DeiT-B.
In this paper, we train a vision transformer on a single 8-GPU node in two
to three days (53 hours of pre-training, and optionally 20 hours of fine-tuning) that is competitive with convnets having a similar number of parameters and efficiency. It uses Imagenet as the sole training set.",,,,"models, train, inference: https://github.com/facebookresearch/deit/blob/main/README_deit.md 

Apache-2.0 license"
Switch,1/11/2021,"Highly cited,SOTA improvement",1.571E+12,8.22e+22,5.76E+11,,648,1024,0.279674797,139663.55942731188,Google TPU v3,C4,,,,,Open weights (unrestricted),Unreleased,Open source,TRUE,1028782.017,Third-party estimation,""" On ANLI (Nie et al., 2019), Switch XXL improves over the prior state-of-the-art to get a 65.7 accuracy versus the prior best of 49.4 (Yang et al., 2020)... Finally, we also conduct an early examination of the model’s knowledge with three closed-book knowledge-based tasks: Natural
Questions, WebQuestions and TriviaQA, without additional pre-training using Salient Span Masking (Guu et al., 2020). In all three cases, we observe improvements over the prior stateof-the-art T5-XXL model (without SSM)""","""Combining expert, model and data parallelism, we design two large Switch Transformer models, one with 395 billion and 1.6 trillion parameters""
Table 9 gives more precise count of 1571B parameters","Table 4
https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf",,"""In our protocol we pre-train with 2^20 (1,048,576) tokens
per batch for 550k steps amounting to 576B total tokens.""

1 token ~ 0.75 words","see table 4 in https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf
",,,,"Apache 2 for weights: https://huggingface.co/google/switch-c-2048 

paper links to this repo but not clear that the training hyperparams for Switch are here:
https://github.com/google-research/t5x

"
BigSSL,1/10/2021,SOTA improvement,8000000000,,42626880000,,,,,,,,,,,,,,,,,,"""In particular, on an ASR task with 34k hours of labeled data, by fine-tuning an 8 billion parameter pre-trained Conformer model we can match state-of-the-art (SoTA) performance with only 3% of the training data and significantly improve SoTA with the full training set""","""... we study the utility of large models, with the parameter count ranging from 600M to 8B...""",,,"Sum all values in Table VII, and add 34k for English VAD, and 926k for English Youtube = 3116k hours

Note this involves significant self-training: ""Noisy student training (NST) [23], [41] is a self-training
method where a teacher model generates pseudo-labels for a
large unlabeled dataset, which is in turn used to train a student
model with augmentation.""

1 hour ~ 13,680 words
13680 * 3116000 = 42626880000",,,,,
DALL-E,1/5/2021,"Significant use,Highly cited",12000000000,4.7e+22,250000000,1.76,,1024,,118437.35864214256,NVIDIA Tesla V100 DGXS 16 GB,DALL-E,,,,,API access,Unreleased,,TRUE,571581.9187928778,Third-party estimation,,DALL·E is a 12-billion parameter version of GPT-3 trained to generate images from text descriptions,"source: https://lair.lighton.ai/akronomicon/

archived: https://github.com/lightonai/akronomicon/tree/main/akrodb","To scale up to 12-billion parameters, we created a dataset of
a similar scale to JFT-300M (Sun et al., 2017) by collecting
250 million text-images pairs from the internet. This dataset
does not include MS-COCO, but does include Conceptual
Captions and a filtered subset of YFCC100M (Thomee et al.,
2016). As MS-COCO was created from the latter, our training data includes a fraction of the MS-COCO validation images (but none of the captions).","""To scale up to 12-billion parameters, we created a dataset of a similar scale to JFT-300M (Sun et al., 2017) by collecting
250 million text-images pairs from the internet. ""

number of epochs: 
1024 batch size * 430,000 updates / 250,000,000 = 1.76","""We trained the model using 1024, 16 GB NVIDIA V100 GPUs and a total batch size of 1024, for a total of 430,000 updates.
At the start of training, we use a linear schedule to ramp up the step size to 4.5 · 10−4 over 5000 updates, and halved the
step size each time the training loss appeared to plateau. We did this a total of five times, ending training with a final step
size that was 32 times smaller than the initial one. """,,,,
CLIP (ViT L/14@336px),1/5/2021,"Highly cited,SOTA improvement",370000000,1.05e+22,400000000,,288,256,,24638.518413409514,NVIDIA V100,Unspecified unreleased,,,,,Open weights (unrestricted),Unreleased,Open source,,171474.57563786334,Third-party estimation,,"Image encoder
Vision Transformer
Table 1 in https://arxiv.org/pdf/2010.11929.pdf
Authors fine-tuned ViT L/14 at additional 336px resolution, hence the @336 (See ViT)
307M params

Text encoder
~Transformer (from paper)
63M params",https://docs.google.com/document/d/156miAJkFN9DDX06C3s03UDsretCtymCKiGDddLBCgQE/edit?usp=sharing,"Custom image-text pairs from the internet

we constructed a new dataset of 400 million (image,
text) pairs collected form a variety of publicly available
sources on the Internet. To attempt to cover as broad a set
of visual concepts as possible, we search for (image, text)
pairs as part of the construction process whose text includes
one of a set of 500,000 queries",,"“The largest ResNet model, RN50x64, took 18 days to train on 592 V100 GPUs while the largest Vision Transformer took 12 days on 256 V100 GPUs”","https://www.kdnuggets.com/2021/03/beginners-guide-clip-model.html
",,,"MIT License

https://github.com/OpenAI/CLIP"
CLIP (ResNet-50),1/5/2021,"Highly cited,SOTA improvement",88600000,,400000000,,,,,,,,,,,,,,,,,,,"Image encoder
~ResNet-50 (from paper)
25.6M params

Text encoder
~Transformer (from paper)
63M params",,Custom image-text pairs from the internet,,,,,,
ERNIE-Doc (247M),12/31/2020,SOTA improvement,247000000.00000003,2.91e+19,,190.88,,,,,,WikiText-103,,,,,Open weights (unrestricted),Unreleased,,,,,"""ERNIE-DOC improved the state-of-the-art language modeling result of perplexity to 16.8 on WikiText103""",,,,,,,,,"weights available, not sure there's training code for WT-103: https://github.com/PaddlePaddle/ERNIE/tree/repro/ernie-doc"
CT-MoS (WT2),12/25/2020,SOTA improvement,45000000,5.62e+17,,1000,,,,,,WikiText-2,,,,,Unreleased,Unreleased,,,,,"""Experimental results confirm that the
proposed method significantly improves state-of-the-art language models, achieving a perplexity of 55.31 and 62.89 on
the test set of Penn Treebank and WikiText-2""",,,,,,,,,
DensePhrases,12/23/2020,SOTA improvement,,2.09952e+18,58000000,4,20,8,,,NVIDIA TITAN Xp,"SQuAD,NQ (Natural Questions)",Speculative,,,,Open weights (unrestricted),Open source,Open source,,4466.099780544917,Hardware,"from abstract ""our model DensePhrases improves over previous phrase retrieval models by 15%-25% absolute accuracy and matches the performance of state-of-the-art retriever-reader models. """,may be possible to estimate from batch size (8) and maximum memory of GPUs (96GB)," flops = (8) * (1215 * 10**10) * (20 * 3600) * 3 // 10 = 2099520000000000000
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)

model of GPU from appendix B (Titan Xp)
number of GPUs from table in appendix A
flops from https://www.techpowerup.com/gpu-specs/titan-xp.c2948","from appendix D ""The number of generated questions is 327,302 and 1,126,354 for SQuAD and Natural Questions, respectively.""","from appendix D ""The number of generated questions is 327,302 and 1,126,354 for SQuAD and Natural Questions, respectively.""
assuming 40 words per question we get around ~ 58M",appendix A row 3,,,,Apache 2.0: https://github.com/princeton-nlp/DensePhrases
VQGAN + CLIP,12/17/2020,"Highly cited,SOTA improvement",,,,,,,,,,,Unknown,,,,,,,,,,,,,,I'm confused - I guess they pretrained on several different datasets? I think the model is also able to do zero-shot learning,,,,,
ESM1b,12/15/2020,"Highly cited,SOTA improvement",652400000,5.1e+21,,56,,128,,924.3248891034536,NVIDIA V100,UniRef50,Confident,,,,Open weights (unrestricted),Unreleased,Open source,,85756.40336300137,"Hardware,Operation counting","""We apply the representations to a range of prediction tasks and find that they improve state-of-art features across the applications.""",See Table 9,"Information: 
128 NVIDIA V100 GPUs [Pre-training details]
8.5 hours on 64 GPUs per epoch, 56 epochs of UR50/S [Appendix B, ESM-1b Hyperparameter optimization, Experimental set-up]
128 NVIDIA V100 GPU, assuming  V100 PCIe half precision 130 TFLOPS and 0.3 utilization rate

Estimate: (8.5*56*3600) s * 1.3e14 FLOP/s * 0.3 *64 = 4.277e21 FLOP

6NC method:
UR50/S has 27.1M sequences, which are capped at 1024 amino acids. 
27.1M * 1024 * 56 * 652.4M * 6 = 6.08e21 FLOP

Geometric mean: 5.1e21","In our experiments, we explore datasets withup to 250 million sequences of the UniParc database (33), whichhas 86 billion amino acids.",,,,,,MIT: https://github.com/facebookresearch/esm
CPM-Large,12/1/2020,SOTA improvement,2600000000,1.8e+21,16700000000,,336,64,,7340.099044719796,NVIDIA V100,Unspecified unreleased,,,,,Open weights (unrestricted),Unreleased,,,42884.58641355919,Third-party estimation,"""CPM outperforms CDial-GPT with a large margin in the few-shot experiment, showing the generalization ability of our model.""","""To the best of our knowledge, CPM, with 2.6 billion parameters and 100GB Chinese training data, is the largest Chinese pre-trained language mode""","source: https://lair.lighton.ai/akronomicon/

archived: https://github.com/lightonai/akronomicon/tree/main/akrodb","we construct a new sub-word vocabulary, containing both words and characters.","""language model, with 2.6 billion parameters and 100GB Chinese training data.""

We use the conversion factor 1GB ~ 167M words","""It takes two weeks to train our largest model using 64 NVIDIA V100.""","https://towardsdatascience.com/the-future-of-ai-is-decentralized-848d4931a29a#:~:text=Training%20GPT%2D3%20reportedly%20cost,a%20single%20training%20run%C2%B9.",,,"MIT license

https://github.com/TsinghuaAI/CPM-1-Generate"
AlphaFold 2,11/30/2020,"Historical significance,Highly cited",93000000,2.99e+21,530000,,264,,,3841.772612266474,Google TPU v3,"PDB (Protein Data Bank),UniRef30 (FKA UniClust30),UniRef90,MGnify,BFD (Big Fantastic Dataset),UniProtKB",Likely,,,,Open weights (unrestricted),Unreleased,Open source,,,Hardware,"""Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known"" [Abstract]

>17790 citations","https://arxiv.org/abs/2207.05477 reimplements AlphaFold 2 in a more efficient way, and states there are 93M parameters in the original version (Table 1)","123 teraFLOPS / TPU v3 chip * 128 cores * (1 chip / 2 cores) * 11 days * 40% utilization = 2.99e21 FLOP
https://www.wolframalpha.com/input?i=123+teraFLOPS+*+128+*+11+days+*+0.4

""Training regimen"" section: 
""We train the model on Tensor Processing Unit (TPU) v3 with a batch size of 1 per TPU core, hence the model uses 128 TPU v3 cores. [...] The initial training stage takes approximately 1 week, and the fine-tuning stage takes approximately 4 additional days.""","""Inputs and data sources"" section:
""The following versions of public datasets were used in this study. Our models were trained on a copy of the PDB downloaded on 28 August 2019. For finding template structures at prediction time, we used a copy of the PDB downloaded on 14 May 2020, and the PDB70 clustering database downloaded on 13 May 2020. For MSA lookup at both training and prediction time, we used Uniref90 v.2020_01, BFD, Uniclust30 v.2018_08 and MGnify v.2018_12. For sequence distillation, we used Uniclust30 v.2018_08 to construct a distillation structure dataset. Full details are provided in Supplementary Methods 1.2.""

AlphaFold needs multiple genetic (sequence) databases to run:

BFD,
MGnify,
PDB70,
PDB (structures in the mmCIF format),
PDB seqres – only for AlphaFold-Multimer,
UniRef30 (FKA UniClust30),
UniProt – only for AlphaFold-Multimer,
UniRef90","3 different types of input data to the network:
(1) Amino acid sequence
(2) Multiple sequence alignments (MSA) to sequences from evolutionarily related proteins
(3) Template structures (3D atom coordinates of homologous structures), where available

Training data is processed into the following two datasets that are sampled with different probabilities. 
Supplementary Material, Section 1.2.4. Training data:
""With 75% probability a training example comes from the self-distillation set (see subsection 1.3) and with 25% probability the training example is a known structure from the Protein Data Bank""

Supplementary Material, Section 1.3 Self-distillation dataset:
""This gives a final dataset of 355,993 sequences"". An initial model was used to predict structures for these sequences.

PDB dataset size in 2020: https://www.rcsb.org/stats/growth/growth-released-structures
172788

Therefore, estimate for number of protein structures available for training (for which amino acid sequence, MSA and homologue template info is also available as input to network): 528781 [~530k]",7 days pretrain and 4 days finetune,,,,"While the AlphaFold code is licensed under the Apache 2.0 License, the AlphaFold parameters and CASP15 prediction data are made available under the terms of the CC BY 4.0 license

code in this repo is inference code:
https://github.com/google-deepmind/alphafold"
KEPLER,11/23/2020,SOTA improvement,125000000,1.24e+20,3300000000,,,,,,,"Wikipedia,BookCorpus (BooksCorpus, Toronto Book Corpus),Wikidata5M",,,,,Unreleased,Open source,Open source,,,Hardware,"""Experimental results show that KEPLER achieves state-of-the-art performances
on various NLP tasks""",,"From author communication

""About 128 GPU-days using Nvidia V100 (16GB). ""

precision: float16

V100 GPU for float16: 28000000000000 (2.8E+13)

0.4 * 28TFLOP/s * 128 GPU-days * 24h/day * 3600s/h
= 1.24E+20


","From author communication

    For the language modeling objective, we use Wikipedia+BookCorpus datasets (about 13GB).    For the knowledge embedding objective, we use Wikidata5m (about 1GB).","For BookCorpus + English Wikipedia: 800M + 2500M

For Wikidata5M: 20614279
See table 1. Contains ""entities"", ""relations"", and ""triplets""",,,,,"MIT License, includes train code https://github.com/THU-KEG/KEPLER"
SimCLRv2,10/26/2020,Highly cited,795000000,,1280000,,,,,,,,,,,,,,,,,,,"From author communication

We trained different model sizes (from 24M to 795M), and they're summarized in Table 1 of the paper (https://arxiv.org/pdf/2006.10029.pdf).",,,,,,,,
wave2vec 2.0 LARGE,10/22/2020,"Highly cited,SOTA improvement",317000000,1.9e+21,727776000,,,,,5021.241075362514,NVIDIA Tesla V100 DGXS 32 GB,"LibriSpeech,LibriLight",,,,,Open weights (unrestricted),Open source,Open source,,,Hardware,"Arguably an ""important"" paper? 

Abstract: 
""We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler.""","Section 5.1:
""We consider two model sizes: BASE (95m parameters) and LARGE (317m parameters)
","From surveying the authors:

We trained the base model on 64 V100 GPUs for 400k updates. This takes about 3 days to complete. The large model is trained on 128 V100 GPUs for 1 million updates, and this takes about 7 days to complete.

V100 GPU peak: 125TFLOP/s (https://www.nvidia.com/en-gb/data-center/tesla-v100/)
Assume 40% utilization based on default for non-Language domain (https://epoch.ai/blog/estimating-training-compute)

64 GPUs * 40% * 125TFLOP/s * 7 days * 24h/day * 3600s/h
~= 1.9E+21 FLOP",,"pg 4, section 4.1

""As unlabeled data we consider the Librispeech corpus [40] without transcriptions containing 960 hours of audio (LS-960) or the audio data from LibriVox (LV-60k). For the latter we follow the preprocessing of [27] resulting in 53.2k hours of audio.""

53.2k h * 13,680 words/h = 727776000 words",,,,,"https://github.com/facebookresearch/fairseq/blob/1bba712622b8ae4efb3eb793a8a40da386fe11d0/examples/wav2vec/README.md

fairseq(-py) is MIT-licensed. The license applies to the pre-trained models as well. Repo contains weights and pretrain and finetune code"
ViT-Huge/14,10/22/2020,Highly cited,632000000,4.262e+21,1280000,14,,,0.32,6724.023241261178,Google TPU v3,"ImageNet-1k,ImageNet21k,JFT-300M",Confident,,,,Open weights (unrestricted),Open source,Open source,,,Hardware,,Table 1 https://arxiv.org/pdf/2010.11929.pdf,"Table 6: 4.262e21 FLOPs

Agrees with Table 2 (2.5k TPUv3-core days), if MFU is around 0.32. 

2500 * 24 * 3600 * (0.5 * 1.23e14) * 0.32 = 4.25e21","To explore model scalability, we use the ILSVRC-2012 ImageNet dataset with 1k classes and 1.3M images (we refer to it as ImageNet in what follows), its superset ImageNet-21k with 21k classes and 14M images (Deng et al., 2009), and JFT (Sun et al., 2017) with 18k classes and 303M high-resolution images. ",,,,,,"Apache-2.0 license
https://github.com/google-research/vision_transformer"
ViT-Base/32,10/22/2020,Highly cited,86000000,,,7,,,,,,JFT-300M,,,,,,,,,,,,Table 1 https://arxiv.org/pdf/2010.11929.pdf,,,,,,,,
German ELECTRA Large,10/21/2020,SOTA improvement,335000000,1.42829568e+21,36383733333.333336,,168,64,,2392.2883349806307,Google TPU v3,"Wikipedia,OPUS,OSCAR,OpenLegalData",Confident,,,,Open weights (unrestricted),,,,64355.01603597277,"Hardware,Operation counting",'we were able to attain SoTA performance across a set of document classification and named entity recognition (NER) tasks for both models of base and large size.',335M from Table 5,"flops = (64) * (123* 10**12) * (7 * 24 * 3600) * (0.3) = 1.4e21
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)

'large models were trained on pods of 16 TPUs v3 (128 cores).' - from section 4.1 it was trained for 7 days from Table 2

Agrees with 6CN:
Tokens seen: 512 (seq len) * 1024 (batch size) * 1 million (steps) = 5.24e11
FLOPs: 6 * 335M * 5.24e11 = 1.05e21",Table 1 in the paper,"163.4GB from Table 1 in the paper
assuming 167M words per GB (German Language) we have 163.4 * 167M * 4/3 tokens per word = 36,383,733,333",7 days from Table 2,,,,MIT: https://huggingface.co/deepset/gelectra-large
GBERT-Large,10/21/2020,SOTA improvement,335000000,2.2444646e+21,27287800000,,264,64,,3771.133385,Google TPU v3,"Wikipedia,OPUS,OSCAR,OpenLegalData",Likely,,,2048,Open weights (unrestricted),Unreleased,,,64355.01603597277,Hardware,'we were able to attain SoTA performance across a set of document classification and named entity recognition (NER) tasks for both models of base and large size.',335M from Table 5,"flops = (64) * (123* 10**12) * (11 * 24 * 3600) * (0.3) = 2.24e21
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)

'large models were trained on pods of 16 TPUs v3 (128 cores).' - from section 4.1it was trained for 11 days from Table 2",Table 1 in the paper,"163.4GB from Table 1 in the paper
assuming 167M words per GB (German Language) we have 163.4 * 167M = 27287800000.0",11 days from Table 2,,,,MIT: https://huggingface.co/deepset/gbert-large
mT5-XXL,10/20/2020,"Highly cited,SOTA improvement",13000000000,8.2e+22,1E+12,1,,,,,,mC4,Confident,,,1048576,Open weights (unrestricted),Open source,Open source,TRUE,,Operation counting,"""Table 2 presents our main results, with perlanguage breakdowns for each task given in Appendix B. Our largest model mT5-XXL exceeds state-of-the-art on all classification and QA tasks and is near SOTA on NER (69.2 vs. 70.1).""",13 billion,"""We pre-train our mT5 model variants for 1 million steps on batches of 1024 length-1024 input sequences, corresponding to roughly 1 trillion input tokens total.""

1 million steps * 1024 batchsize * 1024 length * 13 billion params * 6 = 8.2e22

Ignores fine-tuning compute; this is likely a small fraction of pre-training compute.","""The C4 dataset was explicitly designed to be English only: any page that was not given a probability of at least 99% of being English by langdetect2 was discarded. In contrast, for mC4 we use cld33 to identify over 100 languages.
Since some of these languages are relatively scarce on the internet, we make use of all of the 71 monthly web scrapes released so far by Common Crawl. This is dramatically more source data than was used for C4, for which the April 2019 web scrape alone was enough to provide plenty of English-language data.""","The model was trained on a subset of 1 trillion tokens.
Full mC4 corpus has data ""totaling 6.6B pages and 6.3T tokens""
Distribution by language is in Appendix A.",,,,"""We pre-train our mT5 model variants for 1 million steps on batches of 1024 length-1024 input sequences, corresponding to roughly 1 trillion input tokens total."""," Apache 2.0 license
training code: https://github.com/google-research/multilingual-t5"
Conformer + Wav2vec 2.0 + Noisy Student,10/20/2020,SOTA improvement,1000000000,7.6e+21,,,168,256,,9449.541661137231,Google TPU v3,LibriLight,Confident,,,,Unreleased,Unreleased,,,257422.8158198682,Hardware,"""By doing so, we are able to achieve
word-error-rates (WERs) 1.4%/2.6% on the LibriSpeech test/test-other sets against
the current state-of-the-art WERs 1.7%/3.3%.""",1B for XXL model,"""We train with global batch size 2048 on 256/512 Google TPU V3 cores for 3-4 days for the XL/XXL models respectively...
We fine-tune the pre-trained checkpoints (400k steps) with global batch
size 1024/512 on 256/512 Google TPU v3 cores for 1-3 days for the XL/XXL models""

TPU v3 chips are 123 teraflop/s. 2 chips per core

512 cores * 7 days * 24 * 3600 * 123 tflops * (1 chip/2 cores) * 0.4 (assumed utilization) = 7.6e21","""We pre-train the Conformer encoder akin to wav2vec 2.0 pre-training [6] with 60k hours of unlabeled audio from the ""unlab-60k"" subset of Libri-Light. Unlike in the original work which takes raw waveforms as input, we use log-mel spectrograms... 

The 960h of transcribed audio of the LibriSpeech dataset is used as the supervised data""",,7 days,,,,
LUKE,10/2/2020,SOTA improvement,483000000,1.8144e+22,3500000000,119.46,720,16,,4186.383565732907,NVIDIA V100,Wikipedia,Likely,RoBERTa Large,,2048,Open weights (unrestricted),Open source,Open source,,10728.016680946608,Hardware,"from abstract ""In particular, it obtains state-of-the-art results on five well-known datasets: Open Entity (entity typing), TACRED (relation classification), CoNLL-2003 (named entity recognition), ReCoRD (cloze-style question answering), and SQuAD 1.1 (extractive question answering).""","""The total number of parameters is approximately 483 M, consisting of 355 M in RoBERTa and 128 M in our entity embeddings""","Uses RoBERTa Large as a base model, which used 1.66e22 FLOPs in training.

LUKE's additional training was:
(16) * (1.25e14) * (30 * 24 * 3600) * (0.3) = 1.5552e21
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)

from appendix A: ""Werun the pretraining on NVIDIA’s PyTorch Docker
container 19.02 hosted on a server with two Intel Xeon Platinum 8168 CPUs and 16 NVIDIA Tesla V100 GPUs. The training takes approximately 30 days.""

Assuming 16 bit tensor core computations, 1.25e14 FLOP/s per V100

Total: 1.65888e22 + 1.5552e21 = 1.8144e22","""As input corpus for pretraining, we use the December 2018 version of Wikipedia, comprising approximately 3.5 billion words and 11 million entity annotations. ""","""As input corpus for pretraining, we use the December 2018 version of Wikipedia, comprising approximately 3.5 billion words and 11 million entity annotations. """,see compute notes,,"LUKE's additional training was:
(16) * (1.25e14) * (30 * 24 * 3600) * (0.3) = 1.5552e21
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)

from appendix A: ""Werun the pretraining on NVIDIA’s PyTorch Docker
container 19.02 hosted on a server with two Intel Xeon Platinum 8168 CPUs and 16 NVIDIA Tesla V100 GPUs. The training takes approximately 30 days.""

Assuming 16 bit tensor core computations, 1.25e14 FLOP/s per V100LUKE's additional training was:LUKE's additional training was:
(16) * (1.25e14) * (30 * 24 * 3600) * (0.3) = 1.5552e21(16) * (1.25e14) * (30 * 24 * 3600) * (0.3) = 1.5552e21
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)

from appendix A: ""Werun the pretraining on NVIDIA’s PyTorch Dockerfrom appendix A: ""Werun the pretraining on NVIDIA’s PyTorch Docker
container 19.02 hosted on a server with two Intel Xeon Platinum 8168 CPUs and 16 NVIDIA Tesla V100 GPUs. The training takes approximately 30 days.""container 19.02 hosted on a server with two Intel Xeon Platinum 8168 CPUs and 16 NVIDIA Tesla V100 GPUs. The training takes approximately 30 days.""

Assuming 16 bit tensor core computations, 1.25e14 FLOP/s per V100Assuming 16 bit tensor core computations, 1.25e14 FLOP/s per V100",table in appendix A,"apache 2.0: https://github.com/studio-ousia/luke?tab=readme-ov-file

data is wikimedia, which has a commercial license: https://dumps.wikimedia.org/legal.html

pretraining: https://github.com/studio-ousia/luke/blob/master/pretraining.md "
ProBERTa,9/1/2020,SOTA improvement,44000000,9.72e+18,58320000,,18,4,,26.206992435694065,NVIDIA V100,UniProtKB/Swiss-Prot,Confident,,,,,,,,2682.896227030356,Hardware,"""Furthermore, we used embeddings from PRoBERTa for a fundamentally different problem, PPI prediction, using two different
datasets generated from the HIPPIE database and found that with
sufficient data, it substantially outperforms the current state-of-theart method in the conservative scenario.""","""In total, our model has approximately 44M trainable parameters.""","""we pre-train PRoBERTa on 4 NVIDIA V100 GPUs in 18 hours""
4 * 125 tFLOP/s * 18 * 3600 * 0.3 (assumed utilization) = 9.72e18","""Pre-training data: We use UniProtKB/Swiss-Prot (450K unique sequences with a mean tokenized length of 129.6 tokens), a collection of experimentally annotated and reviewed amino acid sequences""

Fine tuning uses a subset of 313,214 sequences which have annotated labels.","450k sequences * 129.6 tokens per sequence = 58,320,000 tokens",,,,,
ERNIE-GEN (large),8/6/2020,SOTA improvement,340000000,2e+20,86000000000,,,,,,,"CC-News,BookCorpus (BooksCorpus, Toronto Book Corpus),WebText2,Wikipedia,C4",Speculative,,,,Open weights (non-commercial),Open (non-commercial),Open access (non-commercial),,,Operation counting,"""Empirically, ERNIE-GEN is particularly effective and
achieves state-of-the-art results on a range of NLG tasks
including abstractive summarization (Gigaword and CNN/DailyMail), question generation (SQuAD), dialogue response generation (Persona-Chat) and generative question answering (CoQA)""","""We train a base model ERNIEGENBASE (L=12, H=768, A=12, Total Parameters=110M)1
and a large model ERNIE-GENLARGE (L=24, H=1024,
A=16, Total Parameters=340M) with parameters initialized
by BERTBASE and BERTLARGE respectively""","430GB text for 1 epoch

approx 430 * 200 million words = 86B words, or 100B tokens per https://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.ieihc08p8dn0

6 * 340 million params * 100 billion tokens ~= 2e20","""Recent works for pre-training verify that larger scaled pretraining corpora can improve the performances on downstream tasks. We pre-train ERNIE-GENLARGE model on
the 430GB text corpora with 1 epoch and 1M training steps.
Our 430GB text corpora is extracted from the corpus used by
RoBERTa [Liu et al., 2019], T5 [Raffel et al., 2019] and ALBERT [Lan et al., 2020]. We fine-tune ERNIE-GENLARGE
on two abstractive summarization datasets including Gigaword and CNN/Daily Mail, the evaluation results are reported
in Table 9""

RoBERTa and T5 datasets are CC-News, BookCorpus, Wikipedia, WebText2, and C4","approx 430 * 200 million words = ~86B words, per https://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.ieihc08p8dn0",,,,,"https://github.com/PaddlePaddle/ERNIE/tree/repro/ernie-gen

code/weights with unclear license"
DeLighT,8/3/2020,SOTA improvement,99000000,2.4e+19,,62.14,,,,,,WikiText-103,,,,,Unreleased,Open source,Open source,,,,"""Comparison with state-of-the-art methods on machine translation corpora. DeLighT delivers
similar or better performance than state-of-the-art models with fewer parameters.""",,,,,,,,,"MIT, training and evaluation for WT103: https://github.com/sacmehta/delight/blob/master/readme_files/lm/wikitext103.md "
EfficientDet,7/27/2020,"Highly cited,SOTA improvement",77000000,,,600,,,,,,COCO 2017,,,,,Open weights (unrestricted),Open source,Open source,,,,"""EfficientDet-D7 achieves stateof-the-art 55.1 AP on COCO test-dev with 77M parameters and 410B FLOPs""","""EfficientDet-D7 achieves stateof-the-art 55.1 AP on COCO test-dev with 77M parameters and 410B FLOPs""",,,,,,,,"Repo is Apache 2.0:
https://github.com/google/automl/tree/master/efficientdet"
Hopfield Networks (2020),7/16/2020,SOTA improvement,,,,,,,,,,"BACE,SIDER",Unknown,,,,Open weights (unrestricted),Unreleased,Open source,,,,"""Hopfield layers yielded a new state-ofthe-art when compared to different machine learning methods. Finally, Hopfield
layers achieved state-of-the-art on two drug design datasets""",,,"""We test the Hopfield layer HopfieldLayer, on four drug
design datasets. These datasets represent four main areas of modeling tasks in drug design, concretely
to develop accurate models for predicting a) new anti-virals (HIV) by the Drug Therapeutics Program
(DTP) AIDS Antiviral Screen, b) new protein inhibitors, concretely human β-secretase (BACE) inhibitors by Subramanian et al. (2016), c) metabolic effects as blood-brain barrier permeability (BBBP)
(Martins et al., 2012) and d) side effects of a chemical compound from the Side Effect Resource
(SIDER) Kuhn et al. (2016). """,,,,,,"copyleft-like license, derivative works must retain this license. code here:
https://github.com/ml-jku/hopfield-layers/blob/master/LICENSE"
SemExp,7/2/2020,SOTA improvement,,,,,,,,,,"Gibson,Matterport3D (MP3D)",Unknown,,,,Open weights (unrestricted),Open source,Open source,,,,"""Our method achieves state-of-the-art performance on the object goal navigation task and won the CVPR2020 Habitat ObjectNav challenge""",,,"""We use the Gibson [46] and Matterport3D (MP3D) [6] datasets""","""Our training and test set consists of a total of 86 scenes (25 Gibson tiny and 61 MP3D) and 16 scenes (5 Gibson tiny and 11 MP3D), respectively""",,,,,MIT code/weights: https://github.com/devendrachaplot/Object-Goal-Navigation
GShard (dense),6/30/2020,SOTA improvement,2300000000,4.765e+22,346666666666.6667,,1008,1024,,256224.76861369325,Google TPU v3,,Confident,,,4000000,Unreleased,Open source,Unreleased,TRUE,1030932.0891586556,"Operation counting,Hardware","""such a giant model can efficiently be trained on 2048 TPU v3 accelerators in 4 days to achieve far superior quality for translation from 100 languages to English compared to the prior art""","""Our best quality dense single Transformer model (2.3B parameters) achieving ∆BLEU of 6.1, was trained with GPipe [15] on 2048 TPU v3 cores for 6 weeks or total of 235.5 TPU v3 core-years.""","Trained for a total of 235.5 TPU v3 core-years.
Hardware estimate: 235.5 * 365.25 * 24 * 3600 * (1.23e14 / 2) * 0.3 = 1.371e23

Footnote 10 indicates 300k steps and 4M tokens/step -> 1.2T tokens
Arithmetic estimate: 6 * 2.3B * 1.2T = 1.656e22 FLOPs

Geometric mean: sqrt(1.371e23 * 1.656e22) = 4.765e22",,"""We focus on improving the translation quality (measured in terms of BLEU score [48]) from all 100 languages to English. This resulted in approximately 13 billion training examples to be used for model training""

Each example is a sentence pair. Assuming 20 words per sentence and 4/3 tokens per word, that is 13*20*4/3 billion tokens",6 weeks = 1008 hours,,,"Table 3, bolded row is best model","training code is open, Apache: https://github.com/tensorflow/lingvo/tree/master/lingvo/tasks/lm"
iGPT-XL,6/17/2020,Highly cited,6801000000,3.3e+22,1229920.32,,,,,98082.33822642289,NVIDIA Tesla V100 DGXS 32 GB,ILSVRC 2012 subset of ImageNet,,,,,Open weights (unrestricted),Open source,Open source,TRUE,,Third-party estimation,,source: https://openai.com/blog/image-gpt/#rfref53,"Taken from here
https://www.lesswrong.com/posts/wfpdejMWog4vEDLDg/ai-and-compute-trend-isn-t-predictive-of-what-is-happening",,"""We use the ImageNet ILSVRC 2012 training dataset, splitting off 4% as our experimental validation set and report results on the ILSVRC 2012 validation set as our test set.""

https://image-net.org/challenges/LSVRC/2012/

""The goal of this competition is to estimate the content of photographs for the purpose of retrieval and automatic annotation using a subset of the large hand-labeled ImageNet dataset (10,000,000 labeled images depicting 10,000+ object categories) as training.""
",,,,,"Modified MIT, code and weights:

https://github.com/openai/image-gpt?tab=License-1-ov-file#readme

train code: https://github.com/openai/image-gpt/blob/master/src/run.py "
iGPT-L,6/17/2020,Highly cited,1362000000,8.91e+21,1229920.32,,,,,30093.444683107013,NVIDIA Tesla V100 DGXS 32 GB,ILSVRC 2012 subset of ImageNet,,,,,Open weights (unrestricted),Open source,Open source,,,Hardware,,source: https://openai.com/blog/image-gpt/#rfref53,"We have that ""iGPT-L was trained for roughly 2500 V100-days"" [1]

I assume this is the NVIDIA Tesla V100 GPU. In the specifications, the NVIDIA Tesla V100 has 7 to 8.2 TFLOPS of peak double precision performance and 14 to 16.4 TFLOPS of peak single precision performance and 112 to 130 TFLOPS of peak tensor performance [2].

I suppose the one that makes sense using if peak tensor performance, for ~125 TFLOPS peak tensor performance more or less.
Following OpenAIs AI and compute we apply a 0.33 utitilization factor [3].

In total we get 2500 V100-days * (24*60*60) seconds/day * 125 TFLOPS * 0.33 = 8.91e+21 FLOPS = 89.1 PF-days.

[1] https://openai.com/blog/image-gpt/
[2] https://images.nvidia.com/content/technologies/volta/pdf/volta-v100-datasheet-update-us-1165301-r5.pdf
[3] https://openai.com/blog/ai-and-compute/",,"""We use the ImageNet ILSVRC 2012 training dataset, splitting off 4% as our experimental validation set and report results on the ILSVRC 2012 validation set as our test set.""

https://image-net.org/challenges/LSVRC/2012/

""The goal of this competition is to estimate the content of photographs for the purpose of retrieval and automatic annotation using a subset of the large hand-labeled ImageNet dataset (10,000,000 labeled images depicting 10,000+ object categories) as training.""
",,,,,modified MIT: https://github.com/openai/image-gpt?tab=License-1-ov-file#readme
GPT-3 175B (davinci),5/28/2020,"Highly cited,Training cost",1.75E+11,3.14e+23,3.74E+11,0.6,355.2,10000,0.1968,2056969.3385324872,NVIDIA Tesla V100 DGXS 32 GB,"Common Crawl,WebText2,Wikipedia,Books1,Books2",Confident,,,3200000,API access,Unreleased,,TRUE,5595164.712836602,Reported,,"""we train GPT-3, an autoregressive language model with 175 billion parameters""","Table D.1
https://arxiv.org/abs/2005.14165",Table 2.2 (other datasets also used),"From table 2.2, we determine that there are 410 + 19 + 12 + 55 + 3 = 499 billion tokens. 

We multiply this by 0.75 to give 374B words. 

3.74e11

========================
[Anson: I think the calculation below doesn't look at all the data, the CommonCrawl data only constitutes 60% of the data. Multiplying by 5/3 gives 4.75e11]

""The CommonCrawl data was downloaded from 41 shards of monthly CommonCrawl covering 2016 to 2019, constituting 45TB of compressed plaintext before filtering and 570GB after filtering, roughly equivalent to 400 billion byte-pair-encoded tokens. ""

Converted to words using 
http://extraconversion.com/data-storage/gigabits/gigabits-to-words.html

2.85e11",14.8 days according to https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf,,,"3.2M, per table 2.1","https://openai.com/blog/openai-api
"
DETR,5/26/2020,Highly cited,60000000,4e+20,123000,500,,,,959.9097627206426,NVIDIA V100,COCO 2017,Confident,,,64,Open weights (unrestricted),Open source,Open source,,,Hardware,,60M per Table 1,"""Training the baseline model for 300 epochs on 16 V100 GPUs takes 3 days, with 4 images per GPU (hence a total batch size of 64). For the longer schedule used to compare with Faster R-CNN we train for 500 epochs with learning rate drop after 400 epochs. This schedule adds 1.5 AP compared to the shorter schedule.""

48 V100-days for baseline DETR model. Larger model had 1.5x the params and 5/3 as many epochs, so required ~2.5x as much training compute.

125 teraflop/s * 2.5 * 48 * 24 * 3600 * 0.3 (assumed utilization) ~ 4e20","""We perform experiments on COCO 2017 detection and panoptic segmentation datasets [24,18], containing 118k training images and 5k validation images""",,,,,,Apache 2.0: https://github.com/facebookresearch/detr
Retrieval-Augmented Generator,5/22/2020,"Highly cited,SOTA improvement",626000000,,,,,,,,NVIDIA Tesla V100 PCIe 32 GB,"Wikipedia,NQ (Natural Questions)",Confident,,,,Open weights (unrestricted),Unreleased,,,,,"""Our RAG models achieve state-of-the-art results on open Natural Questions [29], WebQuestions [3] and CuratedTrec [2] ""","""Our RAG models contain the trainable parameters for the BERT-base query and document encoder of DPR, with 110M parameters each (although we do not train the document encoder ourselves) and 406M trainable parameters from BART-large, 406M parameters, making a total of 626M trainable parameters""","not enough info, e.g. no training time reported:

""We train with mixed precision floating point arithmetic [40], distributing training across 8, 32GB NVIDIA V100 GPUs, though training and inference can be run on one GPU""",,,,,,,"It's in HF transformers library:
https://huggingface.co/docs/transformers/en/model_doc/rag

this library has an apache license: https://github.com/huggingface/transformers/blob/main/LICENSE"
Conformer,5/16/2020,"Highly cited,SOTA improvement",118800000,,,,,,,,,LibriSpeech,Confident,,,,Unreleased,Unreleased,,,,,"""Conformer significantly outperforms the previous Transformer and CNN based models achieving state-of-the-art accuracies. On the widely used LibriSpeech benchmark, our model achieves WER of 2.1%/4.3% without using a language model and 1.9%/3.9% with an external language model on test/testother""",118.8M for Conformer(L),,,,,,,,
ContextNet,5/7/2020,SOTA improvement,112700000,,1710000000,,,,,,,LibriSpeech,Likely,,,,Unreleased,Unreleased,,,,,"""We demonstrate that on the widely used Librispeech
benchmark, ContextNet achieves a word error rate (WER) of
2.1%/4.6% without external language model (LM), 1.9%/4.1%
with LM and 2.9%/7.0% with only 10M parameters on the
clean/noisy LibriSpeech test sets. This compares to the
best previously published model of 2.0%/4.6% with LM and
3.9%/11.3% with 20M parameters""",Table 5,"Uses pre-trained joint network from https://arxiv.org/pdf/1811.06621, so total training compute should factor this in.",,"970 hours of speech in the LibreSpeech experiments. There is mention of a ""large scale experiment"" which trained on audio from YouTube videos. Tracing the references, it appears to be similar to the dataset used in https://arxiv.org/abs/1610.09975, which has 125,000 hours of transcribed audio for training. However, they mention in footnote 2 that the training and evaluation set have changed from previous experiments.
125k hours at 13,680 words per hour = 1.71B words",,,,,
NAS+ESS (156M),5/6/2020,SOTA improvement,156000000,2.89e+18,,30,,,,,,Penn TreeBank,,,,,Unreleased,Unreleased,,,,,"""Our ESS method
achieves state-of-the-art result on the PTB task""",,,,,,,,,
UnifiedQA,5/2/2020,SOTA improvement,11000000000,1.65e+19,97309860,1.88,36,8,,,Google TPU v3,,Confident,T5-11B,3.8e+19,27,Unreleased,,,,8059.314206761232,"Operation counting,Hardware","""We then introduce UNIFIEDQA (§3.2) that is a QA system
trained on datasets in multiple formats, indicating
new state-of-the-art results on 10 datasets and generalization to unseen datasets.""",11B (based on T5-11B),"A.2: ""In the experiments, we use v3-8 TPUs for T5 models... pretraining UNIFIEDQA approximately takes about 36 hours on T5(11B)""

4 * 1.23e14 * 36 * 3600 * 0.3 = 1.91e19

Alternatively, input (ouput) size of 512 (100) tokens, batch size of 8, trained for 100k steps. Input tokens only need the forward pass.
((2 * 11B * 512 * 8) + (6 * 11B * 100 * 8)) * 100k = 1.43e19

Took geometric mean of these estimates:
sqrt(1.91e19*1.43e19) = 1.65e19","""We empirically chose the following 8 seed datasets for training UNIFIEDQA, 3 based on their effectiveness in our pilot study (details deferred to Section 5) assessing which datasets are most valuable for out-of-format training:
• EX: SQuAD 1.1, SQuAD 2.0
• AB: NarrativeQA
• MC: RACE, ARC, OBQA, MCTest
• YN: BoolQ""","Table 2:
SQuAD 1.1: 87k examples, avg total length of 136.2 + 3.0
SQuAD 2.0: 130k examples, avg total length of 139.9 + 2.6
NarrativeQA: 65k examples, avg total length of 563.6 + 6.2
RACE: 87k examples, avg total length of 317.9 + 6.9
ARC (easy): 2k examples, avg total length of 39.4 + 3.7
ARC (hard): 1k examples, avg total length of  47.4 + 5.0
OBQA: 4k examples, avg total length of 28.7 + 3.6
MCTest: 1.4k examples, avg total length of 245.4 + 4.0
BoolQ: 9k examples, avg total length of 105.1 + 1.0

Total tokens: 97,309,860","""pretraining UNIFIEDQA approximately takes about 36 and 55 hours, on T5(11B) and BART models, respectively.""",,"""• Infrastructure: In the experiments, we use v3-8 TPUs for T5 models, and eight 32GB GPUs for
BART models.
• Time spent to build UNIFIEDQA: pretraining UNIFIEDQA approximately takes about 36 and 55
hours, on T5(11B) and BART models, respectively.""

8 * 123 TFLOPS * 36 * 3600 * 0.3 (utilization assumption) = 3.8e19",,
ATLAS,5/2/2020,SOTA improvement,11000000000,3.825792e+19,,,36,,,59.12208690735248,Google TPU v3,SQuAD 1.1,Confident,,,,Open weights (unrestricted),Open source,Open source,,,Hardware,"from abstract: ""Finally, simply fine-tuning this pre-trained QA model into specialized models results in a new state of the art on 6 datasets""","11B from appendix A.2 : Model sizes: ""Most of the experiments are done on T5(11B) which has 11 billion parameters. We also report experiments with BART (large) with 440 million parameters.""","flops = (8) * (123 * 10**12) * (36 * 3600) * (0.3)
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)

from Appendix A.2: ""Time spent to build UNIFIEDQA: pretraining UNIFIEDQA approximately takes about 36 and 55 hours, on T5(11B) and BART models, respectively.""
so 36h for T5

""Infrastructure: In the experiments, we use v3-8 TPUs for T5 models, and eight 32GB GPUs for BART models.""

from https://cloud.google.com/tpu/docs/system-architecture-tpu-vm#tpu_chip
tpu chip have peak flops 123 teraflops
so 8 chips have peak flops 123 * 8","from appendix A.1 - multiple QA datasets, In section 3 there is description how batches are created from multiple datasets.","from appendix A.1 - multiple QA datasets - it may be possible to estimate by summing sizes of all datasets
I am not sure if all data is used as system is trained for 100K steps (from appendix A.2)
with batch size 8 (appendix A.1)","Appendix A.2: Time spent to build UNIFIEDQA: pretraining UNIFIEDQA approximately takes about 36 and 55 hours, on T5(11B) and BART models, respectively.",,,,"Apache 2.0 license, includes models and training code: https://github.com/allenai/unifiedqa"
Once for All,4/29/2020,SOTA improvement,7700000,1.78428096e+21,,180,,,,1753.9255676777682,NVIDIA V100,ImageNet,,,,,Open weights (unrestricted),Open source,Open source,,,Hardware,"""In particular, OFA achieves a new SOTA 80.0% ImageNet top-1 accuracy under the mobile setting""",,"4.2k V100-hours (table 1)
0.33 utilization rate
",,,,from Table 1,,,"MIT license: https://github.com/mit-han-lab/once-for-all

repo contains inference and training code. models available via library"
Go-explore,4/27/2020,SOTA improvement,,,,,,,,,,,Unknown,,,,Unreleased,Open (non-commercial),Open access (non-commercial),,,,"""GoExplore solves all heretofore unsolved Atari games (meaning those for which algorithms could not previously
outperform humans when evaluated following current community standards for Atari3) and surpasses the state
of the art on all hard-exploration games""",,,,,,,,,non-commercial code: https://github.com/uber-research/go-explore/blob/master/LICENSE
CURL,4/8/2020,SOTA improvement,907264,,,,,,,,,,,,,,Open weights (unrestricted),Open source,Open source,,,,,,,"RL on Atari:

""We measure the data-efficiency and performance of our
method and baselines at 100k and 500k environment steps
on DMControl and 100k interaction steps (400k environment steps with action repeat of 4) on Atari, which we will
henceforth refer to as DMControl100k, DMControl500k
and Atari100k for clarity. While Atari100k benchmark has been common practice when investigating data-efficiency
on Atari (Kaiser et al., 2019; van Hasselt et al., 2019; Kielak,
2020), the DMControl benchmark was set at 500k environment steps because state-based RL approaches asymptotic
performance on many environments at this point, and 100k
steps to measure the speed of initial learning. A broader
motivation is that while RL algorithms can achieve superhuman performance on Atari games, they are still far less
efficient than a human learner. Training for 100-500k environment steps corresponds to a few hours of human time.""",,,,,,"MIT: https://github.com/MishaLaskin/curl

"
Agent57,3/30/2020,SOTA improvement,,,,,,,,,,,Unknown,,,,Unreleased,Unreleased,,,,,"""We propose Agent57, the first deep RL agent that outperforms the standard human benchmark on all 57 Atari games""",,,,,,,,,
MetNet,3/24/2020,SOTA improvement,,,,,,,,,,,Unknown,,,,Unreleased,Unreleased,,,,,"""MetNet improves upon the current operational NWP system HRRR for up to 8 hours of lead time""
... 
""Numerical Weather Prediction is the most successful framework to perform medium- and longrange (up to 6 days with high confidence) forecast to date (Bauer et al., 2015).""",,,"""Precipitation provides a benchmark for a highly varying and densely measured target (Agrawal
et al.). We cast precipitation forecasting as a structured prediction problem where the output comes
in the form of a three-dimensional tensor. Each value of the tensor corresponds to a time and a
location and indicates the corresponding rate of precipitation measured in mm/h. Target precipitation rates are estimated by the Multi Radar Multi Sensor (MRMS) ground based radars as a
function of the returned radar echoes (Zhang et al., 2016). The spatial size obtained from MRMS
is 7000 × 2500 covering the continental United States. Each pixel covers 0.01◦ of longitude and
latitude corresponding to approximately 1 km2
. In addition to MRMS frames, the available input
data include the 16 spectral bands of the optical Geostationary Operational Environmental Satellite
16 (GOES-16). Figure 1 contains examples of MRMS and GOES-16 frames.""",,,,,,
ELECTRA,3/23/2020,Highly cited,335000000,3.1e+21,25000000000,,,,,,,"BookCorpus (BooksCorpus, Toronto Book Corpus),Wikipedia,ClueWeb,Gigaword",,,,262144,Open weights (unrestricted),Open source,Open source,,,Reported,,https://github.com/google-research/electra,"Table 8: ""ELECTRA-1.75M"" used 3.1e21 train FLOPs. Note that the actual parameter count is 335M. The 1.75M refers to the number of training steps.

This doesn't quite line up with a 6ND estimate, 
6 * 335M * (1.75M * 2048 * 128) = 9.22e20 FLOPs
I'm inferring 128 sequence length, possibly this is 256 or 512?","""For most experiments we pre-train on the same data as BERT, which consists
of 3.3 Billion tokens from Wikipedia and BooksCorpus (Zhu et al., 2015). However, for our Large
model we pre-trained on the data used for XLNet (Yang et al., 2019), which extends the BERT
dataset to 33B tokens by including data from ClueWeb (Callan et al., 2009), CommonCrawl, and
Gigaword (Parker et al., 2011).""","33B tokens or ~25B words

""For most experiments we pre-train on the same data as BERT, which consists
of 3.3 Billion tokens from Wikipedia and BooksCorpus (Zhu et al., 2015). However, for our Large
model we pre-trained on the data used for XLNet (Yang et al., 2019), which extends the BERT
dataset to 33B tokens by including data from ClueWeb (Callan et al., 2009), CommonCrawl, and
Gigaword (Parker et al., 2011).""",table 1,,,,"models and training code, Apache 2.0: https://github.com/google-research/electra"
Tensor-Transformer(1core)+PN (WT103),3/17/2020,SOTA improvement,85300000,1.58e+18,,30,,,,,,WikiText-103,,,,,Open weights (unrestricted),Open source,Open source,,,,"""The results are reported in Table 1. In the first section of
rows, we report state-of-the-art results for these two tasks with comparable model sizes""",,,,,,,,,copyleft license: https://github.com/sIncerass/powernorm/blob/master/LICENSE
Routing Transformer (WT-103),3/12/2020,SOTA improvement,79500000,,,,,,,,,WikiText-103,,,,,Open weights (unrestricted),Unreleased,Open source,,,,"""Additionally, we set a new state-of-the-art on the newly released PG-19 data-set, obtaining a test perplexity of 33.2 with a 22 layer Routing Transformer model trained on sequences of length 8192""",,,,,,,,,"code/weights: https://github.com/google-research/google-research/tree/master/routing_transformer

repo is Apache 2.0: https://github.com/google-research/google-research/blob/master/LICENSE"
TransformerXL + spectrum control,3/11/2020,SOTA improvement,151000000,4.6e+17,,250,,,,,,WikiText-103,,,,,Unreleased,Unreleased,,,,,"""We demonstrate that our spectrum control method outperforms the state-of-the-art Transformer-XL modeling for language model""",,,,,,,,,
TCAN (WT2),2/28/2020,SOTA improvement,33000000,,,,,,,,,WikiText-2,,,,,Unreleased,Open source,Open source,,,,"""We improve the state-of-theart results of ... 9.20 on WikiText-2""",,,,,,,,,MIT license for code: https://github.com/haohy/TCAN
Feedback Transformer,2/21/2020,SOTA improvement,126000000,4.41e+19,,267.23,,,,,,WikiText-103,,,,,Unreleased,Unreleased,,,,,"""As shown in Table 4, the Feedback
Transformer model achieves a new SOTA performance (on Enwiki8) of 0.96 bit-per-byte despite its small size.""",,,,,,,,,
Turing-NLG,2/13/2020,SOTA improvement,17000000000,1.57e+22,46400000000,3.39,,256,,51659.713290894986,NVIDIA Tesla V100 DGXS 32 GB,,Likely,,,524288,Unreleased,Unreleased,,,143400.44308513464,"Third-party estimation,Operation counting","from paper: ""Turing Natural Language Generation (T-NLG) is a 17 billion parameter language model by Microsoft that outperforms the state of the art on many downstream NLP tasks""",,"source: https://lair.lighton.ai/akronomicon/
157 PF-days * 3600 * 24 * 10^15  = 1.35648e+22

archived: https://github.com/lightonai/akronomicon/tree/main/akrodb

6ND=6*17000000000*46400000000=4.7328e+21 (confidence regarding dataset size - likely)",,"Authors say they pretrain on the same data as for Megatron-LM. 

From the Megatron-LM paper: https://arxiv.org/pdf/1909.08053.pdf

""The resulting aggregate corpus contains 174 GB of deduplicated text.""

174GB * 2e8words/GB = 3.48e10 words
3.48e10 words (if english) *4/3 = 46400000000 tokens

confidence - likely",,,,,
SimCLR,2/13/2020,Highly cited,375000000,,,1000,,,,,Google TPU v3,ILSVRC 2012 subset of ImageNet,,,,,Open weights (unrestricted),Open source,Open source,,,,,source: https://openai.com/blog/image-gpt/,,"""Dataset and Metrics. Most of our study for unsupervised
pretraining (learning encoder network f without labels) is done using the ImageNet ILSVRC-2012 dataset (Russakovsky et al., 2015). Some additional pretraining experiments on CIFAR-10 (Krizhevsky & Hinton, 2009) can be found in Appendix B.9.""",,,,,,Apache 2.0: https://github.com/google-research/simclr
ALBERT-xxlarge,2/9/2020,Highly cited,235000000,2.39e+21,3300000000,79.4,32,512,,4439.921298510215,Google TPU v3,"Wikipedia,BookCorpus (BooksCorpus, Toronto Book Corpus)",,,,2097152,Open weights (unrestricted),Open source,Open source,,516264.2595914164,Hardware,,,"32 hours of training
512 TPU V3s
0.33 utilization rate
","""To keep the comparison as meaningful as possible, we follow the BERT (Devlin et al., 2019) setup in using the BOOKCORPUS (Zhu et al., 2015) and English Wikipedia (Devlin et al., 2019) for pretraining baseline models. These two corpora consist of around 16GB of uncompressed text. W""","Pretraining same as for BERT - Wikipedia and BookCorpus

""For the pre-training corpus we
use the BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2,500M words)""",,,,"Sequences are capped at 512 tokens; 10% of the time they'll use an input less than 512 long. Batches are over 4096 sequences. Tokens per batch: 2,097,152",Apache 2.0 code/weights. repo includes training code: https://github.com/google-research/ALBERT
TaLK Convolution,2/8/2020,SOTA improvement,240000000,2.78e+19,,187.43,,,,,,WikiText-103,,,,,Unreleased,Unreleased,Unreleased,,,,"""[We] set a new state-of-the-art result on the
IWSLT De-En and CNN-DailyMail datasets""",Table 5,,,,,,,,"MIT, code and weights (though this repo is for translation not WT-103):

https://github.com/lioutasb/TaLKConvolutions?tab=readme-ov-file"
Perceiver IO (optical flow),2/8/2020,SOTA improvement,27900000,,,480,,,,,,AutoFlow,,,,,Unreleased,Unreleased,,,,,"""Perceiver IO... achieves state-of-the-art performance on Sintel optical flow estimation""","Optical flow model (SOTA) was 27.9M params. There are other, larger models described in this paper, e.g. for language.

""For the pixel- and patch-based models, total computational
complexity for a forward pass on a 368 × 496 image is roughly 987 billion FLOPs, and there are
roughly 27.9 million parameters.""",,"""In all cases, we train on the AutoFlow dataset (Sun et al., 2021), which consists of 400, 000 image pairs, for 480 epochs using a cosine learning rate schedule which starts at a learning rate of 4e-4.
We use a batch size of 512. We use the LAMB (You et al., 2021) optimizer.""",,,,,,
Theseus 6/768,2/7/2020,SOTA improvement,66000000,,,,,,,,NVIDIA V100,GLUE,,BERT-Large,2.7e+18,,Open weights (unrestricted),Open source,Open source,,,,"""Our approach outperforms existing knowledge distillation approaches on GLUE benchmark""","66M, Table 1",,"fine-tuned on training sets from GLUE benchmark:

""We test our approach under a task-specific compression setting (Sun et al., 2019; Turc et al., 2019)
instead of a pretraining compression setting (Sanh
et al., 2019; Sun et al., 2020). That is to say, we use
no external unlabeled corpus but only the training set of each task in GLUE to compress the
model. """,,,,"Actually BERT-base, 110M params. Up to 20 V100-hours depending on task. 

125 trillion * 20 * 3600 * 0.3 (utilization assumption) = 2.7e18",,Apache 2.0: https://github.com/JetRunner/BERT-of-Theseus
Meena,1/28/2020,SOTA improvement,2600000000,1.12e+23,40000000000,164,720,1024,0.34306622,206760.3812904988,Google TPU v3,,Confident,,,82655,Unreleased,Unreleased,,TRUE,1032664.6317417204,"Hardware,Operation counting,Third-party estimation","""We also propose a human evaluation metric called Sensibleness and
Specificity Average (SSA)... the full version of Meena (with a filtering mechanism and tuned decoding) scores 79% SSA, 23% higher in absolute SSA than the existing chatbots we evaluated""","""We present Meena, a multi-turn open-domain chatbot trained end-to-end on data mined and filtered from public domain social media conversations. This 2.6B parameter neural network is simply trained to minimize perplexity of the next token.""","https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf
Table 4

In the paper: ""We trained our best model for 30 days on a TPUv3 Pod (2,048 TPU cores) on the Meena dataset containing 40B words (or 61B BPE tokens) [...] by the end of training, the model had traversed the full
training set 164 times (or epochs) and observed a total of about 10T tokens""

Hardware: 30 * 24 * 3600 * (2048/2) * 1.23e14 * 0.3 = 9.794e22
Ops counting: 6 * 10T * 2.6B = 1.56E23
Geometric mean: sqrt(9.79e22*1.56E23) = 1.24e23, very close to the figure in the link above.",,"""The final Meena dataset contains 341GB of text
(40B words)""

Converting from GB to words yields 6.8e10, which is in the same OOM","We trained our best model for 30 days on a TPUv3 Pod (2,048 TPU cores)",,,"61B tokens over 738k training steps, or 82655 tokens per batch on average. Not certain about warmup, etc",
ContextNet + Noisy Student,1/19/2020,SOTA improvement,,8.16e+21,,,1440,,,14226.054462994534,Google TPU v3,"LibriSpeech,LibriLight",Confident,,,,Unreleased,Unreleased,,,,Hardware,"""We are thus able to improve upon the previous state-of-the-art clean/noisy test WERs achieved on LibriSpeech 100h (4.74%/12.20%) and LibriSpeech (1.9%/4.1%)""",,"""We train 6 generations of models numbered 0 to 5, where
we count the baseline model trained with the supervised set
as the zeroth generation. Each generation is trained ... on 32 Google
Cloud TPU chips for 10 days.""

The TPU version is likely v3 given this is a 2020 paper.

we get 6 * 10 * 24 * 3600 * 32 * 123 tflops * 0.4  (assumed utilization) = 8.16e21","""LibriSpeech 100-860 is a semi-supervised task where the clean 100h subset of LibriSpeech [6] is taken to be the supervised set, while the remaining 860h of audio is taken to be the unlabeled set. The unlabeled audio consists of 360h of clean data and 500h of noisy data. We tokenize the transcripts using a WPM model [37] with vocabulary size 16k constructed from the clean 100h subset transcripts.""

Inputs are mel-spectrograms, but unclear the duration of each.",,roughly 10 days,,,,
AlphaFold,1/15/2020,"SOTA improvement,Highly cited",16340840,1e+20,300000000.99999946,,120,,,,,"PDB (Protein Data Bank),UniRef30 (FKA UniClust30)",Speculative,,,,Unreleased,Unreleased,,,,"Hardware,Third-party estimation","""AlphaFold represents a considerable advance
in protein-structure prediction."" [Abstract]","""Neural network hyperparameters"" section of https://www.nature.com/articles/s41586-019-1923-7:
“7 × 4 Blocks with 256 channels, cycling through dilations 1, 2, 4, 8”
“48 × 4 Blocks with 128 channels, cycling through dilations 1, 2, 4, 8”

""Distogram prediction"" section:
""For the final layer, a position-specific bias was used""

Extended Data Fig.1 (b): 
Shows that each block consists of 9 layers:
(1) Batch norm
(2) Elu
(3) Project down (halves number of dimensions)
(4) Batch norm
(5) Elu
(6) 3x3 kernel with dilation
(7) Batch norm
(8) Elu
(9) Project up (doubles number of dimensions)

Dilations don't change the number of parameters in each filter
Assuming that projection layers are convolutional layers with 1x1 kernels

Parameter estimate for each layer in a 256 channel block:
(1) 256*2            = 512
(2) 0
(3) 1*1*256*128 = 32768
(4) 128*2            = 256 
(5) 0
(6) 3*3*128*128 = 147456
(7) 128*2            = 256 
(8) 0
(9) 1*1*128*256 + 256 = 33024
Total                             = 214272

Parameter estimate for each layer in a 128 channel block:
(1) 128*2            = 256
(2) 0
(3) 1*1*128*64   = 8192
(4) 64*2              = 128
(5) 0
(6) 3*3*64*64     = 36864
(7) 64*2              = 128
(8) 0
(9) 1*1*64*128 + 128 = 8320
Total                   = 53897

Estimate total network = 7*4*214272 + 48*4*53897 = 5992616 + 10348224
                                     = 16340840
                                     ~ 16e6

Within a factor of 2 of the estimate of 21M parameters stated in: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7305407/

[Previous approximation: 7 * 4 * 256 * 3 * 3 * 256 + 48 * 4 * 128 * 3 * 3 * 128 = 44826624]","Estimated in the blogpost below

https://www.lesswrong.com/posts/wfpdejMWog4vEDLDg/ai-and-compute-trend-isn-t-predictive-of-what-is-happening

""AlphaFold: they say they trained on GPU and not TPU. Assuming V100 GPU, it's 5 days * 24 hours/day * 3600 sec/hour * 8 V100 GPU * 100*10^12 FLOP/s * 33% actual GPU utilization = 10^20 FLOP.""","""Our models are trained on structures extracted from the PDB"" [""Data"" section]

""For each training sequence, we searched for and aligned to the training sequence similar protein sequences in the Uniclust3035 dataset"" [""Data"" section]","Training Domains: 29,427
Average Residues per Domain: 100
Data Points per Domain: 100 × 100 = 10,000
Total Data Points = 29,427 × 10,000 = 294,270,000 ≈ 3.0 × 10⁸","""Training time: about 5 days for 600,000 steps""",,,,
Big Transfer (BiT-L),12/24/2019,SOTA improvement,928000000,,,40,,,,,Google TPU v3,JFT-300M,,,,,Unreleased,Unreleased,,,,,"""We transfer BiT to many diverse tasks... These tasks include ImageNet’s ILSVRC-2012 [10], CIFAR-10/100 [27], Oxford-IIIT Pet [41], Oxford
Flowers-102 [39] (including few-shot variants), and the 1000-sample VTAB-1k benchmark [66], which consists of 19 diverse datasets. BiT-L attains state-ofthe-art performance on many of these tasks",,,"""We train networks on three different scales of datasets. The largest, BiT-L
is trained on the JFT-300M dataset [51], which contains 300 M noisily labelled images""",,,,,,
DD-PPO,12/19/2019,SOTA improvement,,7.8e+20,,,66,64,,1926.8992900057376,NVIDIA V100,,Likely,,,,Unreleased,Unreleased,,,43046.65435038904,Hardware,"""This agent achieves state-of-art on the Habitat Challenge 2019 RGB track (rank 2 entry has 0.89 SPL).""","no parameter count but some architecture details: ""The policy is parameterized by a 2-layer LSTM with a 512-dimensional hidden state. It takes three inputs: the previous action, the target relative to the current state, and the output of the visual encoder. The LSTM’s output is used to produce a softmax distribution over the action space and an estimate of the value function. See Appendix C for full details.""","""Using DD-PPO, we train agents for 2.5 Billion steps of experience with 64 Tesla V100 GPUs in 2.75 days – 180 GPU-days of training""

125 teraFLOP/s (exact V100 model not specified) * 180 * 24 * 3600 * 0.4 (assumed utilization) = 7.8e20","""We experiment with several different sources of data. First, we utilize the training data released
as part of the Habitat Challenge 2019, consisting of 72 scenes from the Gibson dataset (Xia et al.,
2018). We then augment this with all 90 scenes in the Matterport3D dataset (Chang et al., 2017) to
create a larger training set (note that Matterport3D meshes tend to be larger and of better quality).2
Furthermore, Savva et al. (2019) curated the Gibson dataset by rating every mesh reconstruction on
a quality scale of 0 to 5 and then filtered all splits such that each only contains scenes with a rating of
4 or above (Gibson-4+), leaving all scenes with a lower rating previously unexplored. We examine
training on the 332 scenes from the original train split with a rating of 2 or above (Gibson-2+).""",,2.75 days,,,,MIT license for environment used to train. doesn't seem like it has training code for this model. https://github.com/facebookresearch/habitat-lab
OpenAI Five Rerun,12/13/2019,"Highly cited,SOTA improvement",159000000,1.3e+22,53084160000,,,512,,,NVIDIA P100,,,,,,Unreleased,Unreleased,,TRUE,286996.7074234706,Third-party estimation,"""On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game.""","""We define a policy (π) as a function from the history of observations to a probability distribution
over actions, which we parameterize as a recurrent neural network with approximately 159 million
parameters (θ)."" pg. 3 of paper

source: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389","THIS CALCULATION IS FOR RERUN

source: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389",,"54k iterations (Fig 7)
with a batch size of 983040 (Table 2)",,,,,
OpenAI Five,12/13/2019,"Highly cited,SOTA improvement",159000000,6.7e+22,4.54321E+11,,7104,1536,,,NVIDIA P100,,Confident,,,,Unreleased,Unreleased,,TRUE,860990.1222704119,,"""On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game.""","""We define a policy (π) as a function from the history of observations to a probability distribution over actions, which we parameterize as a recurrent neural network with approximately 159 million parameters (θ)."" pg. 3 of paper

source: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389","""770±50 PFlops/s·days of compute"" for the model that played against world champions. They did a single training run that took 10 months.

While the model was playing against world champions, they continued training for a few days, so that the resulting model used even more training compute: 820±50 PFlops/s·days.

Finally, they also trained a Rerun model with 150±5 PFlops/s·days of compute.

Source: Dota 2 with Large Scale Deep Reinforcement Learning
https://arxiv.org/abs/1912.06680

You cannot multiply the hardware quantity by training time to get the quantity of GPU-hours! Page 5: "" the number of GPUs (up to 1536 at the peak)""

From this NVIDIA blogpost, it appears they were using P100s:
https://developer.nvidia.com/blog/ai-learns-to-play-dota-2-with-human-precision/#:~:text=AI%20Learns%20to%20Play%20Dota,The%20neural",,"""Although the Dota 2 engine runs at 30 frames per second, OpenAI Five only acts on every 4th
frame which we call a timestep""
--> 7.5 timesteps/s

""OpenAI Five is a single training run that ran from June 30th, 2018 to April 22nd, 2019. "" --> 296 days

296 * 24*3600 * 7.5 = 1.92e8

This number seems a little low? The DQN paper had 1e7 timesteps. Might be to do with sample efficiency?

EDIT 14/06/2022
Multiple copies of OpenAI Five were trained in parallel, so the total training time is much higher than 296 days.
Table 1 shows 220,000 GPU iterations, each iteration has a batch size of between 1M and 3M timesteps (Table 2), so the total number of episodes is on the order of 2e11","""OpenAI Five is a single training run that ran from June 30th, 2018 to April 22nd, 2019. "" --> 296 days","Cannot multiply the hardware quantity by training time to get the quantity of GPU-hours! Page 5: "" the number of GPUs (up to 1536 at the peak)""",,,
MMLSTM,12/5/2019,SOTA improvement,75000000,2.32e+18,,50,,,,,,WikiText-103,,,,,Unreleased,Unreleased,,,,,"""In experiments, we demonstrate the language model with MMLSTMs surpasses the existing state-of-the-art model on Penn Treebank (PTB) and WikiText-2 (WT2) datasets""",,,,,,,,,
StarGAN v2,12/4/2019,"Highly cited,SOTA improvement",,,,,,,,,,"CelebA,AFHQ",Unknown,,,,Open weights (non-commercial),Open (non-commercial),Open access (non-commercial),,,,"""Votes from AMT workers for the most preferred method
regarding visual quality and style reflection (%). StarGAN v2 outperforms the baselines with remarkable margins in all aspects.""",,,"""Datasets. We evaluate StarGAN v2 on CelebA-HQ [21] and our new AFHQ dataset (Appendix A)""",,,,,,https://github.com/clovaai/stargan-v2?tab=readme-ov-file non-commercial
Transformer-XL DeFINE (141M),11/27/2019,SOTA improvement,141000000,6.2e+18,,20,,,,,,"WikiText-103,Penn TreeBank",,,,,Unreleased,Unreleased,,,,,"""Compared to state-of-the-art methods including adaptive input representations,
this technique results in a 6% to 20% drop in perplexity""",,,,,,,,,
Photo-Geometric Autoencoder,11/25/2019,SOTA improvement,,,,30,,,,,,"CelebA,3DFAW,BFM",Unknown,,,,Open weights (unrestricted),Open source,Open source,,,,"""Our model outperforms a
current state-of-the-art 3D reconstruction method that uses 2D keypoint supervision""",,,"""We test our method on three human face datasets: CelebA [35], 3DFAW [21, 27, 73, 69] and BFM [47]""",,,,,,MIT license: https://github.com/elliottwu/unsup3d
Transformer - LibriVox + Decoding/Rescoring,11/19/2019,SOTA improvement,296000000,,,,,,,,,"LibriSpeech,LibriVox",Confident,,,,Open weights (unrestricted),,,,,,"""Results with decoding/rescoring are shown in Table 2, where we reach 2.09% and 4.11% on test-clean and test-other , respectively, and are further improvements on the state-of-the-art.""",Table 2,"""Models are trained on 64 GPUs each with an overall batch size of 256 for ResNet and TDS and 320 for Transformer. With only LIBRISPEECH, all models converged in under a week; with pseudo-labels from LIBRIVOX, training required 2-3 weeks""

GPU not specified","""LIBRIVOX2
is a large collection of freely-available audio books. Using tools provided with the LIBRILIGHT dataset [26], we select 72K hours of read speech from English book listings and run several preprocessing
steps. After filtering samples to remove readings of duplicate text and corrupted audio, we remove all audio for which
the speaker has overlap with a sample in LIBRISPEECH... the resulting audio corpus contains 53.8K hours of read speech.""","""the resulting audio corpus contains 53.8K hours of read speech""",,,,,BSD license: https://github.com/jakeju/wav2letter?tab=License-1-ov-file#readme
MuZero,11/19/2019,"Highly cited,SOTA improvement",36864000,4.8e+19,20000000000,,,,,,,,,,,,Unreleased,Unreleased,,,,Hardware,,"Both the representation and dynamics function use the same architecture asAlphaZero, but with 16 instead of20 residual blocks [15]. We use 3x3 kernels and 256 hidden planes for each convolution.

Previous downsampling:
•  1 convolution with stride 2 and 128 output planes, output resolution 48x48.•  2 residual blocks with 128 planes•  1 convolution with stride 2 and 256 output planes, output resolution 24x24.•  3 residual blocks with 256 planes.•  Average pooling with stride 2, output resolution 12x12.•  3 residual blocks with 256 planes.•  Average pooling with stride 2, output resolution 6x6.","third-generation Google Cloud TPU
(For each board game, we used 16 TPUs for training and 1000 TPUs for self-play)
For each game in Atari, we used 8 TPUs for training and 32 TPUs for self-play
Training for 12 hours (for Atari)
Data from Parameter, Compute and Data Trends in Machine Learning
Google v3 TPU: 1.23E+14 FLOP/s (although with the caveat that it might be not applicable)
Utilization rate 
In LaMDA: Language Models for Dialog Applications, they report for TPU V3: 56.5%
Calculations for Atari:
12 hours → 43200 seconds
(8 TPUs for training) * (1.23*10^14 FLOP/s) * (43.2 *10^3 s) * (0.565 utilization rate) = 2.4017472 * 10^19 FLOP
Training time missing for boardgames
Assumption also 12 hours 
Also: 2.4017472 * 10^19 FLOP
Total cost ≈ 4.8 * 10^19 FLOP",,"Table 1
https://arxiv.org/pdf/1911.08265.pdf",,,,,
MoCo,11/13/2019,Highly cited,375000000,,,,,,,,,"ImageNet,Instagram-1B",,,,,Open weights (non-commercial),Open (non-commercial),Open access (non-commercial),,,,,https://openai.com/blog/image-gpt/#rfref53,,"""We study unsupervised training performed in:
ImageNet-1M (IN-1M): This is the ImageNet [11] training set that has ∼1.28 million images in 1000 classes (often
called ImageNet-1K; we count the image number instead,
as classes are not exploited by unsupervised learning). This
dataset is well-balanced in its class distribution, and its images generally contain iconic view of objects.
Instagram-1B (IG-1B): Following [44], this is a dataset
of ∼1 billion (940M) public images from Instagram. The
images are from ∼1500 hashtags [44] that are related to the
ImageNet categories. This dataset is relatively uncurated
comparing to IN-1M, and has a long-tailed, unbalanced
distribution of real-world data. This dataset contains both
iconic objects and scene-level images.""",,,,,,"non-commercial. the released models seem to be trained on ImageNet, not Instagram
https://github.com/facebookresearch/moco"
Noisy Student (L2),11/11/2019,"Highly cited,SOTA improvement",480000000,2.612e+22,81000000,,144,1024,,43900.60644295845,Google TPU v3,"ImageNet,JFT",,,,,Unreleased,Open source,Open source,TRUE,1033553.9740854003,Hardware,"""Noisy Student Training achieves 88.4% top-1 accuracy on ImageNet, which is 2.0% better than the state-of-the-art model""",,"""Our largest model, EfficientNet-L2, needs to be trained for 6 days on a Cloud TPU v3 Pod, which has 2048 cores, if the unlabeled batch size is 14x the labeled batch size""
TPU v3 gets 1.23e14 FLOP/s per chip, with 2 cores per chip

1024 * 1.23e14 * 6 * 24 * 3600 * 0.4 = 2.612e22",,"""Due to duplications, there are only 81M unique images among these 130M images.""",6 days,,,,apache 2.0 license: https://github.com/google-research/noisystudent train script: https://github.com/google-research/noisystudent/blob/master/local_scripts/imagenet/train.sh 
Sandwich Transformer,11/10/2019,SOTA improvement,209000000,1.58e+20,,180,,,,,,"BookCorpus (BooksCorpus, Toronto Book Corpus),enwik8,text8",,,,,Unreleased,Open (non-commercial),Open access (non-commercial),,,,"""Sandwich transformers achieve state-of-the-art results on the enwik8 character-level language modeling dataset and on an additional word-level corpus,
but have no significant effect on machine translation""",209M,,,,,,,,non-commercial training and inference code: https://github.com/ofirpress/sandwich_transformer
CamemBERT,11/10/2019,SOTA improvement,335000000,8.3e+20,31900000000,13,24,,,2319.5419478533995,NVIDIA V100,CCNet,Confident,,,,Open weights (unrestricted),Unreleased,,,,"Hardware,Operation counting","""Our best performing model CamemBERT reaches or improves the state of the art in all four downstream tasks."" (part-of-speech tagging, dependency parsing, named entity recognition and natural language inference tasks)","CamemBERT Large, Table 4","""Unless otherwise specified, our models use the BASE architecture, and are pretrained for 100k backpropagation steps on 256 Nvidia V100 GPUs (32GB each) for a day""

256 V100-days

256 * 125 teraflops * 24 * 3600 * 0.3 (assumed utilization)
= 8.3e20


""Following (Liu et al., 2019), we optimize the model using Adam (Kingma and Ba, 2014) (β1 = 0.9, β2 = 0.98) for 100k steps with large batch sizes of 8192 sequences, each sequence containing at most 512 tokens""

Using compute = 6*N*D, that's 6 * (100k * 8192 * 512) * 335M= 8.43e20","""we train another model with the LARGE architecture, referred to as CamemBERTLARGE, for a fair comparison with XLM-RLARGE. This model is trained with the CCNet corpus, described in Sec. 6, for 100k steps""

Other models in paper are trained with the French portion of OSCAR. See footnote 12."," 31.9B tokens, Table 6.",1 day for each model (may not have been a full 24 hours),,,,"MIT: 
https://camembert-model.fr/"
XLM-RoBERTa,11/5/2019,"Highly cited,SOTA improvement",550000000,2.076e+22,1.2525E+11,,,500,,,NVIDIA Tesla V100 DGXS 32 GB,CC100,Confident,,,,Open weights (non-commercial),Open (non-commercial),Open access (non-commercial),,280388.1069024762,Operation counting,"citation ""which obtains state-of-the-art perfor-
mance on cross-lingual classification, sequence la-
beling and question answering""","The number of parameters in the model is specified as ""550M params"" for XLM-R.","""We use the multilingual MLM loss and train our XLM-R model for
1.5 Million updates on five-hundred 32GB Nvidia
V100 GPUs with a batch size of 8192. ""

6ND:
Sequence length was probably 512, based on follow up paper (XLM-R XXL)
6 * 550e6 * 1.5e6 * 8192 * 512 = 2.076e22
","The training dataset and size are mentioned as ""using more than two terabytes of filtered CommonCrawl data"" and the model being trained on ""100 languages"".",size of CC100 - copied from other rows,,,,,"non-commercial: https://github.com/facebookresearch/XLM?tab=License-1-ov-file#readme

data is wikipedia"
Base LM + kNN LM + Continuous Cache,11/1/2019,SOTA improvement,247000000.00000003,7.3e+18,,200,,,,,,WikiText-103,,,,,Unreleased,Open source,Open source,,,,"""GNN-LM achieves a new state-of-the-art perplexity of 14.8 on WikiText-103""",,,,,,,,,"Training code, MIT: https://github.com/urvashik/knnlm"
AlphaStar,10/30/2019,Highly cited,139000000,5.9250000000001e+22,,,1056,384,,125758.09814850632,Google TPU v3,,,,,,Unreleased,Open source,Open source,TRUE,387634.3155427717,Hardware,,"AlphaStar has 139 million weights, but only 55 million weights are required during inference.","384 TPUv3 chips for 44 days. Assume 33% utilization.
https://www.wolframalpha.com/input?i=123+teraFLOPS+*+384+*+0.33+*+44+days",,"Multiple data types. First supervised learning, then other stuff","""Each agent was trained using 32 third-generation tensor
processing units (TPUs) over 44 days""",,,,"Apache 2.0, training tools: https://github.com/google-deepmind/alphastar

training instructions here: https://github.com/google-deepmind/alphastar/blob/main/alphastar/unplugged/README.md "
BART-large,10/29/2019,Highly cited,406291456,,,,,,,,,Wikipedia,,,,,Open weights (unrestricted),Open source,Open source,,,,,"""In total, BART contains roughly 10% more parameters than the equivalently sized BERT model.""

I counted the parameters in the huggingface model
https://huggingface.co/facebook/bart-large/tree/main

from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained(""facebook/bart-large"")
model = AutoModel.from_pretrained(""facebook/bart-large"")
sum(p.numel() for p in model.parameters() if p.requires_grad)",,"""All models are of comparable size and are trained for 1M steps
on a combination of books and Wikipedia data""",,,,,,"Models:
https://github.com/facebookresearch/fairseq/blob/main/examples/bart/README.md

MIT license:
https://github.com/facebookresearch/fairseq/blob/main/LICENSE"
T5-11B,10/23/2019,Highly cited,11000000000,3.3e+22,2E+11,,481.9,512,0.3707,75524.39074218823,Google TPU v3,C4,Confident,,,65536,Open weights (unrestricted),Open source,Open source,TRUE,516885.91216129856,"Reported,Operation counting,Third-party estimation",,The full 11-billion parameter model,"https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf
Table 4, 4.05e22

update: 3.3e22 per FLAN paper from Google 
https://arxiv.org/pdf/2210.11416.pdf

6ND rule suggests somewhat more FLOPs:
6 * 1T * 11B = 6.6e22",,"""This produces a collection of text that is not only
orders of magnitude larger than most data sets used for pre-training (about 750 GB) but also comprises reasonably clean and natural English text. We dub this data set the “Colossal Clean Crawled Corpus” (or C4 for short) and release it as part of TensorFlow Datasets""

750GB * 200M word/GB * 4/3 tokens per word = 2e11.

Total tokens seen is about 1T:  ""We therefore pre-train our models for 1 million steps on a batch size of 2^11 sequences of length 512, corresponding to a total of about 1 trillion pre-training tokens""","4.05*10^22 FLOP at 37.073% utilization on 512 TPU v3 chips (123 TFLOPS) -> 482 hours
https://www.wolframalpha.com/input?i=4.05*10%5E22+seconds+%2F+%28512*123*10%5E12%29+*%28123%2F45.6%29",,,"""We use a maximum sequence length of 512 and a batch size of 128 sequences. Whenever possible, we “pack” multiple sequences into each entry of the batch10 so that our batches contain roughly 2^16 = 65,536 tokens""","Apache for code and weights:
https://github.com/google-research/text-to-text-transfer-transformer

Data is C4 which is open"
T5-3B,10/23/2019,Highly cited,2800000000,9.0000000001e+21,25500000000,0.17,,,,1613.1944624436098,Google TPU v3,C4,Confident,,,,Open weights (unrestricted),Open source,Open source,,,"Third-party estimation,Reported",,"page 37, 3B and 11B. ""To further explore what kind of performance is possible when using larger models, we consider two additional variants. In both cases, we use d_model = 1024, a 24 layer encoder and decoder, and dkv = 128. For the “3B” variant, we use dff = 16,384 with 32-headed attention, which results in around 2.8 billion parameters; for “11B” we use dff = 65,536 with 128-headed attention producing a model with about 11 billion parameters""","Akronomicon states 1.04e+22 FLOP. Archived source: https://github.com/lightonai/akronomicon/tree/main/akrodb
However, this seems dubiously high.

""We pre-train each model for 2^19 = 524,288 steps on C4 before fine-tuning.""
""In total, this batch size and number of steps corresponds to pre-training on 2^35 ≈ 34B tokens.""
""To compare these mixing strategies on equal footing with our baseline pre-train-then-fine-tune results, we train multi-task models for the same total number of steps: 2^19 + 2^18 = 786,432""
Using the 6DN approximation gives: 6 FLOP/token/param * 2^35 pretrain tokens * (1+1/2 finetune tokens per pretrain token) * 1 iteration of training data* 2.8 billion parameters = 8.659e20 FLOP
https://www.wolframalpha.com/input?i=6+*+2%5E35+*+2.8+billion+*+1.5

update: 9.0E+21 per FLAN paper from Google 
https://arxiv.org/pdf/2210.11416.pdf",,"""This produces a collection of text that is not only orders of magnitude larger than most data sets used for pre-training (about 750 GB) but also
comprises reasonably clean and natural English text. We dub this data set the “Colossal Clean Crawled Corpus” (or C4 for short) and release it as part of TensorFlow Datasets""
750GB * 200M word/GB = 1.5e11

""In total, this batch size and number of steps corresponds to pre-training on 2^35 ≈ 34B tokens.""
""Note that 2^35 tokens only covers a fraction of the entire C4 data set, so we never repeat any data during pre-training.""
The fraction is 25.5 billion / 150 billion = 0.17 epochs.",,,,,"Apache for code and weights:
https://github.com/google-research/text-to-text-transfer-transformer

Data is C4 which is open
training script: https://github.com/google-research/text-to-text-transfer-transformer?tab=readme-ov-file#training "
M4-50B,10/11/2019,SOTA improvement,50000000000,,,,,,,,,,Confident,,,,Unreleased,Unreleased,,,,,,"(sparse architecture)

""By modifying the Transformer architecture through the substitution of the vanilla feed-forward layers with sparsely-gated mixture of experts, we drastically scale up the model capacity, allowing us to successfully train and pass 50 billion parameters, which further improved translation quality across the board.""","Sparse architecture, so training compute is uncertain","""we push the limits of research on multilingual NMT by training a single NMT model on 25+ billion sentence pairs, from 100+ languages to and from English, with 50+ billion parameters.""",25+ billion sentence pairs,,,,,
DistilBERT,10/2/2019,Highly cited,66000000,1.24416e+19,,,,,,,,"Wikipedia,BookCorpus (BooksCorpus, Toronto Book Corpus)",,,,,Open weights (unrestricted),Open source,Open source,,,Hardware,,Table 3,"Section 3: DistilBERT was trained on 8 16GB V100 GPUs for approximately 90 hours.

1.6e13*8*60**2*90*0.3 = 1.2e19","Section 3: We train DistilBERT on the same corpus as the original BERT model: a concatenation of English Wikipedia and Toronto Book Corpus [Zhu et al., 2015].",,,,,,"code, including train: https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation

weights: https://huggingface.co/distilbert/distilbert-base-uncased

repo license is apache: https://github.com/huggingface/transformers/blob/main/LICENSE

Wikipedia is open, BookCorpus is not"
AlphaX-1,10/2/2019,SOTA improvement,579000000,7.6e+18,1480000,,,,,,NVIDIA GeForce GTX 1080 Ti,"ImageNet,COCO",,,,,Unreleased,Open (non-commercial),Open access (non-commercial),,,,"""In 12 GPU days and 1000 samples, AlphaX found an architecture that reaches 97.84\% top-1 accuracy on CIFAR-10, and 75.5\% top-1 accuracy on ImageNet, exceeding SOTA NAS methods in both the accuracy and sampling efficiency""","Table 3: multiadds for AlphaX-1 579M, parameters 5.4M",,"12800000 + 200000=1480000
I assume they used 1,281,167 training images when referred to Imagenet and 200 000 when referred to MS COCO

""We set up the ImageNet training using
the standard mobile configuration with the input image size
of (224 × 224)[45]. More details are available in the appendix. AlphaX sampled 1000 networks, and we selected
the top 20 networks in the pre-training to fine-tune another
530 epochs. 

We use AlphaX-1 model pre-trained on ImageNet
dataset. The training dataset is MSCOCO for object
detection[15] which contains 90 classes of objects. Each
image is scaled to 300 × 300 in RGB channels. We trained
the model with 200k iterations with 0.04 initial learning rate
and the batch size is set to 24. We applied the exponential learning rate decay schedule with the 0.95 decay factor. Our
model uses momentum optimizer with momentum rate set
to 0.9. We also use the L2 weight decay for training. We
process each image with random horizontal flip and random
crop[22]. We set the matched threshold to 0.5, which means
only the probability of an object over 0.5 is effective to appear on the image. We use 8000 subsets of validation images in MSCOCO validation set and report the mean average precision (mAP) as computed with the standard COCO metric library[16].""
",,,,,,"code, no license specified: https://github.com/linnanwang/AlphaX-NASBench101
training: https://github.com/linnanwang/AlphaX-NASBench101/blob/master/net_training.py "
ALBERT,9/26/2019,Highly cited,18000000,,3300000000,79.4,,,,,Google TPU v3,"BookCorpus (BooksCorpus, Toronto Book Corpus),Wikipedia",,,,2097152,Open weights (unrestricted),Open source,Open source,,,,,Section 3.2 of paper,,,"Pretraining same as for BERT - Wikipedia and BookCorpus

""For the pre-training corpus we
use the BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2,500M words)""",,,,,Apache 2.0: https://github.com/google-research/ALBERT
Adaptive Inputs + LayerDrop,9/25/2019,SOTA improvement,423000000.00000006,,,,,,,,,WikiText-103,,,,,Open weights (unrestricted),Open source,Open source,,,,"""In neural machine translation on newstest2014, our 12 encoder layer Transformer model with LayerDrop further improves the state of the art, reaching 30.2 BLEU""",,,,,,,,,"https://github.com/facebookresearch/fairseq/blob/main/examples/layerdrop/README.md
Repo has MIT license
WT training: https://github.com/facebookresearch/fairseq/tree/main/examples/language_model "
Megatron-LM (8.3B),9/17/2019,"Highly cited,SOTA improvement",8300000000,9.1e+21,34800000000,4.4,327,512,0.2269,106142.2892017932,NVIDIA Tesla V100 DGXS 32 GB,,Likely,,,,Unreleased,Open source,Unreleased,TRUE,287273.8627260526,"Hardware,Operation counting,Third-party estimation","""Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA"" 

GPT-2 model here meaning model similar to GPT-2","Source: https://lair.lighton.ai/akronomicon/

Archived source: https://web.archive.org/web/20211220142906/https://lair.lighton.ai/akronomicon/

Data also available on GitHub: https://github.com/lightonai/akronomicon/blob/main/akrodb/NVIDIA/Megatron-LM.json","source: https://lair.lighton.ai/akronomicon/

archived: https://github.com/lightonai/akronomicon/tree/main/akrodb


other estimates:

8.3B is a GPT-2-based model (Table 2). ""For GPT-2 models, all training is performed with sequences of 1024 subword units at a batch size of 512 for 300k iterations"" 

I interpret the above as 1024*512*300k = 157B training tokens 

6 * 157 billion * 8.3 billion  = 7.8e21

Also, their training setup achieved 15.1 petaFLOPS or 1.5e16 FLOPS.
(512 V100s is 512 * 125 teraflops = 64 petaFLOPS so they had ~25% utilization)
2.1 days per epoch, ~4.4 epochs
2.1 * 4.4 * 24 * 3600 * 1.5e16 = 1.197e22

These are both close to the akronomicon estimate","""we aggregate several of the largest language
modeling datasets. We create an aggregate dataset consisting of Wikipedia (Devlin et al., 2018), CC-Stories (Trinh &
Le, 2018), RealNews (Zellers et al., 2019), and OpenWebtext (Radford et al., 2019). To avoid training set leakage
into our downstream tasks we remove the Wikipedia articles
present in the WikiText103 test set (Merity et al., 2016).""","""The resulting aggregate
corpus contains 174 GB of deduplicated text.""","Reported throughput is 15.1 teraFLOPS per GPU on 512 GPUs
Assume total compute is 9.1e21 FLOP.
Then training time is 327 hours.
https://www.wolframalpha.com/input?i=9.1*10%5E21+FLOP+%2F+%28512*15.1+teraFLOPS%29","327 hours * 512 GPUs * $0.55/V100 GPU-hour = $92,083
Convert to 2020 dollars: $78,689",,,"code (2.5B model is a GPT model): https://github.com/NVIDIA/Megatron-LM?tab=readme-ov-file#megatron-overview  
open license: https://github.com/NVIDIA/Megatron-LM?tab=License-1-ov-file#readme "
Megatron-BERT,9/17/2019,"Highly cited,SOTA improvement",3900000000,2.2e+22,46400000000,,374,512,0.2269,,NVIDIA Tesla V100S PCIe 32 GB,,Confident,,,524288,Unreleased,Open source,Unreleased,TRUE,287273.8627260526,"Operation counting,Third-party estimation","""Our BERT model achieves SOTA results on the RACE dataset""","2.1Source: https://lair.lighton.ai/akronomicon/

Archive on GitHub: https://github.com/lightonai/akronomicon/tree/main/akrodb","A third-party source: https://lair.lighton.ai/akronomicon/ claims 5.7e22

The authors report experimenting on 1 V100 GPU and achieving throughput of 39 TFLOPS which is 30% of the peak throughput. Therefore the GPU has a peak throughput of 130 TFLOPS so it is specifically the NVIDIA V100S PCIe.
https://images.nvidia.com/content/technologies/volta/pdf/volta-v100-datasheet-update-us-1165301-r5.pdf

Param-based calculation:
6ND = 6*3.9e9*(2e6+1e4)*1024*512 = 2.5e22 FLOP

1024 is the batch size, 512 is the sequence length (not explicitly stated but they say non-specified hyperparameters follow cited papers).

Time-based calculation:
The 8.3B GPT-like arch took 2.1 days per epoch on 512 GPUs, batch size 512. An epoch was 68.5k iterations with sequence length 1024.

Halving the model size should ~halve the iteration time.
Doubling the batch size should ~double the iteration time.
Halving the sequence length should ~quarter the iteration time (quadratic scaling).

Hence 3.1e-5 days/iteration * 2 * 1/2 * 1/4 = 7.8e-6 days/iteration.

2e6 iterations => seems like 15.6 days training.

On 512 GPUs they achieve a peak throughput of 15.1 PFLOPS.
C=15.1 PFLOPS * 58 days = 2.0e22 FLOP.

If we disregard the Akronomicon estimate and aggregate our two, geometric mean is 2.2e22 FLOP",,"""The resulting aggregate corpus contains 174 GB of deduplicated text.""
174e9 bytes * (1 word / 5 bytes) * (4 tokens / 3 words) = 4.64e10 tokens","The 8.3B GPT-like arch took 2.1 days per epoch on 512 GPUs, batch size 512, sequence length 1024. An epoch was 68.5k iterations.

BERT: batch size 1024, sequence length 512, 2e6 iterations total.

Halving the model size should ~halve the iteration time.
Doubling the batch size should ~double the iteration time.
Halving the sequence length should ~quarter the iteration time (quadratic scaling).

Hence 3.1e-5 days/iteration * 2 * 1/2 * 1/4 = 7.8e-6 days/iteration.

2e6 iterations => seems like 15.6 days training.",,,"""we set the batch size to 1024 and use a learning rate of 1.0e4 warmed up over 10,000 iterations and decayed linearly
over 2 million iterations. Other training parameters are kept
the same as (Devlin et al., 2018).""

in Devlin et al (BERT), sequences are 512 tokens","training code 

https://github.com/NVIDIA/Megatron-LM/blob/main/pretrain_bert.py 

MIT-like license:
https://github.com/NVIDIA/Megatron-LM/blob/main/LICENSE "
ResNet-152 + ObjectNet,9/6/2019,Highly cited,38000000,1.94e+19,50000,,,,,,,ObjectNet,,,,,Unreleased,Unreleased,,,,Hardware,,,"3-5 days of training (say, 4.5), 50 teraFLOP/second at 50% utilization rate (reported) = 1.94E19",,"In total, 95,824 images were collected from 5,982 workers out of which 50,000 images were retained
after validation and included in the dataset",,,,,
UDSMProt,9/4/2019,SOTA improvement,28303800,6.37e+17,,30,,,,,,"SwissProt,a subset of UniProtKB",Likely,,,,Open weights (unrestricted),Open source,Open source,,,Operation counting,"""The proposed method performs on par with state-of-the-art algorithms that were tailored to these specific tasks or, for two out of three tasks, even outperforms them.""","Python code:  
# Given LSTM parameters
emb_sz = 400  # embedding size, typically equal to the input size for the first layer
nh = 1150     # number of hidden units
nl = 3        # number of layers

# The formula for a single LSTM layer parameters is:
# P = 4 * ((input_dim + hidden_dim) * hidden_dim + hidden_dim)

# First layer parameters (input_dim is the embedding size)
first_layer_params = 4 * ((emb_sz + nh) * nh + nh)

# For subsequent layers, input_dim is equal to hidden_dim (nh)
subsequent_layer_params = 4 * ((nh + nh) * nh + nh)

# Total parameters for all layers
total_params = first_layer_params + (nl - 1) * subsequent_layer_params

print(total_params)","Pretraining:
Table 7 gives max of 499k sequences each at (seemingly) L=1024:
499k * 1024 * 28.3M * 6 = 8.7e16

Finetuning:
Largest downstream task has 104940 sequences (Table 5), each sequence has L=1024 residues, 28.3M parameters, and 30 epochs.
105k * 1024 * 30 * 28.3 * 6 = 5.5e17.",,560K proteins,,,,,"BSD license, models and code
https://github.com/nstrodt/UDSMProt "
"Mogrifier (d2, MoS2, MC) + dynamic eval",9/4/2019,SOTA improvement,35000000,,,145,,,,,,WikiText-2,,,,,Unreleased,Unreleased,,,,,"""We establish a new state of the art on all datasets with the exception of Enwik8""",,,,,,,,,Github has dead link: https://github.com/google-deepmind/lamb/blob/master/experiment/mogrifier/README.md
EN^2AS with performance reward,7/22/2019,SOTA improvement,23000000,,,,,,,,,,,,,,Unreleased,Unreleased,,,,,"""The best architecture obtained by our algorithm with
the same search space achieves the state-of-the-art test error rate of 2.51% on CIFAR-10""",,,,,,,,,
Pluribus,7/11/2019,SOTA improvement,,6.6e+16,,,,,,,,,,,,,Unreleased,Unreleased,,,,Hardware,"first to beat humans at multiplayer poker: ""Developing a superhuman AI for multiplayer poker was the widely,recognized main remaining milestone. In this paper we describe Pluribus, an AI capable of defeating elite human professionals in six-player no-limit Texas hold’em poker, the most commonly played poker format in the world.""",,"Trained in 8 days on a 64 core CPU
https://ai.facebook.com/blog/pluribus-first-ai-to-beat-pros-in-6-player-poker/

""We trained the blueprint strategy for Pluribus in eight days on a 64-core server and required less than 512 GB of RAM. No GPUs were used. At typical cloud computing instance rates, it would cost less than $150 to train.""

Guess: trained on i7 Intel CPU, approx 5e9 FLOP/s for each core.

 https://epoch.ai/blog/estimating-training-compute
8 days, 64 cores, 5e9 FLOP/s, 30% utilization",,,,,,,
BigBiGAN,7/4/2019,SOTA improvement,86000000,,,,,,,,,ImageNet,,,,,Open weights (unrestricted),Unreleased,,,,,"""BigBiGAN, an unsupervised learning approach based purely on generative models, achieves state-of-the-art results in image representation learning on ImageNet""",https://openai.com/blog/image-gpt/#rfref53,,"""We train a BigBiGAN on unlabeled ImageNet, freeze its learned
representation, and then train a linear classifier on its outputs, fully supervised using all of the training
set labels""",,,,,,"model (Apache 2.0 license): https://www.kaggle.com/models/deepmind/bigbigan

they share a notebook but with a broken link"
RoBERTa Large,7/1/2019,Highly cited,355000000,8.5067e+21,32000000000,,120,1024,,82771.07593250347,NVIDIA Tesla V100 DGXS 32 GB,"CC-News,BookCorpus (BooksCorpus, Toronto Book Corpus),WebText2,Wikipedia",Confident,,,,Open weights (unrestricted),Open source,Open source,TRUE,575049.4486114612,"Hardware,Operation counting",,,"Section 5: We pretrain our model using 1024 V100 GPUs for approximately one day.

Note this is the base pretraining comparable to BERT, 100k steps. Subsequently they do more: ""increasing the number of pretraining steps
from 100K to 300K, and then further to 500K"".

So assume 5x the 1024 V100 GPUs for 1d estimate. Mixed precision tensor cores get 1.25e14 FLOP/s.

1024 * 1.25e14 * 5 * 24 * 3600 * 0.3 = 1.65888e22

6ND estimate: batches are 8k sequences of 512 tokens; 500k updates means the model saw 500k * 8k * 512 = 2.048T tokens
6 * 2.048T * 355M = 4.36224e21

geometric mean: sqrt(1.65888e22 * 4.36224e21) = 8.5067e21","""We consider five English-language corpora of
varying sizes and domains, totaling over 160GB
of uncompressed text. We use the following text
corpora:
• BOOKCORPUS (Zhu et al., 2015) plus English
WIKIPEDIA. This is the original data used to
train BERT. (16GB).
• CC-NEWS, which we collected from the English portion of the CommonCrawl News
dataset (Nagel, 2016). The data contains 63
million English news articles crawled between
September 2016 and February 2019. (76GB after filtering).4
• OPENWEBTEXT (Gokaslan and Cohen, 2019),
an open-source recreation of the WebText corpus described in Radford et al. (2019). The text
is web content extracted from URLs shared on
Reddit with at least three upvotes. (38GB).5
• STORIES, a dataset introduced in Trinh and Le
(2018) containing a subset of CommonCrawl
data filtered to match the story-like style of
Winograd schemas. (31GB).""",160GB*200M words/GB * (4 words / 3 tokens) = 3.2e10 tokens,"First the model is pretrained for 100k steps on 1024 GPUs for 1 day, then pretraining is increased to 500k steps, so assuming they used the same number of GPUs, this would have taken 5 days.",,,,"code and weights: https://github.com/facebookresearch/fairseq/blob/main/examples/roberta/README.md
pretrain code: https://github.com/facebookresearch/fairseq/blob/main/examples/roberta/README.pretraining.md 

repo is MIT license

"
Tensorized Transformer (257M),6/24/2019,SOTA improvement,257000000,4.76e+18,,30,,,,,,WikiText-103,,,,,Unreleased,Open (non-commercial),Open access (non-commercial),,,,"""Table 2: Results and compression with state-of-the-art results on PTB and WikiText-103""",,,,,,,,,"code, no license: https://github.com/szhangtju/The-compression-of-Transformer"
Walking Minotaur robot,6/19/2019,SOTA improvement,,,,,,,,,,,Unknown,,,,Unreleased,Unreleased,,,,,,,,,,,,,,
LaNet-L (CIFAR-10),6/17/2019,SOTA improvement,44100000,,60000,600,,,,,,CIFAR-10,Confident,,,,Open weights (non-commercial),Open (non-commercial),Open access (non-commercial),,,,"""In practice, LaNAS finds a network that achieves SOTA 99.0% accuracy on CIFAR-10""",44.1M,"LaNet-L was trained on 150 GPU-days, however the GPU was not specified","Trained on CIFAR-10, no pretraining",,,,,,"code and weights here, non-commercial license: https://github.com/facebookresearch/LaMCTS/tree/main/LaNAS/LaNet/CIFAR10"
PG-SWGAN,6/15/2019,SOTA improvement,,,,,,,,,,"CIFAR-10,LSUN,CelebA",Unknown,,,,Unreleased,Open (non-commercial),Open access (non-commercial),,,,"""For fair comparison, we equip the same progressive growing architecture with our proposed SWGAN objective and its dual
SWD blocks (PG-SWGAN). As shown in Fig. 3 (Right)
and Fig. 5, our PG-SWGAN can outperform PG-WGAN in
terms of both qualitative and quantitative comparison on the
CelebA-HQ and LSUN datasets""",,,,,,,,,"looks like code but no weights, no license specified: https://github.com/musikisomorphie/swd"
FixRes ResNeXt-101 WSL,6/14/2019,SOTA improvement,829000000,,940000000,,,,,,,ImageNet,,,,,Open weights (non-commercial),Open (non-commercial),Open access (non-commercial),,,,"""To the best of our knowledge our ResNeXt-101 32x48d surpasses all other models available in the literature""",,,,"""Conversely, when training a ResNeXt-101 32x48d pre-trained in weakly-supervised fashion on 940 million public images at resolution 224x224 and further optimizing for test resolution 320x320, we obtain a test top-1 accuracy of 86.4% (top-5: 98.0%) (single-crop)""",,"https://medium.com/swlh/deepmind-achieved-starcraft-ii-grandmaster-level-but-at-what-cost-32891dd990e4#:~:text=According%20to%20the%20analysis%20by,Source%3A%20DeepMind.",,,code/weights with non-commercial license: https://github.com/facebookresearch/FixRes?tab=License-1-ov-file#readme
Char-CNN-BiLSTM,6/13/2019,SOTA improvement,,,,,,,,,,,Unknown,,,,Unreleased,Unreleased,,,,,"""Notably, our language model achieves a test perplexity of 37.49 on PTB, which to our knowledge is state-of-the-art among models trained only on PTB.""",,,,,,,,,
AWD-LSTM + MoS + Partial Shuffled,6/10/2019,SOTA improvement,35000000,3.28e+17,,750,,,,,,WikiText-2,,,,,Open weights (non-commercial),Open (non-commercial),Open access (non-commercial),,,,"""our method improves on the single model state-of-the-art results for language modeling on Penn Treebank (PTB) and Wikitext-2, achieving test perplexity scores of 46.01 and 38.07, respectively""",,,,,,,,,"code and weights. no license provided:
https://github.com/ChengyueGongR/advsoft"
Transformer-XL Large + Phrase Induction,6/4/2019,SOTA improvement,257000000,1.1058826e+19,,1,,,,,,WikiText-103,,Transformer-XL (257M),,,Unreleased,Open source,Open source,,,,"""We achieved a new state-of-the-art performance of 17.4 perplexity on the Wikitext-103 dataset""",,"Fine-tuned from pre-trained Transformer-XL Large (1.09e19 FLOP). Additional 1.6e17 FLOP of fine-tuning from one epoch on WikiText-103:
6 * 257M * 103M = 1.588e17.

Total: 1.11e19",,,,,,,"code license, BSD-3: https://github.com/luohongyin/PILM?tab=BSD-3-Clause-1-ov-file#readme

training: https://github.com/luohongyin/PILM/blob/master/train_span_wt103.sh "
AMDIM,6/3/2019,Highly cited,626000000,,,,,,,,,"ImageNet,CIFAR-10",,,,,Open weights (unrestricted),Open source,Open source,,,,,source: https://openai.com/blog/image-gpt/#rfref13e,,"""We evaluate our model using standard datasets: CIFAR10, CIFAR100, STL10 [Coates et al., 2011], ImageNet1 [Russakovsky et al., 2015], and Places205 [Zhou et al., 2014].""",,,,,,MIT: https://github.com/Philip-Bachman/amdim-public
XLNet,6/1/2019,Highly cited,340000000,6.19e+21,,63.76260261,,,,,Google TPU v3,"Wikipedia,BookCorpus (BooksCorpus, Toronto Book Corpus)",Confident,,,8192,Open weights (unrestricted),Open source,Open source,TRUE,,"Hardware,Operation counting",,"Same size as BERT-Large, which was 340M","""Specifically, we train on 512 TPU v3 chips for 500K steps with an Adam weight decay optimizer, linear learning rate decay, and a batch size of 8192, which takes about 5.5 days.""

123 teraflops * 5.5 days * 24 * 3600 * 512 * 0.3 utilization (assumption) ~= 8977858560*10^12=8.9*10^21

Alternatively, 500k steps * batch size 8192 * sequence length 512 = 2.1T training passes. 340 million * 6 * 2 trillion = 4.3e21 FLOP. 

Geometric mean: sqrt(8.9e21 * 4.3e21) = 6.19e21","""Following BERT [10], we use the BooksCorpus [40] and English Wikipedia as part of our pretraining
data, which have 13GB plain text combined. In addition, we include Giga5 (16GB text) [26],
ClueWeb 2012-B (extended from [5]), and Common Crawl [6] for pretraining. We use heuristics
to aggressively filter out short or low-quality articles for ClueWeb 2012-B and Common Crawl,
which results in 19GB and 110GB text respectively. After tokenization with SentencePiece [17], we
obtain 2.78B, 1.09B, 4.75B, 4.30B, and 19.97B subword pieces for Wikipedia, BooksCorpus, Giga5,
ClueWeb, and Common Crawl respectively, which are 32.89B in total.""",,,,,,Apache 2.0 for code and weights: https://github.com/zihangdai/xlnet
XLM,6/1/2019,"Highly cited,SOTA improvement",665000000,,,,,,,,,,,,,,Open weights (non-commercial),Open (non-commercial),Open source,,,,"""On supervised machine translation, we obtain a new state of the art of 38.5 BLEU on WMT’16 Romanian-English, outperforming the previous best approach by more
than 4 BLEU""",,,"subset of Wikipedia: ""We use WikiExtractor2
to extract raw sentences
from Wikipedia dumps and use them as monolingual data for the CLM and MLM objectives.""",,,,,,weights/code non-commercial: https://github.com/facebookresearch/XLM?tab=License-1-ov-file#readme
DLRM-2020,5/31/2019,SOTA improvement,1E+11,4e+18,,,,,,,,,,,,,Unreleased,Open source,Open source,,,Reported,"""In this paper, we develop a state-of-the-art deep learning recommendation model
(DLRM)""","Figure 1

https://arxiv.org/abs/2104.05158","Figure 1

https://arxiv.org/abs/2104.05158",,,,,,,"MIT, training/inference code: https://github.com/facebookresearch/dlrm"
MnasNet-A3,5/29/2019,Highly cited,5200000,1.5e+21,1280000,,108,256,,9551.591619865148,Google TPU v3,ImageNet,Speculative,,,,Open weights (unrestricted),Open source,Open source,,258868.3859107431,Hardware,,From https://arxiv.org/pdf/1807.11626.pdf,"""each architecture search takes 4.5 days on 64 TPUv2 devices""
This seems to be referring to a TPUv2 pod, consisting of 64 four-chip modules. The total performance is 11.5 petaFLOPS.
https://en.wikipedia.org/wiki/Tensor_Processing_Unit#Second_generation_TPU
Assuming a 33% utilization rate:

4.5 days * 64 * 180 teraFLOPS * 0.33 = 1.48*10^21 FLOP

However, it is unclear if ""64 TPUv2 devices"" refers to chips or modules, so the true compute might be 1/4 of this amount.",,"""In this paper, we directly perform our architecture search on the ImageNet training set but with fewer training steps (5 epochs). As a common practice, we reserve randomly selected 50K images from the training set as the fixed validation set. """,,,,,"Apache license: https://github.com/tensorflow/tpu/blob/master/LICENSE
model repo is here, includes training code: https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet 

"
MnasNet-A1 + SSDLite,5/29/2019,Highly cited,4900000,1.5e+21,118000,,108,256,,9551.591619865148,Google TPU v3,COCO,Speculative,,,,Open weights (unrestricted),Open source,Open source,,258868.3859107431,Hardware,,From https://arxiv.org/pdf/1807.11626.pdf,"""each architecture search takes 4.5 days on 64 TPUv2 devices""
This seems to be referring to a TPUv2 pod, consisting of 64 four-chip modules. The total performance is 11.5 petaFLOPS.
https://en.wikipedia.org/wiki/Tensor_Processing_Unit#Second_generation_TPU
Assuming a 33% utilization rate:

4.5 days * 64 * 180 teraFLOPS * 0.33 = 1.48*10^21 FLOP

However, it is unclear if ""64 TPUv2 devices"" refers to chips or modules, so the true compute might be 1/4 of this amount.",,,,,,,"Apache license: https://github.com/tensorflow/tpu/blob/master/LICENSE
model repo is here, includes training code: https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet 

"
EfficientNet-L2,5/28/2019,Highly cited,480000000,,,,,,,,,ImageNet,,,,,Open weights (unrestricted),Open source,Open source,,,,,,,,,,,,,Apache license: https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet
CPC v2,5/22/2019,SOTA improvement,303000000,,,,,,,,,ImageNet,,,,,Unreleased,Unreleased,,,,,"""this unsupervised representation substantially improves transfer learning to object detection on the
PASCAL VOC dataset, surpassing fully supervised pre-trained ImageNet classifiers""",source: https://openai.com/blog/image-gpt/#rfref25d,,"""In all cases, the dataset of unlabeled images Du we pre-train
on is the full ImageNet ILSVRC 2012 training set""",,,,,,
AWD-LSTM-DRILL + dynamic evaluation† (WT2),5/14/2019,SOTA improvement,34000000,4.24e+17,,1000,,,,,,WikiText-2,,,,,Open weights (unrestricted),Open (restricted use),Open access (restricted use),,,,"""our models improve over the state-of-the-art by +1.6 perplexity on PennTreebank and by +3.9 perplexity on
Wikitext-2""",,,,,,,,,"copyleft license (restricts derivative works to be open)
https://github.com/idiap/drill?tab=GPL-3.0-1-ov-file#readme

train/eval script: https://github.com/idiap/drill/blob/master/main.py "
ResNeXt-101 Billion-scale,5/2/2019,SOTA improvement,193000000,,,,,,,,,YFCC-100M,,,,,Open weights (non-commercial),Unreleased,,,,,"""We demonstrate the performance of our method on popular classification benchmarks for both images and videos and significantly outperforms the state of the art.""",,,,,,,,,"non-commercial for weights: 
https://github.com/facebookresearch/semi-supervised-ImageNet1K-models"
ResNet-50 Billion-scale,5/2/2019,Highly cited,25000000,,1090000000,,,,,,,"YFCC-100M,IG-1B-Targeted",,,,,Open weights (non-commercial),Unreleased,,,,,,25M parameters vanilla ResNet50,,"""The following web-scale datasets are used for
semi-supervised learning experiments involving an unlabeled dataset U.
• YFCC-100M [38] is a publicly available dataset of about
90 million images from Flickr website with associated
tags. After removing duplicates, we use this data for
most experiments and ablations.
• IG-1B-Targeted: Following [27], we collected a dataset
of 1B public images with associated hashtags from a
social media website. We consider images tagged with
at least one of the 1500 hashtags associated with one of
the 1000 ImageNet-1k classes.""","1 billion + 90 million, per above",,,,,non-commercial for weights: https://github.com/facebookresearch/semi-supervised-ImageNet1K-models
Neuro-Symbolic Concept Learner,4/26/2019,SOTA improvement,,,,,,,,,,"CLEVR,VQS,ImageNet",Unknown,,,,Unreleased,Open source,Open source,,,,"""NS-CL’s modularized design enables interpretable, robust, and accurate visual reasoning: it achieves state-of-the-art performance on the CLEVR datase""",,,"CLEVR, ImageNet, VQS
5000 in CLEVR
64509 in VQS
and whole ImageNet for pretraining
""We train NS-CL on 5K images (<10% of CLEVR’s 70K training images). We generate 20 questions for each image for the entire curriculum learning process""

section 4.3 ""All models use a pre-trained semantic parser on the full CLEVR dataset""

""The only extra supervision of the visual perception module comes from the pre-training of the perception modules on ImageNet (Deng et al., 2009). To quantify the influence of this pre-training""

In appendix G.2 (VQS Dataset):""All models are trained on the first 63,509 images of the training set, and tested on the test split. For hyper-parameter tuning and model selection, the rest 5,000 images from the training set are used for validation.","CLEVR, ImageNet, VQS
5000 in CLEVR
64509 in VQS
and whole ImageNet for pretraining
""We train NS-CL on 5K images (<10% of CLEVR’s 70K training images). We generate 20 questions for each image for the entire curriculum learning process""

section 4.3 ""All models use a pre-trained semantic parser on the full CLEVR dataset""

""The only extra supervision of the visual perception module comes from the pre-training of the perception modules on ImageNet (Deng et al., 2009). To quantify the influence of this pre-training""

In appendix G.2 (VQS Dataset):
""All models are trained on the first 63,509 images of the training set, and tested on the test split. For hyper-parameter tuning and model selection, the rest 5,000 images from the training set are used for validation.",,,,,MIT code: https://github.com/vacancy/NSCL-PyTorch-Release
DANet,4/21/2019,Highly cited,,,,,,,,,,"Cityscapes,COCO-Stuff,PASCAL-Context",Unknown,,,,Open weights (unrestricted),Open source,Open source,,,,,,,,,,,,,"MIT for code and weights: https://github.com/junfu1115/DANet/
train code: https://github.com/junfu1115/DANet/tree/master/experiments/recognition "
BERT-Large-CAS (PTB+WT2+WT103),4/20/2019,SOTA improvement,395000000,5.21e+20,,50,,,,,,"Penn TreeBank,WikiText-2,WikiText-103",,,,,Unreleased,Open source,Open source,,,,"""CAS achieves perplexities between 20.42 and 34.11 on all problems, i.e. on average an improvement of 12.0 perplexity units compared to state-of-the-art LSTMs""",,,,,,,,,Apache 2.0 license: https://github.com/cgraywang/gluon-nlp-1/blob/lmtransformer/scripts/language_model/train/transformer_lm.py
SpecAugment,4/18/2019,Highly cited,,,,,,,,,,"LibriSpeech,Switchboard,Fisher",Unknown,,,,Unreleased,Unreleased,,,,,,,,,,,,,,LibriSpeech is open source
Transformer-XL + RMS dynamic eval,4/17/2019,SOTA improvement,257000000,,,,,,,,,WikiText-103,,,,,Unreleased,Open source,Open source,,,,"""By applying dynamic evaluation to Transformer-XL models, we improve the state of the art on enwik8 from 0.99 to 0.94 bits/char, text8 from 1.08 to 1.04 bits/char, and WikiText-103 from 18.3 to 16.4 perplexity points.""",,,,,,,,,"Apache for code: https://github.com/benkrause/dynamiceval-transformer
wt103 train script: https://github.com/benkrause/dynamiceval-transformer/blob/master/tf/sota/wt103.sh "
WeNet (Penn Treebank),4/8/2019,SOTA improvement,23000000,7.30000001e+17,,6000,24,1,,,NVIDIA V100,Penn TreeBank,Likely,,,70000,Unreleased,Unreleased,,,674.5252054193401,"Hardware,Operation counting","""We show that an architecture found by WeNets arXiv:1904.03819v1 [cs.NE] 8 Apr 2019 WeNet: Weighted Networks for Recurrent Network Architecture Search achieves state-of-the-art results on the Penn Treebank language dataset""",Table 1,"PTB has 912344 tokens. The model has 23M parameters and was trained for 6k epochs. If the model was dense, 6 FLOP/token/param/epoch * 6k epochs * 23M params * 912k tokens = 1.05e18 FLOP.

Alternatively, the model was trained on 1 V100 GPU and ""In terms of efficiency, the overall cost... is within 1 GPU day"" so the training time was around or below 24 hours. Half precision and 30% utilization would be a pretty good match for the arithmetic estimate: 24 hours * 30% * 28 TFLOPS = 7.3e17 FLOP.",,,,,,"They use BPTT with length 35. During architecture search data batch size is 20 and network batch size is 100. While training the architecture they end up finding, batch size is 64. So effective batch size is 35 * 20 * 100 = 70,000 during architecture search, and 35 * 64 = 2,240 during final training. ",PTB dataset
True-Regularization+Finetune+Dynamic-Eval,4/8/2019,SOTA improvement,7000000,,,,,,,,,Penn TreeBank,,,,,Unreleased,Unreleased,,,,,"""In the first experiment, the student model achieves state-of-the-art perplexity results on the Penn Treebank dataset [1] with a model size one third of that of the
previously published best model""",,,,,,,,,
Cross-lingual alignment,4/4/2019,SOTA improvement,,2.56e+18,,,,,,,NVIDIA GeForce GTX 1080 Ti,"Wikipedia,CoNLL2017",,ELMo,,,Open weights (unrestricted),Open source,Open source,,,Hardware,"""our method consistently outperforms the previous state-of-the-art on 6 tested languages""",,"From author communication:

Precision: float32

Hardware: 4 GPU  NVIDIA 1080Ti

NVIDIA 1080Ti: 1.06E+13

Compute: 7 GPU-days

0.4 * 1.06E+13 FLOP/s * 7 days * 24h/day * 3600s/h
= 2.56E+18",,,,,,,"MIT license
https://github.com/TalSchuster/CrossLingualContextualEmb"
FAIRSEQ Adaptive Inputs,4/1/2019,Highly cited,247000000.00000003,7.3e+18,,,,,,,,WikiText-103,,,,,Unreleased,Open source,Open source,,,,,,,,,,,,,"weights and training, Repo is MIT-licensed
https://github.com/facebookresearch/fairseq/blob/main/examples/language_model/README.adaptive_inputs.md "
SciBERT,3/26/2019,"Highly cited,SOTA improvement",110000000,8.926848e+19,3300000000,,168,4,,247.26289010271603,Google TPU v3,,Confident,,,,Open weights (unrestricted),Open source,Open source,,4047.748042705093,Hardware,"""We demon-
strate statistically significant improvements
over BERT and achieve new state-of-the-
art results on several of these tasks""","110M
size of bert base from https://huggingface.co/google-bert/bert-base-uncased
relevant citation: 
""We use the original BERT code to
train SCIBERT on our corpus with the same con-
figuration and size as BERT-Base. We train 4
different versions of SCIBERT: (i) cased or un-
cased and (ii) BASEVOCAB or SCIVOCAB. The
two models that use BASEVOCAB are finetuned
from the corresponding BERT-Base models. The
other two models that use the new SCIVOCAB are
trained from scratch.""","4*123e12*0.3*(7*24*3600) = 8.926848e+19
(num gpu) * (peak compute) * (assumed utilization rate) * (time in seconds)
We have:
 4 TPUv3 chips.123teraFLOPS per chip.
7 days of training
""We use a single TPU v3 with 8 cores. Training the SCIVOCAB models from scratch on our corpus takes 1 week (5 days with max length 128, then 2 days with max length 512). ""

If this compute estimate is accurate and BERT is approximately dense, then C=6eND -> e=C/6ND ~= 40 epochs.","""We train SCIBERT on a random
sample of 1.14M papers from Semantic
Scholar (Ammar et al., 2018). ""","""The average paper length is 154 sentences (2,769 tokens) resulting in a corpus size of 3.17B tokens, similar to the 3.3B tokens on which BERT was trained.""",1 week,,,,"apache 2.0, code and weights: https://github.com/allenai/scibert/"
NMT Transformer 437M,2/28/2019,SOTA improvement,437700000,,,,,,,,,,Confident,,,,Unreleased,Unreleased,,,,,"""We report results on the publicly available TED talks multilingual corpus where we show that massively multilingual many-to-many models are effective in low resource settings, outperforming the previous state-of-the-art while supporting up to 59 languages.""","""Regarding the model, for these experiments we
use a larger Transformer model with 6 layers in
both the encoder and the decoder, model dimension set to 1024, hidden dimension size of 8192,
and 16 attention heads. This results in a model
with approximately 473.7M parameters.""",,"""Since we are not aware of a publicly available resource for this purpose, we construct an in-house
dataset. This dataset includes 102 language pairs
which we “mirror” to-and-from English, with up
to one million examples per language pair. This
results in 103 languages in total, and 204 translation directions which we train simultaneously.""

96M total examples, per Table 4","96M total examples, per Table 4. One sentence per example?",,,,,
KataGo,2/27/2019,SOTA improvement,2500000,2.32e+19,241000000,,456,,,104.91425851608678,NVIDIA Tesla V100 DGXS 16 GB,,Speculative,,,,Open weights (unrestricted),Open source,Open source,,,Hardware,Better than ELF OpenGo while using 1/50th the compute.,https://arxiv.org/abs/2210.00849 gives parameter count for AlphaZero in Fig 1b.,"""[KataGo] surpasses the strength of ELF OpenGo after training on about 27 V100 GPUs for 19 days""
14.13 teraFLOP/s * 19 days = 2.32e+19 FLOP","Self-play: ""In total, KataGo’s main run lasted for 19 days using a maximum of 28 V100 GPUs at any time (averaging 26-27) and generated about 241 million training samples across 4.2 million games.""",241 million training samples across 4.2 million games,27 processors for 19 days,,,,"permissive license https://github.com/lightvector/KataGo/blob/master/LICENSE

training here: https://github.com/lightvector/KataGo/blob/master/SelfplayTraining.md "
ProxylessNAS,2/23/2019,Highly cited,,3.70656e+19,1280000,,,,,122.74110657965193,NVIDIA V100,ImageNet,,,,,Open weights (unrestricted),Open source,Open source,,,Hardware,,,"For their searched Imagenet models, they used 200 GPU hours on a V100 GPU.

At FP32, a V100 GPU has a peak performance of 1.56E+14 FLOPS.

Utilization rate of 0.33.",,,,,,,MIT for code+weights https://github.com/MIT-HAN-LAB/ProxylessNAS
GPT-2 (1.5B),2/14/2019,Highly cited,1500000000,1.920000000001e+21,10666666666.666666,20,,,,,Google TPU v3,WebText,,,,,Open weights (unrestricted),Unreleased,,TRUE,,Operation counting,,"""GPT-2 is a large transformer-based language model with 1.5 billion parameters""","Estimating based on compute = 6 FLOP/token/param * epochs * parameters * tokens.

40GB dataset is approximately 8B words, or 1/0.75 * 8B = 10.66B tokens.

The number of epochs is not reported, but another paper [1] claims in table 1 that it is 20 or 100 epochs, and another paper [2] claims 12 epochs based on communication with the GPT-2 authors (page 4).

12 epochs is the modal, most credible value. Mean of probability mass is probably around 20 epochs, so calculating from that value:

6 * (40 * 200 million * 1/0.75 * 20) * 1.5 billion parameters = 1.92e21
https://www.wolframalpha.com/input?i=6+FLOP+*+20+*+%2840+billion+%2F+5+*+%284%2F3%29%29+*+1.5+billion

[1] https://arxiv.org/abs/1906.06669 One Epoch Is All You Need
[2] https://www.usenix.org/system/files/sec21-carlini-extracting.pdf Extracting Data From Large Language Models

It also appears the model was trained on TPU v3 chips:
https://huggingface.co/openai-community/gpt2",,"“All results presented in this paper use a preliminary version of WebText which does not include links created after Dec 2017 and which after de-duplication and some heuristic based cleaning contains slightly over 8 million documents for a total of 40 GB of text.”
40GB is approximately 8e9 words.
",,,,,"modified MIT
https://github.com/openai/gpt-2?tab=License-1-ov-file#readme"
Hanabi 4 player,2/1/2019,Historical significance,764000,4.3e+18,,,,,,,,,,,,,Unreleased,Unreleased,,,,Hardware,Adapted some SOTA RL algorithms to a new task that posed research challenges,source: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389,14.13e+12 FLOP/s * 7 days * 86400 s/day * 0.50 utilization = 4.3e+18 FLOP,,,,"7 days on V100 –> 7 * 24 * $0.55 = $92.40
Adjust to 2020 dollars: $78.32",,,
MT-DNN,1/31/2019,"Highly cited,SOTA improvement",330000000,,,,,,,,,"GLUE,SciTail",,,,,Open weights (unrestricted),Open source,Open source,,,,"""MT-DNN extends the model proposed in Liu et al. (2015) by incorporating a pre-trained bidirectional transformer language model, known as BERT (Devlin et al., 2018). MT-DNN obtains new state-of-the-art results on ten NLU tasks, including SNLI, SciTail, and eight out of nine GLUE tasks, pushing the GLUE benchmark to 82.7% (2.2% absolute improvement)""",,,"GLUE, SNLI, and SciTail ",,,,,,MIT for code/weights: https://github.com/namisan/mt-dnn
Transformer-XL (257M),1/9/2019,Highly cited,257000000,1.09e+19,,,,,,,,WikiText-103,,,,,Open weights (unrestricted),Open source,Open source,,,,,"Transformer-XL Large, Table 1",,,,,,,,"Apache 2.0, includes train code
https://github.com/kimiyoung/transformer-xl?tab=Apache-2.0-1-ov-file#readme"
Decoupled weight decay regularization,1/4/2019,Highly cited,36500000,2.47e+18,50000,,,,,,,CIFAR-10,,,,,Open weights (unrestricted),Open source,Open source,,,Operation counting,,"From author communication

WideResNet 28-10 models with 36.5 million parameters (3.65E+07)","From author communication

Per image: 5.24 billion FLOPs (5.24E+09)  Per training run: 50k times 5.24E+09 times 1800 epochs = 2.47E+18 FLOPs",,,,,,,"license: https://github.com/loshchil/AdamW-and-SGDW/blob/master/LICENSE

code, including checkpoints: https://github.com/loshchil/AdamW-and-SGDW/blob/master/README.md "
Transformer ELMo,1/1/2019,SOTA improvement,56000000,,,,,,,,,,,,,,Unreleased,Unreleased,,,,,"""Our model is the Reconciled Span Parser (RSP; Joshi et al., 2018), which, using ELMo representations, achieved state of the art performance for this
task. As shown in Table 2, the LSTM based models demonstrate the best performance with a 0.2% and 1.0% improvement over the Transformer and CNN models, respectively""",,,More info on this is extractable with some time,,,,,,
GPipe (Transformer),11/16/2018,"Highly cited,SOTA improvement",6000000000,,20000000000,,,,,,,,,,,,,,,,,,"""We train a single 6-billion-parameter,
128-layer Transformer model on a corpus spanning over 100 languages and achieve better quality than all bilingual models.""",Section 5: ,,,"[WORDS]

Section 5: ""We use a
corpus of parallel documents over 102 languages and English, containing a total of 25 billion training examples, ranging from 10^4 to 10^9 per language""

10^9 sentences * 20 words per sentence",,,,,
GPipe (Amoeba),11/16/2018,Highly cited,557000000,,1281167,,,,,,,ImageNet,,,,,,,,,,,,Section 4,,,Table 4,,,,,
Multi-cell LSTM,11/15/2018,SOTA improvement,7200000,2010000000000000.0,,50,,,,,,,,,,,Unreleased,Unreleased,,,,,"""The proposed multi-cell LSTM language models outperform the state-of-the-art results on well-known Penn Treebank (PTB) setup""",,,,,,,,,
Fine-tuned-AWD-LSTM-DOC (fin),11/12/2018,SOTA improvement,46000000,5.188e+16,1044112,15,,,,,,Penn TreeBank,Confident,AWD-LSTM-DOC (fin) (23M),,,Unreleased,Unreleased,,,,Operation counting,"""The novel approach that we propose allows us to reach state-of-theart quality on Penn Treebank: perplexity decreases from 52.4 to 52.1.""","This is the model trained on Penn Treebank, which uses as a base model the 23M model from Table 7 in https://aclanthology.org/D18-1489.pdf

They additionally train a discriminator with the same architecture, so total parameters is 2*23M = 46M","Base model uses 4.323e16 FLOPs.
They then train a discriminator using the same architecture for 30 epochs, and then use the discriminator to fine-tune the base model for another 15 epochs. Both of these latter training steps require running forward passes on both the discriminator and the language model, but only doing a backward pass on one of them.

Discriminator training: 
2*23M*30*1044112 + 6*23M*30*1044112 = 5.763e15

LM fine-tuning:
2*23M*15*1044112 + 6*23M*15*1044112 = 2.882e15

Total:
4.323e16 + 5.763e15 + 2.882e15 = 5.188e16",,"Per https://arxiv.org/pdf/1904.04733:
""The most common split of this corpus, where sections from 0 to 18 are used for training (38 219 sentences, 912 344 tokens), sections from 19 to 21 are used for validation (5 527 sentences, 131 768 tokens), and sections from 22 to 24 are used for testing (5 462 sentences, 129 654 tokens).""

So dev set is 912,344 + 131768 = 1,044,112",,,,,
Mesh-TensorFlow Transformer 4.9B (language),11/5/2018,SOTA improvement,4900000000,1.617408e+20,6333333333.333333,10,13,256,,935.3300509163912,Google TPU v2,"Wikipedia,One Billion Word benchmark",Confident,,,,Unreleased,Open source,Open source,,161450.39937033202,Hardware,"'Using TPU meshes of up to 512 cores, we train Transformer models with up to 5 billion parameters, surpassing state of the art results on WMT'14 English-to-French translation task and the one-billion-word language modeling benchmark.'",4.9B from section 9.1 : ''The largest model (4.9B parameters) took 13 hours to train on a 512-core TPUv2 cluster.',"flops = (256) * ( 45 * 10**12) * (13 * 3600) * (0.3) = 1.6e20
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)

from section 9.1 : ''The largest model (4.9B parameters) took 13 hours to train on a 512-core TPUv2 cluster.'
from https://en.wikipedia.org/wiki/Tensor_Processing_Unit 
45TFLOPs per chips",from section 9.1 Wikipedia and one-billion-word language modeling benchmark.,"from section 9.1. Experiments done on a ""billion word benchmark"" and a 5B token wikipedia dataset. At 4/3 tokens per word, 1.3B tokens in the first.","from section 9.1 ""For the billion-word language modeling benchmark, we trained the models for 10 epochs. The largest model (4.9B parameters) took 13 hours to train on a 512-core TPUv2 cluster.""",,,,"code here, apache license: https://github.com/tensorflow/mesh/tree/master/mesh_tensorflow/transformer 

https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/transformer/transformer.py "
Mesh-TensorFlow Transformer 2.9B (translation),11/5/2018,SOTA improvement,2900000000,6.84288e+19,1800000000,10,22,64,,395.71656000308855,Google TPU v2,WMT14,Likely,,,,Unreleased,Open source,Open source,,40362.59984258301,Hardware,"'Using TPU meshes of up to 512 cores, we train Transformer models with up to 5 billion parameters, surpassing state of the art results on WMT'14 English-to-French translation task and the one-billion-word language modeling benchmark.'","2.9B from section 9.1 : ""On the WMT14 En-Fr translation tasks (3), we trained the models for 3 epochs. The largest model
(2.9B parameters) was trained for 22 hours on a 128-core TPUv2 cluster.""","flops = (64) * ( 45 * 10**12) * (22 * 3600) * (0.3) = 6.8e19
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)

from section 9.1 : ""On the WMT14 En-Fr translation tasks (3), we trained the models for 3 epochs. The largest model
(2.9B parameters) was trained for 22 hours on a 128-core TPUv2 cluster.""
from https://en.wikipedia.org/wiki/Tensor_Processing_Unit 
45TFLOPs per chips","from section 9.1 ""On the WMT14 En-Fr translation tasks (3), we trained the models for 3 epochs. The largest model
(2.9B parameters) was trained for 22 hours on a 128-core TPUv2 cluster.""","Per Attention is All You Need, WMT 2014 En-Fr is ~36 million sentence pairs. If the average sentence is ~25 tokens (ballpark), dataset size is 
36M * 25 * 2 = 1.8B tokens","from section 9.1 ""On the WMT14 En-Fr translation tasks (3), we trained the models for 3 epochs. The largest model
(2.9B parameters) was trained for 22 hours on a 128-core TPUv2 cluster.""",,,,"code here, apache license https://github.com/tensorflow/mesh/tree/master/mesh_tensorflow/transformer 

https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/transformer/transformer.py  "
MemoReader,10/31/2018,SOTA improvement,,,,,,,,,NVIDIA M40,TriviaQA,Unknown,,,,Unreleased,,,,,,"""TriviaQA. As shown in Table 2, our model,
even without DEBS, outperforms the existing
state-of-the-art method such as ‘BiDAF + SA +
SN’ by a large margin in all the cases""",,"""Our model does require more memory than existing methods, but a single GPU (e.g., M40 with 12GB memory) was enough to train model within a reasonable amount of time""

""Reasonable"" could mean anything, maybe hours to a few days.",,,"""reasonable amount of time"" with a single GPU",,,,
TrellisNet,10/15/2018,SOTA improvement,180000000,2.78e+18,,25,,,,,,WikiText-103,,,,,Unreleased,Open source,Open source,,,,"""Experiments demonstrate that trellis networks outperform the current state of the art methods on a variety of challenging benchmarks, including word-level language modeling and character-level language modeling
tasks""","180M, Table 2",,,,,,,,MIT license for code: https://github.com/locuslab/trellisnet/tree/master/TrellisNet/word_WT103 
MetaMimic,10/11/2018,SOTA improvement,22000000,,,,,,,,,,,,,,,,,,,,"""By retaining and taking advantage of all its experiences,
MetaMimic also substantially outperforms the state-of-the-art D4PG RL agent, when D4PG
uses only the current task experiences.""","""This representational demand motivates the introduction of high-capacity deep neural networks. We found the architecture, shown in Figure 3, with residual connections, 20 convolution layers with 512 channels
for a total of 22 million parameters, and instance normalization to drastically improve performance, as shown in Figure 6 of the Experiments section.""",,,,,,,,
BERT-Large,10/11/2018,Highly cited,340000000,2.85e+20,3300000000,40,96,64,0.2801,1751.4770087736404,Google TPU v2,,,,,128000,Open weights (unrestricted),Open source,Open source,,40374.23895246312,"Operation counting,Hardware",,,more info here https://docs.google.com/document/d/1B8x6XYcmB1u6Tmq3VcbAtj5bzhDaj2TcIPyK6Wpupx4/edit?usp=sharing,,"""For the pre-training corpus we
use the BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2,500M words)""","from appendix A.2: ""Training of BERTLARGE was performed
on 16 Cloud TPUs (64 TPU chips total). Each pre-
training took 4 days to complete.""",,,,"apache 2.0
train+inference code and models here: https://github.com/google-research/bert "
Transformer (Adaptive Input Embeddings) WT103,9/28/2018,SOTA improvement,247000000,4.47e+19,100000000,,67,8,,2880.917278699733,NVIDIA V100,WikiText-103,Confident,,,,Open weights (unrestricted),Open source,Open source,,5408.076505855887,"Hardware,Operation counting","""On the WikiText-103 benchmark we achieve 18.7 perplexity, an improvement of 10.5 perplexity compared to the previously best published result""",Table 2,"8 V100s * 67 hours per Table 2.
125e12 FLOP/sec * 8 * 67 * 3600 * 0.3 (utilization assumption) = 7.2e19 FLOP

They also say they trained for 286k steps in batches of 65,536 tokens.
6 * 247M * (286k * 65536) = 2.78e19

geometric mean: sqrt(7.2e19 * 2.78e19) = 4.47e19","The training data of WIKITEXT-103 comprises about 100M tokens""","""The training data of WIKITEXT-103 comprises about 100M tokens""
Datasets are not combined but used to train separate models",,,,,"MIT for code and weights: https://github.com/facebookresearch/fairseq/blob/main/examples/language_model/README.adaptive_inputs.md 

inference in other readme: https://github.com/facebookresearch/fairseq/blob/main/examples/language_model/README.md "
BigGAN-deep 512x512,9/28/2018,Highly cited,112694781,1.8e+21,292000000,,48,256,,5170.456705747183,Google TPU v3,JFT-300M,Likely,,,,Open weights (unrestricted),Unreleased,,TRUE,259587.67228108257,Third-party estimation,,"I used the publicly available implementation available at [1]

There I loaded the biggan-deep512/1 model, and ran script [2] to compute the number of parameters

[1] https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/biggan_generation_with_tf_hub.ipynb

[2]
n_params = 0
for var in module.variables:
  n_params += np.prod(var.shape.as_list())
  pass

print(n_params)","3e21, estimate taken from:

https://www.lesswrong.com/posts/wfpdejMWog4vEDLDg/ai-and-compute-trend-isn-t-predictive-of-what-is-happening",,"""To confirm that our design choices are effective for even larger and more complex and diverse datasets, we also present results of our system on a subset of JFT-300M (Sun et al., 2017). The full JFT-300M dataset contains 300M real-world images labeled with 18K categories. Since the category distribution is heavily long-tailed, we subsample the dataset to keep only images with the 8.5K most common labels. The resulting dataset contains 292M images – two orders of magnitude larger than ImageNet. ""","""We train on a Google TPU v3 Pod, with the number of cores proportional to the resolution: 128 for 128×128, 256 for 256×256, and 512 for 512×512. Training takes between 24 and 48 hours for most models""",,,,"repo license is Apache:

https://github.com/tensorflow/tfhub.dev/blob/master/assets/docs/deepmind/models/biggan-deep-512/1.md"
LSTM+NeuralCache,9/24/2018,SOTA improvement,2100000,1020000000000000.0,,39,,,,,,,,,,,Unreleased,Unreleased,,,,,"""We obtain a 29.9%/32.1% (validation/test set) relative improvement in perplexity with respect to a baseline LSTM LM on the WikiText-2 dataset, outperforming previous work on neural cache LMs"" 
... 

""we observe that neural cache models
consistently outperform regular cache models on this dataset.""",,,,,,,,,
"AWD-LSTM-MoS + dynamic evaluation (WT2, 2018)",9/18/2018,SOTA improvement,35000000,,,,,,,,,WikiText-2,,,,,Unreleased,Open (non-commercial),Open source,,,,"""Specifically, in language modeling and machine translation, we achieve better performance than the state-of-the-art results on PTB, WT2
and WMT14 English-German datasets.""",,,,,,,,,"code, no license: https://github.com/ChengyueGongR/Frequency-Agnostic "
Transformer + Simple Recurrent Unit,9/17/2018,SOTA improvement,90000000,1.1e+19,,40,,8,,45.37321924485877,NVIDIA V100,WMT English-German,Confident,,,,Unreleased,Unreleased,,,5408.764840583733,Hardware,"""We use the state-of-the-art Transformer
model of Vaswani et al. (2017) as our base architecture... When SRU is incorporated into the architecture,
both the 4-layer and 5-layer model outperform the
Transformer base model""","5-layer model, Table 3","""We use a single NVIDIA Tesla V100 GPU for each model. The published results were obtained
using 8 GPUs in parallel, which provide a large effective batch size during training. To approximate
the setup, we update the model parameters every 5×5120 tokens and use 16,000 warm-up steps
following OpenNMT suggestions. We train each
model for 40 epochs (250,000 steps), and perform
3 independent trials for each model configuration.
A single run takes about 3.5 days with a Tesla V100 GPU.""

125 trillion * 3.5 * 24 * 3600 * 0.3 = 1.1e19","""We train translation models on the WMT English→German dataset, a standard
benchmark for translation systems (Peitz et al.,
2014; Li et al., 2014; Jean et al., 2015). The
dataset consists of 4.5 million sentence pairs""",,,,,,"repo, but no training code for translation: https://github.com/taolei87/sru "
ESRGAN,9/1/2018,Highly cited,,,,,,,,,,"DIV2K,Flickr2K,OutdoorSceneTraining (OST)",Unknown,,,,,,,,,,,,,,,,,,,
(ensemble): AWD-LSTM-DOC (fin) × 5 (WT2),8/30/2018,SOTA improvement,185000000,6.93e+17,,300,,,,,,WikiText-2,,,,,Open weights (unrestricted),Open source,Open source,,,,"""The proposed method improves the current state-of-the-art language model and achieves the best score on the Penn Treebank and WikiText-2, which are the standard benchmark datasets""",,,,,,,,,"code and weights, MIT: https://github.com/nttcslab-nlp/doc_lm?tab=readme-ov-file "
Big Transformer for Back-Translation,8/28/2018,"Highly cited,SOTA improvement",,4.7808e+20,3390000000,,27.666,128,,2442.1618775733145,NVIDIA Tesla V100 DGXS 16 GB,WMT English-German,Likely,,,,Open weights (unrestricted),Open source,Open source,,72133.58132994265,Hardware,"""Finally, we scale to hundreds of millions of monolingual sentences and achieve a new state of the art of 35 BLEU on the WMT'14 English-German test set. ""","""We re-implemented the Transformer model in py-
torch using the fairseq toolkit.1 All experiments
are based on the Big Transformer architecture with
6 blocks in the encoder and decoder. We use the
same hyper-parameters for all experiments, i.e.,
word representations of size 1024, feed-forward
layers with inner dimension 4096. ""

I am not sure what authors mean by 'Big Transformer architecture'","(128) * (1.25e14) * (27*3600 + 40*60) * (0.3)  = 4.7808e20
(number of gpus) * (peak flops) * (seconds) * (assumed utilization rate)  

""We run experiments on DGX-1 machines with 8Nvidia V100 GPUs and machines are interconnected by Infiniband. Experiments are run on 16
machines and we perform 30K synchronous updates.""
""We also use the NCCL2 library [...] with 16-bit floating point
operations""

NCCL2 supported tensor core operations at 1.25e14 FLOP/s on a V100 for FP16 

in section 5.6 we have

""train this system we perform 300K training up-
dates in 27h 40min on 128 GPUs;""","""Finally, for WMT English-German we train
on all 226M available monolingual training sen-
tences and perform 250K updates in 22.5 hours on 128 GPUs. ""","""Finally, for WMT English-German we train on all 226M available monolingual training sentences and perform 250K updates in 22.5 hours on 128 GPUs.""

We assume that 1 sentence have 15 words","""training updates in 27h 40min on 128 GPUs""",,,,"Code and weights, MIT license: https://github.com/facebookresearch/fairseq/blob/main/examples/backtranslation/README.md "
AWD-LSTM-MoS+PDR + dynamic evaluation (WT2),8/14/2018,SOTA improvement,35000000,,,,,,,,,WikiText-2,,,,,Unreleased,Unreleased,,,,,"""our Past Decode Regularization (PDR) method achieves a word level perplexity of 55.6 on the Penn Treebank and 63.5 on the WikiText-2 datasets using a single softmax. We also show gains by using PDR in combination with a mixture-of-softmaxes, achieving a word level perplexity of 53.8 and 60.5 on these datasets. In addition, our method achieves 1.169 bits-per-character on the Penn Treebank Character dataset for character level language modeling. These results constitute a new state-of-the-art in their respective settings.""",,,,,,,,,
Big-Little Net (speech),7/10/2018,SOTA improvement,3320000,4.290048e+17,27360000,16,,,,,,"Switchboard,Fisher",Speculative,,,,Open weights (unrestricted),Open source,Open source,,,Operation counting,"""Furthermore, our model surpasses state-of-the-art CNN acceleration approaches by a large margin in accuracy and FLOPs reduction. On the task of speech recognition, our proposed multi-scale CNNs save 30% FLOPs with slightly better word error rates, showing good generalization across domains.""",table 3,980000000 (number of FLOPs from table 3) * 27360000 (dataset size) * 16 (number of epochs from appendix B.1) = 429004800000000000,"""We train ResNet style acoustic models in the hybrid framework on Switchboard+Fisher (2000h) and provide results on Hub5 (Switchboard and Call Home portions). Switchboard is a large dataset with 2000 hours of transcribed speech from 28, 000 speakers""","""We train ResNet style acoustic models in the hybrid framework on Switchboard+Fisher (2000h) and provide results on Hub5 (Switchboard and Call Home portions). Switchboard is a large dataset with 2000 hours of transcribed speech from 28, 000 speakers""

2000h * 13680 words per hour = 27360000

https://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.3pbt0hfgv7pq",,,,,"apache for code/weights: 
https://github.com/IBM/BigLittleNet"
Big-Little Net,7/10/2018,SOTA improvement,77360000,2.46048e+17,1280000,110,,,,,NVIDIA Tesla K80,ImageNet,Likely,,,256,Open weights (unrestricted),Open source,Open source,,,Operation counting,"""On object recognition task, we demonstrated that our approach provides approximately 2× speedup over baselines while
improving accuracy, and the result significantly outperforms the state-of-the-art networks by a large margin in terms of accuracy and FLOPs reduction""",Table 2,"Using the 6ND formula: 
6×number of tokens×number of parameters×number of epochs
6×1.28×10^6×77360000×110=6.5353728e+16 FLOPs

9.32*10^9 (flops per inference)*1.28×10^6(dataset size)/16 (batch size) * 110 epochs * 3 (to account for backpropagation)= 2.46048e+17 FLOPs",,,,,,"""All the models were trained with 110 epochs, batch size 256""","Apache 2 license
https://github.com/IBM/BigLittleNet"
RCAN,7/8/2018,Highly cited,16000000,,,,,,,,,DIV2K,Unknown,,,,,,,,,,,"""EDSR has much larger number of parameters (43 M) than ours
(16 M), but our RCAN obtains much better performance.""",,,,,,,,
Population-based DRL,7/3/2018,SOTA improvement,122000000,3.49e+19,,,,,,,,,,,,,Unreleased,Unreleased,,,,Third-party estimation,"Qualitatively clearly SOTA: ""In this work, we demonstrate for the first time that an agent can achieve human-level in a popular 3D multiplayer first-person video game, Quake III Arena Capture the Flag (28), using only pixels and game points as input... proved far stronger than existing state-of-the-art agents""","Calculated from the architecture schematic in Figure S11 on pg 55 of the Capture the Flag supplementary materials. This is dominated by the size of the vision module, which is 116 million parameters, followed by the temporal processors which is 4.3 million parameters. The RL policy itself is only 0.79 million parameters. Also, I'm pretty uncertain if I'm right about how I calculated these parameters.

Source: 
https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389","Source: 
https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389",,,,,,,
ShuffleNet v2,6/30/2018,Highly cited,2280000,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QT-Opt,6/27/2018,Highly cited,1200000,3.4875e+19,5984870,80,104.2,,,1317.8035861002786,NVIDIA P100,,Likely,,,,Unreleased,,,,,Hardware,,"""The Q-function Qθ(s, a) is represented in our system by a large convolutional neural network with 1.2M parameters""","""We distribute training across 10 GPUs, using asynchronous SGD with momentum... This system allows us to train the Q-function at 40 steps per second with a batch size of 32 across 10 NVIDIA P100 GPUs.""

""We found empirically that a large number of gradient steps (up to 15M) were needed to train an effective Q-function...""

15M steps * 0.025 seconds/step *  9.30E+12 FLOP/sec/GPU * 10 GPU = 3.4875E+19","""... we collected over 580k grasps over the course of several weeks across 7 robots""
","Observations take up 4TB of disk space, and the input space is a 472x472 RGB image.

Assuming 24 bit depth color (8 bits per channel), that suggests 472 * 472 * 3 * 8 bits = 668.352 kB per image (this could be off by a factor of 2 depending on actual bit depth)

4 TB / 668.352 kB = 5,984,870 images; around 10 per grasp attempt.

15M gradient steps with batchsize 32 implies:
15M steps * 32 images/step * 1/5984870 images ~= each image seen 80 times","""We distribute training across 10 GPUs, using asynchronous SGD with momentum... This system allows us to train the Q-function at 40 steps per second with a batch size of 32 across 10 NVIDIA P100 GPUs.""

""We found empirically that a large number of gradient steps (up to 15M) were needed to train an effective Q-function...""

15M steps * 0.025 seconds/step *  1/3600 hours/second = 104.2 hours","Using cost from ML Hardware Data spreadsheet,
$0.919/hr/GPU * 104.2 hours * 10 GPUs = $957.60

Likely an underestimate, as the cloud pricing comes from 2023 and incorporates 5 additional years of depreciation on the P100.",,,There is a public implementation of the architecture at https://github.com/quantumiracle/QT_Opt (not from any of the paper's co-authors)
DARTS,6/24/2018,Highly cited,33000000,1.1e+16,,300,,,,,,WikiText-2,,,,,Unreleased,Open source,Open source,,,,,,,,,,,,,"apache 2, training/test for wikitext: https://github.com/quark0/darts/tree/master/rnn "
MobileNetV2,6/18/2018,Highly cited,3400000,,,,,,,,,,,,,,,,,,,,,Rados,,,,,,,,
Relational Memory Core,6/5/2018,SOTA improvement,,,,,,,,,,WikiText-103,Unknown,,,,Unreleased,Unreleased,,,,,"""Finally, we test the RMC on a suite of tasks that may profit from more capable relational reasoning across sequential information, and show large gains in RL domains (e.g. Mini PacMan), program evaluation, and language modeling, achieving state-of-the-art results on the WikiText-103, Project Gutenberg, and GigaWord datasets.""",,,,,,,,,"looks like code for the architecture, but not experiment code: https://github.com/google-deepmind/sonnet/blob/v1/sonnet/python/modules/relational_memory.py "
GPT-1,6/1/2018,Highly cited,117000000,1.7578125e+19,1000000000,,720,8,,,NVIDIA Quadro P600,"BookCorpus (BooksCorpus, Toronto Book Corpus)",,,,,Open weights (unrestricted),Open source,Open source,,722.0759543,Operation counting,,"""The model had 117M parameters in total.""

source: https://medium.com/walmartglobaltech/the-journey-of-open-ai-gpt-models-32d95b7b7fb2","COMPUTE = FORWARD COMPUTE PER TOKEN * 3 BACKWARD FORWARD ADJUSTMENT * EPOCHS * DATASET SIZE

""We train for 100 epochs on minibatches of 64 randomly sampled, contiguous sequences of 512 tokens.""
","""We use the BooksCorpus dataset [71] for training the language model""","""BookCorpus is a large collection of free novel books written by unpublished authors, which contains 11,038 books (around 74M sentences and 1G words) of 16 different sub-genres (e.g., Romance, Historical, Adventure, etc.).""
https://paperswithcode.com/dataset/bookcorpus

BookCorpus seems to have about 5000MB of content
source: https://huggingface.co/datasets/bookcorpusopen

Assuming a byte-pair encoder similar to GPT-2, there are 8 bytes / token.

So approximately 5000MB / 8 bytes / token = 5e9 / 8 tokens","""1 month on 8 GPUs."" from the reference link",,,,"MIT, code and weights
https://github.com/openai/finetune-transformer-lm/blob/master/LICENSE"
aLSTM(depth-2)+RecurrentPolicy (WT2),5/22/2018,SOTA improvement,32000000,7.59e+16,,190,,,,,,,,,,,Unreleased,Open source,Open source,,,,"""Without tuning for WT2, both outperform previously published results in 150 epochs (table 3) and converge to new state of the art performance in 190 epochs""",,,,,,,,,BSD-3 license: https://github.com/flennerhag/alstm/tree/master/examples 
Dropout-LSTM+Noise(Bernoulli) (WT2),5/3/2018,SOTA improvement,51000000,1.27e+17,,200,,,,,,,,,,,Unreleased,Unreleased,,,,,"this is the best model in this paper per Table 4
""On language modeling benchmarks, Noisin improves over dropout by as much as 12.2% on the Penn Treebank and 9.4% on the Wikitext-2 dataset""",,,,,,,,,
ResNeXt-101 32x48d,5/2/2018,"Highly cited,SOTA improvement",829000000,8.74395e+21,9525000000,,496,336,,,NVIDIA V100,"ImageNet,Instagram",Confident,,,,Open weights (non-commercial),Unreleased,Open source,TRUE,227533.94953388025,Operation counting,"""We show improvements on several image classification and object detection tasks, and report the highest ImageNet-1k single-crop, top-1 accuracy to date: 85.4%","Table 6
","Table 6: 153e9 mult-adds.
Section 2.4: ""minibatches of 8,064 images"".

Compute = 2 * 3 * mult-adds * dataset size = 2 * 3 * 153e9 * 9525e6 = 8.74e21 FLOP

Likely trained on V100s, since Facebook had just upgraded their Big Basin GPU cluster to V100s as of March 2018. The previous iteration of Big Basin had 32 clusters of 8xP100s, while Big Basin v2 had 42 clusters of 8xV100s, which matches the 336 GPUs used in this paper.","Instagram images, captioned with hashtags",Table 3: (300+1925+300+7000) million images,"""Mahajan et al. (2018) required 19 GPU years to train their ResNeXt101-32x48d"" https://arxiv.org/abs/2103.00020
Models were trained on 336 GPUs, so that suggests 20.65 days or 496 hours",,,,"models, non-commercial: https://github.com/facebookresearch/WSL-Images "
Diffractive Deep Neural Network,4/14/2018,Highly cited,8000000000,,55000,,,,,,,MNIST,Likely,,,,,,,,,,,"""For example, using five 3D-printed transmission layers, containing a total of 0.2 million neurons and ~8.0 billion connections that are trained using deep learning, we experimentally demonstrated the function of a handwritten digit classifier.""

My understanding is that every connection correspond to the parameter to learn.",,"""For this task, phase-only transmission masks were designed by training a 5-layer D2NN with ~55,000 images from MNIST handwritten digit database (14). ""","size of MNIST
""For this task, phase-only transmission masks were designed by training a 5-layer D2NN with ~55,000 images from MNIST handwritten digit database (14). """,,,,,
YOLOv3,4/8/2018,Highly cited,56933216,5.093919992e+19,1281167,,,,,,"NVIDIA M40,NVIDIA GeForce GTX TITAN X",ImageNet,,,,,Unreleased,Unreleased,,,,Operation counting,,"Feature extractor (ignoring biases)
32*3*3*3 +
64*3*3*32 +
32*1*1*64 +
64*3*3*32 +
128*3*3*64 +
2*(64*1*1*128 +
128*3*3*64) +
256*3*3*128 +
8*(128*1*1*256 +
256*3*3*128) +
512*3*3*256 + 
8*(256*1*1*512 + 
512*3*3*256) + 
1024*3*3*512 + 
4*(512*1*1*1024 +
1024*3*3*512) +
4*4*1024*1000

source: table 1
This is assuming the average pooling step changes the output size from 8x8 to 4x4.

The weights file is 237MB. If the weights are saved as float32, 4 bytes per weight, then there are approximately 237M/4=59M parameters, consistent with the calculation above.","We use the formula training_compute = ops_per_forward_pass * 3.5 * n_epochs * n_examples

Assuming 160 epochs of training as in https://arxiv.org/pdf/1612.08242.pdf",,Source: https://image-net.org/download.php,,,,,"code and weights, unclear license: https://pjreddie.com/darknet/yolo/ "
"LSTM (Hebbian, Cache, MbPA)",3/27/2018,SOTA improvement,530442240,3.33e+19,175181505,80,144,8,,590.5320553042133,NVIDIA P100,Project Gutenberg,Confident,,,51200,Unreleased,Unreleased,,,4516.475094515345,"Hardware,Operation counting","""We also show improved performance for word-based language models on news reports (GigaWord), books (Project Gutenberg) and Wikipedia articles (WikiText-103) --- the latter achieving a state-of-the-art perplexity of 29.2.""","Single layer LSTM with hidden dimension of 2048. Vocabulary for Gutenberg is 242,621; input and output embeddings are tied.

Embedding layer (tied): 242,621 * 2048 = 496,887,808
LSTM layer: 4 * (2048 + 2048) * 2048 = 33,554,432

Total: 496,887,808 + 33,554,432 = 530,442,240","They do training runs on a vision task and three language datasets. The largest dataset by size is GigaWord, but the largest training run is on the Gutenberg dataset, at 15B tokens. 

I assume the input embedding is done with an embedding lookup for efficiency rather than a dense matrix multiplication, so we only count FLOPs on the de-embedding.

Ops counting:
6 * 15B * 530,442,240 = 4.774e19

Hardware:
(8 * 1.87e13) * (6 * 24 * 3600) * 0.3 = 2.327e19

Geometric mean: sqrt(4.774e19 * 2.327e19) = 3.33e19

(Note they say 15B steps, but they also say it is 80 epochs on a 175M token dataset, and that it took 6 days on 8 P100s, both of which would agree with 15B tokens, not 15B steps)","They also do training runs on Omniglot, Wikitext-103, and GigaWord.","Omniglot: 32k images
Wikitext-103: ""Over 100 million tokens""
Gutenberg: 175,181,505 tokens
GigaWord v5: 4B tokens

Gigaword is the largest dataset, but the largest training run uses Project Gutenberg.",6 days,,,"Sequence length of 100, total of 512 batches. Batches are split between 8 GPUs.",
4 layer QRNN (h=2500),3/22/2018,SOTA improvement,26000000,2.4e+17,,14,,,,,,WikiText-103,,,,,Unreleased,Open source,Open source,,,,"""QRNNs achieve stateof-the-art results on character-level (Penn Treebank, enwik8) and word-level (WikiText-103)
datasets, respectively""",,,,,,,,,BSD-3 license: https://github.com/salesforce/awd-lstm-lm 
Rotation,3/21/2018,Highly cited,86000000,,,,,,,,,CIFAR-10,,,,,,,,,,,,https://openai.com/blog/image-gpt/#rfref53,,,,,,,,
LSTM (2018),3/4/2018,Highly cited,13000000,,,,,,,,,Penn TreeBank,,,,,Open weights (unrestricted),Open source,Open source,,,,,,,,,,,,,"code, MIT: https://github.com/locuslab/TCN/blob/master/TCN/word_cnn/README.md 
train script: https://github.com/locuslab/TCN/blob/master/TCN/word_cnn/word_cnn_test.py "
Chinese - English translation,3/1/2018,SOTA improvement,,,,,,,,,,,Unknown,,,,,,,,,,"""We find that our latest neural machine translation system has reached a new state-of-the-art, and that the translation quality is at human parity when compared to professional human translations""",,,,,,,,,
Residual Dense Network,2/24/2018,Highly cited,,,,200,,,,,,DIV2K,Unknown,,,,,,,,,,,,,,,,,,,
Spectrally Normalized GAN,2/16/2018,Highly cited,,,,,,,,,,CIFAR-10,Unknown,,,,,,,,,,,,,,,,,,,
TCN (P-MNIST),2/15/2018,SOTA improvement,42000,,,,,,,,,P-MNIST,Confident,,,,,,,,,,"""For the permuted sequential MNIST, TCNs outperform state of the art results using recurrent nets (95.9%) with Zoneout+Recurrent BatchNorm (Cooijmans et al., 2016; Krueger et al., 2017), a highly optimized method for regularizing RNNs""",,,,,,,,,
ENAS,2/9/2018,Highly cited,24000000,2.01e+16,,150,,,,,,Penn TreeBank,,,,,Unreleased,Open source,Open source,,,,,,,,,,,,,code for PTB. Apache license: https://github.com/google-research/google-research/tree/master/enas_lm 
DeepLabV3+,2/7/2018,Highly cited,,,,,,,,,,"ImageNet-1k,COCO,JFT-300M",Unknown,,,,,,,,,,,,,,,,,,,
IMPALA,2/5/2018,"Highly cited,SOTA improvement",1600000,1.68e+20,2.4E+11,,100,1,,53.42846804494412,NVIDIA P100,,,,,,Unreleased,Open source,Open source,,564.8930885996127,Third-party estimation,"""IMPALA is able to achieve better performance than previous agents with less data""","""Figure 3 in the paper states that the large architecture has 1.6 million parameters. I am using the large model because it was the only one trained on all the Atari games at once, which seems like the most impressive task in the suite.""

Source: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389","Source: Ajeya Cotra and Tom Davidson, https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389",,"From fig 6, there were 1e10 environment frames, and 24 agents. Thus we note down 2.4e11 for the ""dataset size""",Maximum training time for IMPALA is 100 hours according to Figure 6. This seems to refer to the 1 GPU model. The 8 GPU model looks to have been trained about 1/8 as long.,,,,"training code, Apache license: https://github.com/google-deepmind/scalable_agent "
AmoebaNet-A (F=448),2/5/2018,Highly cited,469000000,3.85296912e+20,1280000,,168,450,,11766.339677271537,NVIDIA Tesla K40s,ImageNet-1k,,,,,Unreleased,Unreleased,,,249117.8520724292,Hardware,,Table 2,"450 K40 GPUs for 20k models (approx. 7 days).
(From Imagenet paper-data, Besiroglu et al., forthcoming) ",,,"""Each experiment ran on 450 K40 GPUs for 20k models (approx. 7 days).""",,,,"has code, but looks like just a toy model: https://colab.research.google.com/github/google-research/google-research/blob/master/evolution/regularized_evolution_algorithm/regularized_evolution.ipynb "
AmoebaNet-A (F=190),2/5/2018,Highly cited,87000000,,,,,,,,,,,,,,,,,,,,,Table 2,,,,,,,,
QRNN,2/1/2018,SOTA improvement,135000000,3.6e+17,,14,,,,,,WikiText-103,,,,,Unreleased,Unreleased,,,,,"""we reduce our per-epoch time substantially and achieve a new state-of-the-art on WikiText-103 despite training for 14 epochs""",,,,,,,,,
ELMo,2/1/2018,Highly cited,94000000,3300100000000010.0,,,,,,,,,Speculative,,,,,,,,,Third-party estimation,,,3300e12 - https://github.com/amirgholami/ai_and_memory_wall,,,,,,,
ULM-FiT,1/18/2018,Highly cited,441000000,2.72538e+17,103000000,,,,,,,"IMDb,Yelp,Trec-6,DBpedia,AG news,WikiText-103",Speculative,AWD-LSTM,,,Open weights (unrestricted),Unreleased,Unreleased,,,Operation counting,,https://files.fast.ai/models/wt103/?C=S;O=D,TRUE,,"We pretrain the language model on Wikitext-103
(Merity et al., 2017b) consisting of 28,595 preprocessed Wikipedia articles and 103 million words.

Fine-tuning datasets:
TREC-6 Question 5.5k
IMDb Sentiment 25k
Yelp-bi Sentiment 560k
Yelp-full Sentiment 650k
AG Topic 120k
DBpedia Topic 560k

560+120+650+560+25+5.5=1920.5k = 1920500",,,,,https://nlp.fast.ai/category/classification.html
Refined Part Pooling,1/9/2018,Highly cited,,2.6244e+16,,,1,2,,,NVIDIA TITAN Xp,"ImageNet-1k,Market-1501",Confident,,,,,,,,1130.1481800387885,Hardware,,,12150000000000*3600*2*0.3=2.6244e+16 FLOP,,,"""With two NVIDIA TITAN XP GPUs and Pytorch as the platform, training an IDE model and a standard
PCB on Market-1501 (12,936 training images) consumes
about 40 and 50 minutes, respectively""",,,,
Tacotron 2,12/19/2017,Highly cited,,,340000,,,,,,,,Confident,,,,,,,,,,,"some architecture details:

""Input characters are represented using a learned 512-dimensional
character embedding, which are passed through a stack of 3 convolutional layers each containing 512 filters with shape 5 × 1, i.e., where
each filter spans 5 characters, followed by batch normalization [18]
and ReLU activations. As in Tacotron, these convolutional layers
model longer-term context (e.g., N-grams) in the input character
sequence. The output of the final convolutional layer is passed into a
single bi-directional [19] LSTM [20] layer containing 512 units (256
in each direction) to generate the encoded features.""",,"""We train all models on an internal US English dataset[12], which
contains 24.6 hours of speech from a single professional female
speaker.""","""We train all models on an internal US English dataset[12], which contains 24.6 hours of speech from a single professional female speaker.""

13,680 words/hour * 24.6 = 336528 words",,,,,
AlphaZero,12/5/2017,Highly cited,,3.667927300468287e+22,700000,,24,5064,,229918.6146969874,"Google TPU v2,Google TPU v1",,,,,,Unreleased,Unreleased,,TRUE,5647.409436097064,Third-party estimation,,,Extracted from AI and Compute (https://openai.com/blog/ai-and-compute/) charts by using https://automeris.io/WebPlotDigitizer/.,,"""We trained a separate instance of AlphaZero for each game. Training proceeded
for 700,000 steps""",,,,,
2-layer-LSTM+Deep-Gradient-Compression,12/5/2017,Highly cited,6020000,1340000000000000.0,,40,,,,,,,,,,,Unreleased,Unreleased,,,,,,,,,,,,,,"repo, but no code for language modeling: https://github.com/synxlin/deep-gradient-compression "
PNASNet-5,12/2/2017,Highly cited,,6.62904e+19,1280000,,,,,,,ImageNet-1k,,,,,,,,,,Comparison with other models,,,"8 times less compute than Zoph (2018), which used 500 p100s for 4 days.
(From Imagenet paper-data, Besiroglu et al., forthcoming) ",,,,,,,
PNAS-net,12/2/2017,Highly cited,86000000,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TriNet,11/21/2017,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
"AWD-LSTM-MoS + dynamic evaluation (WT2, 2017)",11/10/2017,SOTA improvement,35000000,4.37e+17,,1000,,,,,,,,,,,Unreleased,Open source,Open source,,,,"""Experimental results confirm that the
proposed method significantly improves state-of-the-art language models, achieving a perplexity of 55.31 and 62.89 on
the test set of Penn Treebank and WikiText-2""",,,,,,,,,MIT code: https://github.com/zihangdai/mos 
Fraternal dropout + AWD-LSTM 3-layer (WT2),10/31/2017,SOTA improvement,34000000,9.85e+16,,520,,,,,,WikiText-2,,,,,Unreleased,Open source,Open source,,,,"""We evaluate our model and achieve state-of-the-art results in sequence
modeling tasks on two benchmark datasets – Penn Treebank and Wikitext-2""",,,,,,,,,BSD-3 license: https://github.com/kondiz/fraternal-dropout 
DCN+,10/31/2017,SOTA improvement,,,107785,,,,,,,SQuAD,Confident,,,,Unreleased,,,,,,"""On the Stanford Question Answering Dataset, our model achieves state-of-the-art results with 75.1% exact match accuracy and 83.1% F1, while the ensemble obtains 78.9% exact match accuracy and 86.0% F1. ""
https://paperswithcode.com/paper/dcn-mixed-objective-and-deep-residual
","Not directly repoted - It may be possible to extract number from:
https://github.com/lmn-extracts/dcn_plus/tree/master/question_answering","in Figure 4 we see that network was trained on 140k iterations
from https://github.com/lmn-extracts/dcn_plus/tree/master we see that batch size is 64
It should be possible to compute inference FLOPs from repository and estimate training compute","From start of section 3: ""We train and evaluate our model on the Stanford Question Answering Dataset (SQuAD). ""

","from https://paperswithcode.com/dataset/squad SQuAD have 107,785 question-answer pairs
download-ed dataset from: https://www.kaggle.com/datasets/stanfordu/stanford-question-answering-dataset?resource=download
wc -w on train-v.1.1 returns 4017471 words so around 5.4M tokens

Looks like they probably trained on each token in SQuAD rather than QA pairs, but uncertain.",,,,,
S-Norm,10/29/2017,SOTA improvement,,,2000000000,1,,,,,,TriviaQA,Confident,,,,,,,,,,"""Overall, we are able to achieve a score of 71.3 F1 on the web portion of TriviaQA, a large improvement from the 56.7 F1 of the previous best system.""",Not stated. Probably obtainable from github: https://github.com/allenai/document-qa/tree/master,,,"""530k question-document training pairs""

average question length of 14 words and document length of 2895 words, per
 https://www.cs.utexas.edu/~eunsol/files/papers/acl17jcwz.pdf

530,000 * 2895 words on average * 1.33 tokens/word = ~2,000,000,000",,,,,
PhraseCond,10/28/2017,SOTA improvement,,,90000,,,,,,,SQuAD 1.1,Confident,,,,,,,,,,"""We demonstrate the effectiveness of our proposed model PhaseCond on the SQuAD dataset, showing that our model significantly outperforms both state-of-the-art single-layered and multiple-layered attention models.""",Unclear how many layers they use for self-attention (N) and fusion (L and K). Could calculate if these were known.,,"from start of section 3: ""This paper focuses on the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) to train and evaluate our model. SQuAD, which has gained a significant attention recently, is a largescale dataset consisting of more than 100,000 questions manually created through crowdsourcing on 536 Wikipedia articles. The dataset is randomly partitioned into a training set (80%), a development set (10%), and a blinded test set (10%).""","10% held out for test, so 100k * 0.9 = 90k",,,,,
ProgressiveGAN,10/27/2017,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
CapsNet (MultiMNIST),10/26/2017,Highly cited,11360000,,,,,,,,,,,,,,,,,,,,,"""This model has 24.56M parameters which is 2 times more parameters
than CapsNet with 11.36M parameters.""",,,,,,,,
CapsNet (MNIST),10/26/2017,Highly cited,8200000,,60000,,,,,,,MNIST,,,,,,,,,,,,"""In terms of number of parameters the baseline has 35.4M while CapsNet
has 8.2M parameters and 6.8M parameters without the reconstruction subnetwork""","It should be feasible to estimate this from the information in the paper, but it would require carefully checking the FLOP involved for capsules.",,Section 5: The dataset has 60K and 10K images for training and testing respectively.,,,,,
LRSO-GAN,10/22/2017,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
AlphaGo Master,10/19/2017,Highly cited,,2.00010000000001e+23,,,72,,,471445.3247973037,Google TPU v1,,,,,,Unreleased,Unreleased,,TRUE,,Benchmarks,,,"This is a guess. There was no single journal publication that accompanied this model, that gave information about architecture/model training time etc. All I could find was that it has the same architecture as AlphaGo Zero, and that it had roughly the same power consumption as AGZ. See for instance: 
https://deepmind.com/blog/article/alphago-zero-starting-scratch

Since AGZ reaches the ELO of AlphaGo Master in about 25-30 days (60-75% of the total training time), I estimate the compute to be around 60-75% that of AGZ. I round this to 2e23, and I expect this to only be accurate within an OOM.",,,"""Training started from completely random behaviour and continued without human intervention for approximately three days.""",,,,
AlphaGo Zero,10/18/2017,Highly cited,46400244,3.41e+23,5800000000,,480,,,613480.6258010615,Google TPU v1,,,,,,Unreleased,Unreleased,,TRUE,,"Third-party estimation,Hardware",,Quick calculation,"source: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389


AGZ had two models, one of which was small and another of which was large. The compute for AGZ is for the large model, which has 40 residual blocks instead of 20.

A second way of looking at this... we believe multiple TPUs were used for training. 29 million games * 211 moves per game on average * 0.8 seconds per move = 4.8952E+09 seconds of player-time across all TPUs.

4.8952E+09 seconds of player-time / (40 days * 24 * 60 * 60 seconds of real time) ~= 1,416 players

4 TPUs per player => 4.8952E+09 * 4 = 1.95808E+10 TPU-seconds
Total compute = 1.95808E+10 TPU-seconds * 92E+12 FLOP/(TPU-second) * 0.4 = 7.2e23 FLOP

So similar to the Cotra and Davidson estimate (within a factor of 2).",,"""Over the course of training, 29 million games of self-play were generated""

Approx 200 moves per Go game on average

https://homepages.cwi.nl/~aeb/go/misc/gostat.html

Thus 200 * 29e6 = 5.8e9",,,,,
AWD-LSTM+WT+Cache+IOG (WT2),9/26/2017,SOTA improvement,53000000,3310000000000000.0,,5,,,,,,,,,,,Unreleased,Open (non-commercial),Open source,,,,"""IOG achieves comparable scores to the state-of-the-art on the Penn Treebank
dataset and outperforms the WikiText-2 dataset""",,,,,,,,,"license, looks non-commercial: https://github.com/nttcslab-nlp/iog?tab=License-1-ov-file#readme 

https://github.com/nttcslab-nlp/iog "
LSTM + dynamic eval,9/21/2017,SOTA improvement,50000000,,,,,,,,,WikiText-2,,,,,Unreleased,Open source,Open source,,,,"""Dynamic evaluation outperforms existing adaptation approaches in our comparisons. Dynamic evaluation improves the state-of-the-art word-level perplexities on the Penn Treebank and WikiText-2 datasets to 51.1 and 44.3 respectively""",table 2,,,,,,,,BSD-2: https://github.com/benkrause/dynamic-evaluation 
ISS,9/15/2017,SOTA improvement,11100000.000000002,3400000000000000.0,,55,,,,,,,,,,,Unreleased,Open source,Open source,,,,"""Moreover, ISS learning can find a
smaller RHN model with width 726, meanwhile improve the state-of-the-art perplexity as shown by the second entry in Table 2.""",,,,,,,,,code (apache): https://github.com/wenwei202/iss-rnns/tree/master/ptb 
PyramidNet,9/6/2017,SOTA improvement,26000000,2340000000000000.0,50000,300,,,,,,"CIFAR-10,CIFAR-100",Likely,,,,Open weights (unrestricted),Open source,Open source,,,Operation counting,"""In tests using CIFAR-10, CIFAR-100, and ImageNet1k datasets, our PyramidNets outperform all previous state-of-the-art deep network architectures.""",best model had 26M params,6ND=6*26000000*50000*300=2.34e+15,"""Our PyramidNets are trained using backpropagation [15] by Stochastic Gradient Descent (SGD) with Nesterov momentum for 300 epochs on CIFAR-10 and CIFAR-100 datasets.""
"" CIFAR-10 and CIFAR-100 each contain 32×32-pixel color images, consists of 50,000 training images and 10,000 testing images.""",,,,,,https://github.com/jhkim89/PyramidNet
SENet (ImageNet),9/5/2017,Highly cited,28100000,,,,,,,,,ImageNet,,,,,,,,,,,,Table 16,,,,,,,,
GL-LWGC-AWD-MoS-LSTM + dynamic evaluation (WT2),8/29/2017,SOTA improvement,38000000,4.74e+17,,1000,,,,,,WikiText-2,,,,,Unreleased,Unreleased,,,,,"""Our GL-LSTM model
overcame the state-of-the-art results with only two layers and 19M parameters, and further improved
the state-of-the-art results with the third layer phase""",,,,,,,,,
Libratus,8/19/2017,SOTA improvement,,5.51e+20,,,,,,,,,,,,,Unreleased,Unreleased,,TRUE,,Hardware,Claims to be first ML system to reach superhuman level at No Limit Poker Texas Hold Em,,"""In total, Libratus used about 25 million core hours. Of those, about 13 million core hours were used for exploratory experiments and evaluation. About 6 million core hours were spent on the initial abstraction and equilibrium finding component, another 3 million were used for nested subgame solving, and about 3 million were used on the self-improvement algorithm.""

""Like many data-centric supercomputers, Bridges offers a relatively a modest number of FLOPS, but lots of memory: 895 teraflops and 130 TB, respectively.""

I just used the first bullet point (as those are usually independent systems and you only benchmark one of them).
The first system has 752 nodes a 2CPUs a 14cores each.

source: https://www.top500.org/news/bridges-supercomputer-boots-up-at-pittsburgh/



1. 12M core hours for 196 cores
2. We have  895 TFLOPS for 752 nodes a 2 CPUs a 14 cores
2.1 That's 42.5 GFLOPS per core.
3. Running this for 12M h
3.1 12 * 10^6 * 60 * 60 * 42.5 * 10^9 FLOP/S = 1.823e21 FLOPs
4. Assuming 30% utilization
 1.823e21 * 0.3
→ 5.51e20 FLOPs",,,"""In total, Libratus used about 25 million core hours. Of those, about 13 million core hours were used for exploratory experiments and evaluation. About 6 million core hours were spent on the initial abstraction and equilibrium finding component, another 3 million were used for nested subgame solving, and about 3 million were used on the self-improvement algorithm.""",,,,
Adversarial Joint Adaptation Network (ResNet),8/17/2017,Highly cited,60000000,,4652,,,,,,,"Office-31,ILSVRC 2012 subset of ImageNet",Speculative,,,,,,,,,,,"Model is based on ResNet (60m params), might have more parameters though

""We implement all deep methods based on the Caffe framework, and fine-tune from Caffe-provided models of AlexNet (Krizhevsky et al., 2012) and ResNet (He et al., 2016), both are pre-trained on the ImageNet 2012 dataset.""",,"""Office-31 (Saenko et al., 2010) is a standard benchmark for
domain adaptation in computer vision, comprising 4,652
images and 31 categories collected from three distinct domains: Amazon (A), which contains images downloaded
from amazon.com, Webcam (W) and DSLR (D)... We evaluate all
methods across three transfer tasks A → W, D → W and W
→ D, which are widely adopted by previous deep transfer
learning methods""

""We implement all deep methods based on the Caffe framework, and fine-tune from Caffe-provided models of AlexNet
(Krizhevsky et al., 2012) and ResNet (He et al., 2016), both
are pre-trained on the ImageNet 2012 dataset.""",,,,,,
NeuMF (Pinterest),8/16/2017,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
Cutout-regularized net,8/15/2017,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,https://www.yuzeh.com/data/agz-cost.html,,,
EI-REHN-1000D,8/14/2017,SOTA improvement,19000000,1.06e+16,,100,,,,,,,,,,,Unreleased,Unreleased,,,,,"""The proposed networks showed better performance than other state-of-the-art recurrent networks in all three experiments.""",,,,,,,,,
OpenAI TI7 DOTA 1v1,8/11/2017,"Historical significance,SOTA improvement",,6.046095222592002e+20,,,,,,,,,,,,,,,,TRUE,,Third-party estimation,,"Section 4 states: ""we used a model with over 150 million parameters"" but this is for the 5v5 agent, not the 1v1.",Extracted from AI and Compute (https://openai.com/blog/ai-and-compute/) charts by using https://automeris.io/WebPlotDigitizer/.,,,,,,,
RetinaNet-R101,8/7/2017,Highly cited,53000000,2.065392e+18,135000,,35,8,,,NVIDIA M40,COCO,,,,,,,,,4528.993268174125,Hardware,,source: table 2 in https://arxiv.org/pdf/1911.09070.pdf,"""We use synchronized SGD over 8 GPUs with a total of 16 images per minibatch (2 images per GPU). Unless otherwise specified, all models are trained for 90k iterations with an initial learning rate of 0.01, which is then divided by 10 at 60k and again at 80k iterations. We use horizontal image flipping as the only form of data augmentation unless otherwise noted. Weight decay of 0.0001 and momentum of 0.9 are used. The training loss is the sum the focal loss and the standard smooth L1 loss used for box regression [10]. Training time ranges between 10 and 35 hours for the models in Table 1e.""

NVIDIA M40 GPU

35*60**2*0.3*8*6.83E+12 = 2.07e18",,trainval135k split,"""We use synchronized SGD over 8 GPUs with a total of 16 images per minibatch (2 images per GPU). Unless otherwise specified, all models are trained for 90k iterations with an initial learning rate of 0.01, which is then divided by 10 at 60k and again at 80k iterations. We use horizontal image flipping as the only form of data augmentation unless otherwise noted. Weight decay of 0.0001 and momentum of 0.9 are used. The training loss is the sum the focal loss and the standard smooth L1 loss used for box regression [10]. Training time ranges between 10 and 35 hours for the models in Table 1e.""

NVIDIA M40 GPU

35*60**2*0.3*8*6.83E+12 = 2.07e18",,,,
RetinaNet-R50,8/7/2017,Highly cited,34000000,,,,,,,,,,,,,,,,,,,,,source: table 2 in https://arxiv.org/pdf/1911.09070.pdf,,,,,,,,"likely code and weights here, would need to sort through whether this specific model is here: https://github.com/facebookresearch/Detectron "
AWD-LSTM - 3-layer LSTM (tied) + continuous cache pointer (WT2),8/7/2017,"Highly cited,SOTA improvement",33000000,3.09e+17,,750,,,,,,WikiText-2,,,,,Unreleased,Open source,Open source,,,,"""we achieve an even lower state-of-the-art perplexity of 52.8 on Penn Treebank and 52.0 on WikiText-2.""",,,,,,,,,"bsd-3 license: https://github.com/salesforce/awd-lstm-lm 
train/eval code: https://github.com/salesforce/awd-lstm-lm/blob/master/main.py "
GSM,7/30/2017,SOTA improvement,,,4000000,,,,,,,SQuAD,Likely,,,,,,,,,,"""At the time of submission of the paper, our model holds the first place on the SQuAD leaderboard for both single and ensemble model.""",It could be possible to estimate it from section 3.,,"""We specially focus on the SQuAD dataset to train and evaluate our model, ""","from https://paperswithcode.com/dataset/squad SQuAD have 107,785 question-answer pairs
download-ed dataset from: https://www.kaggle.com/datasets/stanfordu/stanford-question-answering-dataset?resource=download
wc -w on train-v.1.1 returns 4017471 words so around 4M words",,,,,
ConvS2S (ensemble of 8 models),7/25/2017,"Highly cited,SOTA improvement",,5.64e+19,46600000,,,,,,NVIDIA M40,"WMT English-German,WMT14,Gigaword",Likely,,,,,,,,,Hardware,"""We achieve a new state of the art on several public translation benchmark data sets. On the WMT’16 EnglishRomanian task we outperform the previous best result by 1.9 BLEU, on WMT’14 English-French translation we improve over the LSTM model of Wu et al. (2016) by 1.6 BLEU in a comparable setting, and on WMT’14 EnglishGerman translation we ouperform the same model by 0.5
BLEU""",,"All models are implemented in Torch (Collobert et al., 2011) and trained on a single Nvidia M40 GPU except for WMT’14 English-French for which we use a multi-GPU setup on a single
machine. We train on up to eight GPUs synchronously by
maintaining copies of the model on each card and split the
batch so that each worker computes 1/8-th of the gradients;
at the end we sum the gradients via Nvidia NCCL.

1. English-Romanian: ""Training took between 6 and 7.5 days on a single GPU.""
7 days * 24 * 3600 * 6.8e12 FLOP/s (Nvidia M40, fp32) * 0.3 = 1.2e18 FLOP

2. English-German: "" We trained this model on a single GPU over a
period of 18.5 days with a batch size of 48"".
18.5 days * 24 * 3600 * 6.8e12 FLOP/s (Nvidia M40, fp32) * 0.3 = 3.3e18 FLOP

3. English-French: ""Our results are based on training
with 8 GPUs for about 37 days and batch size 32 on each
worker.6 ""
37 days * 24 * 3600 * 8 * 6.8e12 FLOP/s (Nvidia M40, fp32) * 0.3 = 5.2e19 FLOP

the minimum compute needed to train ensemble model: 1.2e18 FLOP + 3.3e18 FLOP + 5.2e19 FLOP = 5.65e19 FLOP

I am not sure how much to add more (they say ensemble model consists of 8 models), probably summarization training takes at least 1.2e18 FLOP more. 

",," 2.8M + 4.5M + 35.5M + 3.8M = 46.6M

We consider three major WMT translation tasks as well as
a text summarization task.

WMT’16 English-Romanian. We use the same data and pre-processing as Sennrich et al. (2016b) but remove sentences with more than 175 words. This results in 2.8M sentence pairs for training and we evaluate on newstest2016.2

WMT’14 English-German. We use the same setup as Luong et al. (2015) which comprises 4.5M sentence pairs for training and we test on newstest2014.

WMT’14 English-French. We use the full training set of
36M sentence pairs, and remove sentences longer than 175
words as well as pairs with a source/target length ratio exceeding 1.5. This results in 35.5M sentence-pairs for training.

Abstractive summarization. We train on the Gigaword
corpus (Graff et al., 2003) and pre-process it identically
to Rush et al. (2015) resulting in 3.8M training examples
and 190K for validation.",,,,,
PSPNet,7/21/2017,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
NASNet-A,7/21/2017,Highly cited,89000000,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AWD-LSTM,7/18/2017,SOTA improvement,24000000,,,,,,,,,WikiText-2,,,,,Unreleased,Unreleased,,,,,"""We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora""",,,,,,,,,
JFT,7/10/2017,Highly cited,44654504,8.43e+20,300000000,4,1440,50,,17239.593034773116,NVIDIA Tesla K80,JFT-300M,Confident,,,32,,,,TRUE,33978.95160170179,Hardware,,"Uses ResNet-101 architecture, which has 44,654,504 parameters:
https://resources.wolframcloud.com/NeuralNetRepository/resources/ResNet-101-Trained-on-ImageNet-Competition-Data/","Tesla K80 performance: 8.13 TFLOP/s

Assume 40% utilization

60 days * 50 GPUs * 40% utilization * 8.13 TFLOP/s/GPU = 8.43*10^20 FLOP",,,,,,,
ShuffleNet v1,7/3/2017,Highly cited,2430000,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NoisyNet-Dueling,6/30/2017,SOTA improvement,,,,,,,,,,,Unknown,,,,Unreleased,Unreleased,,,,,,,,,,,,,,
DeepLabV3,6/17/2017,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
HRA,6/13/2017,SOTA improvement,,,,,,,,,,,Unknown,,,,,,,,,,"""With the best combination, HRA not only outperforms the state-of-the-art on both metrics, it
also significantly outperforms the human score, convincingly demonstrating the strength of HRA.""",,,,,,,,,
Transformer,6/12/2017,"Highly cited,Historical significance",213000000,7.4245248e+18,1866666666.6666667,3,84,8,,438.0356518424336,NVIDIA P100,"WMT English-German,WMT14",Confident,,,,Unreleased,Unreleased,,,4532.065457226254,Hardware,The original transformer,"This page suggests the transformer has 213M parameters.

""Although there are others architectures that make use of attention layers, none achieves so good results so fast. Not only that, but the only model that can compite against Transformer is the Slicenet22, proposed just fifteen days before. It takes much longer to train, due to the huge amount of parameters it requires (348 million against the 213 millions of Transformer), and the BLEU scores it achieves are slightly worse on average. In short, up to date it offers no profit over Transformer.""

https://ricardokleinklein.github.io/2017/11/16/Attention-is-all-you-need.html","""The model was trained during 300000 steps, roughly 3.5 days, using 8 NVIDIA P100 GPUs.""

source: https://ricardokleinklein.github.io/2017/11/16/Attention-is-all-you-need.html

NVIDIA Tesla P100 has 9.3 teraFLOPS single-precision performance

source: https://www.nvidia.com/en-gb/data-center/tesla-p100/

We assume 0.33 utilization performance, in line with OpenAI's ""AI and compute"" article

source: https://openai.com/blog/ai-and-compute/",,"""We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary ""

In total, this is 40.5 million sentence-pairs. Assuming each sentence pair is 15-20 words in each language, this is 1.2-1.6 billion words.

Convert to tokens: 1.4B * 4/3 = 1.87B tokens","We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using
the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the
bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps
(3.5 days).",,,,
EDSR,6/10/2017,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
Reading Twice for NLU,6/8/2017,SOTA improvement,,,,,,,,,,"TriviaQA,SQuAD",Unknown,,,,,,,,,,"""Our results are competi-
tive with the best systems, achieving a new state
of the art on the recent TriviaQA benchmarks.""",,,"""We use 2 recent DQA
benchmark training and evaluation datasets,
SQuAD (Rajpurkar et al., 2016) and TriviaQA
(Joshi et al., 2017). ""","both datasets have around 100k training examples.
SQuAD have around 4M words. TriviaQA is larger
""We use 2 recent DQAbenchmark training and evaluation datasets,
SQuAD (Rajpurkar et al., 2016) and TriviaQA
(Joshi et al., 2017). """,,,,,
PointNet++,6/7/2017,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
Inflated 3D ConvNet,6/1/2017,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
SRGAN,5/25/2017,Highly cited,,,,,,,,,,,Unknown,,,,Unreleased,Unreleased,,,,,,,,,,,,,,
Mnemonic Reader,5/8/2017,SOTA improvement,,,107785,,,,,,,SQuAD,Confident,,,,,,,,,,"from the abstract "" Extensive experiments on the Stanford Question Answering Dataset (SQuAD) show that our model achieves state-of-the-art results. Meanwhile, our model outperforms previous systems by over 6% in terms of both Exact Match and F1 metrics on two adversarial SQuAD datasets. """,may be possible to estimate architecture description,may be possible to estimate from architecture description,"at the start of section 5.1 Implementation Details (page 5) ""We mainly focus on the SQuAD dataset [Rajpurkar et al.,2016] to train and evaluate our model""",size of SQuAD,,,,,
DeepLab (2017),4/27/2017,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
MobileNet,4/17/2017,Highly cited,4200000,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WGAN-GP,3/31/2017,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
Mask R-CNN,3/30/2017,Highly cited,,,,,,,,,,COCO,Unknown,,,,,,,,,,,,,,,"Training with
ResNet-50-FPN on COCO trainval35k takes 32 hours
in our synchronized 8-GPU implementation (0.72s per 16-
image mini-batch), and 44 hours with ResNet-101-FPN",,,,
Prototypical networks,3/15/2017,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
DnCNN,2/1/2017,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
MoE-Multi,1/23/2017,"Highly cited,SOTA improvement",8700000000,9.393905664e+19,1.33E+11,,288,64,,3863.735305110515,NVIDIA Tesla K40t,,,,,1365333.3333333333,Unreleased,,,TRUE,35592.293705651486,Hardware,"""On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost""","Table 5

https://arxiv.org/abs/1701.06538","12 days 
64 NVIDIA K40 GPUs (see hardware data sheet for performance)
0.33 util rate
 ",,"""We constructed a similar training set consisting of shuffled unique sentences from Google’s internal news corpus, totalling roughly 100 billion words""
Assuming 100 words = 133 tokens",12 days,,,"""Training was done synchronously on a cluster of up to 64 GPUs as described in section 3. Each training batch consisted of a set of sentence pairs containing roughly 16000 words per GPU."" Although they appear to use word-level tokenization in other experiments, here they use subword tokens: ""Similar to GNMT, to effectively deal with rare words, we used subword units (also known as “wordpieces"") (Schuster & Nakajima, 2012) for inputs and outputs in our system."" In total 64 GPUs * 16k words/GPU * 4/3 tokens/word = 1,365,333",
OR-WideResNet,1/7/2017,SOTA improvement,18200000,,50000,,,,,,NVIDIA Tesla K80,CIFAR-10,Confident,,,,,,,,,,"""In Sec. 4.3, we upgrade the VGG [38], ResNet [18], and the
WideResNet [45] to ORNs, and train them on CIFAR10 and
CIFAR100 [22], showing the state-of-the-art performance
on the natural image classification task.""",18.2M for largest OR-WideResNet model.,, CIFAR-10 and CIFAR-100,,,,,,
DeepStack,1/6/2017,SOTA improvement,2500000,1.446336e+19,10000000,,218,20,,,,,Speculative,,,,,,,,,Hardware,"first human-competitive poker AI, confirmed by website: https://www.deepstack.ai/","Figure 3, p.9

source: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389","The largest source of compute necessary for training seems to be the data generation job on 20 GPUs. We count this towards the training compute because it requires simulation using the network. This is analogous to the AlphaGo systems simulating Go games.

From p.26: ""For the flop network, one million poker flop situations (from after the flop cards are dealt) were generated and solved. These situations were solved using DeepStack’s depth limited solver with the turn network used for the counterfactual values at public states immediately after the turn card. We used a cluster of 20 GPUS and one-half of a GPU year of computation time.""

Assume they used P100 GPUs because they were common at the time (P100 was released in 2016 and this paper was published in 2017).

But assume low utilization of 10% to hedge on (a) lower-performing GPUs being used, (b) non-FLOP computations taking up a lot of the data generation job.

Calculation:
6 months * 30 days * 24 hours * 3600 seconds * 9.3e12 FLOP/s * 0.1 utilization = 1.446336e+19 FLOP.",,"""The turn network was trained by solving 10 million randomly generated poker turn
games. These turn games used randomly generated ranges, public cards, and a random pot
size (10).""",from compute notes - around 9 days  - half a year of GPU compute using 20 GPUs,,,,
YOLOv2,12/25/2016,Highly cited,51000000,,,,,,,,,,,,,,Open weights (non-commercial),Unreleased,,,,,,Source: https://resources.wolframcloud.com/NeuralNetRepository/resources/YOLO-V2-Trained-on-MS-COCO-Data_1,,,,,,,,"weights here, no license specified: https://pjreddie.com/darknet/yolo/ "
GCNN-14,12/23/2016,Highly cited,,,,35,,,,,,WikiText-103,Unknown,,,,Unreleased,Unreleased,,,,,,,,,,,,,,
Diabetic Retinopathy Detection Net,12/13/2016,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
GAN-Advancer,12/5/2016,Highly cited,,,,,,,,,,,Unknown,,,,Unreleased,Open (non-commercial),Open access (non-commercial),,,,,,,,,,,,,"code, no weights, unclear license:
https://github.com/openai/improved-gan 
experiment code: https://github.com/openai/improved-gan/tree/master/mnist_svhn_cifar10 "
PointNet,12/2/2016,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
Elastic weight consolidation,12/2/2016,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
Image-to-image cGAN,11/21/2016,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
RefineNet,11/20/2016,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
PolyNet,11/17/2016,SOTA improvement,92000000,6.4e+19,1280000,,,32,,615.5480459425461,NVIDIA GeForce GTX TITAN X,ImageNet,Likely,,,,,,,TRUE,18174.382148313794,"Comparison with other models,Operation counting","""The Very Deep PolyNet, designed following this direction, demonstrates substantial improvements over the state-of-the-art on the ILSVRC 2012 benchmark. Compared to Inception-ResNet-v2, it reduces the top-5 validation error on single crops from 4.9% to 4.25%, and that on multi-crops from 3.7% to 3.45%.""",,"Section 5: ""ResNet-500 [has] similar computation
costs to our Very Deep PolyNet"".

ResNet-152 has 11.3e9 FLOP per forward pass (https://arxiv.org/abs/1512.03385, Table 1). Hence ResNet-500 has approx 3.7e10 = 11.3e9*500/152 FLOP per forward pass.

560k iterations, batch size 512:
Train compute = 3.7e10*3*2*560e3 * 512 = 6.4e19",Section 4,,,,,,
ResNeXt-50,11/16/2016,Highly cited,25000000,,,,,,,,,,,,,,,,,,,,,"""If you’re thinking about ResNets, yes, they are related. ResNeXt-50 has 25M parameters (ResNet-50 has 25.5M).""

https://towardsdatascience.com/illustrated-10-cnn-architectures-95d78ace614d",,,,,,,,
Deeply-recursive ConvNet,11/11/2016,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
NASv3 (CIFAR-10),11/5/2016,Highly cited,37400000,2.2e+21,50000,,,800,,,,,Likely,,,,,,,TRUE,,"Third-party estimation,Operation counting",,Table 1,"50 epochs * 50,000 images * 10.0 GFLOPSs * 12800 networks * 2 add-multiply * 3 backward pass 
= 1.9e6 PF = 22 pfs-days

source: https://openai.com/blog/ai-and-compute/",,CIFAR-10 (does not factor in augmentation procedures),,,,,
NAS with base 8 and shared embeddings,11/5/2016,Highly cited,54000000,1.05e+16,,35,,,,,,Penn TreeBank,,,,,Unreleased,Unreleased,,,,,,,,,,,,,,
BIDAF,11/5/2016,"Highly cited,SOTA improvement",2600000,3.4686144e+18,47160000,8,60,8,,41.25014556779628,NVIDIA GeForce GTX TITAN X,"SQuAD,DMQA,GloVe",Confident,,,,Open weights (unrestricted),Open source,Open source,,4544.272433902901,Hardware,"""Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test. ""","There are two similar models described in sections ""Models details""
citation from the paper about model for SQuAD ""The model has about 2.6 million parameters""
citation about model for cloze test
""The model architecture used for this task is very similar to that for SQuAD (Section 4) with only a few small changes to adapt it to the cloze test. ""
","flops = (8) * (6691 * 10**9) * (60 * 3600) * 3 // 10
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate) =

citation from the section about cloze test experiments ""The entire training process takes roughly 60 hours on eight Titan X GPUs. The other hyper-parameters are identical to the model described in Section 4"" (section 4 is about SQuAD experiments and cloze test experiments require more compute and data).
flops  6.691 TFLOPS from https://www.techpowerup.com/gpu-specs/geforce-gtx-titan-x.c2632","""In a cloze test, the reader is asked to fill in words that have been removed from a passage, for measuring one’s ability to comprehend text. Hermann et al. (2015) have recently compiled a massive Cloze-style comprehension dataset, consisting of 300k/4k/3k and 879k/65k/53k (train/dev/test)
examples from CNN and DailyMail news articles, respectively. ""","""In a cloze test, the reader is asked to fill in words that have been removed from a passage, for measuring one’s ability to comprehend text. Hermann et al. (2015) have recently compiled a massive Cloze-style comprehension dataset, consisting of 300k/4k/3k and 879k/65k/53k (train/dev/test)
examples from CNN and DailyMail news articles, respectively. ""
assuming 40 words per example we get around 47160000 words (SQuAD have around 40 words per example - so I think it should be similar case for this dataset)",see compute notes,,,,"apache 2.0, code + weights: https://github.com/allenai/bi-att-flow"
VD-LSTM+REAL Large,11/4/2016,SOTA improvement,51000000,2.13e+16,,75,,,,,,Penn TreeBank,,,,,Unreleased,Unreleased,,,,,"""Our framework leads to state of the art performance on the Penn Treebank""",,,,,,,,,
SPIDER2,10/28/2016,SOTA improvement,409536,1.822e+16,13893600,120,,,,,,Unspecified,Likely,,,,Open weights (non-commercial),,,,,Operation counting,"The method provides state-of-the-art, all-in-one accurate prediction of local structure and solvent accessible surface area. ","Three networks, each three layers. First takes in 459 inputs and outputs 12, second and third take in 459 + (12*17) = 663 inputs.

Network 1: (459 * 150 + 150) + (150 * 150 + 150) + (150 * 150 + 150) + (150 * 12 + 12) = 116,112
Networks 2 and 3: (663 * 150 + 150) + (150 * 150 + 150) + (150 * 150 + 150) + (150 * 12 + 12) = 146,712
Total: 116,112 + (2 * 146,712) = 409,536","120 epochs, dataset 5789 proteins. There are about 300 residues per protein (115,479 residues / 418 proteins) according to https://www.ncbi.nlm.nih.gov/pmc/articles/PMC22960/. 
First network gets 27 features per residue, second and third get 39.
FLOPs from first: 6 * 116112 * (27 * 300 * 5789 * 120) = 3.92e15
FLOPs from 2nd and 3rd: 2 *6 * 146712 * (39 * 300 * 5789 * 120) = 1.43e16
Total: 1.822E16",,"5,789 nonredundant, high resolution structure.
Assuming ~200 residues per protein, 5,789 * 200 = 1,157,800 residues. Each residue has 12 associated features being predicted on.
1,157,800 * 12 = 13,893,600","The authors had a website where sequences could be submitted for processing through the model: ""Each prediction is usually completed within 10 min, but may take up to a few hours depending on how busy the server is and how long the protein chain is [...] Using an external PSSM file can skip the most time consuming step of generating the evolution profile by PSIBLAST, and the executive time reduce to a few seconds"" 

Rough estimate:
It looks like the PSIBLAST step only needs doing once per input, and this takes the majority of the time. If the inference server uses the same hardware that was used for training, 10 mins * 5789 sequences =  965 hours for PSIBLAST calculation. Then assume training on a sequence takes 3x as long as inference (forward + backward pass uses 6 FLOPs per parameter, vs 2 for forward only), so 120 epochs would take:
3 seconds * 3 * 5789 * 120 = 1,737 hours
Total: around 2,702 hours
(This seems on the long side – probably they had better hardware for training, or else there's an incorrect assumption here)",,,,"some kind of download, unclear license

http://zhouyq-lab.szbl.ac.cn/download/"
Differentiable neural computer,10/12/2016,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
Xception,10/7/2016,Highly cited,22855952,4.36e+20,350000000,,720,60,,12617.626833005994,NVIDIA Tesla K80,JFT,Confident,,,,,,,TRUE,40913.209297738365,Hardware,,Table 3,60 K80 GPUs * 30 days * 8.5 TFLOPS/GPU * 0.33 utilization  = 4.36e20,"Also ImageNet, but JFT is significantly larger","""JFT is an internal Google dataset for large-scale image classification dataset, first introduced by Hinton et al. in [5], which comprises over 350 million high-resolution images annotated with labels from a set of 17,000 classes. To evaluate the performance of a model trained on JFT, we use an auxiliary dataset, FastEval14k""","""while the JFT experiments took over one month each.""",,,,
Zoneout + Variational LSTM (WT2),9/26/2016,Highly cited,21000000,1.68e+16,,64,,,,,,WikiText-2,,,,,Unreleased,Unreleased,,,,,,,,,,,,,,
Pointer Sentinel-LSTM (medium),9/26/2016,"Highly cited,SOTA improvement",21000000,7490000000000000.0,,64,,,,,,Penn TreeBank,,,,,Unreleased,Unreleased,,,,,"""Our pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM""",,,,,,,,,
GNMT,9/26/2016,Highly cited,278000000,6.620000000001e+21,388960152200.7108,1,,,,193787.11043815003,NVIDIA Tesla K80,,,,,,Hosted access (no API),Unreleased,,TRUE,,"Hardware,Third-party estimation",,"Table 5 in 'Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer'

https://arxiv.org/abs/1701.06538","From AI and Compute:
""sqrt(10 * 100) factor added because production model used 2-3 orders of magnitude more data, but only 1 epoch rather than 10.
96 K80 GPU’s * 9 days * 8.5 TFLOPS * 0.33 utilization * sqrt(10 * 100)  
= 6.9e6 PF = 79 pfs-days""
source: https://openai.com/blog/ai-and-compute/

https://www.wolframalpha.com/input?i=96+*+9+days+*+8.5+TFLOPS+*+0.33+*+sqrt%281000%29
",,"[WORDS]
"" On WMT En→Fr, the training set contains 36M sentence pairs. On WMT En→De, the training set contains 5M sentence pairs.""
""we also test GNMT on Google’s translation production corpora, which are two to three decimal orders of magnitudes bigger than the WMT corpora for a given language pair.""

41M sentence pairs * 2 sentences per pair * 15 words/sentence * 10^2.5","Test model used 96 K80 for 9 days, then this was scaled up by 31x for the production model, but unclear how many GPUs were used or how long it was trained for. The production run used 96 * 9 days * sqrt(1000) ~= 655730 chip-hours.",,,,presumably deployed via Google translate
Wide Residual Network,9/19/2016,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
TSN,9/17/2016,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
Stacked hourglass network,9/17/2016,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
ResNet-1001,9/17/2016,Highly cited,10200000,,,,,,,,,"CIFAR-10,CIFAR-100",,,,,,,,,,,,,"""On CIFAR, ResNet-1001 takes about 27 h to train on 2 GPUs""",,,,,,,
ResNet-200,9/17/2016,Highly cited,,2.9741645e+19,1281167,,500,,,,,ImageNet,Speculative,,,,Unreleased,Open (non-commercial),Open access (non-commercial),TRUE,,Hardware,,,"""ResNet-200 takes about 3 weeks to train on 8 GPUs"". didn't specify which GPU
upd: 
common GPU performance for 2016 is 6.83E+12 FLOPs/s (https://epoch.ai/blog/estimating-training-compute#forward-pass-compute-and-parameter-counts-of-common-layers) 
then 6.83E+12*3*7*24*3600*8*0.3=2.9741645e+19 (Speculative)",,,"""about 3 weeks""",,,," https://github.com/KaimingHe/resnet-1k-layers
no definite license"
MS-CNN,9/17/2016,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
Youtube recommendation model,9/15/2016,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
WaveNet,9/12/2016,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
Multi-task Cascaded CNN,8/26/2016,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
DenseNet-264,8/25/2016,Highly cited,34000000,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SimpleNet,8/22/2016,SOTA improvement,5480000,,,,,,,,NVIDIA GeForce GTX 980,"CIFAR-10,ImageNet",Confident,,,,,,,,,,"""We achieved state-of-theart result on CIFAR10 outperforming several heavier architectures""",SOTA CIFAR-10 model was 5.48m params,,"""We experimented on CIFAR-10/100 Krizhevsky & Hinton (2009), SVHN Netzer et al. (2011), MNIST
Lecun et al. (1998) and ILSVRC 2012 classification task Russakovsky et al. (2015) datasets in order
to evaluate and compare our architecture""",,,,,,
Character-enriched word2vec,7/15/2016,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,"repo here, unclear how it corresponds to models in this paper:
https://github.com/facebookresearch/fastText "
VD-RHN,7/12/2016,SOTA improvement,32000000,3570000000000000.0,,20,,,,,,Penn TreeBank,,,,,Unreleased,Open source,Open source,,,,"""On the larger Wikipedia datasets for character prediction (text8 and enwik8), RHNs outperform all previous results and achieve an entropy of 1.27 bits per character.""",,,,,,,,,MIT for code: https://github.com/jzilly/RecurrentHighwayNetworks 
fastText,7/6/2016,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
Wide & Deep,6/24/2016,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
R-FCN,6/21/2016,Highly cited,,6.1492939794e+16,94427,,,,,,,"PASCAL VOC 2007,PASCAL VOC 2012,COCO",,,,,,,,,,Hardware,,,"1,464  images in 2012 VOC (https://paperswithcode.com/dataset/pascal-voc)/
9,963 images in 2007 VOC (https://www.tensorflow.org/datasets/catalog/voc)
83K training images in MS COCO  (https://paperswithcode.com/dataset/coco)

They used a Nvidia K40 GPU and report training time/image in seconds (table 3)

Assumed a 0.33 util rate",,,,,,,
DMN,6/20/2016,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
Spatiotemporal fusion ConvNet,6/1/2016,Highly cited,,,97200,,,,,,,UCF101,,,,,,,,,,,,,,,"[SECONDS OF VIDEO]

They use UCF101, whose paper says
""We introduce UCF101 which is currently the largest dataset of human actions. It consists of 101 action classes, over 13k clips and 27 hours of video data""",,,,,
Part-of-sentence tagging model,5/29/2016,Highly cited,,1.454112e+17,912344,50,12,1,,,NVIDIA GeForce GTX TITAN X,"WSJ,Penn TreeBank",Confident,,,,,,,,569.1734872517258,Hardware,,Architecture described in Table 1,"12 hours of training for POS tagging
GeForce GTX TITAN X GPU
0.33 utilization rate
",WSJ subset of Penn Treebank,Table 2,"""the model training requires about 12 hours for POS tagging and 8
hours for NER""",,,,
Named Entity Recognition model,5/29/2016,Highly cited,,9.69408e+16,204567,50,8,1,,,NVIDIA GeForce GTX TITAN X,CoNLL2003,Confident,,,,,,,,569.1734872517258,Hardware,,Architecture in Table 1,"8 hours of training for NER
GeForce GTX TITAN X GPU
0.33 utilization rate
",Table 2,Table 2. 204567 tokens,"""the model training requires about 12 hours for POS tagging and 8
hours for NER""",,,,
Gated HORNN (3rd order),4/30/2016,SOTA improvement,8970000,,,,,,,,,Penn TreeBank,,,,,Unreleased,Unreleased,,,,,"""Both FOFEbased pooling and gated HORNNs have achieved the stateof-the-art performance, i.e., 100 in perplexity on this task.
To the best of our knowledge, this is the best reported performance on PTB under the same training condition.""",,,,,,,,,
Symmetric Residual Encoder-Decoder Net,3/30/2016,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
Binarized Neural Network (MNIST),3/17/2016,Highly cited,37000000,,60000,1000,,,,,,MNIST,Speculative,,,,,,,,,,,"Parameter count is not explicitly stated, but they give details:

""The MLP we train on MNIST consists of 3 hidden layers of 4096 binary units (see Section 1) and a L2-SVM output layer""

Approximately 37m, based on 784 pixels * 4096 + 2 * 4096^2",,,"60k training images, 10k test in MNIST",,,,,
SqueezeNet,2/24/2016,Highly cited,1200000,,,,,,,,,,,,,,,,,,,,,"The paper says ""SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters.""

AlexNet has 60 million parameters.",,,,,,,,
Inceptionv4,2/23/2016,Highly cited,43000000,,,,,,,,,,,,,,,,,,,,,"""The folks from Google strike again with Inception-v4, 43M parameters.""

https://towardsdatascience.com/illustrated-10-cnn-architectures-95d78ace614d",,,,,,,,
Inception-ResNet-V2,2/23/2016,Highly cited,56000000,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A3C FF hs,2/4/2016,"Highly cited,SOTA improvement",,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
Convolutional Pose Machines,1/30/2016,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
AlphaGo Lee,1/27/2016,Highly cited,,1.9e+21,29400000,,696,,,,,,Speculative,,,,Unreleased,Unreleased,,TRUE,,Comparison with other models,,,"This number is pretty uncertain. I expect it to be right to around a factor of 3, at least compared to AlphaGo Fan.

The architecture used was pretty much the same as AlphaGo Fan, but it was ""trained for longer"" and had around 5.33x the number of convolutional layers of AlphaGo Fan (256/48 = 5.33). 

The convolutional layers are the major contributor to the training compute, so I somewhat arbitrarily just multiply the compute for AlphaGo Fan by 5. Thus 3.8e20 * 5 = 1.9e21

Otherwise there has been little said about this model specifically - I've mainly relied on the source for AlphaGo Zero and AlphaGo Fan, linked below

AlphaGo Fan: https://www.nature.com/articles/nature16961

AlphaGo Zero: https://www.nature.com/articles/nature24270.epdf?author_access_token=VJXbVjaSHxFoctQQ4p2k4tRgN0jAjWel9jnR3ZoTv0PVW4gB86EEpGqTRDtpIz-2rmo8-KG06gqVobU5NSCFeHILHcVFUeMsbvwS-lxjqQGg98faovwjxeTUgZAUMnRQ",,"We trained the policy network pσ to classify positions according to expert moves played in the KGS data set. This data set contains 29.4 million positions from 160,000 games played by KGS 6 to 9 dan human players; 35.4% of the games are handicap games.","Training times are given for several components:
- Policy network classifier: 3 weeks
- Policy network RL: 1 day
- Value network regression: 1 week
- Rollout policy: ""Similar to the policy network, the weights π of the rollout policy are trained from 8 million positions from human games on the Tygem server to maximize log likelihood by stochastic gradient descent. Rollouts execute at approximately 1,000 simulations per second per CPU thread on an empty board."" could suggest (8M sims / 1000 sims/sec) / 3600 sec/hr = 2.2 hours, if each position corresponds to only one simulation (unclear)

Total: 29 days or 696 hours",,,,
"Variational (untied weights, MC) LSTM (Large)",12/16/2015,"Highly cited,SOTA improvement",66000000,5620000000000000.0,,16,,,,,,,,,,,Unreleased,Unreleased,,,,,"""The new approach outperforms existing techniques, and to the best of our knowledge improves on the single model state-of-the-art in language modelling with the Penn Treebank (73.4 test perplexity)""",,,,,,,,,
Advantage Learning,12/15/2015,SOTA improvement,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
BPL,12/11/2015,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
ResNet-152 (ImageNet),12/10/2015,Highly cited,60200000,1.041408e+19,1280000,120,,,,,,ILSVRC 2012 subset of ImageNet,,,,256,,,,TRUE,,Operation counting,,Taken from https://arxiv.org/abs/1605.07146,"11.3 *10^9 mult-adds per forward pass (Table 1)
2 FLOPS/ mult-add
3 for forward & backward pass
1.2 * 10^6 examples in dataset
128 epochs","They won ILSVRC 2015, but actually the classification dataset is the same as 2012","""We evaluate our method on the ImageNet 2012 classification dataset [36] that consists of 1000 classes. The models are trained on the 1.28 million training images""",,,,,
ResNet-110 (CIFAR-10),12/10/2015,Highly cited,1700000,,,,,,,,,,,,,,,,,,,,,Table 6,,,,,,,,
SSD,12/8/2015,Highly cited,,,115000,,,,,,NVIDIA GeForce GTX TITAN X,,Confident,VGG16,,,Open weights (unrestricted),,,,,,Also listed in Denis Panjuta's List of 100+ AI Algorithms,Can be calculated from the VGG-16 paper and Figure 2,,,"Multiple datasets were used (PASCAL VOC, ILSVRC, COCO) but afaict COCO is the largest. Per this paper https://arxiv.org/abs/1703.06870v1, final paragraph before section 4.1, ""As in previous work [3, 21], we train using the union of 80k train images and a 35k subset of val images (trainval35k), and report ablations on the remaining 5k subset of val images (minival)"". So trainval35k is 80k + 35k = 115k","Note on training hardware below: unclear if the Titan X was for testing or training. ""We measure the speed with batch size 8 using Titan X and cuDNN v4 with Intel Xeon E5-2667v3@3.20GHz."" I don't know if it's common to use the same hardware for both testing and training.

",,,,
DeepSpeech2 (English),12/8/2015,Highly cited,38000000,2.6e+19,163339200,,120,16,0.4484,206.30577788417415,NVIDIA GeForce GTX TITAN X,,Confident,,,,,,,TRUE,9126.870567999174,"Operation counting,Third-party estimation",,All networks have 38 million parameters.,"1 timestep = (1280 hidden units)^2 * (7 RNN layers * 4 matrices for bidirectional + 2 DNN layers) * (2 for doubling parameters from 36M to 72M) = 98 MFLOP
20 epochs * 12,000 hours * 3600 seconds/hour * 50 samples/sec * 98 MFLOP * 3 add-multiply * 2 backprop 
= 26,000 PF = 0.30 pfs-days

See also AI and Compute by Dario Amodei and OpenAI https://openai.com/research/ai-and-compute",,"""Our English speech system is trained on 11,940 hours of speech, while the Mandarin system is trained on 9,400 hours.""

11,940 * 13,680 = 163339200","""5 days"" from AI and Compute https://openai.com/index/ai-and-compute/",,,,
Inception v3,12/2/2015,Highly cited,23626728,,1200000,,,,,,,ILSVRC 2012 subset of ImageNet,,,,,,,,,,,,Table 3 from Xception paper,,,"The full dataset is a lot larger and has far more categories. When people say ""ImageNet"" they're usually referring to the subset of the full dataset with 1000 categories and 1.2million images, found here: https://image-net.org/challenges/LSVRC/2012/",,,,,
Netflix Recommender System,12/1/2015,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
Multi-scale Dilated CNN,11/23/2015,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
AlphaGo Fan,10/1/2015,"Highly cited,SOTA improvement",8209984,3.8e+20,,,,,,,,,,,,,Unreleased,Unreleased,,TRUE,,Hardware,,"The input to the policy network is a 19 × 19 × 48 image stack consisting of 48 feature planes. The first hidden layer zero pads the input into a 23 × 23 image, then convolves k filters of kernel size 5 × 5 with stride 1 with the input image and applies a rectifier nonlinearity. Each of the subsequent hidden layers 2 to 12 zero pads the respective previous hidden layer into a 21 × 21 image, then convolves k filters of kernel size 3 × 3 with stride 1, again followed by a rectifier nonlinearity. The final layer convolves 1 filter of kernel size 1 × 1 with stride 1, with a different bias for each position, and applies a softmax function. The match version of AlphaGo used k = 192 filters; Fig. 2b and Extended Data Table 3 additionally show the results of training with k = 128, 256 and 384 filters.

The input to the value network is also a 19 × 19 × 48 image stack, with an additional binary feature plane describing the current colour to play. Hidden layers 2 to 11 are identical to the policy network, hidden layer 12 is an additional convolution layer, hidden layer 13 convolves 1 filter of kernel size 1 × 1 with stride 1, and hidden layer 14 is a fully connected linear layer with 256 rectifier units. The output layer is a fully connected linear layer with a single tanh unit.","Assume 0.3 utilisation rate, 1e13 GPU FLOP/s [single precision]. Trained in three stages using 50 GPUs over 3 weeks + 1 day + 1 week

Training compute = (50 GPUs)(29 days)(86400s/day)(0.3 utilisation rate)(1e13 FLOP/s) = 3.8e20 FLOPs",,Supervised learning + self-play,,,,,
Deep Deterministic Policy Gradients,9/9/2015,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
BPE,8/31/2015,Highly cited,,,37500000,,,,,,,WMT'15,,,,,,,,,,,,,,,"[WORDS]
""We perform experiments on data from the shared translation task of WMT 2015. For English→German, our training set consists of 4.2 million sentence pairs, or approximately 100 million tokens. For English→Russian, the training set consists of 2.6 million sentence pairs, or approximately 50 million tokens""

100M tokens, around half will be in English, 0.75 words per token

",,,,,
LSTM-Char-Large,8/26/2015,Highly cited,19000000,2650000000000000.0,,25,,,,,,Penn TreeBank,,,,,Unreleased,Open source,Open source,,,,,,,,,,,,,"code, MIT license: https://github.com/yoonkim/lstm-char-cnn "
"Listen, Attend and Spell",8/20/2015,Highly cited,,,,,,,,,,,Unknown,,,,Unreleased,Unreleased,,,,,,,,,,,,,,
Search-Proven Best LSTM,7/6/2015,Highly cited,20000000,3340000000000000.0,,30,,,,,,,,,,,Unreleased,Unreleased,,,,,,,,,,,,,,
BatchNorm,6/15/2015,Highly cited,13600000,,,72,,,,,,ImageNet,Confident,GoogLeNet / InceptionV1,,,,,,,,,,"""The network contains 13.6 · 106 parameters""",,,,,,,,
YOLO,6/8/2015,Highly cited,271684800,,,,,,,,,,,,,,,,,,,,,"Calculation based on figure 3 of the paper:
7 * 7 * 3 * 64 + 3 * 3 * 64 * 192 + 1 * 1 * 192 * 128 + 3 * 3 * 128 * 256 + 1 * 1 * 256 * 256 + 3 * 3 * 256 * 512 + 4 * (1 * 1 * 512 * 256 + 3 * 3 * 256 * 512) + 1 * 1 * 512 * 512 + 3 * 3 * 512 * 1024 + 2 * (1 * 1 * 1024 * 512 + 3 * 3 * 512 * 1024) + 4 * (3 * 3 * 1024 * 1024) + 7 * 7 * 1024 * 4096 + 4096 * 7 * 7 * 30",,,,,,,,
Faster R-CNN,6/4/2015,Highly cited,,,,,,,,,,,Unknown,,,,Open weights (unrestricted),Open source,Open source,,,,,,,,,,,,,"MIT license for repo:
https://github.com/ShaoqingRen/faster_rcnn 

contains weights and training scripts"
Trajectory-pooled conv nets,5/19/2015,Highly cited,9106245,,,,,,,,,"ImageNet,UCF101",,,,,,,,,,,,"The input layer takes either a single RGB frame (224x224x3) for the spatial stream or a stack of 10 optical flow frames (224x224x20) for the temporal stream.
The first convolutional layer has 96 filters of size 7x7 with stride 2.
This is followed by max pooling with size 3x3 and stride 2.
The second convolutional layer has 256 filters of size 5x5 with stride 2.
After that is another max pooling layer (3x3, stride 2).
The third convolutional layer has 512 filters of size 3x3 with stride 1.
The fourth convolutional layer has 512 filters of size 3x3 with stride 1.
The fifth convolutional layer has 512 filters of size 3x3 with stride 1.
Next is a max pooling layer (3x3, stride 2).
The fully-connected layers have 4096, 2048, and 101 neurons respectively.

(7*7*20+1)*96 + (5*5*20+1)*256 + (3*3*20+1)*512 + (3*3*20+1)*512 + (3*3*20+1)*512 + 2*4096 + (4096+1)*2048 + (2048+1)*101 = 9106245",,,"They pretrain on ImageNet, and use UCF101 for actions. Its paper says ""We introduce UCF101 which is currently the largest dataset of human actions. It consists of 101 action classes, over 13k clips and 27 hours of video data"".",,,,,
Deep LSTM video classifier,5/1/2015,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
Fast R-CNN,4/30/2015,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
genCNN + dyn eval,3/17/2015,SOTA improvement,8000000,7.3e+16,,,,,,,,Penn TreeBank,,,,,Unreleased,Unreleased,,,,,"""genCNN outperforms the state-ofthe-arts with big margins.""",,,,,,,,,
Constituency-Tree LSTM,2/28/2015,Highly cited,205190,,,,,,,,,,,,,,,,,,,,,"Table 1

https://arxiv.org/abs/1503.00075",,,,,,,,
DQN-2015,2/25/2015,Highly cited,1693362,,50000000,,,,,,,,,,,,,,,,,,,"""The input to the neural network consists of an 84x84x4 image produced by the preprocess-ing mapw. The first hidden layer convolves 32 filters of 8x8 with stride 4 with theinput image and applies a rectifier nonlinearity. The second hidden layer con-volves 64 filters of 4x4 with stride 2, again followed by a rectifier nonlinearity.This is followedby a thirdconvolutional layer thatconvolves 64 filtersof 3x3 withstride 1 followed by a rectifier. The final hidden layer is fully-connected and con-sists of 512 rectifier units. The output layer is a fully-connected linear layer with asingle output for each valid action. The number of valid actions varied between 4 and 18 on the games we considered.""

Example num params here: https://colab.research.google.com/drive/1Ty6SFYWd7EcKoxJohucL2OdiLR_3oXnI?usp=sharing","This should be calculatable, just needs careful reasoning about compute per frame.",,"Methods: ""we trained for a total of 50 million frames""",,,,,
TRPO,2/19/2015,Highly cited,33500,,,,30,,,,,,Confident,,,,Unreleased,,,,,,Also listed in Denis Panjuta's List of 100+ AI Algorithms,,,,,"""The 500 iterations of our algorithm took about 30 hours (with slight variation between games) on a 16-core computer.""",,,,
CRF-RNN,2/11/2015,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
"MSRA (C, PReLU)",2/6/2015,Highly cited,87048800,2.397403008e+19,1280000,,588,,,,NVIDIA Tesla K40t,,,,,,,,,TRUE,,Hardware,,"I used the architecture in table 3
I ignored biases, and assumed a SPP bin size of 256

","""training C on eight K40 GPUs, takes about 3-4 weeks""
0.33 util rate
(From Imagenet paper-data, Besiroglu et al., forthcoming) ","They won ILSVRC 2015, but actually the classification dataset is the same as 2012","""We perform the experiments on the 1000-class ImageNet 2012 dataset"", paper; ImageNet 2012 train set size from https://huggingface.co/datasets/imagenet-1k",,,,,
DeepLab,12/22/2014,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
ADAM (CIFAR-10),12/22/2014,Highly cited,,6.048e+16,,,,,,,,,,,,,,,,,,Third-party estimation,,CIFAR-10 with c64-c64-c128-1000 architecture,"From https://openai.com/blog/ai-and-compute/ Appendix

less than 0.0007 pfs-days (86400*10^15*0.0007)",,,,,,,
Fractional Max-Pooling,12/18/2014,SOTA improvement,27000000,1e+17,,250,18,,,,NVIDIA GeForce GTX 780,CIFAR-100,Likely,,,,,,,,,Hardware,"""for instance, we improve on the state-of-the art for CIFAR-100 without even using dropout.""",27M weights in largest CIFAR-100 model,"For the 12M param model, training required ""18 hours on a GeForce GTX 780"". So would be somewhat larger for 27M.

4 TFLOPS * 18 * 3600 * 0.4 = 1e17",,,,,,,
NTM,12/10/2014,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
SNM-skip,12/3/2014,SOTA improvement,62000000000,2.97600000001e+20,1000000000,,,,,,,One Billion Word benchmark,Speculative,,,,,,,TRUE,,Operation counting,'When using skip-gram features the models are able to match the state-of-the-art recurrent neural network (RNN) LMs; combining the two modeling techniques yields the best known result on the benchmark. ' - from abstract,62B from Table 2,https://www.wolframalpha.com/input?i=0.8+billion+*+62+billion+*+6+FLOP,'Our experimental setup used the One Billion Word Benchmark corpus' from section 4.1 - 'Total number of training tokens is about 0.8 billion',1B from 'Our experimental setup used the One Billion Word Benchmark corpus' from section 4.1 - 'Total number of training tokens is about 0.8 billion',,,,,
Cascaded LNet-ANet,11/28/2014,Highly cited,,,,,,,,,,"ILSVRC 2012 subset of ImageNet,CelebA",Unknown,,,,,,,,,,,,,,,,,,,
Fully Convolutional Networks,11/14/2014,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
SC-NLM,11/10/2014,Highly cited,,,600000,,,,,,,"COCO,Flickr30K Entities",Confident,,,,,,,,,,,,,,"Our LSTM encoder and SC-NLM decoder were trained by concatenating the Flickr30K dataset with the recently released Microsoft COCO dataset [46], which combined give us over 100,000 images and over 500,000 descriptions for training",,,,,
LRCN,11/7/2014,Highly cited,142552000,,40000,,,,,,,TaCoS,,,,,,,,,,,,"1st model: CaffeNet fc6 feature extractor (4096-length vectors) -> LSTM with 1024 hidden units

2nd model: CaffeNet fc6 feature extractor (4096-length vectors) -> 2 layer LSTM with 1000 hidden units

3rd mode: Like the second, but has encoder and decoder LSTMs (both with 2 layers)

AlexNet (close relative to CaffeNet) has 61M params.

LSTM RNN number of parameters is given by L*(n*m + n^2 + n) where L:= Number of layers, n:= hidden units, m:= input vector length
",,"Largest model is for image captioning:
Pretrained with ILSVRC 2021 (1.2M images)
Trained on 40k video-sentence pairs from TaCoS",,,,,,
Spatially-Sparse CNN,9/23/2014,SOTA improvement,,,,,,,,,,CIFAR-10,Unknown,,,,,,,,,,SOTA per https://paperswithcode.com/sota/image-classification-on-cifar-10,Parameter count not stated but is probably derivable from the paper.,,,,,,,,
Deeply-supervised nets,9/18/2014,"Highly cited,SOTA improvement",,,870000,,,,,,,"MNIST,CIFAR-10,CIFAR-100,SVHN (Street View House Numbers)",,,,,,,,,,,,,,"According to the paper, the Deeply-Supervised Nets (DSN) model was trained and evaluated on these image classification datasets:

MNIST - handwritten digits dataset with 60,000 training images and 10,000 test images.
CIFAR-10 - 60,000 32x32 color images across 10 classes, with 50,000 for training and 10,000 for testing.
CIFAR-100 - similar to CIFAR-10 but with 100 classes and 600 images per class.
SVHN - Street View House Numbers dataset with over 600,000 images of digits for training and 26,000 images for testing.",60000+50000+60000+600000,,,,,
GoogLeNet / InceptionV1,9/17/2014,Highly cited,6797700,1.51e+18,1200000,827,,,,,,"ILSVRC 2014 subset of ImageNet,ImageNet",Confident,,,,,,,TRUE,,Third-party estimation,,Computed summing the parameters on table 1 of section 5,"AI and Compute  (https://openai.com/blog/ai-and-compute/) charts imply a value of 1.51e18 (value extracted using WebPlotDigitizer  https://automeris.io/WebPlotDigitizer/ ).

Based on the paper, there are 1.5B multiply-adds per inference, and 1.2M images in the training set, but an unknown number of epochs. They decrease the learning rate by 4% every 8 epochs, so there are likely many. If the figure from AI and Compute is taken as true, there were likely 140 epochs",,"""The ILSVRC 2014 classification challenge involves the
task of classifying the image into one of 1000 leaf-node categories in the Imagenet hierarchy. There are about 1.2 million images for training, 50,000 for validation and 100,000 images for testing""
...
""We participated in the challenge with no external data used for training.""","""Although we used CPU based implementation only, a rough estimate suggests that the GoogLeNet network could be trained to convergence using few high-end GPUs within a week""",,,,
SPN-4+KN5,9/14/2014,SOTA improvement,5000000,4.4e+16,,,,,,,,Penn TreeBank,,,,,Unreleased,Open (non-commercial),Open access (non-commercial),,,,"""Our empirical comparisons with
six previous language models indicate that our SPN has superior
performance""",,,,,,,,,"code, no license specified: https://github.com/stakok/lmspn/tree/master/SPNLM 
training code: https://github.com/stakok/lmspn/blob/master/SPNLM/README.doc "
Seq2Seq LSTM,9/10/2014,Highly cited,1920000000,5.6e+19,652000000,,240,,,,,WMT14,,,,,,,,TRUE,,"Operation counting,Hardware",,"The resulting LSTM has 384M parameters of which 64M are pure recurrent connections (32M for the “encoder” LSTM and 32M
for the “decoder” LSTM).
The paper uses an ensemble of 5 LSTMs.","384E+6 parameters * 2 FLOP/parameter * (348E+6 + 304E+6 points per epoch) * 7.5 epochs * 3 FLOP/point ~= 1.126656e+19 FLOP
Times 5 independent models in ensemble => 5.6E+19 FLOP

If we assume NVIDIA K40 (in use at the time): 10 days * 24 * 60 * 60 seconds/day * 8 GPUs * 33% * 5e12 FLOP/s * 5 models in ensemble ~= 5.7E+19 FLOP",,"[WORDS]
""We used the WMT’14 English to French dataset. We trained our models on a subset of 12M sentences consisting of 348M French words and 304M English words, which is a clean “selected”
subset from [29].""",Training took about 10 days,,,,
Large regularized LSTM,9/8/2014,Highly cited,66000000,9.1e+16,,55,,,,,,Penn TreeBank,,,,,Unreleased,Open source,Open source,,,,,,,,,,,,,"Apache: https://github.com/wojzaremba/lstm 
train: https://github.com/wojzaremba/lstm/blob/master/main.lua "
VGG19,9/4/2014,Highly cited,144000000,,1300000,,,,,,,ILSVRC 2012 subset of ImageNet,,,,,,,,,,,,"Source: Table 2
https://arxiv.org/abs/1409.1556",,,"""In this section, we present the image classification results achieved by the described
ConvNet architectures on the ILSVRC-2012 dataset (which was used for ILSVRC 2012–2014 challenges). The dataset includes images of 1000 classes, and is split into three sets: training (1.3M images), validation (50K images), and testing (100K images with held-out class labels).""",,,,,
VGG16,9/4/2014,Highly cited,138000000,1.2291e+19,1300000,74,504,4,,,NVIDIA GTX Titan Black,ILSVRC 2012 subset of ImageNet,Confident,,,256,,,,TRUE,2295.5738918135626,Hardware,,"Source: Table 2
https://arxiv.org/abs/1409.1556","3 weeks * 4 Titan Black GPUs * 0.30 utilization

Section 3.3: ""On a system equipped with four NVIDIA Titan Black GPUs, training a single net took 2–3 weeks depending on the architecture.""

Titan Black performance: 5.645 TFLOPS (assuming FP32)

https://www.wolframalpha.com/input?i=5.645+TFLOPS+*+3+weeks+*+4+*+0.3


",,"""In this section, we present the image classification results achieved by the described ConvNet architectures on the ILSVRC-2012 dataset (which was used for ILSVRC 2012–2014 challenges). The dataset includes images of 1000 classes, and is split into three sets: training (1.3M images), validation (50K images), and testing (100K images with held-out class labels).""

This is confirmed by section 3.1 Training:
""The batch size was set to 256""
""In total, the learning rate was decreased 3 times, and the learning was stopped after 370K iterations (74 epochs)""
256 * 370k/74 = 1.3M",,,,,
RNNsearch-50*,9/1/2014,Highly cited,,1.5552e+18,348000000,,,,,,NVIDIA Quadro K6000,WMT'14 + selection,,,,,,,,TRUE,,Third-party estimation,,,"From https://openai.com/blog/ai-and-compute/ Appendix.

0.018 pfs-days
(86400*10^15*0.018)

252 hours in a Quadro K-6000 GPU",,"[WORDS]
""WMT ’14 contains the following English-French parallel corpora: Europarl (61M words), news
commentary (5.5M), UN (421M) and two crawled corpora of 90M and 272.5M words respectively,
totaling 850M words. Following the procedure described in Cho et al. (2014a), we reduce the size of
the combined corpus to have 348M words using the data selection method by Axelrod et al. (2011).""",,,,,
SmooCT,7/1/2014,SOTA improvement,,6.9e+16,12000000000,,48,,,,,,,,,,,,,,,Hardware,First RL system to achieve superhuman level at Poker Limit Texas Hold Em,,"""Each three-player agent was trained for about 12 billion episodes, requiring about 48 hours of training time [...] on a modern computer without using parallelization""

Assume an Intel i7 so 400e9 FLOP/s.
6.9e16 = 400e9*60*60*48",,"""Each three-player agentwas trained for about 12 billion episodes""

An episode seems to be a round of betting.",,,,,
Multiresolution CNN,6/23/2014,Highly cited,126125568,,50000000,,,,,,,,,,,,,,,,,,,"""Using shorthand notation, the full [single frame] architecture is C(96, 11, 3)-N-P-C(256, 5, 1)-N-P-C(384, 3, 1)-C(384, 3, 1)-C(256, 3, 1)-P-FC(4096)-FC(4096), where C(d, f, s) indicates a convolutional layer with d filters of spatial size f ×f, applied to the input with stride s""

Two such single-frame architectures are concatenated as shown in figure 2

""Since the input is only of half the
spatial size as the full-frame models, we take out the last
pooling layer to ensure that both streams still terminate in a
layer of size 7×7×256. ""

We assume the input are T=10 frames with C=3 color channels each

2*(256*(10*3*5*5+1) + 384*(256*3*3+1) + 384*(384*3*3+1) + 256*(384*3*3+1)) + (2*7*7*256 + 1)*4096 + (4096+1)*4096



",,,"""We further estimate the size of our dataset of sampled frames to be on the order of 50 million examples and that our networks have each seen approximately 500 million examples throughout the training period in total.""

So 5e+7 datapoints and 10 epochs.",,,,,
DeepFace,6/23/2014,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
RNN-WER,6/22/2014,"Highly cited,SOTA improvement",26500000,,1100000,,,,,,,WSJ,Likely,,,,,,,,,,"""Finally, by combining the new model with a baseline, we
have achieved state-of-the-art accuracy on the Wall Street
Journal corpus for speaker independent recognition.""","""The network had five levels of bidirectional LSTM hidden layers, with 500 cells in each layer, giving a total of ∼ 26.5M weights.""",,"""The experiments were carried out on the Wall Street Journal (WSJ) corpus (available as LDC corpus LDC93S6B
and LDC94S13B). The RNN was trained on both the 14
hour subset ‘train-si84’ and the full 81 hour set""

","dataset is 81 hours

At 228 wpm (https://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit)
that's 81*228*60 = 1,108,080

another source says WSJ contains 37k sentences, so this would be ~30 words per sentence which seems high but roughly right: https://www.arxiv-vanity.com/papers/1903.00216/",,,,,
Fragment embedding,6/21/2014,SOTA improvement,144496000,,150000,20,,,,,,Flickr30K Entities,Likely,,,,,,,,,,"""Extensive experimental evaluation shows that reasoning on both the global level of images and sentences and the finer level of their respective fragments significantly improves performance on image-sentence retrieval tasks.""","Model contains a word embedding. a matrix combining two word embeddings, and image embedding (built upon a pretrained RCNN image model.
Word embedding: 400000 * 200 =80000000 (""Here, We is a d × 400, 000 matrix that encodes a 1-of-k vector into a d-dimensional word vector representation (we use d = 200).""
Embedding dimension: 1000 (""The size of the embedded space is cross-validated, and we found that values of approximately 1000 generally work well.""
Word combination matrix: 400* 1000=400000
Image embedding: 4096*1000=4096000 (""We use the Caffe [41] implementation of the ImageNet Detection RCNN model [27] to detect objects in all images. On our machine with a Tesla K40 GPU, the RCNN processes one image in approximately 25 seconds. We discard the predictions for 200 ImageNet detection classes and only keep the 4096-D activations"")
CNN: 60,000,000 ""The CNN architecture is identical to the one described in Girhsick et al. [26]. It contains approximately 60 million parameters""
Total parameters: 4096000+80000000+400000+60000000=144,496,000",,"The datasets contain 1,000, 8,000 and 30,000 images respectively and each image is annotated using Amazon Mechanical Turk with 5 independent sentences.","Largest experiment uses 30000 training images, 30000 * 5 = 150,000 sentences",,,,,
SPPNet,6/18/2014,Highly cited,,3.411072e+18,1280000,,672,,,,NVIDIA GeForce GTX TITAN,ImageNet-1k,,,,,,,,TRUE,,Hardware,,,"""All networks in this paper can be
trained on a single GeForce GTX Titan GPU (6 GB memory) within two to four weeks.""
4.7e12 FLOP/s * 4* 7*24*60*60 seconds * 0.3 utilisation",,"Section 3.1: ""We train the networks on the 1000-category training
set of ImageNet 2012.""","""All networks in this paper can be trained on a single GeForce GTX Titan GPU (6 GB memory) within two to four weeks.""",,,,
GANs,6/10/2014,Highly cited,,5.184e+17,60000,,,,,,,CIFAR-10,Speculative,,,,,,,TRUE,,Third-party estimation,,The paper outlines the G-D framework but doesn't provide information about the structures of their generator and discriminator.,"From https://openai.com/blog/ai-and-compute/ Appendix

""Less than 0.006 pfs-days""
(86400*10^15*0.006)

Seems extremely speculative, unless someone at OpenAI privately corresponded with the authors. There is no information about compute or training in the GANs paper.",,"""We trained adversarial nets an a range of datasets including MNIST[23], the Toronto Face Database (TFD) [28], and CIFAR-10 [21].""

MNIST has 60k images 
https://en.wikipedia.org/wiki/MNIST_database

TFD seems to have 2925 examples (?)
https://www.cs.toronto.edu/~urtasun/courses/CSC411/hw3-411.pdf

CIFAR-10 has 60k images
https://www.cs.toronto.edu/~kriz/cifar.html

",,,,,
Two-stream ConvNets for action recognition,6/9/2014,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
GRUs,6/3/2014,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
AdaRNN,6/1/2014,Highly cited,13040,,6248,,,,,,,,Confident,,,,,,,,,,,"D=25 ""For recursive neural models, the dimension of word vector is set to 25, and f = tanh is used as the nonlinearity function. We employ 10 composition matrices in AdaRNN.""
Composition matrices: ""W ∈ R D×2D is the composition matrix, and b is the bias vector.""
C=10 ""We employ 10 composition matrices in AdaRNN.""
Combination matrix: ""S ∈ R C×(2D+|e|) is the matrix used to determine which composition function we use, vl , vr are the left and right child vectors, and e are external feature vector. In this work, e is a one-hot binary feature vector which indicates what the dependency type is.""
|e| > 4: (see Figure 2)
Weights: 10 * 25 * 50 + 10 * (50+4) =13040 (ignoring embedding to the 25 dimension embedding space)",,,"""Training data consists of 6,248 tweets,""",,,,,
Paragraph Vector,5/14/2014,Highly cited,32000000,,75000,,,,,,,IMDb,Confident,,,,,,,,,,," 75000*400+5000*400=32000000
""We learn the word vectors and paragraph vectors using 75,000 training documents""
""In PV-DM, the learned vector representations have 400 dimensions for both words and documents""
Paragraph embedding of dimension number of paragraphs * embedding size
Word embedding of dimension |V|*embedding size
Assuming vocabulary of 5000 since results are compared directly to Maas et. al., 2011",,,"""25,000 labeled training instances, 25,000 labeled test in-
stances and 50,000 unlabeled training instances.""",,,,,
HyperNEAT,3/5/2014,SOTA improvement,239712,,,,,,,,,,,,,,,,,,,,"""Neuroevolution ameliorates these problems and evolved policies achieve state-of-the-art results, even surpassing human high scores on three games""","""The ANN consists of three layers (Fig. 3): a substrate layer inwhich information from the game screen (raw pixels, objects, ornoise) is given as input to the network; a processing layer whichadds a nonlinear internal representation; and a nonlinear outputlayer from which actions are read and conveyed to the Atari em-ulator. Both the input and output layers are fully connected tothe processing layer. The substrate dimensionality of the inputand processinglayers is 810 in the case of the object repre-sentation and 1621 for the pixel and noise representations.3The output layer consists of a 33 substrate mirroring the ninepossible directions of the Atari 2600 joystick and a single noderepresenting thefire button""",,,,,,,,
GloVe (32B),1/1/2014,Highly cited,120000000,,42000000000,,,,,,,Common Crawl,,,,,,,,,,,,400k vocab * 300 vector dimensions,"""The total run-time is split between populating X
and training the model. The former depends on
many factors, including window size, vocabulary
size, and corpus size. Though we did not do so,
this step could easily be parallelized across multiple machines (see, e.g., Lebret and Collobert
(2014) for some benchmarks). Using a single
thread of a dual 2.1GHz Intel Xeon E5-2658 machine, populating X with a 10 word symmetric
context window, a 400,000 word vocabulary, and
a 6 billion token corpus takes about 85 minutes.
Given X, the time it takes to train the model depends on the vector size and the number of iterations. For 300-dimensional vectors with the above settings (and using all 32 cores of the above machine), a single iteration takes 14 minutes. See Fig. 4 for a plot of the learning curve""

""We run 50 iterations for vectors smaller than
300 dimensions, and 100 iterations otherwise (see
Section 4.6 for more details about the convergence
rate).""

But we are interested in the 42B token model",,"""We trained our model on five corpora of varying sizes: a 2010 Wikipedia dump with 1 billion tokens; a 2014 Wikipedia dump with 1.6 billion tokens; Gigaword 5 which has 4.3 billion tokens; the combination Gigaword5 + Wikipedia2014, which has 6 billion tokens; and on 42 billion tokens of web data, from Common Crawl

[To demonstrate the scalability of the model, we also trained it on a much larger sixth corpus, containing 840 billion tokens of web data, but in this case we did not lowercase the vocabulary, so the results are not directly comparable.]""","Section 4.6 in original paper (https://nlp.stanford.edu/pubs/glove.pdf)

85 min to populate coocurrence matrix
+ 25 training iterations

Each iteration takes 14 minutes on 32 cores ",,,,
GloVe (6B),1/1/2014,Highly cited,120000000,,6000000000,,,,,,,Gigaword5 + Wikipedia2014,,,,,,,,,,,,400k vocab * 300 vector dimensions,"""The total run-time is split between populating X
and training the model. The former depends on
many factors, including window size, vocabulary
size, and corpus size. Though we did not do so,
this step could easily be parallelized across multiple machines (see, e.g., Lebret and Collobert
(2014) for some benchmarks). Using a single
thread of a dual 2.1GHz Intel Xeon E5-2658 machine, populating X with a 10 word symmetric
context window, a 400,000 word vocabulary, and
a 6 billion token corpus takes about 85 minutes.
Given X, the time it takes to train the model depends on the vector size and the number of iterations. For 300-dimensional vectors with the above settings (and using all 32 cores of the above machine), a single iteration takes 14 minutes. See Fig. 4 for a plot of the learning curve""

""We run 50 iterations for vectors smaller than
300 dimensions, and 100 iterations otherwise (see
Section 4.6 for more details about the convergence
rate).""

Details of dual 2.1GHz Intel Xeon E5-2658 machine:
https://www.intel.com/content/www/us/en/products/sku/61428/intel-xeon-processor-e52658-20m-2-10-ghz-8-0-gts-intel-qpi/specifications.html",,"""We trained our model on five corpora of varying sizes: a 2010 Wikipedia dump with 1 billion tokens; a 2014 Wikipedia dump with 1.6 billion tokens; Gigaword 5 which has 4.3 billion tokens; the combination Gigaword5 + Wikipedia2014, which has 6 billion tokens; and on 42 billion tokens of web data, from Common Crawl

[To demonstrate the scalability of the model, we also trained it on a much larger sixth corpus, containing 840 billion tokens of web data, but in this case we did not lowercase the vocabulary, so the results are not directly comparable.]""","Section 4.6 in original paper (https://nlp.stanford.edu/pubs/glove.pdf)

85 min to populate coocurrence matrix
+ 25 training iterations

Each iteration takes 14 minutes on 32 cores ",,,,
OverFeat,12/21/2013,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
Image generation,12/20/2013,Highly cited,,475200000000000.0,60000,,,,,,,MNIST,,,,,,,,,,Third-party estimation,,,"From https://openai.com/blog/ai-and-compute/ Appendix

""less than 0.0000055 pfs-days""
(86400*10^15*0.0000055)",,"""We trained generative models of images from the MNIST and Frey Face datasets""

MNIST has 60k images
https://en.wikipedia.org/wiki/MNIST_database

Frey Face has 2k images
https://cs.nyu.edu/~roweis/data.html",,,,,
DOT(S)-RNN,12/20/2013,Highly cited,6160000,,,,,,,,,,,,,,Unreleased,Unreleased,,,,,,,,,,,,,,
DQN,12/19/2013,Highly cited,836096,2300000000000000.0,,,,,,,,,,,,,,,,,,Operation counting,,"""The input to the neural network consists is an 84 × 84 × 4 image produced by φ. The first hidden layer convolves 16 8 × 8 filters with stride 4 with the input image and applies a rectifier nonlinearity [10, 18]. The second hidden layer convolves 32 4 × 4 filters with stride 2, again followed by a rectifier nonlinearity. The final hidden layer is fully-connected and consists of 256 rectifier units. The output layer is a fully connected linear layer with a single output for each valid action. The number of valid actions varied between 4 and 18 on the games we considered.""","Network is 84x84x3 input, 16, 8x8, stride 4, 32 4x4 stride 2, 256 fully connected
First layer: 20*20*3*16*8*8 = 1.23M add-multiplies
Second layer: 9*9*16*32*4*4 = 0.66M add-multiplies
Third layer: 9*9*32*256 = 0.66M add-mutliplies
Total ~ 2.55M add-multiplies
2.5 MFLOPs * 5M updates * 32 batch size * 2 multiply-add * 3 backward pass
= 2.3 PF = 2.7e-5 pfs-days

",,,,,,,
Network in Network,12/16/2013,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
RNN for 1B words,12/11/2013,"Highly cited,SOTA improvement",20000000000,,1000000000,,240,24,,,,One Billion Word benchmark,Speculative,,,,,,,,,,"from abstract: 'We show performance of several well-known types of language models, with the best results achieved with a recurrent neural network based language model. The baseline unpruned Kneser-Ney 5-gram model achieves perplexity 67.6; a combination of techniques leads to 35% reduction in perplexity, or 10% reduction in cross-entropy (bits), over that baseline. '",20B from Table 1,"240 hours on 24 CPUs from Table 1. CPU model is not given, but there is mention of using SIMD instructions. 1 SIMD operation is around 4 FLOP. CPU can have around 3e9 operations per second. so around 12e9*24 * 240*3600 = 2.4e17 operations. This estimation doesn't include use of multiple threads. Including use of threads we would probably have around 10 times more operations  so around 2.4e18 FLOPs. This estimation is speculative.","from abstract: ""With almost one billion words of training data, ""","from abstract: 'With almost one billion words of training data, '","from Table 1,240 hours on 24 CPUs",,,,
DBLSTM,12/8/2013,Highly cited,29900000,,,,,,,,,,,,,,,,,,,,,"""The DBLSTM network had five bidirectional hidden levels, with 500 LSTM cells in each of the forward and backward
layers, and a size 3385 softmax output layer, giving a total of
29.9M weights.""",,,,,,,,
TransE,12/5/2013,Highly cited,942000000,1.340928e+18,17000000,,,,,,,,Speculative,,,,,,,TRUE,,Hardware,,"Based on the TransE architecture, the authors give a formula for how the model size scales with the dimensionality of the dataset. The model scale is proportional to: k*(n_e+n_r) where k is the embeddings dimension, n_e is the number of entities, and n_r is the number of relationships.

They studied using the TransE model for two datasets: FB15k and FB1M. The FB15k model has 810000 parameters.

FB15k has 14951 entities and 1345 relationships. FB1M has 1000000 entities and 23382 relationships. Therefore, the FB1M model will be bigger than the FB15k model by a factor of (23382e6)/(14951*1345) => N = 8.1e5 * (23382e6)/(14951*1345) = 942e6.","8 GPUs (they don't specify which, so I used the average for FP32 for 2017 from the write-up table)
8 hours 
0.33 util rate",,"""it can be successfully trained on a large scale data set with 1M
entities, 25k relationships and more than 17M training samples""",,,,,
DeViSE,12/5/2013,"Highly cited,SOTA improvement",,,5400000000,,,,,,,,Confident,,,,,,,,,,,,,,"""We trained a skip-gram text model on a corpus of 5.7 million documents (5.4 billion words) """,,,,,
TensorReasoner,12/1/2013,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
Visualizing CNNs,11/12/2013,Highly cited,,5.32e+17,,,,,,,NVIDIA GeForce GTX 580,,,,,,,,,TRUE,,"Hardware,Third-party estimation",,,"1 GPU * 12 days * 1.54 TFLOPS/GTX 580 * 0.33 utilization 
= 532 PF = 0.0062 pfs-days

Source: https://openai.com/blog/ai-and-compute",,,,,,,
R-CNN (T-net),11/11/2013,Highly cited,69003872,,,,,,,,,,,,,,,,,,,,,"Computed from architecture description in Caffee

https://nbviewer.jupyter.org/github/BVLC/caffe/blob/master/examples/detection.ipynb",,,,,,,,
Word2Vec (small),10/16/2013,Highly cited,207600000,,692000,,,,,,,,,,,,,,,,,,,"""We discarded from the vocabulary all words that occurred less than 5 times in the training data, which resulted in a vocabulary of size 692K [...] Starting with the same news data as in the previous experiments, we first constructed the phrase based training corpus and then we trained several Skip-gram models using different hyperparameters. As before, we used vector dimensionality 300 and context size 5.""",,,"""For training the Skip-gram models, we have used a large dataset consisting of various news articles (an internal Google dataset with one billion words). We discarded from the vocabulary all words that occurred less than 5 times in the training data, which resulted in a vocabulary of size 692K""",,,,,
Word2Vec (large),10/16/2013,Highly cited,692000000,3.888e+16,33000000000,,24,,,,,,,,,,,,,,,Third-party estimation,,"""To maximize the accuracy on the phrase analogy task, we increased the amount of the training data by using a dataset with about 33 billion words. We used the hierarchical softmax, dimensionality of 1000, and the entire sentence for the context.""","From https://openai.com/blog/ai-and-compute/ Appendix.

""less than 0.00045 pfs days""
(86400*10^15*0.00045)",,"""For training the Skip-gram models, we have used a large dataset consisting of various news articles (an internal Google dataset with one billion words). We discarded from the vocabulary all words that occurred less than 5 times in the training data, which resulted in a vocabulary of size 692K""","Table 5 appears to call the model ""Skip-Phrase"" and says it took 1 day",,,,
RNTN,10/1/2013,Highly cited,,1.422e+16,155063,,5,,,,,,Likely,,,,Unreleased,Unreleased,,,,,,,"""The RNTN would usually achieve its best performance on the dev set after training for 3 - 5 hours.""
","""we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences""
Average FP32 FLOPS for 2013: 1.58E+12 (https://epoch.ai/blog/estimating-training-compute)
Assuming utilization of 0.5
Compute estimate: 0.5 * 5*60*60 * 1.58e12=1.422e16=14220000000000000
","""The sentences in the treebank were split into a train (8544), dev (1101) and test splits (2210)""
Training data: 215154*(8544/11855)=155063
",The RNTN would usually achieve its best performance on the dev set after training for 3 - 5 hours.,,,,
RCTM,10/1/2013,Highly cited,,9331200000000000.0,4100000,,15,3,,,,,Likely,,,,,,,,,Hardware,,,"""The training of an RCTM takes about 15 hours on 3 multicore CPUs""
Given the publication year, a rough estimate for the CPU performance is 16 FP32 per cycle, 4 cores, clock speed 4GHz, utilization of 0.3.
15*60*60*3*4*12*4000000000*0.3=9331200000000000=9.33e15","""The training set used for all the experiments comprises a bilingual corpus of 144953 pairs of sentences less than 80 words in length from the news
commentary section of the Eighth Workshop on Machine Translation (WMT) 2013 training data.""","""The English sentences contain about 4.1M words""",The training of an RCTM takes about 15 hours on 3 multicore CPUs.,,,,
Mitosis,9/22/2013,Highly cited,37230,1.37e+17,1000000,,24,,,,,,,,,,,,,TRUE,,Hardware,ICPR 2012 mitosis detection competition winner,Sum numbers of weights in Table 1.b,"""Training each network requires one day of computation with an optimized GPU implementation""

Assuming 1.58E+12 FLOP/second on FP32 (from the table in the Estimating compute post), we get

3600*24*1.58E+12 = 1.37E+17 FLOP",,"The dataset is built in two stages. First a classifier is trained on small sample, and used to curate a more representative larger dataset.

The final dataset has 1M instances

""We build the actual training set, composed by 1 million instances, which includes
all mitosis pixels (6.6% of the training instances). The remaining 95.4% is sampled
from non-mitosis pixels by assigning to each pixel p a weight D(p).""","""Training each network requires one day of computation with an optimized GPU implementation""",,,,
RNN+weight noise+dynamic eval,8/4/2013,Highly cited,54000000,4210000000000000.0,,14,,,,,,IAM Online Handwriting Database (IAM-OnDB),,,,,Unreleased,Unreleased,,,,,,,,,,,,,,
Fisher Vector image classifier,6/12/2013,Highly cited,,9.08424E+13,,,2,,,,,ImageNet,,,,,,,,,,Hardware,,,"They use a Intel Xeon E5-2470 Processor for 2 hours. This can do 12,617 MOps/Sec 
https://www.cpubenchmark.net/cpu.php?cpu=Intel+Xeon+E5-2470+%40+2.30GHz&id=2003",,,,,,,
SemVec,6/9/2013,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
ReLU-Speech,5/26/2013,"Training cost,SOTA improvement",101706240,1.2773376e+17,,,168,,,,,,Likely,,,,,,,TRUE,,Hardware,,"""The overall input dimensionality is 1040,""
""All layers of our networks have 2560 hidden units ""
""used to generate 7969 context-dependent tied acoustic states""
Largest model: 12 hidden layers (Fig 4)
Parameters: 1040*2560+12*2560*2560+2560*7969=101706240
","""across 4 machines using up to 4 CPUs each""
CPU model not specified, I assumed a Sandy Bridge with 16 FLOP/cycle and 3.3GhZ based on the publication year (4*16*3300000000=211200000000 FLOP/s per machine)
Compute: 4*211200000000*168*60*60*0.3 = 1.53e17

Alternatively, the training set is ""several hundred hours of speech"", with inputs consisting of 26 frames, each frame is 10ms apart.
If we assume 400h of training data, 400h/10ms/26= 5,538,461 inputs
6 * 101706240 * 5,538,461 = 3.38e15 FLOPs per epoch. Number of epochs unstated.",Several hundred hours of speech,,"""The results we report are obtained by training for one week.""",,,,
Multilingual DNN,5/26/2013,"SOTA improvement,Training cost",206899200,,77580000,,672,,,,,,Confident,,,,,,,,,,,"""The input for the DNN is eleven contiguous frames of 40-dimensional log-filterbank features. The DNN consists of four hidden layers each with 2560 nodes""
Network structure: 3 multilingual shared layers, 1 language specific hidden layer + output layer (Figure 2)
Language specific layer output sizes: 1600, 3300, 2900, 5700, 3500, 5500, 6200, 4700, 5100, 4900, 3700 (Table 1)
Shared: 11*40*2560+2560*2560+2560*2560=14233600
Language heads: 11*2560*2560+2560*1600+2560*3300+2560*2900+2560*5700+2560*3500+2560*5500+2560*6200+2560*4700+2560*5100+2560*4900+2560*3700=192665600
Total: 14233600+192665600=206899200=2e8",Could be estimated if we knew framerate of input filterbanks.,,"Trained on 80+100+220+270+920+1140+1450+1460+1490+1490=8620h of speech data (Table 1)
Conversion to words using an estimate of 150 wpm: 8620*60*150=77580000 words
","""increased training time of roughly four weeks""
4*7*24=672 hours of training",,,,
Selective Search,4/2/2013,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
PreTrans-3L-250H,3/22/2013,Highly cited,43000000,,,,,,,,,,,,,,,,,,,,,Table 1,,,,,,,,
Maxout Networks,2/18/2013,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
Textual Imager,1/16/2013,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
DistBelief NNLM,1/16/2013,"Highly cited,SOTA improvement",,2.612736e+18,6000000000,,336,180,,,,,Likely,,,,,,,TRUE,,Hardware,,,"Trained for 14 days on 180 CPU cores (Table 6)
Roughly estimating the performance of CPUs in a HPC around 2013: 16 FP32 operations per cycle, 2.5GHz, 0.3 utilization
Time: 14*24*60*60=1209600s
FLOPs: 0.3*180*16*2500000000=2160000000000
Training compute: 1209600s * 2160000000000 = 2612736000000000000 = 2.61e18
https://www.wolframalpha.com/input?i=16+FLOP+*+2.5+GHz+*+180+*+14+days+*+0.3",,Largest system is trained on 6B words (Table 6),Trained for 14 days (Table 6),,,,
DistBelief Vision,12/3/2012,"SOTA improvement,Highly cited",1700000000,,16000000,,,,,,,ImageNet,Likely,,,,,,,,,,,"""we used Downpour SGD to train the 1.7 billion parameter image model""",,,For visual object recognition we trained a larger neural network with locally-connected receptive fields on the ImageNet data set of 16 million images,,,,,
DistBelief Speech,12/3/2012,"Highly cited,SOTA improvement",47185920,3.114e+17,1100000000,,120,,,,,,Speculative,,,,,,,TRUE,,Operation counting,,"""We used a deep network with five layers: four hidden layer with sigmoidal activations and 2560 nodes each, and a softmax output layer with 8192 nodes.""
""The network was fully-connected layer-to-layer, for a total of approximately 42 million model parameters.""
2560*2560*4+2560*8192=47185920","https://www.wolframalpha.com/input?i=6+FLOP+*+47185920+*+1.1+billion
Number of epochs unknown but most likely 1 and probably under 30.
We could narrow down the uncertainty further if we knew something about the hardware.",,"""We trained on a data set of 1.1 billion weakly labeled examples""",Figure 4,,,,
Bayesian automated hyperparameter tuning,12/2/2012,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
RNN+LDA+KN5+cache,12/1/2012,SOTA improvement,9000000,,,,,,,,,Penn TreeBank,,,,,Unreleased,Unreleased,,,,,"""We report perplexity results on the Penn Treebank data, where we achieve a new state-of-the-art""",,,,,,,,,
AlexNet,9/30/2012,"Highly cited,Historical significance",60000000,4.7e+17,1200000,,132,,0.313,,NVIDIA GeForce GTX 580,ImageNet,Confident,,,,,,,TRUE,,"Operation counting,Hardware,Third-party estimation",,"""Our neural network architecture has 60 million parameters.""","1.2M images * 90 epochs * 0.75 GFLOP * (2 add-multiply) * (3 backward pass) 
= 470 PF = 0.0054 pfs-days

Source: https://openai.com/blog/ai-and-compute/

Hardware method:
2 GTX 580 3GB GPUs for ""between five and six days"". Assuming 5.5 days and 32-bit training:
1.581 TFLOPS * 5.5 days * 2 = 1.5e18 FLOP
Comparing to the operation counting method, this implies around 31% MFU.",,"""ImageNet is a dataset of over 15 million labeled high-resolution images belonging to roughly 22,000 categories. The images were collected from the web and labeled by human labelers using Amazon’s Mechanical Turk crowd-sourcing tool. Starting in 2010, as part of the Pascal Visual Object Challenge, an annual competition called the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) has been held. ILSVRC uses a subset of ImageNet with roughly 1000 images in each of 1000 categories. In all, there are roughly 1.2 million training images, 50,000 validation images, and 150,000 testing images.""","""Our network takes between five and six days to train on two GTX 580 3GB GPUs.""",,,,
LSTM LM,9/9/2012,Highly cited,102720000,1.66e+16,27000000,,,,,,,,Speculative,,,,,,,TRUE,,Operation counting,,"Multiple models were trained, the largest on transcribed French podcast data.
""We trained an LSTM LM using 300 hidden nodes and 27 M running words of indomain training data""
""Corpus sizes in number of running words; the vocabulary size of the Treebank corpus is 10 K, for Quaero French it is 170 K""
Embedding and unembedding: 2*170000*300=102000000
LSTM: 4*600*300=720000
Total: 102000000+720000=102720000=1.03e8
(Assuming the embedding dimension is the same as the LSTM layer)","FLOP per input for LSTM layer is 4*2*(M+N)*M, for N inputs and M outputs.

Embedding FLOPs: 2 * 170000 * 300 = 102,000,000
LSTM FLOPs: 4 * 2 * (300 + 300) * 300 = 1,440,000
Unembedding FLOPs: 2 * 170000 * 300 = 102,000,000
Total: 205,440,000 FLOPs per word per forward pass
For 27M training input words and including backward passes: 27M * 3 * 205,440,000 = 1.66e16

However, it sounds like they're doing something with a secondary acoustic model, so this may be an underestimate.",,"""We trained an LSTM LM using 300 hidden nodes and 27 M running words of indomain training data.""",,,,,
LSTM-300units,9/1/2012,Highly cited,12000000,,,,,,,,,,,,,,Unreleased,Unreleased,,,,,,,,,,,,,,
Context-dependent RNN,7/27/2012,SOTA improvement,,,,,,,,,,,Unknown,,,,,,,,,,New SOTA perplexity on PTB,,,,,,,,,
Unsupervised High-level Feature Learner,7/12/2012,"Highly cited,SOTA improvement",1000000000,6e+17,10000000,,72,,,,,,Likely,,,,,,,TRUE,,Operation counting,"""we trained our network to obtain 15.8% accuracy in recognizing 20,000 object categories from ImageNet, a leap of 70% relative improvement over the previous state-of-the-art""","""To answer this, we train a 9-layered locally connected sparse autoencoder with pooling and local contrast normalization on a large dataset of images (the model has 1 billion connections, the dataset has 10 million 200x200 pixel images downloaded from the Internet)""","Assuming 1 epoch, 10 million images and 1 billion parameters, 6*N*D = 6*10^17 FLOP",10 million 200x200 images extracted from Youtube videos,10 million 200x200 images extracted from Youtube videos,"""We train this network using model parallelism and asynchronous SGD on a cluster with 1,000 machines (16,000 cores) for three days. """,Hardware not reported,,,
MV-RNN,7/12/2012,Highly cited,3510255,,,,,,,,,,,,,,,,,,,,,"""We represent a word as both a continuous vector and a matrix of parameters. We initialize all word vectors x ∈ Rn with pre-trained 50-dimensional word vectors from the unsupervised model of Collobert and Weston (2008). [...] Every word is also associated with a matrix X.  [...] If the vectors have dimensionality n, then each word’s matrix has dimensionality X ∈ Rn×n.""

""We propose the following combination function which is input dependent:
p = fA,B(a, b) = f(Ba, Ab) = g(W x (Ba Ab)) ,(2)
where A, B are matrices for single words, the global W ∈ Rn×2n is a matrix that maps both transformed words back into the same n-dimensional space.""

""For computing nonterminal phrase matrices, we define the function
P = fM(A, B) = WMA, B, (3)
where WM ∈ Rn×2n, so P ∈ Rn×n just like each input matrix.""

""If every word is represented by an n-dimensional vector and additionally by an n × n matrix, the dimensionality of the whole model may become too large with commonly used vector sizes of n = 100. In order to reduce the number of parameters, we represent word matrices by the following low-rank plus diagonal approximation: A = UV + diag(a), (5)
where U ∈ Rn×r, V ∈ Rr×n, a ∈ Rnand we set the rank for all experiments to r = 3.""

""We train these representations by adding on top of each parent node a simple softmax classifier
to predict a class distribution over, e.g., sentiment or relationship classes: d(p) = softmax(Wlabelp). If there are K labels, then d ∈ RK is a K-dimensional multinomial distribution""

In total there are V*(n+n*r + r*n) + n*2n + n*2n + (n+1)*k parameters, where n is the vector dimension, r is the low-rank decomposition dimension, V is the vocabulary size and k is the number of classes.

In the experiments we have that n=50, r=3, k=? and V=?. I'm guesstimating k=5 and V=10k.",,,,,,,,
Dropout (TIMIT),6/3/2012,Highly cited,48840185,,41620,,,,,,NVIDIA GeForce GTX 580,TIMIT,,,,,Unreleased,Open (non-commercial),Unreleased,,,,,The input to the net is 21 adjacent frames with an advance of 10ms per frame. The neural net has 4 fully-connected hidden layers of 4000 units per layer and 185 “softmax” output units that are subsequently merged into the 39 distinct classes used for the benchmark.,,,"4162 utterances, guesstimated avg 10 words per utterance",,,,,http://www.cs.toronto.edu/~nitish/dropout see model files
Dropout (MNIST),6/3/2012,Highly cited,5592010,6039370800000000.0,60000,,,,,,NVIDIA GeForce GTX 580,MNIST,,,,,Unreleased,Open (non-commercial),Unreleased,,,Operation counting,,,"Num mul-add / forward pass
2 FLOPs / mult-add
3 total mult-add / fp mult-add
3000 epochs
60000 training samples",,"The MNIST database contains 60,000 training images and 10,000 testing images (Wikipedia)",,,,,http://www.cs.toronto.edu/~nitish/dropout see model files
Dropout (ImageNet),6/3/2012,Highly cited,,2.731968e+17,1000000,,96,,,,NVIDIA GeForce GTX 580,ImageNet,,,,,Unreleased,Unreleased,Unreleased,TRUE,,Hardware,,"""We achieved comparable performance of 48.6% error using a single neural network with five convolutional hidden layers interleaved with “max-pooling” layer followed by two globally
connected layers and a final 1000-way softmax layer""","""a single NVIDIA GTX 580 GPU... Training on ImageNet takes
roughly four days with dropout and two days without.""
1.581 TFLOP/s * 4 day * 86400 s/day * 0.5 utilization",,"In 2010, a subset of 1000 classes
with roughly 1000 examples per class was the basis of an object recognition competition...",4 days with dropout; 2 days without dropout,,,,
Dropout (CIFAR),6/3/2012,Highly cited,,4268700000000000.0,60000,,1.5,,,,NVIDIA GeForce GTX 580,CIFAR-10,,,,,Unreleased,Open (non-commercial),Unreleased,,,Hardware,,,"""a single NVIDIA GTX 580 GPU. Training on CIFAR-10 takes roughly 90 minutes"" p17
1.581 TFLOP/s * 90 min * 60 s/min * 0.5 utilization",,,90 minutes,,,,http://www.cs.toronto.edu/~nitish/dropout see model files
MCDNN (MNIST),2/13/2012,Highly cited,1994300,3726979200000000.0,60000,,,,,,,MNIST,,,,,,,,,,Operation counting,,"""We train five DNN columns per normalization, resulting in a total of 35 columns for the entire MCDNN.
[Each DNN has an architecture] 1x29x29-20C4-MP2-40C5-MP3-150N-10N DNN""","Num of multiply-adds per forward pass
2 FLOPs/mult-add
3 (fp+bp FLOPs / fp FLOPs)
800 epochs
60.000 training size
35 networks

""Training a DNN takes almost 14 hours and after 500 training epochs little additional improvement is observed""",,"The MNIST database contains 60,000 training images and 10,000 testing images (Wikipedia)",,,,,
HOGWILD!,11/11/2011,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
NLP from scratch,11/8/2011,Highly cited,5000000,,852000000,,72,,,,,,,,,,,,,,,,,"""The capacity of our network architectures lies mainly in the word lookup table, which contains 50 × 100,000 parameters to train. [...] most of the trainable parameters are located in the lookup tables.""",,,"""Section 4 leverages large unlabeled data sets (∼ 852 million words)""","""Chunking and NER take about one hour to train, POS takes few hours, and SRL takes about three days.""
SRL is the longest task.",,,,
Domain Adaptation,11/6/2011,Highly cited,15260,,4652,,,,,,,Dataset introduced in 'Adapting Visual Category Models to New Domains',,,,,,,,,,,,"Did not take into account initial image feature extraction, only novel stuff.

1. Perform PCA on the feature matrices from both domains. Learnable parameters are projection matrices.
= 800 (# features) x 200 (reduced dimension) x 2 (once per subdomain)

2. Perform partial least squares regression. Learnable parameters are

Matrix P with dimensions 200 (# features) x 30 (dimension of latent space)
Matrix Q with dimensions 1 (# responses) x 30 (dimension of latent space)
Projection matrix of X onto latent space:  200 (# features) x 30 (dimension of latent space)
Projection matrix of Y onto latent space:  1 (# responses) x 30 (dimension of latent space)
",,,"Dataset introduced in 'Adapting Visual Category Models to New
Domains'",,,,,
Adaptive Subgrad,10/3/2011,Highly cited,,,,,,,,,,Reuters RCV1,Unknown,,,,,,,,,,,,,"They experiement with their method on different models with different modalities, I entry only information about text classification experiment 

Text Classification (Reuters RCV1): They trained binary classifiers for each of the 4 major categories using ADAGRAD and compared them to other methods like RDA, FOBOS, Passive-Aggressive, and AROW.",,,,,,
Recursive sentiment autoencoder,7/1/2011,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,"They use several datasets for self-supervised and supervised learning
",,,,,
Recursive Neural Network,6/28/2011,Highly cited,,,833333,,,,,,,WSJ,Confident,,,,,,,,,,,,,"We train all models on the Wall Street Journal section of the Penn Treebank using the standard training (2–21), development (22) and test (23) splits.","Full WSJ dataset: 1000000 words (https://catalog.ldc.upenn.edu/LDC99T42) 
Using 20 out of 24 splits for training: 1000000*(20/24)=833333.333",,,,,
Vector Space Model,6/19/2011,"Highly cited,SOTA improvement",255000,,75000,,,,,,,IMDb,Confident,,,,,,,,,,"""We evaluate the model using small,
widely used sentiment and subjectivity corpora and find it out-performs several previously introduced methods for sentiment classification""","""We build a fixed dictionary of the 5,000 most frequent tokens""
""For all word vector models, we use 50-dimensional vectors""
Parameters: 5000*50 + 5000=255000
",,"""We induce word representations with our model using 25,000 movie reviews from IMDB.""","""We train a variant of our model which uses 50,000 unlabeled reviews in addition to the labeled set of 25,000 reviews""",,,,,
Cross-Lingual POS Tagger,6/19/2011,SOTA improvement,,,,,,,,,,,Unknown,,,,,,,,,,"""Across eight European languages, our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline, and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm.""",,,,,,,,,
RNN-SpeedUp,5/22/2011,Highly cited,,,697500,,,,,,,Penn TreeBank,,,,,,,,,,,,,,,"Section 3: ""The data used in the following experiments were obtained from
Penn Tree Bank: sections 0-20 were used as training data (about
930K tokens)""

0.75 words per token for English",,,,,
Deep Autoencoders,4/29/2011,Historical significance,139808256,3.672864e+16,1600000,85,48,1,,,NVIDIA GeForce GTX 285,,Confident,,,,,,,TRUE,476.5921730228709,Hardware,,"2*(3072*8192+8192*4096+4096*2048+2048*1024+1024*512+512*256+256*128+128*64+64*28)=139808256
""n each autoencoder, the hidden layers halve in size until they reach the desired size, except that we use 28 instead of 32""","48*60*60*708500000000*0.3=36728640000000000=3.7e16
GTX 285 with 708.5 GFLOP/s","""We tested our models on two subsets of the 80 million tiny images datase""","""We train on 1.6 million 32 × 32 color images""","""The entire training procedure for each autoencoder took about 2 days on an Nvidia GTX 285 GPU.""",,,,
Deep rectifier networks,4/13/2011,Highly cited,,,,,,,,,,"CIFAR-10,MNIST,NISTP,NORB",Unknown,,,,,,,,,,,,,,,,,,,
Optimized Single-layer Net,4/11/2011,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
YouTube Video Recommendation System,9/26/2010,Highly cited,,,10000000000,,,,,,,,,,,,,,,,,,,,,,"""We currently handle millions of users
and tens of billions of activity events with a total footprint
of several terabytes of data""

If 10M users each watch 1000 videos, that's 10B visualizations, which matches their ""activity events"" count.",,,,,
RNN LM,9/26/2010,Highly cited,70265000,5.396e+16,6400000,1,504,,,,,WSJ,Speculative,,,,,,,TRUE,,Operation counting,,"This database entry refers to the 3xRNN rows in Table 2 (static and dynamic likely use the same model ensemble, but allow the model weights to update once when testing the dynamic version).

I assume the 3xRNN represents interpolation between the three largest models shown explicitly (RNN 250/5, RNN 250/2, and RNN 400/10). This seems likely, since smaller models do considerably worse on their own.

In the following colab notebook, I estimate vocabulary sizes for the NYT Gigaword data at around 54.4k, 41.4k, and 27.6k for merge thresholds of 2, 5, and 10, respectively: https://colab.research.google.com/drive/1K5qH0EqXtFwTLESNtp4oelCM28GpGXt6#scrollTo=tedUkbgklNJ3

So the total number of parameters in each constituent model is:
- RNN 250/2: (250 + 54.4k) * 250 + (250 * 54.4k) = 27,262,500
- RNN 250/5: (250 + 41.4k) * 250 + (250 * 41.4k) = 20,762,500
- RNN 400/10: (400 + 27.6k) * 400 + (400 * 27.6k) = 22,240,000

In total: 70,265,000 parameters","""Convergence is usually achieved after 10-20 epochs.""
Training was done over a 6.4M subset of the NYT section of English Gigaword.

6 * 70,265,000 * 20 * 6.4M = 5.396e16",The training corpus consists of 37M words from NYT section of English Gigaword,"As it is very time consuming to train RNN LM on large data, we have used only up to 6.4M words for training RNN models.","""it takes several weeks to train the most complex models.""
Rough guess, 3 weeks = 504 hours

Assuming these models trained at the same time on different machines.",,,,
Fisher-Boost,9/5/2010,Highly cited,,,,,21.5,,,,,,Unknown,,,,,,,,,,,,,,,"""Extracting and projecting the SIFT features for the 350K training images takes approx. 15h (150ms / image), learning the GMM on a random subset of 1M descriptors approx. 30 min, computing the Fisher vectors. Improving the Fisher Kernel for Large-Scale Image Classification 155
approx. 4h (40ms / image) and learning the 18 classifiers approx. 2h (7 min / class). ""
15 + 0.5 + 4 + 2 = 21.5 hours",,,,
ReLU (NORB),6/15/2010,Highly cited,16210006,,291600,,,,,,,,,,,,,,,,,,,"""The stereo-pair images are subsampled from their original resolution of 108 × 108 × 2 to 32 × 32 × 2 to speed up experiments [...]  the architecture
with the best results have 4000 units in the first layer
and 2000 in the second [...] there are 58,320 test
cases (9,720 cases per class) ""

So the architecture has (32*32*2+1)x4000 + (4000+1)*2000 + (2000+1)*58,320/9,720 parameters",,,"""There are 291,600 training cases (48,600 cases per class) and 58,320 test cases (9,720 cases per class).""",,,,,
ReLU (LFW),6/15/2010,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
Mid-level Features,6/13/2010,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,This is extracting low-level SIFT features then max-pooling them and using in a linear SVM. The training compute could be estimated loosely for the SVM part.,,,,,,,,
Deconvolutional Network,6/13/2010,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
Word Representations,6/1/2010,Highly cited,,,37000000,,,,,,,,,,,,,,,,,,,,,,"Section 6: ""After cleaning, there are 37 million words (58%
of the original) in 1.3 million sentences""",,,,,
Feedforward NN,5/13/2010,Highly cited,7082000,350000000000000.0,,,,,,,,MNIST,,,,,,,,,,Operation counting,,"pg250 of the paper, section 2.3: 
""We optimized feedforward neural networks with one to
five hidden layers, with one thousand hidden units per
layer""

Input is a flattened 32x32 image, which corresponds to an input vector of length 3072

Output is a number from 0-9, so 10 neurons

No. of params: 3072*1000 + 4*1000*1000 + 1000*10 = 7,082,000
","Roughly two times the number of parameters for ops per forward pass. 

So 2*7082000 params*3.5*140 epochs * 50k training images = 3.5e14",,,,,,,
6-layer MLP (MNIST),3/1/2010,Highly cited,12110000,130788000000000.0,60000,,2,,,,"NVIDIA GeForce GTX 280,Intel Core 2 Quad Q9450",MNIST,Likely,,,,,,,,,Operation counting,,Table 1,"""Networks with up to 12 million weights can successfully be trained by plain gradient descent to achieve test errors below 1% after 20-30 epochs in less than 2 hours of training.""

60k images in each MNIST epoch.

Architecture-based estimate: 6 * 12.11M * 60k * 30 = 1.31e14

We can also get a rough hardware estimate. The authors use single precision, GTX280 gets 6.221e11 FLOPs in single precision. Training 30 epochs takes less than 2 hours, but on each epoch the training set is augmented in online fashion, which takes 93 seconds.
(2*3600 - 93*30) * 6.221e11 * 0.3 = 8.23e14

The architecture approach seems less uncertain.",,"""MNIST consists of two datasets, one for training (60,000 images) and one for testing (10,000 images). Many studies divide the training set into two sets consisting of 50,000 images for training and 10,000 for validation. Our network is trained on slightly deformed images, continually generated in on-line fashion; hence we may use the whole un-deformed training set for validation, without wasting training images""","""less than 2 hours of
training""",,,,
Stacked Denoising Autoencoders,1/3/2010,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
Super-vector coding,1/1/2010,SOTA improvement,1025,,1000000,,,,,,,"PASCAL VOC 2007,PASCAL VOC 2009",Speculative,,,,,,,,,,"""Our experiments demonstrate that the proposed classification method achieves state-of-the-art accuracy on the well-known PASCAL benchmarks.""","Somewhat low confidence, but it seems like the number of learnable parameters is the size of the codebook, plus parameters in the SVM used for classification. Since it is a linear SVM, there will be one parameter per input feature, plus a single bias term.

So in total, 512 learnable codebook values, plus 513 SVM parameters = 1025 parameters",,"""PASCAL VOC 2007 consists of 9,963 images which are divided into
three subsets: training data (2501 images), validation data (2510 images), and test data (4952 images). PASCAL VOC 2009 consists of 14,743 images and correspondingly are divided into three subsets: training data(3473 images), validation data(3581 images), and testing data (7689 images).""","""PASCAL VOC 2007 consists of 9,963 images which are divided intothree subsets: training data (2501 images), validation data (2510 images), and
test data (4952 images). PASCAL VOC 2009 consists of 14,743 images and correspondingly are divided into three subsets: training data(3473 images), validation data(3581 images), and testing data (7689 images).""

PASCAL VOC 2009 is the larger experiment; images used in training is 3473 + 3581 = 7,054

For each image, the inputs for the codebook training are generated as follows: ""128-dimensional SIFT vectors are extracted over a grid with spacing of 4 pixels on three patch scales (16x16,25x25 and 31x31). The dimension of descriptors is reduced to 80 by applying principal component analysis (PCA). The codebooks C are trained on one million randomly sampled descriptors""

Loss function for learning the SV coding seems to look at the L2 loss on SIFT vectors, so there should be one gradient per descriptor, i.e. 1M inputs.",,,,,
3D city reconstruction,9/29/2009,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
BellKor 2007,9/21/2009,SOTA improvement,,,100480507,,,,,,,Netflix Prize,,,,,,,,,,,Won Netflix prize,,,,"The training data set consists of 100,480,507
ratings",,,,,
MatrixFac for Recommenders,8/7/2009,Highly cited,,,100480507,,,,,,,Netflix Prize,,,,,,,,,,,,,,,,,,,,
Pragmatic Theory solution (Netflix 2009),8/1/2009,SOTA improvement,,,100480507,,,,,,,Netflix Prize,Confident,,,,,,,,,,Netflix grand prize winner (along with two other teams),"This is an ensemble of many smaller models. Ideally, the number of parameters of all the sub-models should be added up and recorded here.","This is an ensemble of many smaller models. Ideally, the training compute of all the sub-models should be added up and recorded here.",,"""Netflix provided a training data set of 100,480,507 ratings that 480,189 users gave to 17,770 movies.""",,,,,
BigChaos OptiBlend,8/1/2009,SOTA improvement,,,100480507,,,,,,,Netflix Prize,,,,,,,,,,,"won Netflix prize
",,,,"""Netflix provided a training data set of 100,480,507 ratings that 480,189 users gave to 17,770 movies.""",,,,,
BellKor 2009,8/1/2009,Historical significance,,,100480507,,,,,,,Netflix Prize,,,,,,,,,,,Introduced new algorithms; won Netflix Grand Prize,,,,"""Netflix provided a training data set of 100,480,507 ratings that 480,189 users gave to 17,770 movies.""",,,,,
BellKor 2008,8/1/2009,SOTA improvement,,,100480507,,,,,,,Netflix Prize,,,,,,,,,,,Won Netflix prize,,,,"""Netflix provided a training data set of 100,480,507 ratings that 480,189 users gave to 17,770 movies.""",,,,,
GPU DBNs,6/15/2009,Highly cited,100000000,1000000000000000.0,1000000,,,,,,,,,,,,,,,TRUE,,Hardware,,"""For example, we are able to reduce the time required to learn a four-layer DBN with 100 million free parameters from several weeks to around a single day.""",https://www.getguesstimate.com/models/19602,,"Table 2 shows the running time for processing 1 million
examples for RBMs of varying size",,,,,
Conv-DBN,6/14/2009,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
Deep Boltzmann Machines,4/16/2009,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
RBM Image Classifier,4/8/2009,Highly cited,80000000,,2000000,,,,,,,CIFAR-10,Likely,,,,,,,,,,,"Best performing model (see Figure 3.1) had 10,000 hidden units in one hidden layer and 8000 visible units",,"This paper is the origin of CIFAR-10. However, CIFAR-10 is a labeled subset of all the training data used

""The tiny images dataset on which we based all of our experiments was collected by colleagues at MIT and NYU over the span of six months; it is described in detail in [14]. They assembled it by searching the web for images of every non-abstract English noun in the lexical database WordNet[15, 8]. They used several search engines, including Google, Flickr, and Altavista and kept roughly the rst 3000 results for each search term. After collecting all the images for a particular search term, they removed perfect duplicates and images in which an excessively large portion of the pixels were white, as they tended to be synthetic gures rather than natural images. The search term used to nd an image provides it with a rough label, although it is extremely unreliable due to the nature of online image search technology.
In total, the dataset contains 80 million colour images downscaled to 32 × 32 and spread out across 79000 search terms. Most of our experiments with unsupervised learning were performed on a subset of about 2 million images.""

""We paid students to label a subset of the tiny images dataset... We call this the CIFAR-10 dataset, after the Canadian Institute for Advanced Research, which funded the project""",,,,,,
Semantic Hashing,12/10/2008,Highly cited,2600000,,310521,,,,,,,,,,,,,,,,,,,,,,Section 4.1,,,,,
HLBL,12/8/2008,Highly cited,1846400,,14000000,,,,,,,,Confident,,,,,,,,,,,"""Except for where stated otherwise, the models used for the experiments used 100 dimensional feature vectors and a context size of 5.""
""The vocabulary size for this dataset is 17964.""
Embedding: 17964 * 100 = 1796400
Context matrices: 5 * 100 * 100 = 50000
Unembedding: 0 (tied embedding “while the matrix of weights from the hidden layer to the output layer is simply the feature vector matrix R”)
Total: 1796400 + 50000 = 1846400
","6ND: 6 * 1846400 * 14000000 = 155,097,600,000,000 FLOP per epoch.
Not stated how many epochs for training.","""APNews dataset containing the Associated Press news stories from 1995 and 1996.""","""The dataset consists of a 14 million word training set""",HLBL with largest tree (T7) takes 32 minutes per epoch. Unstated how many epochs they trained for.,,,,
ADAPTIVE NLPM,12/8/2008,Highly cited,12198000,,14000000,,,,,,,,Confident,,,,Unreleased,,,,,,,"None given but it does say "" Since each non-leaf node in a tree has its own feature vector, the number of free parameters associated with the tree is linear in this quantity"", and the largest model (T7: ADAPATIVE(0.4) x 4) has 121980 of them. The feature vectors are 100-dimensional. I've done the dubious thing of multiplying the two to give an estimate.",,,"""The dataset consists of a 14 million word training set""","32 minutes per epoch for the largest model, but unfortunately no epoch count given.

""Models were trained using the learning rate of 10^−3 until the perplexity on the validation set started to increase. Then the learning rate was reduced to 3 × 10^−5 and training was resumed until the validation perplexity started increasing again.""",,,,
BigChaos 2008,11/25/2008,Historical significance,,,100480507,,,,,,,Netflix Prize,,,,,,,,,,,Winners of the 2008 Netflix Price,,,,"""Netflix provided a training data set of 100,480,507 ratings that 480,189 users gave to 17,770 movies.""",,,,,
Sparse digit recognition SVM,11/19/2008,SOTA improvement,,,,,,,,,,,Unknown,,,,,,,,,,"""Finally, we train a support vector machine (SVM) on the resulting feature vectors and obtain state-of-the-art classification performance in the digit recognition task defined by the MNIST benchmark""",,,,,,,,,
Boss (DARPA Urban Challenge),7/23/2008,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
Semi-Supervised Embedding for DL,7/5/2008,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
Denoising Autoencoders,7/5/2008,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
Deep Multitask NLP Network,7/5/2008,"Highly cited,SOTA improvement",1500000,,633000000,,168,,,,,"PropBank,Penn TreeBank,Wikipedia",Speculative,,,,,,,,,,,"With a word vector size of 50 and a vocabulary size of 30,000, the embedding matrix has 1,500,000 parameters. There are also some small convolutional and dense layers with far fewer parameters.",,"PropBank (1M words) for semantic role labeling task
Penn Treebank (1M words) for part-of-speech tagging and chunking tasks
Stanford NER dataset for named entity recognition task
Wikipedia text (631M words) for unsupervised pretraining","Section 7: ""631 million words
from Wikipedia""",1 week on 1 computer,,,,
Multiscale deformable part model,6/23/2008,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
BLSTM for handwriting (2),12/3/2007,SOTA improvement,100881,,,,,,,,,,,,,,,,,,,,"""In experiments on an unconstrained
online database, we record excellent results using either raw or preprocessed data, well outperforming a state-of-the-art HMM based system in both cases.""","For the raw input representation,
there were 4 input units and a total of 100,881 weights",,,,,,,,
Enhanced Neighborhood-Based Filtering,10/28/2007,SOTA improvement,,,,,,,,,,,Unknown,,,,,,,,,,"""We evaluate these methods on the Netflix dataset, where they deliver significantly better results than the commercial Netflix Cinematch recommender system.""",,,,,,,,,
BLSTM for handwriting (1),9/23/2007,SOTA improvement,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
Regularized SVD for Collaborative Filtering,8/12/2007,Highly cited,,,100480507,,,,,,,Netflix Prize,,,,,,,,,,,,,,,,,,,,
Fisher Kernel GMM,7/16/2007,Highly cited,,,30000,,2.5,,,,,,Likely,,,,,,,,,,,,2.5 hours on an AMD Opteron 2.4GHz with 4GB RAM,"in-house image dataset of 19 object/scene categories

","""Approximately 30K images were available for training and 5K for testing. Both sets were manually multi-labeled""","""With Fisher kernels, the training
cost is reduced down to approximately 2h30.""",,,,
SB-LM,6/22/2007,"Training cost,Highly cited",3E+11,1.4494464e+18,1.8E+12,,24,1500,,,,,Likely,,,,,,,TRUE,,Hardware,,Table 2,"Assuming a Nehalem based processor with 8 FLOP/cycle (https://www.agner.org/optimize/microarchitecture.pdf#page=105.06) , 2 cores and 2.33 GHz clock speed: 8*2*2330000000=37280000000 FLOP/s
Trained for 1 day on 1500 machines (Table 2)
Compute: 1500*37280000000*1*24*60*60*0.3=1449446400000000000=1.4e18
","Largest training set is dubbed ""web"", and is described as ""general web data, which was collected in January 2006 (2 trillion tokens)""
Table 2 indicates 1.8T tokens",Table 2,Table 2,,,,
KN-LM,6/22/2007,"Training cost,Highly cited",21000000000,7.7303808e+17,31000000000,,48,400,,,,,Likely,,,,,,,TRUE,,Hardware,,Table 2,"Trained for 2 days on 400 machines (Table 2)
Assuming a Nehalem based processor with 8 FLOP/cycle (https://www.agner.org/optimize/microarchitecture.pdf#page=105.06) , 2 cores and 2.33 GHz clock speed: 8*2*2330000000=37280000000 FLOP/s
Compute: 400*37280000000*2*24*60*60*0.3=773038080000000000=7.7e17","Largest dataset (""web"") was deemed to expensive to train with the KN methodology. Largest actually used was ""webnews"", described as ""data collected over several years, up to December 2005, from web pages containing predominantly English news articles (31 billion tokens).""",Table 2,Table 2,,,,
Restricted Bolzmann machines,6/20/2007,Highly cited,,,100480507,,,,,,,Netflix Prize,,,,,,,,,,,,,,,"The training data set consists of 100,480,507
ratings",,,,,
λ-WASP,6/1/2007,SOTA improvement,,,792,,,,,,,,,,,,,,,,,,"""The resulting parser is shown to be the bestperforming system so far in a database query domain""",,,,"""Table 1 summarizes the results at the end of the learning curves (792 training examples for λWASP, WASP and SCISSOR, 600 for Z&C)""",,,,,
Empirical evaluation of deep architectures,6/1/2007,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
Sparse Energy-Based Model,12/4/2006,Highly cited,,,60000,,,,,,,MNIST,,,,,,,,,,,,,,,,,,,,
Greedy layer-wise DNN training,12/4/2006,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
Local Binary Patterns for facial recognition,12/1/2006,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,"Shallowly investigated, couldn't find much.
",,,,,,,,
Sparse Vision Encoding,11/1/2006,Highly cited,,9.6048E+12,2000,,10,,,,,,Likely,,,,Unreleased,,,,,Hardware,"Major caveat: the task in question is ""encoding"", which I'm not sure is a valid task. Unusually low confidence across all answers in this form due to the task and training data being atypical.",,"(Just for natural images)

""All experiments were conducted on a Linux machine with AMD Opteron 2GHz CPU and 2GB RAM. ... For example, we were able to learn a set of 1,024 bases (each 14×14 pixels)in about 2 hours and a set of 2,000 bases (each 20×20 pixels) in about 10 hours.""

I filtered for 2GHz Opteron models that came out in 2005, of which there are five: https://www.techpowerup.com/cpu-specs/?mfgr=AMD&released=2005&generation=AMD%20Opteron&sort=name

Found a source which indicates 3 cycles per 32-bit multiply, and 5 per 64 bit: https://www.cse.wustl.edu/~roger/569M/m2066.pdf 
Assuming 32-bit precision, 2e9 cycles/s / 3 cycle/FLOP = 6.67e8 FLOP/s

10 * 3600 * 6.67e8 * 0.4 = 9.6048e12 FLOPs",,"However, these are 1000 20x20 pixel ""bases"", not images","""...in about 2 hours and a set of 2,000 bases (each 20×20 pixels) in about 10 hours.""",,,,
Dimensionality Reduction,7/18/2006,Highly cited,3800000,,70000,,,,,,,,,,,,,,,,,,,,,,"After fine-tuning on all 60,000 training images, the autoencoder was tested on 10,000 new images and produced much better reconstructions than did PCA
(Fig. 2B)",,,,,
Deep Belief Nets,7/18/2006,Highly cited,1600000,,60000,,,,,,,MNIST,,,,,,,,,,,,,,,"""The network that performed best on the validation set was
then tested and had an error rate of 1.39%. This network was
then trained on all 60,000 training images8 until its error-rate
on the full training set was as low as its final error-rate had
been on the initial training set of 44,000 images.""",,,,,
CTC-Trained LSTM,6/25/2006,Highly cited,114662,,41620,,,,,,,TIMIT,,,,,,,,,,,,"""The hidden layers were fully connected to themselves
and the output layer, and fully connected from the input layer. The input layer was size 26, the softmax output layer size 62 (61 phoneme categories plus the blank label), and the total number of weights was
114, 662.""

https://www.cs.toronto.edu/~graves/icml_2006.pdf",,,"4162 utterances, guesstimated avg 10 words per utterance",,,,,
Spatial Pyramid Matching,6/17/2006,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
DrLIM,6/17/2006,Highly cited,37097,,217470,,,,,,,,,,,,,,,,,,,Architecture described in figure 3,,,"""The dataset was split into 660 training images and a 312
test images. The result of training on all 10989 similar pairs
and 206481 dissimilar pairs is a 3-dimensional manifold in
the shape of a cylinder (see figure 8).""

206481 + 10989 = 217470",,,,,
FAST,5/7/2006,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
TFE SVM,2/2/2006,SOTA improvement,,,,,,,,,,,Unknown,,,,,,,,,,best at affine-transformed digits in table 4,,,,,,,,,
Stanley (DARPA Grand Challenge 2),1/1/2006,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,"""Our  approach  and  the underlying  probabilistic  Markov  model  possess  anumber  of  unknown  parameters.  These  parameters include the height threshold, the statistical acceptance  probability  threshold,  and  various  Markov chain error parameters the noise covariances of theprocess noise and the measurement noise. Stanley uses a discriminative learning algorithm for  locally  optimizing  these  parameters.""",,,,,,,,
Monocular Depth Prediction,12/5/2005,Highly cited,1472256,,2933137,,,,,,,,Speculative,,,,Unreleased,,,,,,1000+ citations,"""In detail, we use different parameters (θr, σ1r, σ2r) for each row in the image, because the images we consider are taken from a horizontally mounted camera, and thus different rows of the image have different statistical properties.""
""We collected a total of 425 image+depthmap pairs, with an image resolution of 1704x2272 and a depthmap resolution of 86x107""

The dimensionality of each parameter set isn't totally clear, but from equation (1) it seems like θ is the same length as the absolute depth input features (646) and σ1 and σ2 are scalar. If so, then we should have:
2272 * (646 + 1 + 1) = 1,472,256 parameters",,"""We used a 3-D laser scanner to collect images and their corresponding depthmaps. The scanner uses a SICK 1-D laser range finder mounted on a motor to get 2D scans. We collected a total of 425 image+depthmap pairs, with an image resolution of 1704x2272 and a depthmap resolution of 86x107""","""We collected a total of 425 image+depthmap pairs, with an image resolution of 1704x2272 and a depthmap resolution of 86x107. In the experimental results reported here, 75% of the images/depthmaps were used for training, and the remaining 25% for hold-out testing""
It seems like they do predictions over the dense values in the depthmap, so
86 * 107 * 425 * 0.75 = 2,933,137",,,,,
RankNet,8/7/2005,Highly cited,5711,3.48208E+12,3464289,22,5.85,,,,,,Confident,,,,Unreleased,,,,,Operation counting,,"Model is ""a two layer net with 10 hidden units""
Input is of size 569 ""In all, we use 569 features""
Parameters:
569*10 + 10 for hidden layer
10*1 + 1 for output layer","FLOPs per forward pass: 2*parameters = 11422
FLOPs per pair (data point): two forward passes and one backward pass (""A forward prop is performed for the first sample; each node’s activation and gradient value are stored; a forward prop is then performed for the second sample, and the activations and gradients are again stored. The gradient of the cost is then *formula*"") = 2*11422 + 2*11422 = 45688
Total FLOPs = (FLOPs per pair = 45688)*(pairs = 3,464,289)*(epochs = 22) = 3.48E12","""The data comprises 17,004 queries for the
English / US market, each with up to 1000 returned
documents.""
...
""This resulted in
our training on 384,314 query/document feature vectors, and on 3,464,289 pairs.""","""This resulted in our training on 384,314 query/document feature vectors, and on 3,464,289 pairs.""",Table 6,,,,
BiLSTM for Speech,8/1/2005,Highly cited,152061,24124575958774.88,36960,,,,,,,TIMIT,,,,,,,,TRUE,,Third-party estimation,,"""The hidden layer sizes were chosen to ensure that all networks had roughly the same number of weights W (≈100,000). However, for the MLPs the network grew with the time-window size, and W varied between 22,061 and 152,061.""",Extracted from AI and Compute (https://openai.com/blog/ai-and-compute/) charts by using https://automeris.io/WebPlotDigitizer/.,,"https://catalog.ldc.upenn.edu/LDC93s1
One sample utterance has around 10 words

3696 utterances * 10 words = around 37k words",,,,,
Histograms of Oriented Gradients,6/25/2005,Highly cited,,,1805,,,,,,,,,,,,,,,,,,,,,," we produced a new and significantly more
challenging data set, ‘INRIA’, containing 1805 64×128 im-
ages",,,,,
ConvNet similarity metric,6/20/2005,Highly cited,,,140000,,,,,,,,,,,,,,,,,,,,,,"The actual training set that was used contained
140,000 image pairs that were evenly split between genuine
and impostor.",,,,,
Hiero,6/1/2005,Highly cited,120000000,,171400000,,,,,,,,,,,,,,,,,,,"Very unsure, but the paper mentions 
""We ran the training process of Section 3 on the same data, obtaining a grammar of 24M rules"" 
and 
""For our experiments we used the following features, analogous to Pharaoh’s default feature set:
• P(γ | α) and P(α | γ), the latter of which is not
found in the noisy-channel model, but has been
previously found to be a helpful feature (Och
and Ney, 2002);
• the lexical weights Pw(γ | α) and Pw(α | γ) (Koehn et al., 2003), which estimate how well the words in α translate the words in γ;
2
• a phrase penalty exp(1), which allows the
model to learn a preference for longer or
shorter derivations, analogous to Koehn’sphrase penalty (Koehn, 2003).""

Suggesting 24M rules * 5 features per rule (?)",,,"[WORDS]
155M words dataset for the language model plus (7.2+9.2)M words for the translation model?",,,,,
SACHS,4/22/2005,Highly cited,178,,5400,,,,,,,,,,,,,,,,,,,From https://www.bnlearn.com/bnrepository/,,,"I think? 

"" The truncated singlecell data set (420 data points) shows a large
(11-arc) decline in accuracy, missing more connections and reporting more unexplained arcs than its larger (5400 data points) counterpart (fig. S4B). ""

Seems potentially wrong by maybe 20%. Might need to add 1200.",,,,,
Hierarchical LM,1/6/2005,Highly cited,,115848000000000.0,1000000,30,13.4,1,,,,Brown corpus,Confident,,,,,,,TRUE,,Hardware,,,"""The computations were performed on Athlon processors with a 1.2 GHz clock""
FP32 per cycle: 4 (""The bottom line is that the Athlon is capable of delivering as many as four 32-bit, single-precision floating-point results per clock cycle"", https://www.pctechguide.com/amd-technology/amd-athlon) 
Training time per epoch: 1609s (table 1)
Epochs: 30 ""Training is performed over about 20 to 30 epochs according to validation set perplexity (early stopping).""
Assumed utilization: 0.5 
Compute estimate: 0.5*1200000000*4*30*1609=115848000000000=1.16e14","""The experiments were performed on the Brown corpus, with a reduced vocabulary size of 10,000 words""","""The corpus has 1,105,515 occurrences of words, split into 3 sets: 900,000 for training, 100,000 for validation (model selection), and 105,515 for testing""","Training time per epoch: 1609s (table 1)
Total training time 30*1609/60/60=13.408h
",,,,
LMICA,12/1/2004,"Training cost,Historical significance",4096000,2782080000000000.0,100000,,,,,,,,Confident,,,,,,,TRUE,,Hardware,,"64*64*1000=4096000
""100000 samples of natural scenes of 64 × 64 pixels were given as X""
""LMICA was carried out in 1000 layers""
","69*60*60*8*2800000000*0.5=2782080000000000=2.78e15
""it consumed about 69 hours with Intel 2.8GHz CPU""
- Assuming they used an Intel Pentium 4 processor with 8 FLOP/cycle (https://en.wikipedia.org/wiki/FLOPS)",,"""100000 samples of natural scenes of 64 × 64 pixels were given as X""",,,,,
Invariant CNN,6/27/2004,"Highly cited,Historical significance",90575,9.7423E+11,24300,10,,,,,,,Confident,,,,,,,,,Operation counting,,"""The network has a total of 90,575 trainable parameters.""","""A full propagation through the network requires 3,896,920 multiply-adds."" - it's not entirely clear whether this refers to a forward pass or forward + backward pass (I assumed the latter)
""We used a stochastic version of the Levenberg-Marquardt algorithm with diagonal approximation of the Hessian [7], for approximately 250,000 online updates.""
3896920*250000=974230000000=9.7e11",,"""normalized-uniform set: 5 classes, centered, unperturbed objects on uniform backgrounds. 24,300 training samples, 24,300 testing samples.""",,,,,
Max-Margin Markov Networks,3/1/2004,Highly cited,,,600,,,,,,,,,,,,,,,,,,,,,,"The data set is divided into 10 folds of ∼ 600 training and ∼ 5500 testing examples.
The accuracy results, ... are averages over the 10 folds",,,,,
CNN Best Practices,8/6/2003,Highly cited,,,50000,,,,,,,MNIST,,,,,,,,,,,,,,,,,,,,
Unsupervised Scale-Invariant Learning,6/18/2003,Highly cited,451,,3500,,,,,,,,,,,,,,,,,,,"See Table 1
",,,"See Table 2 and Figure 1.
There are 7 datasets, each with 200-800 of pictures. I pick 500 as the avg number of pictures",,,,,
Phrase-based translation,5/1/2003,Highly cited,9178890,,20000000,,,,,,,,,,,,,,,,,,,"There are various components to the system:

- Translation probability model phi
- The distortion probability distribution d
- A langage model p_LM
- A length factor w

Several translation probability models are considered. The most performant one is the AP word alignment model. The sentence length preferred by the authors is 3 words maximum. In the biggest corpus considered (320k phrase pairs) it produces a phrase translation probability table of 1996k entries.

The distortion probability model d is taken from  (Marcu and Wong, 2002).

The distortion probability model must have ~10 parameters at most

The language model p_LM is a back off trigram model from (Seymore and Rosenfeld,1997). AFAIK the cutoff used is not specified. Based on the example on section 4.3 of (Seymore and Rosefeld, 1997), a trigram probability model has about 3866964 + 2674322 + 641604 parameters.

""For each possible phrase translation anywhere in the sentence (we call it a translation option), we multiply its phrase translation probability with the language model probability for the generated English phrase. As language model probability we use the unigram probability for the first word, the bigram probability for the second, and the trigram probability for all following words""

The length factor w is an additional single parameter.

""In order to calibrate the output length, we introduce a
factor w for each generated English word in addition to
the trigram language model ""

In summary, the parameter count seems to be dominated by the trigram language model and the word alignment phrase translation model. ",,,"[WORDS]
""We used the freely available Europarl corpus to carry out experiments. This corpus contains over 20 million words in each of the eleven official languages of the European Union, covering the proceedings of the European Parliament 1996-2001. 1755 sentences of length 5-15 were reserved for testing.""

""These results are consistent
over training corpus sizes from 10,000 sentence pairs to
320,000 sentence pairs. ""

So 20 million words or 320k sentence pairs.",,,,,
NPLM,3/15/2003,Highly cited,11904264,1303898760000000.0,1000000,,,,,,,Brown corpus,,,,,,,,TRUE,,Operation counting,,"""The number of free parameters is |V|(1 + nm + h) + h(1 + (n − 1)m) [...] For example, consider the following architecture used in the experiments on the AP (Associated Press) news data: the vocabulary size is |V| = 17,964, the number of hidden units is h = 60, the order of the model is n = 6, the number of word features is m = 100""","""For example, consider the following architecture used in the experiments on the AP (Associated
Press) news data: the vocabulary size is |V| = 17,964, the number of hidden units is h = 60, the order
of the model is n = 6, the number of word features is m = 100. The total number of numerical operations to process a single training example is approximately |V|(1+nm+h)+h(1+nm)+nm""

The first 800,000 words were used for training... reducing the vocabulary size to |V| = 16,383

convergence of the stochastic gradient ascent procedure was obtained after around 10
to 20 epochs for the Brown corpus

NOTE: there are two corpuses. The one represented in this calculation is the Brown one, which got a better improvement over sota",,"""Comparative experiments were performed on the Brown corpus which is a stream of 1,181,041 words, from a large variety of English texts and books. The first 800,000 words were used for training, the following 200,000 for validation (model selection, weight decay, early stopping) and the remaining 181,041 for testing. The number of different words is 47,578 (including punctuation, distinguishing between upper and lower case, and including the syntactical marks used to separate texts and paragraphs). Rare words with frequency ≤ 3 were merged into a single symbol, reducing the vocabulary size to |V| = 16,383.""",,,,,
LDA,2/2/2003,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,Multiple experiments with different tasks and datasets,,,,,
Statistical Shape Constellations,1/1/2003,Historical significance,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
Maximum Entropy Models for machine translation,7/6/2002,Highly cited,,,519523,,,,,,,,,,,,,,,,,,,,,,"[WORDS]
Table 1",,,,,
Tagging via Viterbi Decoding,6/1/2002,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
NEAT,6/1/2002,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
Thumbs Up?,5/28/2002,Highly cited,,,2053,,,,,,,IMDb,,,,,,,,,,,,,,,"yielding a corpus of 752 negative and
1301 positive reviews",,,,,
Decision tree (classification),12/8/2001,Highly cited,120000000,6.3E+13,14460,,,,,,,,,,,,,,,TRUE,,Operation counting,,"From table 1, it looks like the number of weights depends on the dataset size, which in this case is 2*4916 faces+9544 non-faces = 19376, and multiplies that by the number of filters T = 6061, so no. of params = 1.2e8 (Note:I think ""features"" = ""filters"" in this paper)","
The training compute can be tediously worked out from the pseudocode. I think for dataset size D, number of filters T, the training compute is roughly 180k * D * 3 * T = 6.3e13 FLOPs",They scraped the dataset personally for training,Section 5: 4916 hand labeled faces  + 9544 non-face images = 14460,,,,,
Gradient Boosting Machine,10/1/2001,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
Immediate trihead,7/6/2001,SOTA improvement,,,,,,,,,,,Unknown,,,,,,,,,,"""The perplexity for both of these models significantly improve
upon the trigram model base-line as
well as the best previous grammar based language model""",,,,,,,,,
PoE MNIST,11/28/2000,Historical significance,3925310,5.181E+13,60000,500,,,,,,MNIST,Confident,,,,,,,TRUE,,Operation counting,,"10 models, one for each digit. Largest models: 500 epochs, 500 hidden units (Table 2)
""The largest network was the best, even though each digit model contains 392,500 parameters trained on only 4,400 images""
""the classification network had 30 inputs and therefore 300 weights and 10 output biases.""

Total: 392500*10 + 310 = 3,925,310","Each model was trained on 4400 examples: ""The largest network was the best, even though each digit model contains 392,500 parameters trained on only 4,400 images.""

Table 2, largest network trained 500 epochs.
10 * 6 * 392500 * 4400 * 500 = 51,810,000,000,000",,Total training data size is 60000 but the subnetworks were trained on smaller subsets.,,,,,
Neural LM,11/28/2000,"Training cost,Historical significance,Highly cited",6906980,6339000000000000.0,33100000,10,,,,,,Hansard Corpus,Confident,,,,,,,TRUE,,Operation counting,,"(30959*100) + (8*100*120) + (120*30959) = 6,906,980
""This is obtained with a network with the direct architecture, 100 randomly initialized words features, 120 hidden units, and n = 8 words of context.""
""The Hansard corpus (Canadian parliament proceedings, French version) is a stream of about 34 million words, of which 32 millions (set A) was used for training, 1.1 million (set B) was used for validation, and 1.2 million (set C) was used for out-of-sample tests. The original data has 106, 936 different words, and those with frequency <= 10 were merged into a single token, yielding IVI = 30,959 different words.""
","The authors use a trick to avoid having to calculate the final layer for all possible words in the vocabulary. They precompute a ""short list"" of the most common word following any 2 precursor words with a smoothed trigram model, and then only calculate the softmax over words on the short list. This means only a negligible fraction of the unembedding parameters get used, so the effective number of parameters appears to be (30959*100) + (8*100*120) = 3,191,900

""Apparent convergence of the stochastic gradient descent procedure was obtained after around 10 epochs for Hansard""

6ND:
6*3191900*33100000*10=6.339e15
",,"The Hansard corpus (Canadian parliament proceedings, French version) is a stream of about 34 million words, of which 32 millions (set A) was used for training, 1.1 million (set B) was used for validation, and 1.2 million (set C) was used for out-of-sample tests.",,,,,
FrameNet role labeling,9/1/2000,Highly cited,,,50000,,,,,,,FrameNet,,,,,,,,,,,,,,,"Abstract: ""The system is based on statistical classifiers trained on roughly 50,000 sentences""",,,,,
SVD in recommender systems,7/14/2000,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
Perceptron for Large Margin Classification,12/1/1999,Highly cited,,,60000,,,,,,,MNIST,,,,,,,,,,,,,,,"""The dataset consists of 60,000 training examples and 10,000 test examples.""",,,,,
IBM Model 4,7/2/1999,Highly cited,,,800000,,,,,,,,,,,,,,,,,,,,,,"[WORDS]
See FIgure 6",,,,,
LSTM with forget gates,1/2/1999,Highly cited,276,,30000,,,,,,,,,,,,,,,,,,,See Table 1,,,"Training was stopped after at most 30000
training streams, each of which was ended
when the first prediction error or the
100000th successive input symbol occurred

NOTE this is a weird task. Not sure how to measure dataset size (#seqs? #symbols?)",,,,,
LeNet-5,11/1/1998,"Historical significance,Highly cited",60000,2.81094E+12,60000,,,,,,,MNIST,,,,,,,,TRUE,,Operation counting,,"""[LeNet5] contains 390408 connections, but only 60000 trainable free parameters because of the weight sharing""","""[LeNet5] contains 390408 connections"" = multiply-adds
MNIST - 60,000 data points
20 epochs",,"The MNIST database contains 60,000 training images and 10,000 testing images (Wikipedia)",,,,,
Social and content-based classification,7/1/1998,Highly cited,,,45000,,,,,,,,,,,,,,,,,,,,,,"""Our data set consists of more than 45,000 movie rat-
ings collected from approximately 260 users.""",,,,,
Sparse coding model for V1 receptive fields,12/1/1997,Highly cited,,,10,,,,,,,,,,,,,,,,,,,,,,"In Simulation Methods: ""The data for training were taken from ten 512 × 512
pixel images of natural surroundings""",,,,,
LSTM,11/15/1997,Highly cited,10504,2.1008E+13,1273000,,,,,,,,,,,,,,,TRUE,,Operation counting,,"Table 2

http://www.bioinf.jku.at/publications/older/2604.pdf","""Due to limited computation time, training is stopped after 5 million sequence presentations""

Each sequence has p=100 elements in the long-delay setting.

COMPUTE = PRESENTATIONS * PRESENTATION LENGTH * UPDATE COMPUTE PER TOKEN",,"Table 8. The rightmost column lists numbers of training sequences required to achieve the stopping
criterion.

This applies to experiment 5 (multiplication)

Sequences have random lengths, on the order of 100-1000 (table 7 )",,,,,
Bidirectional RNN,11/1/1997,Highly cited,13000,,73920,,,,,,,TIMIT,,,,,,,,,,,,"Page 7: ""The structures of all networks are adjusted so that
each of them has about the same number of free parameters
(approximately 13 000 here""",,,"""the training data set consisting of 3696 sentences
from 462 speakers""

Assuming avg sentence length of 20 words

3696 * 20 total words",,,,,
SVM for face detection,6/17/1997,Highly cited,,,50000,,,,,,,,,,,,,,,,,,,,,,"Section 1: ""The problem that we have to solve involves training a classifier
to discriminate between face and non-face patterns, using a
data set of 50,000points. """,,,,,
Deep Blue,5/1/1997,"Historical significance,Highly cited",8000,,,,,,,,,,Speculative,,,,Unreleased,Unreleased,,,,,"Defeated Kasparov in 1997, which was a famous AI milestone.","""The new chess chip had a completely redesigned evaluation function, going from around 6400 features to over 8000""","The 8000 features were tuned using a mix of human judgment and automated tools using data on chess matches. Unclear how much total ""compute"" went into this.",,,,,,,
HMM Word Alignment,8/5/1996,Highly cited,,,442316,,,,,,,,,,,,,,,,,,,,,,"[WORDS]
Table 1.
I take the sum of all words. Maybe it would be better to use only the sum of English or German words?",,,,,
AdaBoost.M2 Digit Recognition,7/3/1996,Highly cited,,,9709,,,,,,,,Confident,,,,,,,,,,Also listed in Denis Panjuta's List of 100+ AI Algorithms,,,"""The dataset comes from the US Postal Service (USPS)
and consists of 9709 training examples""",,,,,,
System 11,6/18/1996,Highly cited,6452,12930000000,9050,,,,,,,,,,,,,,,,,Operation counting,,"System 11 is a combination of Network 1 and Network 2

Network 1 has 2095 connections and network 2 has 4357 connections (see table 1)","Since there is no parameter sharing, the forward compute is roughly twice that of the number of parameters. We use a 2:1 forward-backward ratio as this is a shallow network, with most connections in the first layer.

Number of passes (Section 2.1):
* ""Nearly 1,050 face examples were gathered from face databases [...]""
* ""Fifteen face examples are generated for the training set from each original image""

Training loop:
1. ""initial set of nonface images by generating 1,000 random images""
2. Train (presumably on whole set)
3. Run + collect false positives
4. ""Select up to 250 of these subimages [...] and add them into the training set [...] Go to step 2""

""A typical training run selects approximately 8,000 nonface images ""

Selecting 8,000 nonface images implies 8000/250 = 32 loops.

Assuming compute is 3 * N * D, we have
* Loop 1: D = 15*1050 + 1000
* Loop 2: D = 15*1050 + 1000 + 250
* So on.

Hence D overall is 32*(15*1050 + 1000) + 250*32/2*(32+1) = 668,000.

Hence compute = 3 * 6452 * 668e3 = 1.3e10.",,"""A typical training
run selects approximately 8000 non-face images from the
146,212,178 subimages that are available at all locations
and scales in the training scenery images.""

""Nearly 1050 face examples were gathered from face databases at CMU and Harvard [...] In the training set,15 face examples are generated from each
original image [...]""

""Create an initial set of non-face images by generating
1000 images with random pixel intensities""",,,,,
MUSIC perceptron,6/3/1996,Historical significance,13607,8.81734E+11,27000,400,,,,,,,Confident,,,,,,,TRUE,,Operation counting,,230*55+56*15+16*6+7*3=13607 (Figure 2),"2*13607*3*10800000=881733600000=8.8e11
Training steps: 400*27000=10800000
""After 400 epochs the error of the network""",,"“The training experiments were carried out on a database of 30,000 photos. Therefor the database was split into ten sets. Nine of them were used for the training and one for the testing.”",,,,,
LISSOM,11/27/1995,"SOTA improvement,Historical significance",432800,1.95545E+11,2000,38,,,,,,,Likely,,,,,,,TRUE,,Operation counting,,"Total connections 32*32*20*20+20*20*48+20*20*10=432800
Input: 32*32, Lissom: 20*20, Output: 10 (Figure 1a), up to 48 lateral connections per Lissom neuron (Figure 1b)
","Lissom connections: 32*32*20*20+20*20*48=428800
Lissom compute: 2*428800*3*38*2000=195532800000=1.96e11
Perceptron connections: 20*20*10=4000
Perceptron compute: 2*4000*3*500*1700=20400000000=2e10
Total compute: 195532800000+12000000=195544800000=1.96e11
""LISSOM was trained with 2000 patterns""
""The initial self-organizing map was formed in 8 epochs over the training set, gradually reducing the neighborhood radius from 20 to 8. The lateral connections were then added to the system, and over another 30 epochs,""
""Of these, 1700 were used to train the perceptron layer, ""
""After the SOM and LISSOM maps were organized, a complete set of activation patterns on the two maps were collected. These patterns then formed the training input for the perceptron layer. Two separate versions were each trained for 500 epochs,""

",,"""LISSOM was trained with 2000 patterns""",,,,,
Support Vector Machines,9/1/1995,Highly cited,100000000,,60000,,,,,,,MNIST,,,,,,,,,,,,"Section 6.2.2: ""...polynomials
of degree 4 (that have more than 10^8 free parameters)...""
They used 4-degree polynomials for MNIST",,,"Section 6.2: ""The large database consists of 60,000 training and 10,000 test patterns""",,,,,
Random Decision Forests,8/14/1995,Highly cited,,,60000,,,,,,,MNIST,,,,,,,,,,,,,,,The images are from the 1992 NIST (National Institute of Standards and Technology) Competition,,,,,
Iterative Bootstrapping WSD,6/26/1995,Highly cited,,,460000000,,,,,,,,,,,,,,,,,,,,,,the data were extracted from a 460 million word corpus,,,,,
Predictive Coding NN,12/2/1994,Historical significance,206910,1.86219E+13,600000,25,,,,,,,Confident,,,,,,,TRUE,,Operation counting,,"5*80*430+430+430*80+80=206910
""P has nk input units and k output units. n is called the ""time-window size""
""Note that the time-window was quite small (n = 5).""
""alphabet consisted of k = 80 possible characters""
""P had 430 hidden units""","2*206910*3*15000000=18621900000000=1.86e13
""The training phase consisted of 25 sweeps through the training set""",,"Training dataset: 15000*40=600000
""The training set for the predictor was given by a set of 40 articles from the newspaper Miinchner M erkur, each containing between 10000 and 20000 characters.""",,,,,
NeuroChess,12/2/1994,"Historical significance,Highly cited",72251,8.58731E+11,120000,,,,,,,,Speculative,,,,,,,TRUE,,Hardware,,"""Prior to learning an evaluation function, the model M (175 input, 165 hidden, and 175 output units)"" = 58,090 parameters
""NeuroChess then learns an evaluation network V (175 input units, 0 to 80 hidden units, and one output units)."" = 14,161 parameters
Total: 58,090 + 14,161 = 72,251","Lower bound: 0.3*2*24*60*60*1400000=72576000000=7.26e10
Upper bound: 0.3*14*24*60*60*1400000*20=10160640000000=1.02e13
Geometric mean: 858730812676=8.59e11 (speculative)
""Thus far, experiments lasted for 2 days to 2 weeks on I to 20 SUN Sparc Stations. ""
SparcStation has 1.4 MFLOPS (https://ieeexplore.ieee.org/document/63671)
",,"""is trained using a database of 120,000 expert games.""",,,,,
Mixture of linear models,12/2/1994,Historical significance,384000,4.536E+11,7000,,12,,,,,,Likely,,,,,,,TRUE,,Hardware,,"“In the example we describe, 7000 training images are sufficient to fit 384,000 parameters“","0.3*12*60*60*35000000=453600000000=4.54e11
Assuming a utilization of 0.3 and interpreting ""overnight"" as 12 hours.
“the training procedure is fast enough to do the fitting overnight on an R4400-based machine. “
R4400 has 35MFLOPS (“Compare this to the 200MHz R4400 which is rated at about 35MFLOPS”, http://www.sgidepot.co.uk/perf.html)",,"""7000 training images are sufficient""","""the training procedure is fast enough to do the fitting overnight on an R4400-based machine.""",,,,
JPMAX,12/2/1994,Historical significance,4446,80828280,1500,,,,,,,,Speculative,,,,,,,,,Operation counting,,"Inputs are 12x12 pixels (Figure 3). They first train the architecture in Figure 2 a), then freeze it and train the additional layer in Figure 3 b).

Figure 2 a): 2*(12*12*15) + 2*15 = 4,350
Figure 2 b): 2*(12*12*15) + 2*15 + 2*(15*3) + 2*3 = 4,446","Training 2a):  ""The learning took about 3000 iterations of steepest descent"" Assuming each iteration refers to a single image.
6 * 4446 * 3000 = 80,028,000

Training 2b): ""While keeping the first layer of weights frozen, this network was trained using exactly the same cost function as the first layer for about 30 iterations using a gradient-based learning method.""

6 * 4446 * 30 = 800,280

Total: 80,828,280",,“Figure 3: 10 of the 1500 training patterns”,,,,,
GroupLens,10/22/1994,Highly cited,,,100000000,,,,,,,,,,,,,,,,,,,"For each pair of users, the system computes the correlation between their scores in the articles they have rated.

Then to make the prediction of a score for a given article and user the system computes a weighted average taking into account the correlations with each other user, the average rating of each user and the average rating of the article.

So the system in total has n+m+n*n ~= n*n parameters, where n is the number of users and m is the number of articles.

To address scaling issues, the system is partioned into clusters of users. It's very unclear what is the number of users per cluster, though the Daily ratings traffic table provided suggests that is around 10k users ",,,"For each pair of users, the system computes the correlation between their scores in the articles they have rated.

Then to make the prediction of a score for a given article and user the system computes a weighted average taking into account the correlations with each other user, the average rating of each user and the average rating of the article.

So the system in total has n+m+n*n ~= n*n parameters, where n is the number of users and m is the number of articles.

To address scaling issues, the system is partioned into clusters of users. It's very unclear what is the number of users per cluster, though the Daily ratings traffic table provided suggests that is around 10k users ",,,,,
Ceramic-MLP,1/7/1994,Historical significance,1888,4531200000,80,5000,,,,,,,Likely,,,,,,,,,Operation counting,,"Parameters: 100*16 + 16*16 + 16*2 = 1888
Architecture: ""The topology of the classifier was X-Y-Y-2, where X is the number of input components, Y is the number of neurons in each hidden layer and the number of neurons in the output layer is two, which is the number of classes. The two hidden layers were considered to have the same number of nodes for simplification purposes. ""
Input size: ""Each pattern consists of a 10 x 10 pixel sub-image.""
Hidden size: ""Experiments have been made on networks with 6, 9, 12 and 16 hidden nodes. ""","Compute estimate: 2*1888*3*400000=4531200000=4.53e9
Training steps: ""In Fig. 6 we report the classification results obtained on the testing set in the 12 and 16 component compressed data after 400000 training iterations""",,"After the pre-processing phase, a training set of 80 patterns and a testing set of 64 patterns were available.",,,,,
ANN Eye Tracker,11/29/1993,Historical significance,5620,17534400000,2000,260,0.6,,,,,,Confident,,,,,,,,,Operation counting,,"15*15*20+20+50*10*2+100=5620
Hidden layer is split, 15*15 image input, 2*50 output neurons (see Figure 2)
Hidden size up to 20 neurons (""This architecture was used with varying numbers of hidden units in the single, divided, hidden layer; experiments with 10, 16 and 20 hidden units were performed. "")","2*5620*3*520000=17534400000
Training examples: 2000*260=520000
""As mentioned before, 2000 image/position pairs were gathered for training""
""All of the networks described in this paper are trained with the same parameters for 260 epochs""","""2000 image/position pairs were gathered for training""",,"""Training the 8x2 hidden layer network using the 15x40 input retina, with 2000 images, takes approximately 30-40 minutes on a Sun SPARC 10 machine. """,,,,
Siamese-TDNN,8/1/1993,"Historical significance,Highly cited",744,1.28696E+13,7701,69,,,,,,,Likely,,,,,,,TRUE,,Operation counting,,"""The input is 8 by 200 units, the first convolutional layer is 6 by 192 units with each unit's receptive field covering 8 by 9 units of the input. The first averaging layer is 6 by 64 units, the second convolution layer is 4 by 57 with 6 by 8 receptive fields and the second averaging layer is 4 by 19""
""Two separate sub-networks based on Time Delay Neural Networks (Lang and Hinton, 1988, Guyon et al. 1990) act on each input pattern to extract features,""
""All weights could be learnt, but the two sub-networks were constrained to have identical weights.""
L1: H=1, W=200, C=8, K=9, D=6
L2: H=1, W=64, C=6, K=8, D=4
Parameters:  7*9*8+5*8*6=744","8073216*3*7701*69=12869570138112=1.29e13
Forward pass flop: 2*(2*200*200*8*6+2*64*64*6*4)=8073216
""We used up to 7,701 signature pairs""
Epochs: 69 (Table 1)
","""In total, 219 people signed between 10 and 20 signatures each, 145 signed genuines, 74 signed forgeries."" ""We used up to 7,701 signature pairs""","""We used up to 7,701 signature pairs""",,,,,
IBM-5,6/15/1993,Highly cited,1658364,,53358600,,,,,,,Proceedings of the Canadian parliament,,,,,,,,,,,,"The model is initiallized with 2.44E+09 translation probabilities, which are progressively culled until 1,658,364 remain. There are other parameters in the models (eg the fertility probabilities that relate each word in the input to the number of words it will align to) but the parameter count is dominated by the translation probabilities.",,,"""They used the algorithm to extract a large number of translations from several years of the proceedings of the Canadian parliament. From these translations, we have chosen as our training data those for which both the English sentence and the French sentence are 30 or fewer words in length. This is a collection of 1,778,620 translations.""",,,,,
Boosting,11/30/1992,Historical significance,2578,,9709,,,,,,,,Likely,,,,,,,,,,,"“The network has 4645 neurons, 2578 different weights, and 98442 connections.“",,,“divided into 9709 training examples and 2007 validation samples.”,,,,,
Cancer drug mechanism prediction,10/16/1992,Historical significance,594,53460000,,,,,,,,,Likely,,,,,,,,,Operation counting,,"“The network shown has 60 input PEs, one for each cell line, and 6 output PEs“
“Neural networks with three to nine hidden layer PEs used”
9*60 + 6*9 = 594","2*594*3*15000=53460000=5.35e7
“The extent of training was 15,000 presentations“",,,,,,,
Golem,10/1/1992,"Historical significance,SOTA improvement",,,1612,,,,,,,,Confident,,,,,,,,,,,,,"""The input to the program consisted of 12 non-homologous proteins (1612 residues)""",Table 1,,,,,
Fuzzy NN,9/1/1992,Highly cited,1166,1403117760,436,,,,,,,,,,,,,,,,,Operation counting,,"Table II: ""he neural network has three hidden layers, with m hidden nodes in each layer"", m = 20, input dim. = 9, output dim. = 6",1166 params * 2 FLOP/param * (3 for forward + backward pass) * 460 epochs * 436 examples,,"""The above-mentioned algorithm was tested on a set of 871 Indian Telugu vowel sounds"" and 50% of the dataset was used. 871*0.5 ~= 436",,,,,
TD-Gammon,5/1/1992,Highly cited,25000,18232157622832.703,6300000,,,,,,,,,,,,,,,TRUE,,Third-party estimation,,"""The best performance was obtained with a network containing 80 hidden units and over 25,000 weights.""",Extracted from AI and Compute (https://openai.com/blog/ai-and-compute/) charts by using https://automeris.io/WebPlotDigitizer/.,,"""This network was trained
for over 300,000 training games""

Each backgammon game has an avg of around 21 movements
https://www.bkgm.com/rgb/rgb.cgi?view+712",,,,,
Weight Decay,12/2/1991,"Highly cited,Historical significance",8386,75474000000,5000,300,,,,,,,Confident,,,,,,,TRUE,,Operation counting,," 7*26*40+40+40*26+26=8386
""The network had 7 x 26 input units, 40 hidden units and 26 output units""","2*8386*3*1500000=75474000000=7.55e10
""It was trained on 400 to 5000 random words from the data base of around 20.000 words,""
""The top full line corresponds to the generalization error after 300 epochs""
",NetTalk dataset of 20000 words,"""It was trained on 400 to 5000 random words from the data base of around 20.000 words,""",,,,,
SRN-Encoded Grammatical Structures,9/1/1991,Highly cited,,,177805,,,,,,,,,,,,,,,,,,,,,,4 training sets of 10k sentences each. Total number of words calculated by multiplying 10k and the avg. number of words per sentence in the training set.,,,,,
RAAM,11/1/1990,Highly cited,1536,,29,,,,,,,,Confident,,,,,,,,,,,"Largest model:
""A 48-16-48 RAAM learned to construct representations ""
Parameters 48*16*2 = 1536",,"Trained on sentence fragments, see Figure 10",29 sentence fragments (Figure 10),,,,,
SexNet compression,10/1/1990,Historical significance,72940,78775200000,90,2000,,,,,,,Confident,,,,,,,TRUE,,Operation counting,,"900*40*2+40+900=72940
“Images sampled at 30x30 were compressed using a 900x40x900 fully connected back-propagation network”","2*72940*3*90*2000=78775200000
“The compression network trained for 2000 runs on each of 90 faces”",,“The compression network trained for 2000 runs on each of 90 faces”,,,,,
SexNet classification,10/1/1990,"Historical significance,Highly cited",1640,,80,,,,,,,,Likely,,,,,,,,,,,Largest classification model: 40*40 + 40=1640 (Figure 2),,,"“Each training on a different 80 faces, leaving a distinct set of 10 untrained faces for testing”",,,,,
ISR network,10/1/1990,Historical significance,,,9000,,,,,,,,Confident,,,,,,,,,,,,,,"“We used a training and test set of about 9,000 and 1,800 characters respectively. “",,,,,
Bankruptcy-NN,6/17/1990,"Historical significance,Highly cited",36,3059337600,370,191400,24,,,,,,Confident,,,,,,,TRUE,,Operation counting,,"""The input layer consisted of the five nodes, one for each of the ratios. The hidden layer consisted of 5 node.;. The output layer consisted of only one neuron""
30 weights + 6 biases = 36"," 2*36*3*74*191400=3,059,337,600=3.06e9
""Convergence was reached after 191,400 iterations""",,"""The first (training) subsample of 74 firms data""
5 inputs per firm
74*5 = 370",,,,,
Innervator,12/1/1989,Highly cited,10,120000000,4,,,,,,,,,,,,,,,,,Operation counting,,Each net has 5 units,10 params * 6 FLOP/param/pass * 4 datapoints * 1000 epochs * 50 individuals * 10 generations,,,,,,,
ALVINN,12/1/1989,Highly cited,36627,10548576000,1200,40,,,,,,Road snapshots,,,,,,,,TRUE,,Operation counting,,"1217*29 + 29*46 =36627
“Each of these 1217 input units is fully connected to the hidden layer of 29 units, which is in turn fully connected to the output layer. The output layer consists of 46 units, divided into two groups.”","2 * 36627 * 3 * 40 * 1200 = 10548576000 = 1.05e10
36627 parameters
""After 40 epochs of training on the 1200 simulated road snapshots""",,"""Training involves first creating a set of 1200 road snapshots depicting roads with a wide variety of retinal orientations and positions, under a variety of lighting conditions and with realistic noise levels""",,,,,
Speaker-independent vowel classification,11/27/1989,Historical significance,3040,7485696000,4104,100,,,,,,,Confident,,,,,,,TRUE,,Operation counting,,"“The MLP consisted of 64 inputs (the DFf coefficients. each nonnalized between zero and one), a single hidden layer of 40 units, and 12 output units;”","2*3040*3*410400=7485696000=7.49e9
“The network was trained on 100 iterations through the 4104 training vectors.”",,,,,,,
Handwritten Digit Recognition System,11/27/1989,Historical significance,2578,1.8144E+11,9840,30,72,,,,,,Confident,,,,,,,TRUE,,Hardware,,"""In summary, the network has 4635 units, 98442 connections, and 2578 independent parameters.“","1.4e6 * 3 * 24 * 60* 60 * 0.5 = 181440000000 = 1.81e11
""A complete training session (30 passes through the training set plus test) takes about 3 days on a SUN SPARCstation 1""
Sparcstation 1 has an estimated compute of 1.4 MFLOPS (source: https://ieeexplore.ieee.org/document/63671 )",,"""After 30 training passes the error rate on training set (7291 handwritten plus 2549 printed digits)""","""A complete training session (30 passes through the training set plus test) takes about 3 days on a SUN SPARCstation 1""",,,,
Truck backer-upper,6/18/1989,Historical significance,805,,,,,,,,,,Confident,,,,,,,,,,,6*25+25+8*45+45*6=805 (see Figure 6),,,,,,,,
Invariant image recognition,6/18/1989,Historical significance,,27000000000,,,6,,,,,,Confident,,,,,,,TRUE,,Hardware,,,"0.5*6*60*60*2.5e6 = 27000000000 = 2.7e10
Trained for 6h on a SUN-4 (section 4)
Assumed utilization of 0.5
SUN-4 is estimated at 2.5e6 FLOP/s (Nordhaus, 2007)",,,Section 4,,,,
Time-delay neural networks,3/3/1989,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
Q-learning,1/1/1989,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
MLP baggage detector,1/1/1989,Historical significance,,,40000,2000,,,,,,,Confident,,,,,,,,,,One of the first real-world use cases of neural networks,"3 layer network, input layer (<20), hidden layer, output layer (3)",,,"""The database contains about 20,000 bags without simulants and a like number with; although, because of changes made in the systems as they were developed, not all measurements are on the same basis.""",,,,,
MLN-ASR,8/1/1988,Historical significance,10000,296425000,1400,,0.1,,,,,,Confident,,,,,,,TRUE,,Hardware,,"“For an MLN of about 10,000 links, the time was 115 CPU msecs for the recognition of a spoken letter and 317 msecs for the learning of a spoken letter on the SUN 4/280. A 20% reduction was obtained on the VAX 8650”","“For an MLN of about 10,000 links, the time was 115 CPU msecs for the recognition of a spoken letter and 317 msecs for the learning of a spoken letter on the SUN 4/280. A 20% reduction was obtained on the VAX 8650”, “Learning and recognition were performed on a VAX 8650.”
Dataset: 70*10*2=1400 (Train) 10*10*2=200 (Test)
“The ten words of the El set were pronounced twice by 80 speakers (40 males and 40 females)”
“The data from 70 speakers were used as a training set while the data from the remaining 10 speakers (6 males and 4 females) were used for the test”
VAX 8650 FLOPS  = 1.67E+06 (Nordhaus)
Training time: 317ms * 1400  * 0.8 = 355040ms = 355s
Estimate: 0.5 * 1.67e6 * 355 = 296425000 = 2.96e8",,,,,,,
Motion-Driven 3D Feature Tracking,7/1/1988,Highly cited,,,1500,,,,,,,,,,,,,,,,,,,"""The simulation studies reported here all involved a 16-bit input pattern. """,,,"""The total number of possible input patterns was 65,536. Training sets of 650 and 1500 patterns picked at random from this total were used.""",,,,,
NetTalk (transcription),6/6/1987,Highly cited,18629,28328002560,1024,55,,,,,,,,,,,,,,TRUE,,Operation counting,,"""The connections in the network are specified by a total of 18629
weight parameters (including a variable threshold for each unit)""",18629 params * 2 FLOP/param * (3 for forward + backward pass) * 55 epochs * 1024 words/epoch * 4.5 letters/word,,"We used the first two pages of transcriptions, which contained 1024 words from a child in firstgrade",,,,,
NetTalk (dictionary),6/6/1987,Highly cited,18629,27664065000,1000,55,,,,,,,,,,,,,,TRUE,,Operation counting,,"""The connections in the network are specified by a total of 18629 weight parameters (including a variable threshold for each unit)""",18629 params * 2 FLOP/param * (3 for forward + backward pass) * 55 epochs * 1000 words/epoch * 4.5 letters/word,,"""A subset of the 1000 most commonly occurring words was selected from this dictionary based on frequency counts in the Brown corpus""",,,,,
Optimized Multi-Scale Edge Detection,11/1/1986,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
Back-propagation,10/1/1986,Highly cited,144,124416000,144,,,,,,,,,,,,,,,TRUE,,Operation counting,,Figure 4 includes a representation of the weights learned by the people to relationship network,"We assume that the number of mult-adds per pass is equal to the number of parameters.

""We trained the network for 1500 sweeps""

There are 12*12 possible pairs of people, so we assume that is the dataset size",,"There are 12*12 possible pairs of people, so we assume that is the dataset size",,,,,
Distributed representation NN,8/15/1986,"Historical significance,Highly cited",432,388800000,100,1500,,,,,,,Confident,,,,,,,TRUE,,Operation counting,,"Parameters: 24*6 + 12*6 + 12*12 + 12*6 =432
""Figure 5: The activity levels in a five-layer network after it has learned. The bottom layer has 24 input units on the left for representing person 1 and 12 units on the right for representing the relationship. The white squares inside these two groups show the activity levels of the units. There is one active unit in the first group (representing Colin) and one in the second group (representing has-aunt). Each of the two groups of input units is totally connected to its own group of 6 units in the second layer. These two groups of 6 must learn to encode the input terms as distributed patterns of activity. The second layer is totally connected to the central layer of 12 units, and this layer is connected to the penultimate layer of 6 units.""
"," 2*432*3*1500*100=388800000=3.9e8
""After 1500 sweeps through all 100 training examples the weights were very stable """,,"""After 1500 sweeps through all 100 training examples the weights were very stable """,,,,,
PDP model for serial order,1/5/1986,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
Error Propagation,1/3/1986,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
Learnability theory of language development,7/1/1984,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
Hierarchical Cognitron,4/1/1984,Historical significance,9315,,5,,,,,,,,Speculative,,,,,,,,,,,"Parameters 5*5*9*3*3 + 3*3*9*3*3*9 + 9*3*3*9 = 9315
""The numbers of excitatory cells in these four layers were: 7x7 in U0, 5x5 in  U1, 3x3 in U2, and 9 in U3""
""Each feature-extracting cell in layer U1 receives excitatory modifiable afferent connections from 3x3 cells in layer U0""
""On the other hand, each feature, each extracting cell in layers U2 and U3 receives excitatory modifiable connections from all 9 cells in each of the 3 x 3 hypercolumns in the layer preceding it. Therefore, it receives 3 x 3 x 9 afferent excitatory modifiable connections altogether""
",,,"""Five training patterns used for the self-organization are shown in Fig. 4""",,,,,
ASE+ACE,9/1/1983,"Highly cited,Historical significance",324,324000000,500000,,2.8,,,,,,Likely,,,,,,,TRUE,,Operation counting,,"The system consists of two parts: ACE and ASE, each with 162 weights (=324 parameters). Found in Figures 2 and 3.","324 * 2 * 500000 = 324000000 = 3.24e8. The calculation assumes ""compute per forward pass"" = ""number of parameters"" = ""compute per backward pass"". Their model only has a single layer and is trained with simple update rules instead of gradient descent. Training details are described in Section IX.

Note that this is the compute for a single run; they appear to have repeated training 10 times for the ASE+ACE system.",,"""Runs consisted of 100 trials unless the run's duration exceeded 500 000 time steps (approximately 2.8 h of simulated real time)"" 
""Almost all runs of the ASE/ACE system [...], were terminated after 500 000"" (Section IX)","""Runs consisted of 100 trials unless the run's duration exceeded 500 000 time steps (approximately 2.8 h of simulated real time)"" 
""Almost all runs of the ASE/ACE system [...], were terminated after 500 000"" (Section IX)",,,,
Hopfield network,4/1/1982,Highly cited,9900,,,,,,,,,,,,,,,,,,,,,"My understanding is that the biggest Hopfield networks they studied had N=100 units. 

Each unit has 99 synapses Tij from each other unit, for a total of 100*99 parameters",,,,,,,,
Kohonen network,7/25/1981,Highly cited,4096,,,,,,,,,,,,,,,,,,,,,"The input vectors are 3D.
I could not find the grid size, but from the images it looks 8x8.
So the network was 8x8x3 parameters.",,,??? Seemingly no info,,,,,
Neocognitron,4/1/1980,Highly cited,1140576,228115200,5,,,,,,,,,,,,,,,TRUE,,Operation counting,,"""The synaptic connections from S-layers to C-layers
are fixed and unmodifiable. [...]
The numbers of excitatory cells in these seven layers are: 16x16 in U0, 16x16x24 in Us1, 10x10x 24 in Uc1, 8x8x24 in Us2, 6x 6x 24 in Uc2, 2x2x24 in Us3, and 24 in Uc3 
[...]
 the number of input synapses to each S-cell is 5 x 5 in layer Us1 and 5x5x24 in layers Us2 and Us3
[...]
The number of excitatory input synapses to each C-cell is 5x5 in layers Uc1 and Uc2, and is 2x2 in
layer Uc3
""

The number of synapses into each S-layer is:

S1: (16*16*24)*(5*5) 
S2: (8*8*24)*(5*5*24)
S3: (2*2*24)*(5*5*24)

We assume one parameter a per synapse into each cell in a S-layer, and one parameter b per each cell in a S-layer.","""It does not necessarily mean that all of these input synapses are
always fully reinforced. In usual situations, only some of these input synapses are reinforced, and the rest of them remains in small values [...] Each of the five stimulus patterns has been presented 20 times to the network. By that time, self organization of the network has almost been completed.""

We multiply by 2 to account for multadds
",,"""In order to self-organize the network, we have presented five stimulus patterns ""0"", ""1"", ""2"", ""3"", and ""4"", which are shown in Fig. 6""",,,,,
Internal functionality of visual invariants,5/2/1979,Historical significance,,,,,,,,,,,Unknown,,,,,,,,,,,,,,??? Seemingly no info,,,,,
TD(0),8/1/1977,Historical significance,,,,,,,,,,,Unknown,,,,,,,,,,,,,,??? Seemingly no info,,,,,
Statistical continuous speech recognizer,4/30/1976,Highly cited,,,12000,,,,,,,,Confident,,,,,,,,,,,,,"800 sentences, counting 15 words per sentence gives 12000
""All the results given are for a training set of 800 sentences and a test set of 100 sentences""","800 sentences, counting 15 words per sentence gives 12000 words
""All the results given are for a training set of 800 sentences and a test set of 100 sentences""",,,,,
Cognitron,9/1/1975,Historical significance,28800,5760000,5,20,,,,,,,,,,,,,,TRUE,,Operation counting,Precursor of the Neocognitron,"4 layers, 288 neurons per layer, weights connect each neuron to only 25 neurons in the previous layer = 4*288*25 parameters","Backward to forward ratio: 1 to 1 as weight updates are calculated from local activation patterns instead of gradient descent.
Total compute estimate: 100*2*4*288*25 = 5760000 = 5.76e6
",,5 examples presented for (at least) 20 cycles = 100 training steps,,,,,
Piecewise linear model,11/1/1973,"Historical significance,Highly cited",357,,314,,,,,,,,Confident,,,,,,,,,,,"16 input features + bias = 17 input features
7*6/2 = 21 Hyperplanes
17*21 = 357 parameters
""For the multicategory problem involving NR categories, a total of NR(NR - 1)/2 hyperplanes are used to partition the pattern space.""
""The input variables to the classifier consisted of the mean variance of the four textural features (f1,f2,f3, andfg obtained from the distance 1 gray-tone spatial-dependence matrices) and eight spectral features (comprised of the mean variance of the image gray-tone values) in each of the four spectral bands""
",,,"""Number of training samples = 314;""",,,,,
Graph-based structural reasoning,9/1/1970,Highly cited,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
Decision tree adaline,5/1/1969,Historical significance,2450,,80,,,,,,,,Confident,,,,,,,,,,,"5 adaline were trained on binary decisions (p. 1)
Each adaline had up to 490 input weights (“meshes”)
Total parameters = 5*490=2450
",,,40 positive and negative training examples (p. 2),,,,,
GLEE,7/1/1968,Historical significance,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
BOXES,7/1/1968,Historical significance,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,
LTE speaker verification system,11/1/1966,Historical significance,2061,105917060,417,131,,,,,,,Likely,,,,,,,TRUE,,Operation counting,,"2 connected systems, 1st level LTE and 2nd level LTE.
1st Level: 1810 parameters (""Thus, every 20 msec after the beginning of the utterance, the 15 filter amplitudes were each represented by a 12-bit code, resulting in a 180-bit time sample of the spectrum for that interval. Each time sample was fed to the first-level LTE's, which reduced it to a 10-bit code"")
2nd Level: 251 parameters (""This resulted in a 250-bit input pattern to the second level for the first half-second of each utterance. Each 250-bit pattern was then classified by the LTE into one of two classes"")","1st and 2nd level system are trained separately, multiple versions of both are trained, I chose the largest clearly described training runs.

1st level LTE compute: 2*1810*28700=103894000=1.04e8
1st level steps: 28700 (""Only 287 samples were selected to train the 10 LTE's. The same algorithm was used as that used with the 100-class gain. Two LTE's
converged before 100 training passes."")

2nd level LTE compute: 2*251*4030=2023060=2e6
2nd level steps: 4030 (31 epochs, 130 training examples, see Table 3)

Total compute: 103894000+2023060=105917060=1.06e8 (assuming no backward pass since they didn't use backpropagation)
",,"Split between both systems, 287 for 1st level, 130 for 2nd level.",,,,,
Heuristic Reinforcement Learning,10/1/1965,"Historical significance,Highly cited",,1080000,,,3,,,,,,Speculative,,,,,,,TRUE,,Hardware,,,"Figure 10 shows their largest system is trained for 3h and was trained on an analog IBM 1620 that was simulated on a digital IBM 1710.
Nordhaus, 2007 lists the IBM 1620 at 200 multiplications per second and doesn’t contain the 1710
Flops estimate: 0.5 * 3 * 60 * 60 * 200 = 1080000 = 1.08e6
Assumed utilization of 0.5
",,,Figure 10,,,,
Print Recognition Logic,1/1/1963,Historical significance,,22500000,,,2.5,,,,,,Speculative,,,,,,,TRUE,,Hardware,,,"0.5*2.5*60*60*5000 = 22500000 = 2.25e7
Assumed utilization of 0.5
Trained for 2-3h on an IBM 7090 (from Introduction)
Estimated IBM 7090 at 5000 FLOP/s based on multiplications per second (Nordhaus, 2007)
Note: the Nordhaus estimate is very different from Wikipedia's estimate of 100000 FLOP/s, which cites a PowerPoint as source.",,,2-3h (from Introduction),,,,
MADALINE I,7/1/1962,Historical significance,,,,,,,,,,,Unknown,,,,,,,TRUE,,,,,,,,,,,,
Linear Decision Functions,6/1/1962,"Historical significance,Highly cited",,1559250,500,,0.4375,,,,,,Speculative,,,,,,,TRUE,,Hardware,,,"0.5*45*35*1980 = 1559250 = 1.56e6
Trained using IBM punched cards, computation took 45 * 35s for all 10 digits (Section Estimating the Linear Decision Function).
Multiplications per second estimate based on publication year: 1.98e3 (regression on Nordhaus data).
Assumed utilization of 0.5",,"""Fifty different people were asked, resulting in a sample size of 50 for each of the ten pattern classes. ""","""Forty-five hyperplanes are required in the complete linear decision function""
""About 35 seconds, on the average, was required to determine a hyperplane, given an initial position.""",,,,
LMS,6/30/1960,Highly cited,17,,6,,,,,,,,,,,,,,,TRUE,,,,,,,,,,,,
ADALINE,6/30/1960,Highly cited,17,9900,100,,,,,,,,,,,,,,,TRUE,,Operation counting,,"""The machine's total experience is stored in the values of the weights a0,...,a16""","""The method of searching that has proven most useful is the method of steepest descent""

Apparently each pattern was only shown once to the system.

So the training compute is (forward pass compute) * (3 for backprop) * dataset size",,"""The best system, arrived at by slow precise adaptation on the full body of 100 noisy patterns, was able to classify these patterns as desired except for twelve errors.""

https://isl.stanford.edu/~widrow/papers/c1960adaptiveswitching.pdf",,,,,
Perceptron (1960),3/30/1960,Historical significance,1000,720000000,5000,,,,,,,,Speculative,,,,,,,TRUE,,Hardware,,""" The first program was designed to handle
up to 1000 A units, and a 72 by 72 sensory mosaic. It
was found that this large sensory system presented
stimuli with a fineness of grain considerably better than
the limits of discrimination of a thousand-unit percep-
tron, and at the same time, required an excessive
amount of time for stimulus transformations, since each
illuminated point in the stimulus must be transformed
individually into its image point.""","4000 * 12000 * 15
from the text ""This program uses the IBM 704 computer to simulate per-ceptual learning, recognition, and spontaneous classification of visual stimuli in the perceptron,""
from https://en.wikipedia.org/wiki/IBM_704 The 704 can execute up to 12,000 floating-point additions per second.
"" For the first system, the computing time averaged about 15 seconds per stimulus cycle, ""
In Fig 10 we see up to 4000 stimuli",,"from the text ""The two main simulation programs total about 5000 words each.""",,,,,
Pattern recognition and reading by machine,12/1/1959,Historical significance,2625,,,,,,,,,,,,,,,,,TRUE,,,,A two bit state is recorded for each of the 75 cell pairs and each of the 25+10 characters recognized.,,,,,,,,
Samuel Neural Checkers,7/1/1959,Highly cited,16,428400000,53000,,,,,,,,,,,,,,,TRUE,,,,"""with 16 terms for generalization learning""

""Mention has been made several times of the procedure
for replacing terms in the scoring polynomial. The program, as it is currently running, contains 38 different
terms (in addition to the piece-advantage term), 16 of
these being included in the scoring polynomial at anyone
time and the remaining 22 being kept in reserve.""","""it can learn to do this in a remarkably short period of time 8 or 10 hours of machine-playing time)""

""The availability of a larger and faster machine (the IBM 704), coupled with many detailed changes in the programming procedure, leads to a fairly interesting game being played, even without any learning.""

""The Type 704 is the first large-scale, commercially available computer to employ fully automatic floating point arithmetic commands. [...]. Floating point addition or subtraction operations require 84 microseconds.""

source: https://www.ibm.com/ibm/history/exhibits/mainframe/mainframe_PP704.html

""An idea of the learning ability of this procedure can be gained by analyzing an initial test series of 28 games""

""Each game averaged 68 moves (34 to a side), of which approximately 20 caused changes to be made in the scoring polynomial.""",,"Based on number of board positions

At the present time the memory tape contains something over 53,000 board positions (averaging 3.8 word search) which have been selected from a much larger
number of positions by means of the culling techniques
described. While this is still far from the number which
would tax the listing and searching procedures used in
the program, rough estimates, based on the frequency
with which the saved boards are utilized during normal
play (these figures being tabulated automatically), indicate that a library tape containing at least 20 times the
present number of board positions would be needed to
improve the midgame play significantly. At the present
rate of acquisition of new positions this would require
an inordinate amount of play and, consequently, of
machine time.",,,,,
Pandemonium (morse),2/1/1959,Highly cited,,600000000,,,,,,,,,Speculative,,,,,,,TRUE,,Hardware,,"The paper mentions 11 function types. Unclear how many times they are called (number of ""demons"" in their Pandemonium implementation).","The paper mentions using an IBM 704, which can execute up to 12,000 floating-point additions per second (https://wikiless.org/wiki/IBM_704). My best guess as to how long it ran for ranges between 1h to 2 days, which when plugged into guesstimate (https://www.getguesstimate.com/models/19625), i.e., taking the log mean, gives a mean estimate of 600M",,??? Might need to make a guesstimate here.,,,,,
Perceptron Mark I,1/1/1957,"Historical significance,Highly cited",1000,694894.9377361819,6,,,,,,,,,,,,,,,TRUE,,Third-party estimation,First modern neural network ,"""Figure 4.8 Illustration of the Mark 1 perceptron hardware. The photograph on the left shows how the inputs were obtained using a simple camera system in which an input scene, in this case a printed character, was illuminated by powerful lights, and an image focussed onto a 20 × 20 array of cadmium sulphide photocells, giving a primitive 400 pixel image. The perceptron also had a patch board, shown in the middle photograph, which allowed different configurations of input features to be tried. Often these were wired up at random to demonstrate the ability of the perceptron to learn without the need for precise wiring, in contrast to a modern digital computer. The photograph on the right shows one of the racks of adaptive weights. Each weight was implemented using a rotary variable resistor, also called a potentiometer, driven by an electric motor thereby allowing the value of the weight to be adjusted automatically by the learning algorithm.""

source: Bishop, Christopher M. (2006). Pattern Recognition and Machine Learning

The Perceptron had a 400-pixel visual input and 1000 neurons in the hidden layer. https://twitter.com/DiegoKuonen/status/1130352233223262208",Extracted from AI and Compute (https://openai.com/blog/ai-and-compute/) charts by using https://automeris.io/WebPlotDigitizer/.,,Appendix II describes an experiment with 6 stimulus patterns,,,,,
Sequence-based pattern recognition,3/1/1955,Historical significance,,,,,,,,,,,Unknown,,,,,,,TRUE,,,,,,,,,,,,
Self Organizing System,3/1/1955,Historical significance,225,,256,,,,,,,,,,,,,,,TRUE,,,,Figure 4 contains the learnt weight matrix,,,""" The modifier was then
disabled so that no further changes in the net could
occur and all 256 possible input patterns were then presented in turn.""

""For these purposes, 16-element nets (8 input and 8
output) were used because it was desired to exhaust all
possible input patterns, and we were limited to about
2^8 inputs by available time. """,,,,,
Genetic algorithm,7/2/1954,Historical significance,,,,,,,,,,,Unknown,,,,,,,TRUE,,,Possibly first computer simulation of a genetic evolution algorithm,,,,,,,,,
SNARC,1/8/1952,Historical significance,40,,,,,,,,,,,,,,,,,TRUE,,,,"The link below seems to suggest the SNARC had 40 cells, each with a dial that acts as a configurable weight.

https://www.webofstories.com/play/marvin.minsky/137",,,,,,,,
Theseus,7/2/1950,Historical significance,40,40,40,,,,,,,,,,,,,,,TRUE,,Other,,"The learned part is the maze configuration. There are 25 squares of the maze. The 16 squares to the left top corner have each one adjacent square down and one adjacent square up, for a total of 16*2 walls. We only need to count the 8 spare walls connecting the squares in the right side and the bottom side. In total there are 16*2+8 walls.","The ""training"" consists on the mouse running around and checking each wall.",,Each wall Theseus bumps into is a datapoint,,,,,
